{"id": "2981549002", "references": ["1533861849", "2964308564", "2963446712", "2963403868", "2194775991", "2310919327", "1677182931", "2157331557", "2064675550", "1836465849"], "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes.", "abstract": "Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at this http URL.", "citation_count": "55", "reference_count": "27", "date": "2019", "authors": ["Greg Yang"], "related_topics": ["Recurrent neural network", "Artificial neural network", "Multilayer perceptron", "Gaussian process", "Normalization (statistics)", "Transformer (machine learning model)", "Tensor", "Algorithm", "Feed forward", "Computation", "Pooling", "Computer science"]}
{"id": "3105081694", "references": ["3001118548", "2963446712", "2108598243", "3008985036", "2194775991", "2919115771", "3008827533", "3007497549", "3010604545", "2962835968"], "title": "COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images.", "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.", "citation_count": "46", "reference_count": "743", "date": "2020", "authors": ["Linda Wang", "Zhong Qiu Lin", "Alexander Wong"], "related_topics": ["Deep learning", "Convolutional neural network", "Image processing", "Benchmark (computing)", "Machine learning", "Key (cryptography)", "Computer science", "Radiography", "Artificial intelligence", "Coronavirus disease 2019 (COVID-19)", "Tomography x ray computed", "X ray image"]}
{"id": "2950893734", "references": ["2963684088", "2963373786", "2964308564", "2963073614", "2963403868", "2964121744", "2099471712", "2962793481", "2963470893", "2117539524"], "title": "Self-Attention Generative Adversarial Networks", "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.", "citation_count": "51", "reference_count": "1,558", "date": "2018", "authors": ["Han Zhang", "Ian Goodfellow", "Dimitris Metaxas", "Augustus Odena"], "related_topics": ["Boosting (machine learning)", "Visualization", "Normalization (statistics)", "Pattern recognition", "Computer science", "Generative grammar", "Adversarial system", "Artificial intelligence", "Self attention"]}
{"id": "3119786062", "references": ["2618530766", "2108598243", "3034978746", "2963403868", "2963091558", "2194775991", "3118608800", "2964121744", "2963341956", "1836465849"], "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.", "citation_count": "49", "reference_count": "315", "date": "2021", "authors": ["Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly", "Jakob Uszkoreit", "Neil Houlsby"], "related_topics": ["Transformer (machine learning model)", "Contextual image classification", "Computer vision", "Image (mathematics)", "Computer science", "Scale (chemistry)", "Structure (mathematical logic)", "Conjunction (grammar)", "Artificial intelligence", "Self attention"]}
{"id": "2145339207", "references": ["2618530766", "2121863487", "1652505363", "2187089797", "2310919327", "2100495367", "2546302380", "1665214252", "2952509347", "2072128103"], "title": "Human-level control through deep reinforcement learning", "abstract": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.", "citation_count": "33", "reference_count": "14,862", "date": "2015", "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "related_topics": ["Reinforcement learning", "Q-learning", "Temporal difference learning", "Artificial neural network", "General video game playing", "Set (psychology)", "Sensory processing", "Reinforcement", "Artificial intelligence", "Computer science"]}
{"id": "2153579005", "references": ["1889268436", "1614298861", "2117130368", "2131462252", "2158139315", "1423339008", "2141599568", "1662133657", "2132339004", "1498436455"], "title": "Distributed Representations of Words and Phrases and their Compositionality", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\r\n\r\nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "citation_count": "19", "reference_count": "25,996", "date": "2013", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "related_topics": ["Word2vec", "Word embedding", "Word order", "Word (computer architecture)", "Principle of compositionality", "Softmax function", "Syntax", "Distributional semantics", "Simple (philosophy)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1533861849", "references": ["2025768430", "2136922672", "2117130368", "1994197834", "2131462252", "2110798204", "2310919327", "2172174689", "1498436455", "2072128103"], "title": "Understanding the difficulty of training deep feedforward neural networks", "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).", "citation_count": "17", "reference_count": "13,013", "date": "2010", "authors": ["Xavier Glorot", "Yoshua Bengio"], "related_topics": ["Deep learning", "Vanishing gradient problem", "Initialization", "Artificial neural network", "Feedforward neural network", "Activation function", "Gradient descent", "Feature (machine learning)", "Artificial intelligence", "Machine learning"]}
{"id": "2964308564", "references": ["1810943226", "1815076433", "6908809", "1753482797", "2157331557", "2964199361", "2294059674", "2132339004", "2064675550", "2153653739"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "abstract": "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "citation_count": "25", "reference_count": "17,608", "date": "2015", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "related_topics": ["Machine translation", "Artificial neural network", "Phrase", "Sentence", "Encoder", "Artificial intelligence", "Bottleneck", "Byte pair encoding", "Computer science", "Closed captioning"]}
{"id": "2963446712", "references": ["2618530766", "2097117768", "2108598243", "2194775991", "2183341477", "1677182931", "2095705004", "1903029394", "2117539524", "1836465849"], "title": "Densely Connected Convolutional Networks", "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&amp;#x2014;one between each layer and its subsequent layer&amp;#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.", "citation_count": "39", "reference_count": "16,384", "date": "2017", "authors": ["Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian Q. Weinberger"], "related_topics": ["Convolutional code", "Network architecture", "Vanishing gradient problem", "Layer (object-oriented design)", "Benchmark (computing)", "Artificial neural network", "Code (cryptography)", "Feature (machine learning)", "Parallel computing", "Artificial intelligence", "Computer science", "Residual neural network"]}
{"id": "2963403868", "references": ["2963420686", "2923014074", "2963091558", "2970597249", "2911489562", "2963341956", "2965373594"], "title": "Attention is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.", "citation_count": "0", "reference_count": "19,086", "date": "2017", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "related_topics": ["Machine translation", "Encoder", "BLEU", "Speech translation", "Artificial neural network", "Transduction (machine learning)", "Byte pair encoding", "Speech recognition", "Computer science", "Transformer (machine learning model)"]}
{"id": "2194775991", "references": ["2102605133", "2618530766", "2155893237", "639708223", "2097117768", "1536680647", "1903029394", "2962835968", "2117539524", "1836465849"], "title": "Deep Residual Learning for Image Recognition", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "citation_count": "52", "reference_count": "79,410", "date": "2016", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "related_topics": ["Deep learning", "Residual", "Convolutional neural network", "Feature learning", "Vanishing gradient problem", "Artificial neural network", "MNIST database", "Object detection", "Test set", "Transfer of learning", "Softmax function", "Pattern recognition", "Machine learning", "Computer vision", "Computer science", "Transformer (machine learning model)", "Artificial intelligence", "Residual neural network"]}
{"id": "2310919327", "references": ["2158778629", "2964311892", "2963399829", "2147880316", "2156163116", "1944615693", "1510526001", "2157364932"], "title": "Gradient-based learning applied to document recognition", "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.", "citation_count": "0", "reference_count": "37,798", "date": "2001", "authors": ["Yann Lecun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "related_topics": ["Artificial neural network", "Convolutional neural network", "Handwriting recognition", "Transformer (machine learning model)", "Language model", "Graph (abstract data type)", "Decision boundary", "Network architecture", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1677182931", "references": ["2102605133", "2618530766", "2155893237", "2097117768", "2108598243", "1536680647", "2095705004", "2962835968", "2117539524", "1836465849"], "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.", "citation_count": "35", "reference_count": "11,676", "date": "2015", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "related_topics": ["Rectifier (neural networks)", "Initialization", "Overfitting", "Artificial neural network", "Contextual image classification", "Pattern recognition", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "2157331557", "references": ["2618530766", "2147768505", "2963504252", "6908809", "2153579005", "1753482797", "2156387975", "2294059674", "2132339004", "2064675550"], "title": "Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "citation_count": "31", "reference_count": "13,086", "date": "2014", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "related_topics": ["Encoder", "Recurrent neural network", "Artificial neural network", "Machine translation", "Phrase", "Feature (machine learning)", "Conditional probability", "Sequence", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2064675550", "references": ["2107878631", "2128499899", "194249466", "2048060899", "2103452139", "1674799117", "2007431958", "2123716044", "2143503258", "2154890045"], "title": "Long short-term memory", "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "citation_count": "34", "reference_count": "49,372", "date": "1997", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "related_topics": ["Vanishing gradient problem", "Backpropagation through time", "Recurrent neural network", "Artificial neural network", "Backpropagation", "Time complexity", "Chunking (psychology)", "Computational complexity theory", "Algorithm", "Artificial intelligence", "Computer science"]}
{"id": "1836465849", "references": ["1533861849", "2097117768", "2146502635", "2310919327", "1677182931", "2095705004", "104184427", "1665214252", "2117539524", "2168231600"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.", "citation_count": "23", "reference_count": "26,211", "date": "2015", "authors": ["Sergey Ioffe", "Christian Szegedy"], "related_topics": ["Normalization (statistics)", "Initialization", "Contextual image classification", "Algorithm", "Machine learning", "Computer science", "Artificial intelligence", "Covariate shift"]}
{"id": "3001118548", "references": ["2006434809", "2104548316", "2166867592", "3000413850", "2131262274", "3017468735", "2026274122", "2132260239", "2903899730", "2725497285"], "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China", "abstract": "A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients. All patients with suspected 2019-nCoV were admitted to a designated hospital in Wuhan. We prospectively collected and analysed data on patients with laboratory-confirmed 2019-nCoV infection by real-time RT-PCR and next-generation sequencing. Data were obtained with standardised data collection forms shared by the International Severe Acute Respiratory and Emerging Infection Consortium from electronic medical records. Researchers also directly communicated with patients or their families to ascertain epidemiological and symptom data. Outcomes were also compared between patients who had been admitted to the intensive care unit (ICU) and those who had not.", "citation_count": "34", "reference_count": "31,613", "date": "2020", "authors": ["Chaolin Huang", "Yeming Wang", "Xingwang Li", "Lili Ren", "Jianping Zhao", "Yi Hu", "Li Zhang", "Guohui Fan", "Jiuyang Xu", "Xiaoying Gu", "Zhenshun Cheng", "Ting Yu", "Jiaan Xia", "Yuan Wei", "Wenjuan Wu", "Xuelei Xie", "Wen Yin", "Hui Li", "Min Liu", "Yan Xiao", "Hong Gao", "Li Guo", "Jungang Xie", "Guangfa Wang", "Rongmeng Jiang", "Zhancheng Gao", "Qi Jin", "Jianwei Wang", "Bin Cao"], "related_topics": ["Middle East respiratory syndrome", "Intensive care unit", "Viral pneumonia", "Medical record", "Epidemiology", "Pneumonia", "Coronavirus", "Betacoronavirus", "Internal medicine", "Medicine"]}
{"id": "2108598243", "references": ["2110764733", "2141282920", "2151103935", "2115733720", "1782590233", "1576445103", "2128017662", "2145607950", "1528789833", "2038721957"], "title": "ImageNet: A large-scale hierarchical image database", "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", "citation_count": "24", "reference_count": "28,822", "date": "2009", "authors": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "related_topics": ["WordNet", "Image retrieval", "Contextual image classification", "Ontology (information science)", "Cluster analysis", "Information retrieval", "Cognitive neuroscience of visual object recognition", "Ontology", "The Internet", "Robustness (computer science)", "Computer science"]}
{"id": "3008985036", "references": ["3032185657", "3037255629", "3006643024", "3028749392", "3042098369", "3006110666", "3002108456"], "title": "Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR", "abstract": "In a series of 51 patients with chest CT and real-time polymerase chain reaction assay (RT-PCR) performed within 3 days, the sensitivity of CT for 2019 novel coronavirus infection was 98% and that ...", "citation_count": "7", "reference_count": "2,128", "date": "2020", "authors": ["Yicheng Fang", "Huangqi Zhang", "Jicheng Xie", "Minjie Lin", "Lingjun Ying", "Peipei Pang", "Wenbin Ji"], "related_topics": ["Real-time polymerase chain reaction", "Polymerase chain reaction", "Pneumonia", "Nuclear medicine", "Tomography", "Medicine", "2019-20 coronavirus outbreak", "Chest ct", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2919115771", "references": ["2025768430", "2136922672", "2022508996", "2163922914", "2310919327", "2160815625", "2100495367", "1993882792", "2064675550", "2145339207"], "title": "Deep learning", "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.", "citation_count": "33", "reference_count": "40,041", "date": "2015", "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"], "related_topics": ["Deep learning", "Object detection", "Cognitive neuroscience of visual object recognition", "Abstraction (linguistics)", "Theano", "Backpropagation", "Representation (systemics)", "Computational model", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "3008827533", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3003573988", "3004318991", "3003465021", "3005079553", "3004239190", "3002108456"], "title": "Clinical Characteristics of Coronavirus Disease 2019 in China", "abstract": "Abstract Background Since December 2019, when coronavirus disease 2019 (Covid-19) emerged in Wuhan city and rapidly spread throughout China, data have been needed on the clinical characteristics of...", "citation_count": "19", "reference_count": "18,220", "date": "2020", "authors": ["Wei-jie Guan", "Zheng-yi Ni", "Yu Hu", "Wenhua Liang", "Chun-quan Ou", "Jianxing He", "Lei Liu", "Hong Shan", "Chunliang Lei", "David S.C. Hui", "Bin Du", "Lan-juan Li", "Guang Zeng", "Kwok-Yung Yuen", "Ruchong Chen", "Chun-Li Tang", "Tao Wang", "Ping-yan Chen", "Jie Xiang", "Shiyue Li", "Jinlin Wang", "Zi Jing Liang", "Yi-xiang Peng", "Li Wei", "Yong Liu", "Ya-hua Hu", "Peng Peng", "Jian-ming Wang", "Ji-yang Liu", "Zhong Chen", "Gang Li", "Zhi-jian Zheng", "Shao-qin Qiu", "Jie Luo", "Chang-jiang Ye", "Shao-yong Zhu", "Nanshan Zhong"], "related_topics": ["Pandemic", "Betacoronavirus", "Viral Epidemiology", "China", "Environmental health", "Medicine", "Coronavirus Infections", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Viral therapy"]}
{"id": "3007497549", "references": ["3004906315", "3001118548", "3006643024", "3004511262", "3003901880", "3006110666", "3008818676", "3005656138", "3006354146"], "title": "Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases.", "abstract": "Chest CT had higher sensitivity for diagnosis of COVID-19 as compared with initial reverse-transcription polymerase chain reaction from swab samples in the epidemic area of China.", "citation_count": "9", "reference_count": "3,893", "date": "2020", "authors": ["Tao Ai", "Zhenlu Yang", "Hongyan Hou", "Chenao Zhan", "Chong Chen", "Wenzhi Lv", "Qian Tao", "Ziyong Sun", "Liming Xia"], "related_topics": ["Real-time polymerase chain reaction", "Reverse transcription polymerase chain reaction", "Pneumonia", "Polymerase chain reaction", "Radiology", "Medicine", "2019-20 coronavirus outbreak", "Chest ct", "Coronavirus disease 2019 (COVID-19)", "Follow up studies"]}
{"id": "3010604545", "references": ["3033952286", "3036958556", "3035464429", "3008962515", "3034059415", "3008452791", "3033453353", "3005079553", "3034408674", "3035275617"], "title": "Detection of SARS-CoV-2 in Different Types of Clinical Specimens.", "abstract": "This study describes results of PCR and viral RNA testing for SARS-CoV-2 in bronchoalveolar fluid, sputum, feces, blood, and urine specimens from patients with COVID-19 infection in China to identify possible means of non-respiratory transmission.", "citation_count": "13", "reference_count": "3,893", "date": "2020", "authors": ["Wenling Wang", "Yanli Xu", "Ruqin Gao", "Roujian Lu", "Kai Han", "Guizhen Wu", "Wenjie Tan"], "related_topics": ["Sputum", "Viral load", "Feces", "Betacoronavirus", "Real-time polymerase chain reaction", "Pneumonia (non-human)", "Polymerase chain reaction", "Urine", "Virology", "Medicine"]}
{"id": "2962835968", "references": ["639708223", "1536680647", "1901129140", "2194775991", "1903029394", "3106250896"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "abstract": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "citation_count": "0", "reference_count": "84,164", "date": "2015", "authors": ["Karen Simonyan", "Andrew Zisserman"], "related_topics": ["Convolution", "Computer vision", "Scale (map)", "Computer science", "Basis (linear algebra)", "Architecture", "Artificial intelligence", "Crowd counting", "Region proposal"]}
{"id": "2963684088", "references": ["2405756170", "2738588019", "2963420272", "2893749619", "2331128040", "2963800363", "2962793481", "2963836885", "2963470893"], "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "abstract": "Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.", "citation_count": "0", "reference_count": "8,928", "date": "2016", "authors": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "related_topics": ["Unsupervised learning", "Supervised learning", "Feature learning", "Artificial intelligence", "Class (computer programming)", "Hierarchy", "Computer science", "Object (computer science)", "Generative grammar", "Generator (mathematics)"]}
{"id": "2963373786", "references": ["2963684088", "2271840356", "830076066", "648143168", "1487641199", "2183341477", "2963685250", "2949416428", "2964153729", "1836465849"], "title": "Improved techniques for training GANs", "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "citation_count": "20", "reference_count": "4,731", "date": "2016", "authors": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "related_topics": ["MNIST database", "Pattern recognition", "Turing test", "Computer science", "Artificial intelligence"]}
{"id": "2963073614", "references": ["2963684088", "1901129140", "2133665775", "2340897893", "2964121744", "1903029394", "2099471712", "2100495367", "2117539524", "1836465849"], "title": "Image-to-Image Translation with Conditional Adversarial Networks", "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.", "citation_count": "52", "reference_count": "9,243", "date": "2017", "authors": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A. Efros"], "related_topics": ["Image translation", "Image (mathematics)", "Machine learning", "Artificial intelligence", "Computer science", "User interface", "Translation (geometry)", "Function (engineering)", "Image resolution", "Iterative reconstruction"]}
{"id": "2964121744", "references": ["2964015378", "2962739339", "2963073614", "2963403868", "1514535095", "2331128040", "2962793481", "2963470893"], "title": "Adam: A Method for Stochastic Optimization", "abstract": "Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "citation_count": "0", "reference_count": "69,925", "date": "2015", "authors": ["Diederik P. Kingma", "Jimmy Lei Ba"], "related_topics": ["Stochastic optimization", "Convex optimization", "Rate of convergence", "Invariant (mathematics)", "Diagonal", "Uniform norm", "Mathematical optimization", "Byte pair encoding", "Regret", "Computer science"]}
{"id": "2099471712", "references": ["2618530766", "2025768430", "2136922672", "1904365287", "2310919327", "1959608418", "3118608800", "2546302380", "2964153729", "2072128103"], "title": "Generative Adversarial Nets", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "citation_count": "34", "reference_count": "27,297", "date": "2014", "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "related_topics": ["Generative model", "Discriminative model", "Approximate inference", "Markov chain", "Minimax", "Adversarial machine learning", "Perceptron", "Backpropagation", "Theoretical computer science", "Machine learning", "Image translation", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2962793481", "references": ["2963684088", "2963373786", "2194775991", "2340897893", "1959608418", "2100495367", "1903029394", "2099471712", "2962835968", "2117539524"], "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X \u2192 Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y \u2192 X and introduce a cycle consistency loss to push F(G(X)) \u2248 X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.", "citation_count": "54", "reference_count": "8,408", "date": "2017", "authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "related_topics": ["Image translation", "Image (category theory)", "Translation (geometry)", "Graphics", "Object (computer science)", "Pattern recognition", "Domain (software engineering)", "Artificial intelligence", "Computer science", "Training set"]}
{"id": "2963470893", "references": ["2618530766", "2097117768", "2194775991", "1677182931", "2964121744", "2099471712", "1849277567", "2962835968", "2117539524", "1836465849"], "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network", "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.", "citation_count": "68", "reference_count": "5,688", "date": "2017", "authors": ["Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "related_topics": ["Image texture", "Image resolution", "Convolutional neural network", "Image translation", "Pixel", "Iterative reconstruction", "Network architecture", "Similarity (geometry)", "Pattern recognition", "Artificial intelligence", "Computer science"]}
{"id": "2117539524", "references": ["2102605133", "2618530766", "1614298861", "2155893237", "2097117768", "2108598243", "2151103935", "2168356304", "1849277567", "2962835968"], "title": "ImageNet Large Scale Visual Recognition Challenge", "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.", "citation_count": "97", "reference_count": "23,640", "date": "2015", "authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "related_topics": ["Object detection", "Cognitive neuroscience of visual object recognition", "Pattern recognition (psychology)", "Contextual image classification", "Object (computer science)", "Field (computer science)", "Benchmark (computing)", "Theano", "Machine learning", "Data science", "Computer science", "Artificial intelligence"]}
{"id": "2618530766", "references": ["1904365287", "2130325614", "2097117768", "2108598243", "2110764733", "2194775991", "2911964244", "3118608800", "2546302380", "1665214252"], "title": "ImageNet classification with deep convolutional neural networks", "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "citation_count": "31", "reference_count": "166,585", "date": "2017", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "related_topics": ["Convolutional neural network", "Dropout (neural networks)", "Overfitting", "Artificial neural network", "Softmax function", "Test data", "Pattern recognition", "Machine learning", "Regularization (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "3034978746", "references": ["3116298410", "3122924117", "3099495704", "3119786062", "3145385912", "3125947392", "3132401450", "3098053103", "3154503084", "3108316907"], "title": "A Simple Framework for Contrastive Learning of Visual Representations", "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.", "citation_count": "0", "reference_count": "1,377", "date": "2020", "authors": ["Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton"], "related_topics": ["Supervised learning", "Linear classifier", "Machine learning", "Memory bank", "Representation (mathematics)", "Matching (statistics)", "Computer science", "Quality (business)", "Simple (abstract algebra)", "Artificial intelligence", "Nonlinear transformation"]}
{"id": "2963091558", "references": ["639708223", "2806070179", "2963403868", "1861492603", "2565639579", "2194775991", "1677182931", "2962835968", "2117539524", "1836465849"], "title": "Non-local Neural Networks", "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.", "citation_count": "53", "reference_count": "2,925", "date": "2018", "authors": ["Xiaolong Wang", "Ross Girshick", "Abhinav Gupta", "Kaiming He"], "related_topics": ["Pose", "Object detection", "Image segmentation", "Contextual image classification", "Block (programming)", "Artificial neural network", "Feature extraction", "Process (computing)", "Segmentation", "Artificial intelligence", "Computer science"]}
{"id": "3118608800", "references": ["2081580037", "2165225968", "2096192494"], "title": "Learning Multiple Layers of Features from Tiny Images", "abstract": "In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.", "citation_count": "3", "reference_count": "11,906", "date": "2009", "authors": ["Alex Krizhevsky"], "related_topics": ["Deep belief network", "Generative model", "Boltzmann machine", "Classifier (UML)", "MNIST database", "Vanishing gradient problem", "Pattern recognition", "Pixel", "Focus (optics)", "Computer science", "Artificial intelligence"]}
{"id": "2963341956", "references": ["2962739339", "2251939518", "2117130368", "2108598243", "2963403868", "2153579005", "2131744502", "2250539671", "2963748441", "2025768430"], "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "citation_count": "52", "reference_count": "22,961", "date": "2018", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina N. Toutanova"], "related_topics": ["Question answering", "Language model", "Natural language understanding", "Named-entity recognition", "SemEval", "Inference", "Winograd Schema Challenge", "Sequence labeling", "Artificial intelligence", "Computer science", "Transformer (machine learning model)"]}
{"id": "2121863487", "references": ["2100677568", "2154642048", "1569320505", "94523489", "1603765807", "1639032689", "1535810436", "3017143921", "3011120880", "2178806388"], "title": "Reinforcement Learning: An Introduction", "abstract": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.", "citation_count": "84", "reference_count": "44,248", "date": "1988", "authors": ["R.S. Sutton", "A.G. Barto"], "related_topics": ["Learning classifier system", "Reinforcement learning", "Apprenticeship learning", "Unsupervised learning", "Temporal difference learning", "Robot learning", "Computational learning theory", "Q-learning", "Artificial intelligence", "Computer science", "Machine learning"]}
{"id": "1652505363", "references": ["2145094598", "1614298861", "2025768430", "2154642048", "2145339207", "2076063813", "2107941094", "2116064496", "1498436455", "2072128103"], "title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations", "abstract": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.", "citation_count": "0", "reference_count": "23,606", "date": "1986", "authors": ["David E. Rumelhart", "James L. McClelland"], "related_topics": ["Competitive learning", "Connectionism", "Parallel processing (DSP implementation)", "Theoretical computer science", "Computer science", "Linear algebra", "Resource (project management)", "Data processing", "Cognition", "Quickprop"]}
{"id": "2187089797", "references": ["2001141328", "2125637308", "2139823104", "2156718197", "2137570937", "2157444450", "2053186076", "2100495367", "1742512077", "2072128103"], "title": "Visualizing Data using t-SNE", "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.", "citation_count": "36", "reference_count": "20,331", "date": "2008", "authors": ["Laurens van der Maaten", "Geoffrey Hinton"], "related_topics": ["Sammon mapping", "t-distributed stochastic neighbor embedding", "Isomap", "Nonlinear dimensionality reduction", "Dimensionality reduction", "Visualization", "Embedding", "Multidimensional scaling", "Data mining", "Mathematics"]}
{"id": "2100495367", "references": ["2136922672", "2001141328", "2032647857", "2021774695", "2121122425", "2053186076", "2293063825"], "title": "Reducing the Dimensionality of Data with Neural Networks", "abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.", "citation_count": "7", "reference_count": "15,414", "date": "2006", "authors": ["G. E. Hinton", "R. R. Salakhutdinov"], "related_topics": ["Autoencoder", "Dimensionality reduction", "Restricted Boltzmann machine", "Artificial neural network", "Deep belief network", "Convolutional Deep Belief Networks", "Gradient descent", "Curse of dimensionality", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2546302380", "references": ["2166049352", "2134557905", "2130325614", "2161969291", "2151103935", "2110798204", "2310919327", "2097018403", "2100495367", "2162915993"], "title": "What is the best multi-stage architecture for object recognition?", "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).", "citation_count": "54", "reference_count": "2,435", "date": "2009", "authors": ["Kevin Jarrett", "Koray Kavukcuoglu", "Marc'Aurelio Ranzato", "Yann LeCun"], "related_topics": ["Feature extraction", "Feature (machine learning)", "Unsupervised learning", "Supervised learning", "Filter bank", "MNIST database", "Filter (signal processing)", "Word error rate", "Normalization (statistics)", "Histogram", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "1665214252", "references": ["2099866409", "2136922672", "2134557905", "2536626143", "1994197834", "1782590233", "2546302380", "2100495367", "2116064496", "2157364932"], "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.", "citation_count": "21", "reference_count": "13,596", "date": "2010", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "related_topics": ["Boltzmann machine", "Binary number", "Vanishing gradient problem", "Sigmoid function", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Rule of inference", "Unit (ring theory)", "Mathematics", "Artificial intelligence", "Infinite number"]}
{"id": "2952509347", "references": ["2099587183", "1625390266", "2132622533", "1515851193", "2126316555", "1502916507", "2101355568", "2013391942", "2952509347", "2145339207"], "title": "The arcade learning environment: an evaluation platform for general agents", "abstract": "In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.", "citation_count": "26", "reference_count": "1,854", "date": "2015", "authors": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "related_topics": ["Reinforcement learning", "Learning environment", "Transfer of learning", "Benchmark (computing)", "Interface (Java)", "Human\u2013computer interaction", "Computer science", "Software", "Set (psychology)"]}
{"id": "2072128103", "references": ["2136922672", "2156909104", "2119821739", "2187089797", "2129131372", "2310919327", "2911964244", "2296616510", "2053186076", "2100495367"], "title": "Learning Deep Architectures for AI", "abstract": "Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.", "citation_count": "227", "reference_count": "9,739", "date": "2009", "authors": ["Yoshua Bengio"], "related_topics": ["Deep belief network", "Unsupervised learning", "Instance-based learning", "Online machine learning", "Feature learning", "Robot learning", "Computational learning theory", "Learning classifier system", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1889268436", "references": ["2251939518", "2117130368", "2163455955", "2151048449", "2097606805", "71795751", "1423339008", "1662133657", "1984052055", "2103305545"], "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "citation_count": "39", "reference_count": "1,449", "date": "2012", "authors": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "related_topics": ["Parse tree", "Principle of compositionality", "Natural language", "Syntax", "Recurrent neural network", "Propositional calculus", "Vector space", "Noun", "Meaning (non-linguistic)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1614298861", "references": ["2271840356", "3104097132", "2100664567", "1895577753", "2153579005", "2250539671", "1888005072", "2964321699", "2123024445", "1486649854"], "title": "Efficient Estimation of Word Representations in Vector Space", "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.", "citation_count": "0", "reference_count": "17,853", "date": "2013", "authors": ["Tomas Mikolov", "Kai Chen", "Greg S. Corrado", "Jeffrey Dean"], "related_topics": ["Word2vec", "Word embedding", "Word (computer architecture)", "Test set", "Similarity (psychology)", "Artificial neural network", "Data set", "Pattern recognition", "Vector space", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2117130368", "references": ["2158823144", "2107008379", "2914746235", "2310919327", "2173629880", "2163568299", "2158847908", "2130903752", "2132339004", "2885050925"], "title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "citation_count": "24", "reference_count": "5,718", "date": "2008", "authors": ["Ronan Collobert", "Jason Weston"], "related_topics": ["Multi-task learning", "Semi-supervised learning", "Language identification", "Language model", "Semantic role labeling", "Sentence", "Convolutional neural network", "Host (network)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2131462252", "references": ["2127314673", "2158195707", "2111305191", "2091812280", "2038721957", "2056590938", "2121227244", "2132339004", "36903255", "1558797106"], "title": "A Scalable Hierarchical Distributed Language Model", "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "citation_count": "12", "reference_count": "1,132", "date": "2008", "authors": ["Andriy Mnih", "Geoffrey E. Hinton"], "related_topics": ["Cache language model", "Language model", "Binary tree", "Tree (data structure)", "Word (computer architecture)", "Probabilistic logic", "Scalability", "Feature (machine learning)", "Artificial intelligence", "Computer science"]}
{"id": "2158139315", "references": ["2158997610", "2117130368", "168564468", "2156515921", "2131462252", "1662133657", "1880262756", "2004763266", "2132339004", "2296073425"], "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &amp; Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "citation_count": "47", "reference_count": "2,502", "date": "2010", "authors": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "related_topics": ["Word (computer architecture)", "Semi-supervised learning", "Chunking (psychology)", "Natural language processing", "Code (cryptography)", "Computer science", "Simple (abstract algebra)", "Artificial intelligence", "General method", "Word representation"]}
{"id": "1423339008", "references": ["2536208356", "2117130368", "2130325614", "2067191022", "1574901103", "2100495367", "1528789833", "2162915993", "1566135517", "2132339004"], "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.", "citation_count": "25", "reference_count": "1,490", "date": "2011", "authors": ["Richard Socher", "Cliff C. Lin", "Chris Manning", "Andrew Y. Ng"], "related_topics": ["Parsing", "Parse tree", "Treebank", "Natural language", "Sentence", "Syntax", "Artificial neural network", "Segmentation", "Natural language processing", "Pattern recognition", "Annotation", "Computer science", "Artificial intelligence"]}
{"id": "2141599568", "references": ["1632114991", "1614298861", "2147152072", "2117130368", "179875071", "2131462252", "1970689298", "2158139315", "2100495367", "2132339004"], "title": "Linguistic Regularities in Continuous Space Word Representations", "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.", "citation_count": "22", "reference_count": "3,585", "date": "2013", "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "related_topics": ["Syntax", "Language model", "Analogy", "Natural language processing", "Speech recognition", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "1662133657", "references": ["2117130368", "2173213060", "2166706824", "2038721957", "2024165284", "1532325895", "1880262756", "3013264884", "1660390307", "1992419399"], "title": "From frequency to meaning: vector space models of semantics", "abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "citation_count": "178", "reference_count": "3,223", "date": "2010", "authors": ["Peter D. Turney", "Patrick Pantel"], "related_topics": ["Statistical semantics", "Distributional semantics", "Semantics", "Random indexing", "Structure (mathematical logic)", "Meaning (linguistics)", "Natural language processing", "Field (computer science)", "Process (engineering)", "Computer science", "Artificial intelligence"]}
{"id": "2132339004", "references": ["2147152072", "2110485445", "2914484425", "2158195707", "1575350781", "2096175520", "2038721957", "2116064496", "2121227244", "1631260214"], "title": "A neural probabilistic language model", "abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.", "citation_count": "33", "reference_count": "7,596", "date": "2003", "authors": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "related_topics": ["Language model", "Cache language model", "Word embedding", "Sentence", "Joint probability distribution", "Word (computer architecture)", "Generalization", "Probabilistic logic", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1498436455", "references": ["2322002063", "1652505363"], "title": "Learning representations by back-propagating errors", "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "citation_count": "2", "reference_count": "23,174", "date": "1988", "authors": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "related_topics": ["Domain (software engineering)", "Task (project management)", "Measure (mathematics)", "Algorithm", "Computer science", "Neurophysiology", "The Internet"]}
{"id": "2025768430", "references": ["2136922672", "1652505363", "1994197834", "2110798204", "2100495367", "2153663612", "2293063825", "2172174689", "1498436455", "2072128103"], "title": "Extracting and composing robust features with denoising autoencoders", "abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "citation_count": "23", "reference_count": "5,670", "date": "2008", "authors": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "related_topics": ["Deep belief network", "Unsupervised learning", "Generative model", "Discriminative model", "Nonlinear dimensionality reduction", "Machine learning", "Pattern recognition", "Perspective (graphical)", "Representation (mathematics)", "Benchmark (computing)", "Mathematics", "Artificial intelligence"]}
{"id": "2136922672", "references": ["2158778629", "2156163116", "2567948266", "2131686571", "2124914669", "2057175746", "2310919327", "2159080219", "2159737176", "2116064496"], "title": "A fast learning algorithm for deep belief nets", "abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "citation_count": "30", "reference_count": "15,773", "date": "2006", "authors": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "related_topics": ["Deep belief network", "Convolutional Deep Belief Networks", "Generative model", "Content-addressable memory", "Restricted Boltzmann machine", "Greedy algorithm", "Artificial neural network", "Boltzmann machine", "Artificial intelligence", "Algorithm", "Mathematics"]}
{"id": "1994197834", "references": ["2136922672", "2134557905", "2124914669", "2110798204", "2153635508", "2147800946", "2100495367", "2159737176", "2116064496", "2613634265"], "title": "An empirical evaluation of deep architectures on problems with many factors of variation", "abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.", "citation_count": "13", "reference_count": "1,061", "date": "2007", "authors": ["Hugo Larochelle", "Dumitru Erhan", "Aaron Courville", "James Bergstra", "Yoshua Bengio"], "related_topics": ["Online machine learning", "Instance-based learning", "Computational learning theory", "Stability (learning theory)", "Deep learning", "Semi-supervised learning", "Active learning (machine learning)", "Ensemble learning", "Unsupervised learning", "Competitive learning", "Artificial neural network", "Wake-sleep algorithm", "Algorithmic learning theory", "Support vector machine", "Machine learning", "Artificial intelligence", "Computer science", "Generalization error"]}
{"id": "2110798204", "references": ["1993845689", "2109779438", "2136922672", "2103626435", "2124914669", "2167967601", "2125569215", "2100495367", "2116064496", "2613634265"], "title": "Greedy Layer-Wise Training of Deep Networks", "abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.", "citation_count": "16", "reference_count": "5,558", "date": "2006", "authors": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "related_topics": ["Deep belief network", "Convolutional Deep Belief Networks", "Deep learning", "Artificial neural network", "Optimization problem", "Generative model", "Initialization", "Context (language use)", "Artificial intelligence", "Theoretical computer science", "Computer science"]}
{"id": "2172174689", "references": ["2102409316", "2136922672", "2156163116", "2105464873", "1802356529", "2310919327", "11828546", "2116064496", "2075187489", "1902027874"], "title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "abstract": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.", "citation_count": "14", "reference_count": "1,552", "date": "2006", "authors": ["Marc'aurelio Ranzato", "Christopher Poultney", "Sumit Chopra", "Yann L. Cun"], "related_topics": ["Encoder", "Sparse approximation", "MNIST database", "Code (cryptography)", "Filter (signal processing)", "Word error rate", "Energy (signal processing)", "Pattern recognition", "Detector", "Computer science", "Artificial intelligence"]}
{"id": "1810943226", "references": ["1632114991", "3023071679", "2143612262", "44815768", "2131462252", "2108677974", "1554663460", "2120861206", "2064675550", "196214544"], "title": "Generating Sequences With Recurrent Neural Networks", "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "citation_count": "30", "reference_count": "3,331", "date": "2013", "authors": ["Alex Graves"], "related_topics": ["Recurrent neural network", "Handwriting", "Sequence", "Speech recognition", "Computer science", "Point (typography)", "Structure (mathematical logic)"]}
{"id": "1815076433", "references": ["2107878631", "2110485445", "2146502635", "2118706537", "1606347560", "2122585011", "2171865010", "2064675550", "1498436455", "196214544"], "title": "On the difficulty of training recurrent neural networks", "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.", "citation_count": "23", "reference_count": "4,011", "date": "2013", "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "related_topics": ["Recurrent neural network", "Dynamical systems theory", "Artificial intelligence", "Clipping (computer graphics)", "Mathematics", "Effective solution"]}
{"id": "6908809", "references": ["2120420045", "2147768505", "19621276", "1994616650", "2168231600", "1498436455"], "title": "ADADELTA: An Adaptive Learning Rate Method", "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "citation_count": "6", "reference_count": "6,447", "date": "2012", "authors": ["Matthew D. Zeiler"], "related_topics": ["Stochastic gradient descent", "Gradient descent", "Online machine learning", "MNIST database", "Machine learning", "Pattern recognition", "Hyperparameter", "Selection (genetic algorithm)", "Computer science", "Scale (ratio)", "Task (project management)", "Artificial intelligence"]}
{"id": "1753482797", "references": ["1889268436", "2117130368", "2251222643", "179875071", "2146502635", "2171928131", "2006969979", "2103305545", "2132339004", "196214544"], "title": "Recurrent Continuous Translation Models", "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is &gt; 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "citation_count": "18", "reference_count": "1,282", "date": "2013", "authors": ["Nal Kalchbrenner", "Phil Blunsom"], "related_topics": ["Rule-based machine translation", "Transfer-based machine translation", "Dynamic and formal equivalence", "Sentence", "Language model", "Perplexity", "Syntax", "Translation (geometry)", "Word order", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2964199361", "references": ["2395935897", "2130942839", "1810943226", "6908809", "1905522558", "1753482797", "1828163288", "2157331557", "2341457423", "2153653739"], "title": "On the Properties of Neural Machine Translation: Encoder--Decoder Approaches", "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2010Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "citation_count": "11", "reference_count": "3,650", "date": "2014", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "related_topics": ["Machine translation", "Convolutional neural network", "Artificial neural network", "Encoder", "Sentence", "Translation (geometry)", "Speech recognition", "Representation (mathematics)", "Computer science", "Focus (optics)"]}
{"id": "2294059674", "references": ["2335728318", "2618530766", "1904365287", "2310919327", "2131241448", "3118608800", "189596042", "2546302380", "2156387975", "2912934387"], "title": "Maxout Networks", "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "citation_count": "23", "reference_count": "2,224", "date": "2013", "authors": ["Ian Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "related_topics": ["MNIST database", "Leverage (statistics)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2153653739", "references": ["1508165687", "1517947178", "1549285799", "2116316001", "2101105183", "2161792612", "1973923101", "2006969979", "1986543644", "2158388102"], "title": "Statistical phrase-based translation", "abstract": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.", "citation_count": "15", "reference_count": "3,371", "date": "2003", "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "related_topics": ["Noun phrase", "Phrase", "Determiner phrase", "Phrase search", "Dependency grammar", "Phrase structure rules", "Generalized phrase structure grammar", "Synchronous context-free grammar", "Hybrid machine translation", "Example-based machine translation", "Computer-assisted translation", "Interactive machine translation", "Transfer-based machine translation", "Machine translation software usability", "Pivot language", "Evaluation of machine translation", "Natural language processing", "Computer science", "BLEU", "Artificial intelligence", "Lexicography"]}
{"id": "2097117768", "references": ["2102605133", "2618530766", "2068730032", "1904365287", "2310919327", "104184427", "1849277567", "2168231600", "2963911037", "2963542991"], "title": "Going deeper with convolutions", "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "citation_count": "21", "reference_count": "31,038", "date": "2015", "authors": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "related_topics": ["Convolutional neural network", "Artificial neural network", "Contextual image classification", "Hebbian theory", "Object detection", "Feature extraction", "Artificial intelligence", "Machine learning", "Computer science", "Residual neural network"]}
{"id": "2183341477", "references": ["2102605133", "2618530766", "2016053056", "2097117768", "2096733369", "1677182931", "1903029394", "2962835968", "2117539524", "1836465849"], "title": "Rethinking the Inception Architecture for Computer Vision", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "citation_count": "20", "reference_count": "13,669", "date": "2016", "authors": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna"], "related_topics": ["Test set", "Set (abstract data type)", "Machine learning", "Computer science", "Inference", "Artificial intelligence", "State (computer science)", "Computation", "Computer vision", "Variety (cybernetics)", "Regularization (mathematics)"]}
{"id": "2095705004", "references": ["2145094598", "2618530766", "2335728318", "2136922672", "2131241448", "3118608800", "2100495367", "2546302380", "2135046866", "2025768430"], "title": "Dropout: a simple way to prevent neural networks from overfitting", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "citation_count": "36", "reference_count": "28,707", "date": "2014", "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "related_topics": ["Overfitting", "Deep learning", "Convolutional neural network", "Supervised learning", "Types of artificial neural networks", "Artificial neural network", "Regularization (mathematics)", "Document classification", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "1903029394", "references": ["2102605133", "2618530766", "2155893237", "2155541015", "2097117768", "2109255472", "2963542991", "1849277567", "2962835968", "1663973292"], "title": "Fully convolutional networks for semantic segmentation", "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.", "citation_count": "42", "reference_count": "23,343", "date": "2015", "authors": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "related_topics": ["Scale-space segmentation", "Segmentation", "Inference", "Image translation", "Pattern recognition", "Artificial intelligence", "Pascal (programming language)", "Image (mathematics)", "Layer (object-oriented design)", "Computer science", "Key (cryptography)"]}
{"id": "2963420686", "references": ["2618530766", "639708223", "2963446712", "2097117768", "2963403868", "2194775991", "1903029394", "2962835968", "2117539524", "1836465849"], "title": "Squeeze-and-Excitation Networks", "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \u201cSqueeze-and-Excitation\u201d (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of   ${\\sim }$    \u223c     25 percent. Models and code are available at  https://github.com/hujie-frank/SENet .", "citation_count": "81", "reference_count": "6,969", "date": "2018", "authors": ["Jie Hu", "Li Shen", "Samuel Albanie", "Gang Sun", "Enhua Wu"], "related_topics": ["Convolutional neural network", "Block (data storage)", "Convolutional code", "Feature (machine learning)", "Construct (python library)", "Code (cryptography)", "Convolution", "Theoretical computer science", "Artificial intelligence", "Computer science"]}
{"id": "2923014074", "references": ["2962739339", "2963846996", "2251939518", "2962736243", "2963918774", "2525127255", "2130158090", "2963748441", "1486649854", "3104033643"], "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.", "citation_count": "16", "reference_count": "1,669", "date": "2018", "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "related_topics": ["Natural language understanding", "Benchmark (computing)", "Test set", "Transfer of learning", "Test data", "Sentence", "Task (project management)", "Feature (machine learning)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2970597249", "references": ["3127550471", "3100307207", "3105966348", "2996035354", "3100345210", "2990704537", "3099342932"], "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.", "citation_count": "0", "reference_count": "2,628", "date": "2019", "authors": ["Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le"], "related_topics": ["Autoregressive model", "Language model", "Question answering", "Sentiment analysis", "Margin (machine learning)", "Ranking", "Machine learning", "Dependency (UML)", "Computer science", "Noise reduction", "Artificial intelligence"]}
{"id": "2911489562", "references": ["2962739339", "2963403868", "2525778437", "2153579005", "2919115771", "2250539671", "2963756346", "2963341956", "2132339004", "2963748441"], "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining.", "abstract": "Motivation  Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.  Results  We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.  Availability and implementation  We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.", "citation_count": "43", "reference_count": "1,040", "date": "2019", "authors": ["Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang"], "related_topics": ["Biomedical text mining", "Named-entity recognition", "Language model", "Question answering", "Relationship extraction", "Deep learning", "Natural language processing", "F1 score", "Source code", "Computer science", "Artificial intelligence"]}
{"id": "2965373594", "references": ["2251939518", "2963403868", "2962784628", "2963026768", "2970597249", "2964121744", "2963341956", "2963748441", "2899771611", "1840435438"], "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.", "citation_count": "44", "reference_count": "5,486", "date": "2019", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov"], "related_topics": ["Hyperparameter", "Language model", "Machine learning", "Replication (statistics)", "Computer science", "Code (cryptography)", "Key (cryptography)", "Artificial intelligence", "Training set"]}
{"id": "2102605133", "references": ["2618530766", "2155541015", "2108598243", "2161969291", "2151103935", "2031489346", "2310919327", "2088049833", "3118608800", "2168356304"], "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.", "citation_count": "46", "reference_count": "19,676", "date": "2014", "authors": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "related_topics": ["Object detection", "Feature (computer vision)", "Convolutional neural network", "Feature extraction", "Support vector machine", "Segmentation", "Pattern recognition", "Minimum bounding box", "Machine learning", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2155893237", "references": ["2102605133", "2618530766", "1825604117", "2155541015", "1872489089", "2147414309", "2088049833", "2962883796", "753012316", "2963542991"], "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.", "citation_count": "10", "reference_count": "14,888", "date": "2014", "authors": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "related_topics": ["Deep learning", "Theano", "CUDA", "Python (programming language)", "Convolutional neural network", "Cloud computing", "Caff\u00e8", "TrueNorth", "Artificial neural network", "Computer architecture", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "639708223", "references": ["2102605133", "2618530766", "2155893237", "2097117768", "1536680647", "2194775991", "1903029394", "2168356304", "2962835968", "2117539524"], "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet  [1]  and Fast R-CNN  [2]  have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model  [3]  , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "citation_count": "39", "reference_count": "27,656", "date": "2017", "authors": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "related_topics": ["Object detection", "Convolutional neural network", "Deep learning", "Artificial neural network", "Feature extraction", "Frame rate", "Artificial intelligence", "Computer science"]}
{"id": "1536680647", "references": ["2102605133", "2618530766", "2155893237", "2108598243", "1861492603", "2109255472", "2164598857", "2168356304", "2962835968", "2963542991"], "title": "Fast R-CNN", "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "citation_count": "25", "reference_count": "14,493", "date": "2015", "authors": ["Ross Girshick"], "related_topics": ["Object detection", "Python (programming language)", "Pascal (programming language)", "Feature extraction", "Computational science", "Real-time computing", "Artificial intelligence", "Computer science", "Open source software"]}
{"id": "2158778629", "references": ["1647075334", "2147880316", "2134557905", "2119823327", "1991848143", "2310919327", "2164598857", "2121927366", "2157364932", "2104095591"], "title": "Toward automatic phenotyping of developing embryos from videos", "abstract": "We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.", "citation_count": "42", "reference_count": "308", "date": "2005", "authors": ["Feng Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "related_topics": ["Image segmentation", "Image processing", "Contextual image classification", "Pixel", "Computer vision", "Component (UML)", "Set (abstract data type)", "Computer science", "Cytoplasm", "Embryo", "Cell wall", "Artificial intelligence"]}
{"id": "2964311892", "references": ["2618530766", "1578099820", "2156718197", "1999192586", "2310919327", "2962820688", "2160815625", "1554944419", "2147860648", "2132914434"], "title": "Spectral Networks and Locally Connected Networks on Graphs", "abstract": "Abstract: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.", "citation_count": "20", "reference_count": "2,211", "date": "2014", "authors": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "related_topics": ["Laplacian matrix", "Convolutional neural network", "Hierarchical clustering", "Domain (software engineering)", "Image (mathematics)", "Translation (geometry)", "Theoretical computer science", "Group (mathematics)", "SIGNAL (programming language)", "Computer science"]}
{"id": "2963399829", "references": ["3035743198", "2971149989", "3129973872", "2963855133", "2966415767", "2970902013", "3098350627", "2964274690"], "title": "mixup: Beyond Empirical Risk Minimization", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.", "citation_count": "0", "reference_count": "1,648", "date": "2017", "authors": ["Hongyi Zhang", "Moustapha Cisse", "Yann N. Dauphin", "David Lopez-Paz"], "related_topics": ["Artificial neural network", "Empirical risk minimization", "Robustness (computer science)", "Artificial intelligence", "Generalization", "Computer science", "Memorization", "Sensitivity (control systems)", "Simple (abstract algebra)", "Generative grammar"]}
{"id": "2147880316", "references": ["1934019294", "2117400858", "1574901103", "2160842254", "2310919327", "2096175520", "3021452258", "1773803948", "2009570821", "3124955340"], "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.", "citation_count": "24", "reference_count": "15,804", "date": "2001", "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C. N. Pereira"], "related_topics": ["Variable-order Markov model", "Maximum-entropy Markov model", "Graphical model", "Conditional random field", "Markov model", "Markov property", "Sequence labeling", "Conditional entropy", "Random field", "Bayesian network", "Discriminative model", "Hidden Markov model", "Structured prediction", "Hidden semi-Markov model", "Probabilistic logic", "Probabilistic relevance model", "Algorithm", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2156163116", "references": ["2068017609", "2166469100", "2310919327", "2027197837", "2159737176", "1554663460", "51975515", "2147345686"], "title": "Best practices for convolutional neural networks applied to visual document analysis", "abstract": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.", "citation_count": "8", "reference_count": "2,771", "date": "2003", "authors": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "related_topics": ["Convolutional neural network", "MNIST database", "Artificial neural network", "Handwriting recognition", "Set (abstract data type)", "Machine learning", "Computer science", "Support vector machine", "Artificial intelligence", "Training set"]}
{"id": "1944615693", "references": ["2155893237", "2618530766", "2097117768", "2108598243", "2161969291", "2151103935", "2310919327", "1849277567", "1677409904", "2962835968"], "title": "Action recognition with trajectory-pooled deep-convolutional descriptors", "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.", "citation_count": "41", "reference_count": "3,123", "date": "2015", "authors": ["Limin Wang", "Yu Qiao", "Xiaoou Tang"], "related_topics": ["Normalization (statistics)", "Normalization (image processing)", "Discriminative model", "Pooling", "Pattern recognition", "Robustness (computer science)", "Machine learning", "Computer science", "Action recognition", "Artificial intelligence"]}
{"id": "1510526001", "references": ["2019575783", "2119821739", "2310919327", "2153635508", "2911964244", "2084812512", "2087347434", "1618905105", "2101276256", "1988195734"], "title": "Probability Estimates for Multi-class Classification by Pairwise Coupling", "abstract": "Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998)", "citation_count": "20", "reference_count": "2,293", "date": "2004", "authors": ["Ting-Fan Wu", "Chih-Jen Lin", "Ruby C. Weng"], "related_topics": ["Multiclass classification", "Class (biology)", "Random forest", "Support vector machine", "Linear system", "Voting", "Pattern recognition", "Mathematics", "Artificial intelligence", "Classification methods", "Pairwise coupling"]}
{"id": "2157364932", "references": ["2107369107", "2994340921", "1802356529", "2144354855", "2121647436", "2310919327", "2053186076", "2095757522", "10021998", "2138451337"], "title": "Learning a similarity metric discriminatively, with application to face verification", "abstract": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.", "citation_count": "17", "reference_count": "2,950", "date": "2005", "authors": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "related_topics": ["Facial recognition system", "Norm (mathematics)", "Discriminative model", "Robustness (computer science)", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence", "Face verification"]}
{"id": "2147768505", "references": ["1533861849", "2025768430", "2136922672", "2117130368", "2546302380", "2100495367", "1993882792", "2116064496", "1498436455", "2072128103"], "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.", "citation_count": "84", "reference_count": "3,179", "date": "2012", "authors": ["G. E. Dahl", "Dong Yu", "Li Deng", "A. Acero"], "related_topics": ["Deep belief network", "Hidden Markov model", "Word error rate", "Artificial neural network", "Mixture model", "Triphone", "Context model", "Context (language use)", "Speech recognition", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2963504252", "references": ["1533861849", "2618530766", "2117130368", "1815076433", "2110798204", "2100495367", "104184427", "1993882792", "2141125852", "2072128103"], "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "abstract": "Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.", "citation_count": "22", "reference_count": "985", "date": "2014", "authors": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "related_topics": ["Artificial neural network", "Deep learning", "Nonlinear system", "Gradient descent", "Convergence (routing)", "Edge of chaos", "Gaussian", "Linearity", "Algorithm", "Computer science", "Artificial intelligence"]}
{"id": "2156387975", "references": ["1533861849", "2025768430", "2136922672", "2129131372", "2310919327", "2097726431", "3118608800", "2546302380", "1665214252", "2072128103"], "title": "Deep Sparse Rectifier Neural Networks", "abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity", "citation_count": "36", "reference_count": "6,986", "date": "2011", "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "related_topics": ["Rectifier (neural networks)", "Winner-take-all", "Artificial neural network", "Logistic function", "Hyperbolic function", "Topology", "Artificial intelligence", "Computer science"]}
{"id": "2107878631", "references": ["1996741810", "2154642048", "2128499899", "2088978850", "19621276", "2148099973", "2581275558", "1527772862", "2125329357", "2016589492"], "title": "Learning long-term dependencies with gradient descent is difficult", "abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. &gt;", "citation_count": "19", "reference_count": "6,898", "date": "1994", "authors": ["Y. Bengio", "P. Simard", "P. Frasconi"], "related_topics": ["Vanishing gradient problem", "Gradient descent", "Recurrent neural network", "Gradient method", "Artificial neural network", "Term (time)", "Pattern recognition (psychology)", "Machine learning", "Face (geometry)", "Dynamical systems theory", "Computer science", "Numerical analysis", "Artificial intelligence", "Recurrent neural nets"]}
{"id": "2128499899", "references": ["1959983357", "2154642048", "2053127376", "2007431958", "2143503258", "2167607759", "2016589492"], "title": "Induction of Multiscale Temporal Structure", "abstract": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.", "citation_count": "8", "reference_count": "156", "date": "1991", "authors": ["Michael C Mozer"], "related_topics": ["Computational problem", "Sequence", "Structure (mathematical logic)", "Backpropagation", "Algorithm", "Computer science", "Musical composition", "Fraction (mathematics)", "Musical form"]}
{"id": "194249466", "references": ["2964308564", "1026270304", "2076063813", "2130942839", "2194775991", "1924770834", "2064675550", "2072128103"], "title": "Untersuchungen zu dynamischen neuronalen Netzen", "abstract": "", "citation_count": "0", "reference_count": "869", "date": "1991", "authors": ["Sepp Hochreiter"], "related_topics": ["Computer science"]}
{"id": "2048060899", "references": ["1991133427", "1959983357", "1966812932", "2048330959", "2173629880", "2101926813", "3017143921", "3121926921", "2176028050", "1498436455"], "title": "A time-delay neural network architecture for isolated word recognition", "abstract": "Abstract   A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.", "citation_count": "18", "reference_count": "815", "date": "1990", "authors": ["Kevin J. Lang", "Alex H. Waibel", "Geoffrey E. Hinton"], "related_topics": ["Time delay neural network", "Word recognition", "Hidden Markov model", "Network architecture", "Artificial neural network", "Vocabulary", "Speech recognition", "Segmentation", "Computer science"]}
{"id": "2103452139", "references": ["2107878631", "2154642048", "2128499899", "2110485445", "1674799117", "2138484437", "2798813531", "2123716044", "2064675550", "2098398123"], "title": "Learning long-term dependencies in NARX recurrent neural networks", "abstract": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.", "citation_count": "32", "reference_count": "702", "date": "1996", "authors": ["Tsungnan Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "related_topics": ["Recurrent neural network", "Artificial neural network", "Nonlinear system identification", "Nonlinear autoregressive exogenous model", "Gradient descent", "Grammar induction", "Gradient method", "Artificial intelligence", "Autoregressive model", "Machine learning", "Nonlinear system", "Computer science", "Recurrent neural nets"]}
{"id": "1674799117", "references": ["2147568880", "2079735306", "3099873379", "1810943226", "1735317348", "1828163288", "2144499799", "2064675550", "2136848157"], "title": "Gradient-based learning algorithms for recurrent networks and their computational complexity", "abstract": "", "citation_count": "0", "reference_count": "548", "date": "1995", "authors": ["Ronald J. Williams", "David Zipser"], "related_topics": ["Computational resource", "Computational learning theory", "Deep learning", "Probabilistic analysis of algorithms", "Artificial neural network", "Recurrent neural network", "Stability (learning theory)", "Types of artificial neural networks", "Computer science", "Artificial intelligence", "Theoretical computer science"]}
{"id": "2007431958", "references": ["2075510082", "2177721432", "1652505363"], "title": "Generalization of back-propagation to recurrent neural networks.", "abstract": "An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\ensuremath{\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.", "citation_count": "3", "reference_count": "1,210", "date": "1987", "authors": ["Fernando J. Pineda"], "related_topics": ["Hopfield network", "Recurrent neural network", "Time delay neural network", "Types of artificial neural networks", "Feedforward neural network", "Probabilistic neural network", "Physical neural network", "Stochastic neural network", "Artificial intelligence", "Computer science"]}
{"id": "2123716044", "references": ["2150355110", "1529008516", "2132152975", "2143787696", "2112462566", "2138484437", "2057653135", "2090248140", "1583833196", "2016589492"], "title": "Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks", "abstract": "Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise. &gt;", "citation_count": "27", "reference_count": "759", "date": "1994", "authors": ["G.V. Puskorius", "L.A. Feldkamp"], "related_topics": ["Extended Kalman filter", "Recurrent neural network", "Kalman filter", "Control theory", "Artificial neural network", "Dynamical systems theory", "Gradient descent", "Control system", "Feed forward", "Control theory", "Computer science", "Moving horizon estimation"]}
{"id": "2143503258", "references": ["1597286183", "1959983357", "2154642048", "1971129545", "2173629880", "2007431958", "2581275558", "1507849272", "3121926921", "2016589492"], "title": "Learning state space trajectories in recurrent neural networks", "abstract": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.", "citation_count": "14", "reference_count": "997", "date": "1989", "authors": ["Barak A. Pearlmutter"], "related_topics": ["Echo state network", "Recurrent neural network", "Time delay neural network", "Feedforward neural network", "Deep learning", "Random neural network", "Probabilistic neural network", "Gradient descent", "Algorithm", "Artificial intelligence", "Computer science"]}
{"id": "2154890045", "references": ["1597286183", "2154642048", "2110485445", "2173629880", "2147800946", "2138484437", "1535810436", "2581275558", "2895674046", "2016589492"], "title": "Gradient calculations for dynamic recurrent neural networks: a survey", "abstract": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed. &gt;", "citation_count": "117", "reference_count": "713", "date": "1995", "authors": ["B.A. Pearlmutter"], "related_topics": ["Backpropagation through time", "Deep learning", "Recurrent neural network", "Types of artificial neural networks", "Artificial neural network", "Backpropagation", "Artificial intelligence", "Fixed point", "Theoretical computer science", "Computer science", "Boltzmann constant", "Recurrent neural nets"]}
{"id": "2146502635", "references": ["2610857016", "2108598243", "1992208280", "1978394996", "2150102617", "2167732364", "2160218441", "2798766386", "3120740533", "2296319761"], "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.", "citation_count": "48", "reference_count": "8,863", "date": "2011", "authors": ["John Duchi", "Elad Hazan", "Yoram Singer"], "related_topics": ["Subgradient method", "Online machine learning", "Empirical risk minimization", "Stochastic optimization", "Regularization (mathematics)", "Mathematical optimization", "Regret", "Function (mathematics)", "Domain (software engineering)", "Mathematics"]}
{"id": "104184427", "references": ["1533861849", "2618530766", "2136922672", "2147768505", "2184045248", "2110798204", "2100495367", "1993882792", "3141595720", "2064675550"], "title": "On the importance of initialization and momentum in deep learning", "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.\r\n\r\nOur success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.", "citation_count": "28", "reference_count": "3,619", "date": "2013", "authors": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "related_topics": ["Initialization", "Stochastic gradient descent", "Recurrent neural network", "Deep learning", "Momentum (technical analysis)", "Schedule", "Machine learning", "Computer science", "Curvature", "Training (meteorology)", "Artificial intelligence"]}
{"id": "2168231600", "references": ["2117130368", "2108598243", "2173213060", "2146502635", "2147768505", "2184045248", "2118858186", "3118608800", "2132339004", "2141125852"], "title": "Large Scale Distributed Deep Networks", "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.", "citation_count": "33", "reference_count": "3,338", "date": "2012", "authors": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Marc'aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le", "Andrew Y. Ng"], "related_topics": ["Deep learning", "Artificial neural network", "Feature learning", "Stochastic gradient descent", "Theano", "Asynchronous communication", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2006434809", "references": ["1703839189", "1852588318", "2140143765", "2107053896", "2131262274", "2166867592", "2112147913", "2163627712", "2045002682", "2119775949"], "title": "Epidemiological, demographic, and clinical characteristics of 47 cases of Middle East respiratory syndrome coronavirus disease from Saudi Arabia: a descriptive study", "abstract": "Summary  Background  Middle East respiratory syndrome (MERS) is a new human disease caused by a novel coronavirus (CoV). Clinical data on MERS-CoV infections are scarce. We report epidemiological, demographic, clinical, and laboratory characteristics of 47 cases of MERS-CoV infections, identify knowledge gaps, and define research priorities.  Methods  We abstracted and analysed epidemiological, demographic, clinical, and laboratory data from confirmed cases of sporadic, household, community, and health-care-associated MERS-CoV infections reported from Saudi Arabia between Sept 1, 2012, and June 15, 2013. Cases were confirmed as having MERS-CoV by real-time RT-PCR.  Findings  47 individuals (46 adults, one child) with laboratory-confirmed MERS-CoV disease were identified; 36 (77%) were male (male:female ratio 3\u00b73:1). 28 patients died, a 60% case-fatality rate. The case-fatality rate rose with increasing age. Only two of the 47 cases were previously healthy; most patients (45 [96%]) had underlying comorbid medical disorders, including diabetes (32 [68%]), hypertension (16 [34%]), chronic cardiac disease (13 [28%]), and chronic renal disease (23 [49%]). Common symptoms at presentation were fever (46 [98%]), fever with chills or rigors (41 [87%]), cough (39 [83%]), shortness of breath (34 [72%]), and myalgia (15 [32%]). Gastrointestinal symptoms were also frequent, including diarrhoea (12 [26%]), vomiting (ten [21%]), and abdominal pain (eight [17%]). All patients had abnormal findings on chest radiography, ranging from subtle to extensive unilateral and bilateral abnormalities. Laboratory analyses showed raised concentrations of lactate dehydrogenase (23 [49%]) and aspartate aminotransferase (seven [15%]) and thrombocytopenia (17 [36%]) and lymphopenia (16 [34%]).  Interpretation  Disease caused by MERS-CoV presents with a wide range of clinical manifestations and is associated with substantial mortality in admitted patients who have medical comorbidities. Major gaps in our knowledge of the epidemiology, community prevalence, and clinical spectrum of infection and disease need urgent definition.  Funding  None.", "citation_count": "31", "reference_count": "1,320", "date": "2013", "authors": ["Abdullah Assiri", "Jaffar A Al-Tawfiq", "Abdullah A Al-Rabeeah", "Fahad A Al-Rabiah", "Sami Al-Hajjar", "Ali Al-Barrak", "Hesham Flemban", "Wafa N Al-Nassir", "Hanan H Balkhy", "Rafat F Al-Hakeem", "Hatem Q Makhdoom", "Alimuddin I Zumla", "Ziad A Memish"], "related_topics": ["Middle East respiratory syndrome", "Epidemiology", "myalgia", "Chills", "Abdominal pain", "Young adult", "Middle East respiratory syndrome coronavirus", "Odds ratio", "Pediatrics", "Medicine"]}
{"id": "2104548316", "references": ["2100820722", "2463755683", "2131262274", "2127949919", "2398786667", "1576737979", "2106882534", "2128788856", "2076620790", "2125251240"], "title": "A novel coronavirus associated with severe acute respiratory syndrome.", "abstract": "background A worldwide outbreak of severe acute respiratory syndrome (SARS) has been associated with exposures originating from a single ill health care worker from Guangdong Province, China. We conducted studies to identify the etiologic agent of this outbreak. methods We received clinical specimens from patients in six countries and tested them, using virus isolation techniques, electron-microscopical and histologic studies, and molecular and serologic assays, in an attempt to identify a wide range of potential pathogens. results No classic respiratory or bacterial respiratory pathogen was consistently identified. However, a novel coronavirus was isolated from patients who met the case definition of SARS. Cytopathological features were noted microscopically in Vero E6 cells inoculated with a throat-swab specimen. Electron-microscopical examination of cultures revealed ultrastructural features characteristic of coronaviruses. Immunohistochemical and immunofluorescence staining revealed reactivity with group I coronavirus polyclonal antibodies. Consensus coronavirus primers designed to amplify a fragment of the polymerase gene by reverse transcription\u2013polymerase chain reaction (RT-PCR) were used to obtain a sequence that clearly identified the isolate as a unique coronavirus only distantly related to previously sequenced coronaviruses. With specific diagnostic RT-PCR primers we identified several identical nucleotide sequences in 12 patients from several locations, a finding consistent with a point source outbreak. Indirect fluorescent antibody tests and enzyme-linked immunosorbent assays made with the new coronavirus isolate have been used to demonstrate a virus-specific serologic response. Preliminary studies suggest that this virus may never before have infected the U.S. population. conclusions A novel coronavirus is associated with this outbreak, and the evidence indicates that this virus has an etiologic role in SARS. The name Urbani SARS-associated coronavirus is proposed for the virus.", "citation_count": "31", "reference_count": "5,221", "date": "2003", "authors": ["Ksiazek Tg", "Erdman D", "Goldsmith Cs", "Zaki", "Peret T", "Emery S", "Tong S", "Urbani C", "Comer Ja", "Lim W", "Rollin Pe", "Dowell Sf", "Ling Ae", "Humphrey Cd", "Shieh Wj", "Guarner J", "Paddock Cd", "Rota P", "Fields B", "DeRisi J", "Yang Jy", "Cox N", "Hughes Jm", "LeDuc Jw", "Bellini Wj", "Anderson Lj"], "related_topics": ["Human coronavirus OC43", "Coronavirus", "Human coronavirus 229E", "Human coronavirus NL63", "Severe acute respiratory syndrome", "Population", "Nidovirales", "Virus", "Virology", "Biology"]}
{"id": "2166867592", "references": ["2129542667", "1703839189", "2126707939", "1987783718", "2116586125", "2132260239", "2170933940", "2111412754", "2025170735", "1963953102"], "title": "Isolation of a Novel Coronavirus from a Man with Pneumonia in Saudi Arabia", "abstract": "A previously unknown coronavirus was isolated from the sputum of a 60-year-old man who presented with acute pneumonia and subsequent renal failure with a fatal outcome in Saudi Arabia. The virus (called HCoV-EMC) replicated readily in cell culture, producing cytopathic effects of rounding, detachment, and syncytium formation. The virus represents a novel betacoronavirus species. The closest known relatives are bat coronaviruses HKU4 and HKU5. Here, the clinical data, virus isolation, and molecular identification are presented. The clinical picture was remarkably similar to that of the severe acute respiratory syndrome (SARS) outbreak in 2003 and reminds us that animal coronaviruses can cause severe disease in humans.", "citation_count": "24", "reference_count": "4,563", "date": "2012", "authors": ["Ali Moh Zaki", "Sander Van Boheemen", "Theo M. Bestebroer", "Albert D.M.E. Osterhaus", "Ron A.M. Fouchier"], "related_topics": ["Middle East respiratory syndrome", "Coronavirus", "Middle East respiratory syndrome coronavirus", "Betacoronavirus", "Pneumonia", "Alphacoronavirus", "Virus", "Outbreak", "Virology", "Medicine"]}
{"id": "3000413850", "references": ["2793022939", "2470646526", "1993577573", "2292021561", "2255243349", "2290466312", "2565805236", "2107277218", "2791599184", "2725497285"], "title": "Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV.", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.", "citation_count": "53", "reference_count": "1,351", "date": "2020", "authors": ["Timothy P. Sheahan", "Amy C. Sims", "Sarah R. Leist", "Alexandra Sch\u00e4fer", "John Won", "Ariane J. Brown", "Stephanie A. Montgomery", "Alison Hogg", "Darius Babusis", "Michael O. Clarke", "Jamie E. Spahn", "Laura Bauer", "Scott Sellers", "Danielle Porter", "Joy Y. Feng", "Tomas Cihlar", "Robert Jordan", "Mark R. Denison", "Ralph S. Baric"], "related_topics": ["Lopinavir/ritonavir", "Lopinavir", "Lung injury", "Viral load", "Ritonavir", "Middle East respiratory syndrome coronavirus", "Respiratory disease", "Pulmonary function testing", "Virology", "Medicine"]}
{"id": "2131262274", "references": ["2130141864", "1991467275", "2463755683", "1982444609", "2123324969"], "title": "A Major Outbreak of Severe Acute Respiratory Syndrome in Hong Kong", "abstract": "background There has been an outbreak of the severe acute respiratory syndrome (SARS) worldwide. We report the clinical, laboratory, and radiologic features of 138 cases of suspected SARS during a hospital outbreak in Hong Kong. methods From March 11 to 25, 2003, all patients with suspected SARS after exposure to an index patient or ward were admitted to the isolation wards of the Prince of Wales Hospital. Their demographic, clinical, laboratory, and radiologic characteristics were analyzed. Clinical end points included the need for intensive care and death. Univariate and multivariate analyses were performed. results There were 66 male patients and 72 female patients in this cohort, 69 of whom were health care workers. The most common symptoms included fever (in 100 percent of the patients); chills, rigors, or both (73.2 percent); and myalgia (60.9 percent). Cough and headache were also reported in more than 50 percent of the patients. Other common findings were lymphopenia (in 69.6 percent), thrombocytopenia (44.8 percent), and elevated lactate dehydrogenase and creatine kinase levels (71.0 percent and 32.1 percent, respectively). Peripheral air-space consolidation was commonly observed on thoracic computed tomographic scanning. A total of 32 patients (23.2 percent) were admitted to the intensive care unit; 5 patients died, all of whom had coexisting conditions. In a multivariate analysis, the independent predictors of an adverse outcome were advanced age (odds ratio per decade of life, 1.80; 95 percent confidence interval, 1.16 to 2.81; P=0.009), a high peak lactate dehydrogenase level (odds ratio per 100 U per liter, 2.09; 95 percent confidence interval, 1.28 to 3.42; P=0.003), and an absolute neutrophil count that exceeded the upper limit of the normal range on presentation (odds ratio, 1.60; 95 percent confidence interval, 1.03 to 2.50; P=0.04). conclusions SARS is a serious respiratory illness that led to significant morbidity and mortality in our cohort.", "citation_count": "5", "reference_count": "2,974", "date": "2003", "authors": ["Nelson Lee", "David Hui", "Alan Wu", "Paul Chan", "Peter Cameron", "Gavin M Joynt", "Anil Ahuja", "Man Yee Yung", "C B Leung", "K F To", "S F Lui", "C C Szeto", "Sydney Chung", "Joseph J Y Sung"], "related_topics": ["Intensive care", "Severe acute respiratory syndrome", "Odds ratio", "Intensive care unit", "Chills", "Confidence interval", "Cohort", "myalgia", "Internal medicine", "Medicine", "Surgery"]}
{"id": "3017468735", "references": ["3009739970", "3001118548", "3005655936", "3002764620", "3015190630", "3004318991", "3010819577", "3006354146", "3007814559", "3007273493"], "title": "A Novel Coronavirus Genome Identified in a Cluster of Pneumonia Cases \u2014 Wuhan, China 2019\u22122020", "abstract": "", "citation_count": "0", "reference_count": "377", "date": "2020", "authors": ["Wenjie Tan", "Xiang Zhao", "Xuejun Ma", "Wenling Wang", "Peihua Niu", "Wenbo Xu", "George F. Gao", "Guizhen Wu"], "related_topics": ["Coronavirus", "Pneumonia", "Disease cluster", "Genome", "Medicine", "Virology"]}
{"id": "2903899730", "references": ["2104548316", "2775086803", "2166867592", "2119111857", "2470646526", "2144081223", "1993577573", "2306794997", "2111211467", "2132260239"], "title": "Origin and evolution of pathogenic coronaviruses", "abstract": "Severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) are two highly transmissible and pathogenic viruses that emerged in humans at the beginning of the 21st century. Both viruses likely originated in bats, and genetically diverse coronaviruses that are related to SARS-CoV and MERS-CoV were discovered in bats worldwide. In this Review, we summarize the current knowledge on the origin and evolution of these two pathogenic coronaviruses and discuss their receptor usage; we also highlight the diversity and potential of spillover of bat-borne coronaviruses, as evidenced by the recent spillover of swine acute diarrhoea syndrome coronavirus (SADS-CoV) to pigs. Coronaviruses have a broad host range and distribution, and some highly pathogenic lineages have spilled over to humans and animals. Here, Cui, Li and Shi explore the viral factors that enabled the emergence of diseases such as severe acute respiratory syndrome and Middle East respiratory syndrome.", "citation_count": "145", "reference_count": "3,159", "date": "2019", "authors": ["Jie Cui", "Fang Li", "Zheng Li Shi"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome", "Middle East respiratory syndrome coronavirus", "Viral evolution", "Viral pathogenesis", "Virology", "Phylogenetics", "Biology", "Acute diarrhoea", "Severe acute respiratory syndrome coronavirus"]}
{"id": "2132260239", "references": ["2100820722", "2127062009", "2090060897", "2004869546", "2030133843", "2084994773", "2149579937", "2107922358", "2125251240"], "title": "Identification of a novel coronavirus in patients with severe acute respiratory syndrome.", "abstract": "BACKGROUND: The severe acute respiratory syndrome (SARS) has recently been identified as a new clinical entity. SARS is thought to be caused by an unknown infectious agent. METHODS: Clinical specimens from patients with SARS were searched for unknown viruses with the use of cell cultures and molecular techniques. RESULTS: A novel coronavirus was identified in patients with SARS. The virus was isolated in cell culture, and a sequence 300 nucleotides in length was obtained by a polymerase-chain-reaction (PCR)-based random-amplification procedure. Genetic characterization indicated that the virus is only distantly related to known coronaviruses (identical in 50 to 60 percent of the nucleotide sequence). On the basis of the obtained sequence, conventional and real-time PCR assays for specific and sensitive detection of the novel virus were established. Virus was detected in a variety of clinical specimens from patients with SARS but not in controls. High concentrations of viral RNA of up to 100 million molecules per milliliter were found in sputum. Viral RNA was also detected at extremely low concentrations in plasma during the acute phase and in feces during the late convalescent phase. Infected patients showed seroconversion on the Vero cells in which the virus was isolated. CONCLUSIONS: The novel coronavirus might have a role in causing SARS.", "citation_count": "9", "reference_count": "5,072", "date": "2003", "authors": ["Christian Drosten", "Stephan G\u00fcnther", "Wolfgang Preiser", "Sylvie van der Werf", "Hans-Reinhard Brodt", "Stephan Becker", "Holger Rabenau", "Marcus Panning", "Larissa Kolesnikova", "Ron A.M. Fouchier", "Annemarie Berger", "Ana-Maria Burgui\u00e8re", "Jindrich Cinatl", "Markus Eickmann", "Nicolas Escriou", "Klaus Grywna", "Stefanie Kramme", "Jean-Claude Manuguerra", "Stefanie M\u00fcller", "Volker Rickerts", "Martin St\u00fcrmer", "Simon Vieth", "Hans-Dieter Klenk", "Albert D.M.E. Osterhaus", "Herbert Schmitz", "Hans Wilhelm Doerr"], "related_topics": ["Coronavirus", "Severe acute respiratory syndrome", "Human coronavirus OC43", "Human coronavirus NL63", "Novel virus", "Virus", "Nidovirales", "Coronaviridae", "Virology", "Biology"]}
{"id": "2725497285", "references": ["2129542667", "1945961678", "2099941783", "2195009776", "2292021561", "2470646526", "2525468044", "2255243349", "2298153446", "2115555188"], "title": "Broad-spectrum antiviral GS-5734 inhibits both epidemic and zoonotic coronaviruses.", "abstract": "Emerging viral infections are difficult to control because heterogeneous members periodically cycle in and out of humans and zoonotic hosts, complicating the development of specific antiviral therapies and vaccines. Coronaviruses (CoVs) have a proclivity to spread rapidly into new host species causing severe disease. Severe acute respiratory syndrome CoV (SARS-CoV) and Middle East respiratory syndrome CoV (MERS-CoV) successively emerged, causing severe epidemic respiratory disease in immunologically naive human populations throughout the globe. Broad-spectrum therapies capable of inhibiting CoV infections would address an immediate unmet medical need and could be invaluable in the treatment of emerging and endemic CoV infections. We show that a nucleotide prodrug, GS-5734, currently in clinical development for treatment of Ebola virus disease, can inhibit SARS-CoV and MERS-CoV replication in multiple in vitro systems, including primary human airway epithelial cell cultures with submicromolar IC50 values. GS-5734 was also effective against bat CoVs, prepandemic bat CoVs, and circulating contemporary human CoV in primary human lung cells, thus demonstrating broad-spectrum anti-CoV activity. In a mouse model of SARS-CoV pathogenesis, prophylactic and early therapeutic administration of GS-5734 significantly reduced lung viral load and improved clinical signs of disease as well as respiratory function. These data provide substantive evidence that GS-5734 may prove effective against endemic MERS-CoV in the Middle East, circulating human CoV, and, possibly most importantly, emerging CoV of the future.", "citation_count": "34", "reference_count": "1,095", "date": "2017", "authors": ["Timothy P. Sheahan", "Amy C. Sims", "Rachel L. Graham", "Vineet D. Menachery", "Lisa E. Gralinski", "James B. Case", "Sarah R. Leist", "Krzysztof Pyrc", "Joy Y. Feng", "Iva Trantcheva", "Roy Bannister", "Yeojin Park", "Darius Babusis", "Michael O. Clarke", "Richard L. Mackman", "Jamie E. Spahn", "Christopher A. Palmiotti", "Dustin Siegel", "Adrian S. Ray", "Tomas Cihlar", "Robert Jordan", "Mark R. Denison", "Ralph S. Baric"], "related_topics": ["Respiratory function", "Middle East respiratory syndrome", "Coronavirus", "Ebola virus", "Viral load", "Disease", "Viral replication", "Respiratory disease", "Virology", "Immunology", "Biology"]}
{"id": "2110764733", "references": ["2166049352", "2134557905", "2156909104", "2107034620", "2154422044", "2156598602", "2038721957", "2164598857", "1566135517", "2138451337"], "title": "LabelMe: A Database and Web-Based Tool for Image Annotation", "abstract": "We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.", "citation_count": "44", "reference_count": "3,313", "date": "2008", "authors": ["Bryan C. Russell", "Antonio Torralba", "Kevin P. Murphy", "William T. Freeman"], "related_topics": ["Image retrieval", "LabelMe", "Automatic image annotation", "Object detection", "Object (computer science)", "Supervised learning", "WordNet", "Cognitive neuroscience of visual object recognition", "Information retrieval", "Data mining", "Computer science"]}
{"id": "2141282920", "references": ["2970081408", "2050457084", "2055225264", "1587328194", "2166770390", "2612148268", "1934863104", "1666447063", "181417509", "2293605478"], "title": "Labeling images with a computer game", "abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "citation_count": "10", "reference_count": "2,875", "date": "2004", "authors": ["Luis von Ahn", "Laura Dabbish"], "related_topics": ["Computer game", "Game design", "Game development tool", "Game design document", "Game art design", "Multimedia", "Computer science", "Block (data storage)", "Image (mathematics)"]}
{"id": "2151103935", "references": ["1949116567", "2124386111", "2012778485", "2154422044", "2124087378", "2033819227", "2111308925", "2165497495", "1676552347", "2124404372"], "title": "Distinctive Image Features from Scale-Invariant Keypoints", "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.", "citation_count": "42", "reference_count": "63,557", "date": "2004", "authors": ["David G. Lowe"], "related_topics": ["3D single-object recognition", "Haar-like features", "Feature (computer vision)", "Scale-invariant feature transform", "Maximally stable extremal regions", "Scale space", "Point set registration", "Hough transform", "Pattern recognition", "Computer vision", "Computer science", "Machine learning"]}
{"id": "2115733720", "references": ["2045656233", "2217896605", "2166049352", "2134557905", "2030536784", "2154422044", "2310919327", "2130416410", "2164598857", "2124386111"], "title": "One-shot learning of object categories", "abstract": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.", "citation_count": "44", "reference_count": "2,041", "date": "2006", "authors": ["Li Fei-Fei", "R. Fergus", "P. Perona"], "related_topics": ["One-shot learning", "Supervised learning", "Maximum a posteriori estimation", "Caltech 101", "Object (computer science)", "Bayesian probability", "Prior probability", "Bayes' theorem", "Probabilistic logic", "Inference", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "1782590233", "references": ["2137659841", "3097096317", "2033419168", "2994340921", "2098693229", "2121647436", "2006793117", "2123921160", "2125310925", "1999478155"], "title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments", "abstract": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.", "citation_count": "40", "reference_count": "5,215", "date": "2008", "authors": ["Gary B. Huang", "Marwan Mattar", "Tamara Berg", "Eric Learned-Miller"], "related_topics": ["Three-dimensional face recognition", "Face detection", "Facial recognition system", "Face (geometry)", "Computer vision", "Computer science", "Database", "Quality (business)", "Range (mathematics)", "Artificial intelligence", "Face synthesis", "Image acquisition"]}
{"id": "1576445103", "references": ["2618530766", "2295107390", "2108598243", "1861492603", "2031489346", "2963173190", "2962835968", "2117539524"], "title": "Caltech-256 Object Category Dataset", "abstract": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.", "citation_count": "0", "reference_count": "2,353", "date": "2007", "authors": ["Gregory Griffin", "Alex Holub", "Pietro Perona"], "related_topics": ["Caltech 101", "Pyramid (image processing)", "Set (abstract data type)", "Clutter", "Object (computer science)", "Matching (graph theory)", "Benchmark (computing)", "Computer vision", "Measure (mathematics)", "Mathematics", "Artificial intelligence"]}
{"id": "2128017662", "references": ["2104978738", "2124404372", "1980911747", "2151103935", "2172188317", "2177274842", "2147717514", "2162006472", "2165497495", "2131846894"], "title": "Scalable Recognition with a Vocabulary Tree", "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u0092s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "citation_count": "19", "reference_count": "4,716", "date": "2006", "authors": ["D. Nister", "H. Stewenius"], "related_topics": ["Vocabulary", "Bag-of-words model in computer vision", "Search engine indexing", "Robustness (computer science)", "Scalability", "Pattern recognition", "Visualization", "Computer science", "Artificial intelligence"]}
{"id": "2145607950", "references": ["2110764733", "2111993661", "2107034620", "1576445103", "2128017662", "2038721957", "2164598857", "2162915993", "1566135517", "2124386111"], "title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition", "abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.", "citation_count": "54", "reference_count": "1,899", "date": "2008", "authors": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "related_topics": ["WordNet", "Object detection", "Image processing", "Cognitive neuroscience of visual object recognition", "Lexical database", "Human visual system model", "Pattern recognition (psychology)", "Object (computer science)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1528789833", "references": ["2024046085", "2147880316", "2154422044", "2057175746", "2168002178", "2164598857", "1666447063", "2124351162", "2169551590", "1484228140"], "title": "TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation", "abstract": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods.\r\n\r\nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).", "citation_count": "25", "reference_count": "1,518", "date": "2006", "authors": ["Jamie Shotton", "John Winn", "Carsten Rother", "Antonio Criminisi"], "related_topics": ["Image segmentation", "One-class classification", "Conditional random field", "Discriminative model", "Context model", "Segmentation", "Feature selection", "Boosting (machine learning)", "Supervised learning", "Cognitive neuroscience of visual object recognition", "Linear discriminant analysis", "Image processing", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2038721957", "references": ["2022166150", "2108598243", "2110764733", "1861492603", "2031489346", "2145607950", "2132339004", "2160660844", "2097726431"], "title": "WordNet : an electronic lexical database", "abstract": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.", "citation_count": "0", "reference_count": "20,553", "date": "2000", "authors": ["Christiane Fellbaum"], "related_topics": ["eXtended WordNet", "EuroWordNet", "WordNet", "Normalized Google distance", "IndoWordNet", "Lexical database", "GermaNet", "Semantic network", "Natural language processing", "Information retrieval", "Computer science", "Artificial intelligence"]}
{"id": "3032185657", "references": ["3001118548", "3014294089", "3003668884", "3010930696", "3009885589", "3008090866", "3008827533", "3008028633", "3007497549", "3005079553"], "title": "Trajectory of the COVID-19 pandemic: chasing a moving target", "abstract": "The spread of COVID-19 has already taken a pandemic form, affecting over 180 countries in a matter of three months. The full continuum of disease ranges from mild, self-limiting illness to severe progressive COVID-19 pneumonia, multiorgan failure, cytokine storm and death. Younger and healthy population is now getting affected than before. Possibilities of airborne and fecal oral routes of transmission has increased the concern. In the absence of any specific therapeutic agent for coronavirus infections, the most effective manner to contain this pandemic is probably the non-pharmacological interventions (NPIs). The damage due to the pandemic disease is multifaceted and crippling to economy, trade, and health of the citizens of the countries. The extent of damage in such scenarios is something that is beyond calculation by Gross Domestic Product rate or currency value of the country. Unfortunately, unlike many other diseases, we are still away from the target antiviral drug and vaccine for severe acute respiratory syndrome (SARS-CoV-2) infection. The prime importance of NPIs like social distancing, staying in home, work from home, self-monitoring, public awareness, self-quarantine, etc. are constantly being emphasized by CDC, WHO, health ministries of all countries and social media houses. This is time of introspection and learning from our mistakes. Countries like China and South Korea who were initially the most hit countries could contain the disease spread by liberal testing of their population, stringent quarantine of people under investigation and isolation of the positive cases. Rest of the countries need to act urgently as well to bring an immediate halt in the community transmission.", "citation_count": "75", "reference_count": "18", "date": "2020", "authors": ["Kamal Kant Sahu", "Ajay Kumar Mishra", "Amos Lal"], "related_topics": ["Population", "Pandemic", "Isolation (health care)", "Quarantine", "Disease", "Transmission (medicine)", "Psychological intervention", "Gross domestic product", "Development economics", "Business"]}
{"id": "3037255629", "references": ["3001118548", "3001897055", "3010930696", "3007940623", "3008827533", "3010604545", "3008028633", "3007497549", "3005079553", "3002108456"], "title": "COVID-19 in children: An ample review", "abstract": "The aim of this review was to describe the current knowledge about coronavirus disease 2019 (COVID-19, which is caused by severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) in children, from epidemiological, clinical, and laboratory perspectives, including knowledge on the disease course, treatment, and prognosis. An extensive literature search was performed to identify papers on COVID-19 (SARS-CoV-2 infection) in children, published between January 1, 2020 and April 1, 2020. There were 44 relevant papers on COVID-19 in children. The results showed that COVID-19 occurs in 0.39\u201312.3% of children. Clinical signs and symptoms are comparable to those in adults, but milder forms and a large percentage of asymptomatic carriers are found among children. Elevated inflammatory markers are associated with complications and linked to various co-infections. Chest computed tomography (CT) scans in children revealed structural changes similar to those found in adults, with consolidations surrounded by halos being somewhat specific for children with COVID-19. The recommended treatment includes providing symptomatic therapy, with no specific drug recommendations for children. The prognosis is much better for children compared to adults. This review highlights that COVID-19 in children is similar to the disease in the adult population, but with particularities regarding clinical manifestations, laboratory test results, chest imaging, and treatment. The prognosis is much better for children compared to adults, but with the progression of the pandemic; the cases in children might change in the future.", "citation_count": "63", "reference_count": "12", "date": "2020", "authors": ["Ioana M Ciuca"], "related_topics": ["Epidemiology", "Asymptomatic carrier", "Disease", "Pediatrics", "Medicine", "Pandemic", "Adult population", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Signs and symptoms"]}
{"id": "3006643024", "references": ["3004906315", "3001118548", "2092969802", "3001897055", "3003668884", "2102634410", "3004802901", "2800783955", "3005079553", "2056155046"], "title": "Time Course of Lung Changes at Chest CT during Recovery from Coronavirus Disease 2019 (COVID-19).", "abstract": "Background Chest CT is used to assess the severity of lung involvement in coronavirus disease 2019 (COVID-19). Purpose To determine the changes in chest CT findings associated with COVID-19 from initial diagnosis until patient recovery. Materials and Methods This retrospective review included patients with real-time polymerase chain reaction-confirmed COVID-19 who presented between January 12, 2020, and February 6, 2020. Patients with severe respiratory distress and/or oxygen requirement at any time during the disease course were excluded. Repeat chest CT was performed at approximately 4-day intervals. Each of the five lung lobes was visually scored on a scale of 0 to 5, with 0 indicating no involvement and 5 indicating more than 75% involvement. The total CT score was determined as the sum of lung involvement, ranging from 0 (no involvement) to 25 (maximum involvement). Results Twenty-one patients (six men and 15 women aged 25-63 years) with confirmed COVID-19 were evaluated. A total of 82 chest CT scans were obtained in these patients, with a mean interval (\u00b1standard deviation) of 4 days \u00b1 1 (range, 1-8 days). All patients were discharged after a mean hospitalization period of 17 days \u00b1 4 (range, 11-26 days). Maximum lung involved peaked at approximately 10 days (with a calculated total CT score of 6) from the onset of initial symptoms (R2 = 0.25, P &lt; .001). Based on quartiles of chest CT scans from day 0 to day 26 involvement, four stages of lung CT findings were defined. CT scans obtained in stage 1 (0-4 days) showed ground-glass opacities (18 of 24 scans [75%]), with a mean total CT score of 2 \u00b1 2; scans obtained in stage 2 (5-8 days) showed an increase in both the crazy-paving pattern (nine of 17 scans [53%]) and total CT score (mean, 6 \u00b1 4; P = .002); scans obtained in stage 3 (9-13 days) showed consolidation (19 of 21 scans [91%]) and a peak in the total CT score (mean, 7 \u00b1 4); and scans obtained in stage 4 (\u226514 days) showed gradual resolution of consolidation (15 of 20 scans [75%]) and a decrease in the total CT score (mean, 6 \u00b1 4) without crazy-paving pattern. Conclusion In patients recovering from coronavirus disease 2019 (without severe respiratory distress during the disease course), lung abnormalities on chest CT scans showed greatest severity approximately 10 days after initial onset of symptoms. \u00a9 RSNA, 2020.", "citation_count": "12", "reference_count": "1,266", "date": "2020", "authors": ["Feng Pan", "Tianhe Ye", "Peng Sun", "Shan Gui", "Bo Liang", "Lingli Li", "Dandan Zheng", "Jiazheng Wang", "Richard L Hesketh", "Lian Yang", "Chuansheng Zheng"], "related_topics": ["Lung", "Respiratory distress", "Pneumonia", "Nuclear medicine", "Stage (cooking)", "Radiography", "Retrospective cohort study", "Quartile", "Medicine", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "3028749392", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3009885589", "3004318991", "3008827533", "3008028633", "3005079553", "3002108456"], "title": "A Collaborative Multidisciplinary Approach to the Management of Coronavirus Disease 2019 in the Hospital Setting.", "abstract": "The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19), which presents an unprecedented challenge to medical providers worldwide. Although most SARS-CoV-2-infected individuals manifest with a self-limited mild disease that resolves with supportive care in the outpatient setting, patients with moderate to severe COVID-19 will require a multidisciplinary collaborative management approach for optimal care in the hospital setting. Laboratory and radiologic studies provide critical information on disease severity, management options, and overall prognosis. Medical management is mostly supportive with antipyretics, hydration, oxygen supplementation, and other measures as dictated by clinical need. Among its medical complications is a characteristic proinflammatory cytokine storm often associated with end-organ dysfunction, including respiratory failure, liver and renal insufficiency, cardiac injury, and coagulopathy. Specific recommendations for the management of these medical complications are discussed. Despite the issuance of emergency use authorization for remdesivir, there are still no proven effective antiviral and immunomodulatory therapies, and their use in COVID-19 management should be guided by clinical trial protocols or treatment registries. The medical care of patients with COVID-19 extends beyond their hospitalization. Postdischarge follow-up and monitoring should be performed, preferably using telemedicine, until the patients have fully recovered from their illness and are released from home quarantine protocols.", "citation_count": "136", "reference_count": "15", "date": "2020", "authors": ["Raymund R. Razonable", "Kelly M. Pennington", "Anne M. Meehan", "John W. Wilson", "Adam T. Froemming", "Courtney E. Bennett", "Ariela L. Marshall", "Abinash Virk", "Eva M. Carmona"], "related_topics": ["Clinical trial", "Intensive care unit", "Telemedicine", "Emergency Use Authorization", "Intensive care medicine", "Respiratory failure", "Liver function tests", "ARDS", "Medicine", "Coagulopathy"]}
{"id": "3042098369", "references": ["3001118548", "3013893137", "3003668884", "3002539152", "3009885589", "3007497549", "3010604545", "3005079553", "3004280078", "3001195213"], "title": "Laboratory Tests for COVID-19: A Review of Peer-Reviewed Publications and Implications for Clinical Use", "abstract": "Diagnostic tests for the coronavirus infection 2019 (COVID-19) are critical for prompt diagnosis, treatment and isolation to break the cycle of transmission. A positive real-time reverse-transcriptase polymerase chain reaction (RT-PCR), in conjunction with clinical and epidemiologic data, is the current standard for diagnosis, but several challenges still exist. Serological assays help to understand epidemiology better and to evaluate vaccine responses but they are unreliable for diagnosis in the acute phase of illness or assuming protective immunity. Serology is gaining attention, mainly because of convalescent plasma gaining importance as treatment for clinically worsening COVID-19 patients. We provide a narrative review of peer-reviewed research studies on RT-PCR, serology and antigen immune-assays for COVID-19, briefly describe their lab methods and discuss their limitations for clinical practice.", "citation_count": "70", "reference_count": "11", "date": "2020", "authors": ["Daniel Shyu", "James Dorroh", "Caleb Holtmeyer", "Detlef Ritter", "Anandhi Upendran", "Raghuraman Kannan", "Dima Dandachi", "Christian Rojas-Moreno", "Stevan P Whitt", "Hariharan Regunath"], "related_topics": ["Serology", "Peer review", "Epidemiology", "Isolation (health care)", "Intensive care medicine", "MEDLINE", "Transmission (medicine)", "Medicine", "Pandemic", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "3006110666", "references": ["3004906315", "2112136274", "3001897055", "3005272159", "2056155046"], "title": "Chest CT for Typical Coronavirus Disease 2019 (COVID-19) Pneumonia: Relationship to Negative RT-PCR Testing.", "abstract": "Some patients with positive chest CT findings may present with negative results of real-time reverse-transcription polymerase chain reaction (RT-PCR) tests for coronavirus disease 2019 (COVID-19). In this study, the authors present chest CT findings from five patients with COVID-19 infection who had initial negative RT-PCR results. All five patients had typical imaging findings, including ground-glass opacity (five patients) and/or mixed ground-glass opacity and mixed consolidation (two patients). After isolation for presumed COVID-19 pneumonia, all patients were eventually confirmed to have COVID-19 infection by means of repeated swab tests. A combination of repeated swab tests and CT scanning may be helpful for individuals with a high clinical suspicion of COVID-19 infection but negative findings at RT-PCR screening.", "citation_count": "5", "reference_count": "1,522", "date": "2020", "authors": ["Xingzhi Xie", "Zheng Zhong", "Wei Zhao", "Chao Zheng", "Fei Wang", "Jun Liu"], "related_topics": ["False Negative Reactions", "Radiology", "Real-time polymerase chain reaction", "Medicine", "2019-20 coronavirus outbreak", "Chest ct", "Coronavirus disease 2019 (COVID-19)", "Radiographic image interpretation", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3002108456", "references": ["3001118548", "2999318660", "2999364275", "2775086803", "2166867592", "2999409984", "2991899552", "2909194930", "2903899730", "2132260239"], "title": "Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study", "abstract": "In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia. In this retrospective, single-centre study, we included all confirmed cases of 2019-nCoV in Wuhan Jinyintan Hospital from Jan 1 to Jan 20, 2020. Cases were confirmed by real-time RT-PCR and were analysed for epidemiological, demographic, clinical, and radiological features and laboratory data. Outcomes were followed up until Jan 25, 2020.", "citation_count": "28", "reference_count": "13,827", "date": "2020", "authors": ["Nanshan Chen", "Min Zhou", "Xuan Dong", "Jieming Qu", "Fengyun Gong", "Yang Han", "Yang Qiu", "Jingli Wang", "Ying Liu", "Yuan Wei", "Jia'an Xia", "Ting Yu", "Xinxin Zhang", "Li Zhang"], "related_topics": ["Pneumonia", "Epidemiology", "Coronavirus", "Retrospective cohort study", "Comorbidity", "Public health", "Pediatrics", "Medicine", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Tomography x ray computed"]}
{"id": "2022508996", "references": ["2156163116", "2130325614", "2310919327", "2143516773", "1423339008", "2546302380", "2110158442", "2031342017", "2169551590", "1999478155"], "title": "Learning Hierarchical Features for Scene Labeling", "abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.", "citation_count": "48", "reference_count": "2,778", "date": "2013", "authors": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "related_topics": ["Image segmentation", "Image texture", "Feature vector", "Feature extraction", "Contextual image classification", "Pixel", "Segmentation", "Deep learning", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2163922914", "references": ["2618530766", "2136922672", "2158899491", "2187089797", "2310919327", "3118608800", "2160815625", "2100495367", "2162915993", "1665214252"], "title": "Representation Learning: A Review and New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.", "citation_count": "237", "reference_count": "9,127", "date": "2013", "authors": ["Y. Bengio", "A. Courville", "P. Vincent"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Active learning (machine learning)", "Instance-based learning", "Feature learning", "Algorithmic learning theory", "Computational learning theory", "Competitive learning", "Multi-task learning", "Stability (learning theory)", "Inductive transfer", "Sequence learning", "Artificial neural network", "Deep learning", "Autoencoder", "Boltzmann machine", "Feature extraction", "Nonlinear dimensionality reduction", "Inference", "Probabilistic logic", "Machine learning", "Artificial intelligence", "Computer science", "Manifold"]}
{"id": "2160815625", "references": ["1533861849", "2145094598", "2136922672", "2147768505", "1994197834", "44815768", "2100495367", "1993882792", "2116064496", "1498436455"], "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.", "citation_count": "63", "reference_count": "10,041", "date": "2012", "authors": ["G. Hinton", "Li Deng", "Dong Yu", "G. E. Dahl", "A. Mohamed", "N. Jaitly", "Andrew Senior", "V. Vanhoucke", "P. Nguyen", "T. N. Sainath", "B. Kingsbury"], "related_topics": ["Acoustic model", "Time delay neural network", "FMLLR", "Hidden Markov model", "Artificial neural network", "Mixture model", "Speech recognition", "Margin (machine learning)", "Frame (networking)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "1993882792", "references": ["2136922672", "2103359087", "2147768505", "1994197834", "2913932916", "44815768", "2159080219", "3118608800", "2100495367", "2116064496"], "title": "Acoustic Modeling Using Deep Belief Networks", "abstract": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.", "citation_count": "41", "reference_count": "1,900", "date": "2012", "authors": ["A. Mohamed", "G. E. Dahl", "G. Hinton"], "related_topics": ["Generative model", "Hidden Markov model", "Deep belief network", "Discriminative model", "Mixture model", "Artificial neural network", "Feature vector", "TIMIT", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "3001897055", "references": ["2104548316", "3027518954", "2257005270", "2166867592", "2955025503", "2306794997", "2792024998", "2132260239", "2903899730", "1909499787"], "title": "A Novel Coronavirus from Patients with Pneumonia in China, 2019.", "abstract": "In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. (Funded by the National Key Research and Development Program of China and the National Major Project for Control and Prevention of Infectious Disease in China.).", "citation_count": "12", "reference_count": "15,426", "date": "2020", "authors": ["Na Zhu", "Dingyu Zhang", "Wenling Wang", "Xingwang Li", "Bo Yang", "Jingdong Song", "Xiang Zhao", "Baoying Huang", "Weifeng Shi", "Roujian Lu", "Peihua Niu", "Faxian Zhan", "Xuejun Ma", "Dayan Wang", "Wenbo Xu", "Guizhen Wu", "George F. Gao", "Wenjie Tan"], "related_topics": ["Coronavirus", "Betacoronavirus", "Infectious disease (medical specialty)", "Clade", "Viral Epidemiology", "Subfamily", "Virology", "Subgenus", "China", "Medicine"]}
{"id": "3003668884", "references": ["3001897055", "3002715510", "3002539152", "3002533507", "2470646526", "3001971765", "2149508011", "3000834295", "1909499787", "2147166346"], "title": "Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.", "abstract": "Abstract Background The initial cases of novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, Hubei Province, China, in December 2019 and January 2020. We analyzed data on the...", "citation_count": "18", "reference_count": "11,868", "date": "2020", "authors": ["Qun Li", "Xuhua Guan", "Peng Wu", "Xiaoye Wang", "Lei Zhou", "Yeqing Tong", "Ruiqi Ren", "Kathy S.M. Leung", "Eric H.Y. Lau", "Jessica Y. Wong", "Xuesen Xing", "Nijuan Xiang", "Yang Wu", "Chao Li", "Qi Chen", "Dan Li", "Tian Liu", "Jing Zhao", "Man Liu", "Wenxiao Tu", "Chuding Chen", "Lianmei Jin", "Rui Yang", "Qi Wang", "Suhua Zhou", "Rui Wang", "Hui Liu", "Yingbo Luo", "Yuan Liu", "Ge Shao", "Huan Li", "Zhongfa Tao", "Yang Yang", "Zhiqiang Deng", "Boxi Liu", "Zhitao Ma", "Yanping Zhang", "Guoqing Shi", "Tommy T.Y. Lam", "Joseph T. Wu", "George F. Gao", "Benjamin J. Cowling", "Bo Yang", "Gabriel M. Leung", "Zijian Feng"], "related_topics": ["Coronavirus", "Pneumonia", "Betacoronavirus", "Outbreak", "Transmission (mechanics)", "Virology", "China", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3002539152", "references": ["2129542667", "2105637133", "2103503670", "2140338292", "2769543984", "2170933940", "2115555188", "2889758689", "2025170735", "2807736175"], "title": "A familial cluster of pneumonia associated with the 2019 novel coronavirus indicating person-to-person transmission: a study of a family cluster.", "abstract": "Summary  Background  An ongoing outbreak of pneumonia associated with a novel coronavirus was reported in Wuhan city, Hubei province, China. Affected patients were geographically linked with a local wet market as a potential source. No data on person-to-person or nosocomial transmission have been published to date.  Methods  In this study, we report the epidemiological, clinical, laboratory, radiological, and microbiological findings of five patients in a family cluster who presented with unexplained pneumonia after returning to Shenzhen, Guangdong province, China, after a visit to Wuhan, and an additional family member who did not travel to Wuhan. Phylogenetic analysis of genetic sequences from these patients were done.  Findings  From Jan 10, 2020, we enrolled a family of six patients who travelled to Wuhan from Shenzhen between Dec 29, 2019 and Jan 4, 2020. Of six family members who travelled to Wuhan, five were identified as infected with the novel coronavirus. Additionally, one family member, who did not travel to Wuhan, became infected with the virus after several days of contact with four of the family members. None of the family members had contacts with Wuhan markets or animals, although two had visited a Wuhan hospital. Five family members (aged 36\u201366 years) presented with fever, upper or lower respiratory tract symptoms, or diarrhoea, or a combination of these 3\u20136 days after exposure. They presented to our hospital (The University of Hong Kong-Shenzhen Hospital, Shenzhen) 6\u201310 days after symptom onset. They and one asymptomatic child (aged 10 years) had radiological ground-glass lung opacities. Older patients (aged &gt;60 years) had more systemic symptoms, extensive radiological ground-glass lung changes, lymphopenia, thrombocytopenia, and increased C-reactive protein and lactate dehydrogenase levels. The nasopharyngeal or throat swabs of these six patients were negative for known respiratory microbes by point-of-care multiplex RT-PCR, but five patients (four adults and the child) were RT-PCR positive for genes encoding the internal RNA-dependent RNA polymerase and surface Spike protein of this novel coronavirus, which were confirmed by Sanger sequencing. Phylogenetic analysis of these five patients' RT-PCR amplicons and two full genomes by next-generation sequencing showed that this is a novel coronavirus, which is closest to the bat severe acute respiatory syndrome (SARS)-related coronaviruses found in Chinese horseshoe bats.  Interpretation  Our findings are consistent with person-to-person transmission of this novel coronavirus in hospital and family settings, and the reports of infected travellers in other geographical regions.  Funding  The Shaw Foundation Hong Kong, Michael Seak-Kan Tong, Respiratory Viral Research Foundation Limited, Hui Ming, Hui Hoy and Chow Sin Lan Charity Fund Limited, Marina Man-Wai Lee, the Hong Kong Hainan Commercial Association South China Microbiology Research Fund, Sanming Project of Medicine (Shenzhen), and High Level-Hospital Program (Guangdong Health Commission).", "citation_count": "31", "reference_count": "6,982", "date": "2020", "authors": ["Jasper Fuk Woo Chan", "Shuofeng Yuan", "Kin Hang Kok", "Kelvin Kai Wang To", "Hin Chu", "Jin Yang", "Fanfan Xing", "Jieling Liu", "Cyril Chik Yan Yip", "Rosana Wing Shan Poon", "Hoi Wah Tsoi", "Simon Kam Fai Lo", "Kwok Hung Chan", "Vincent Kwok Man Poon", "Wan Mui Chan", "Jonathan Daniel Ip", "Jian Piao Cai", "Vincent Chi Chung Cheng", "Honglin Chen", "Christopher Kim Ming Hui", "Kwok Yung Yuen"], "related_topics": ["Coronavirus", "Betacoronavirus", "Epidemiology", "Disease cluster", "Pneumonia", "Outbreak", "Asymptomatic", "Internal medicine", "China", "Medicine"]}
{"id": "3003573988", "references": ["3003668884", "2104595316", "3002533591", "3004397688", "2069251911", "1815575713", "2096145431", "2147166346", "1968393246", "3002764620"], "title": "Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study.", "abstract": "Summary  Background  Since Dec 31, 2019, the Chinese city of Wuhan has reported an outbreak of atypical pneumonia caused by the 2019 novel coronavirus (2019-nCoV). Cases have been exported to other Chinese cities, as well as internationally, threatening to trigger a global outbreak. Here, we provide an estimate of the size of the epidemic in Wuhan on the basis of the number of cases exported from Wuhan to cities outside mainland China and forecast the extent of the domestic and global public health risks of epidemics, accounting for social and non-pharmaceutical prevention interventions.  Methods  We used data from Dec 31, 2019, to Jan 28, 2020, on the number of cases exported from Wuhan internationally (known days of symptom onset from Dec 25, 2019, to Jan 19, 2020) to infer the number of infections in Wuhan from Dec 1, 2019, to Jan 25, 2020. Cases exported domestically were then estimated. We forecasted the national and global spread of 2019-nCoV, accounting for the effect of the metropolitan-wide quarantine of Wuhan and surrounding cities, which began Jan 23\u201324, 2020. We used data on monthly flight bookings from the Official Aviation Guide and data on human mobility across more than 300 prefecture-level cities in mainland China from the Tencent database. Data on confirmed cases were obtained from the reports published by the Chinese Center for Disease Control and Prevention. Serial interval estimates were based on previous studies of severe acute respiratory syndrome coronavirus (SARS-CoV). A susceptible-exposed-infectious-recovered metapopulation model was used to simulate the epidemics across all major cities in China. The basic reproductive number was estimated using Markov Chain Monte Carlo methods and presented using the resulting posterior mean and 95% credibile interval (CrI).  Findings  In our baseline scenario, we estimated that the basic reproductive number for 2019-nCoV was 2\u00b768 (95% CrI 2\u00b747\u20132\u00b786) and that 75\u2008815 individuals (95% CrI 37\u2008304\u2013130\u2008330) have been infected in Wuhan as of Jan 25, 2020. The epidemic doubling time was 6\u00b74 days (95% CrI 5\u00b78\u20137\u00b71). We estimated that in the baseline scenario, Chongqing, Beijing, Shanghai, Guangzhou, and Shenzhen had imported 461 (95% CrI 227\u2013805), 113 (57\u2013193), 98 (49\u2013168), 111 (56\u2013191), and 80 (40\u2013139) infections from Wuhan, respectively. If the transmissibility of 2019-nCoV were similar everywhere domestically and over time, we inferred that epidemics are already growing exponentially in multiple major cities of China with a lag time behind the Wuhan outbreak of about 1\u20132 weeks.  Interpretation  Given that 2019-nCoV is no longer contained within Wuhan, other major Chinese cities are probably sustaining localised outbreaks. Large cities overseas with close transport links to China could also become outbreak epicentres, unless substantial public health interventions at both the population and personal levels are implemented immediately. Independent self-sustaining outbreaks in major cities globally could become inevitable because of substantial exportation of presymptomatic cases and in the absence of large-scale public health interventions. Preparedness plans and mitigation interventions should be readied for quick deployment globally.  Funding  Health and Medical Research Fund (Hong Kong, China).", "citation_count": "44", "reference_count": "3,286", "date": "2020", "authors": ["Joseph T Wu", "Kathy Leung", "Gabriel M Leung"], "related_topics": ["Population", "Mainland China", "China", "Beijing", "Outbreak", "Serial interval", "Public health", "Preparedness", "Socioeconomics", "Geography"]}
{"id": "3004318991", "references": ["3001118548", "2103441770", "3001897055", "3002539152", "2166867592", "3017468735", "2141052558", "2306794997", "2804822363", "3004280078"], "title": "Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding.", "abstract": "Summary  Background  In late December, 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. A novel coronavirus was subsequently identified as the causative pathogen, provisionally named 2019 novel coronavirus (2019-nCoV). As of Jan 26, 2020, more than 2000 cases of 2019-nCoV infection have been confirmed, most of which involved people living in or visiting Wuhan, and human-to-human transmission has been confirmed.  Methods  We did next-generation sequencing of samples from bronchoalveolar lavage fluid and cultured isolates from nine inpatients, eight of whom had visited the Huanan seafood market in Wuhan. Complete and partial 2019-nCoV genome sequences were obtained from these individuals. Viral contigs were connected using Sanger sequencing to obtain the full-length genomes, with the terminal regions determined by rapid amplification of cDNA ends. Phylogenetic analysis of these 2019-nCoV genomes and those of other coronaviruses was used to determine the evolutionary history of the virus and help infer its likely origin. Homology modelling was done to explore the likely receptor-binding properties of the virus.  Findings  The ten genome sequences of 2019-nCoV obtained from the nine patients were extremely similar, exhibiting more than 99\u00b798% sequence identity. Notably, 2019-nCoV was closely related (with 88% identity) to two bat-derived severe acute respiratory syndrome (SARS)-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, eastern China, but were more distant from SARS-CoV (about 79%) and MERS-CoV (about 50%). Phylogenetic analysis revealed that 2019-nCoV fell within the subgenus Sarbecovirus of the genus Betacoronavirus, with a relatively long branch length to its closest relatives bat-SL-CoVZC45 and bat-SL-CoVZXC21, and was genetically distinct from SARS-CoV. Notably, homology modelling revealed that 2019-nCoV had a similar receptor-binding domain structure to that of SARS-CoV, despite amino acid variation at some key residues.  Interpretation  2019-nCoV is sufficiently divergent from SARS-CoV to be considered a new human-infecting betacoronavirus. Although our phylogenetic analysis suggests that bats might be the original host of this virus, an animal sold at the seafood market in Wuhan might represent an intermediate host facilitating the emergence of the virus in humans. Importantly, structural analysis suggests that 2019-nCoV might be able to bind to the angiotensin-converting enzyme 2 receptor in humans. The future evolution, adaptation, and spread of this virus warrant urgent investigation.  Funding  National Key Research and Development Program of China, National Major Project for Control and Prevention of Infectious Disease in China, Chinese Academy of Sciences, Shandong First Medical University.", "citation_count": "36", "reference_count": "7,834", "date": "2020", "authors": ["Roujian Lu", "Xiang Zhao", "Juan Li", "Peihua Niu", "Bo Yang", "Honglong Wu", "Wenling Wang", "Hao Song", "Baoying Huang", "Na Zhu", "Yuhai Bi", "Xuejun Ma", "Faxian Zhan", "Liang Wang", "Tao Hu", "Hong Zhou", "Zhenhong Hu", "Weimin Zhou", "Li Zhao", "Jing Chen", "Yao Meng", "Ji Wang", "Yang Lin", "Jianying Yuan", "Zhihao Xie", "Jinmin Ma", "William J Liu", "Dayan Wang", "Wenbo Xu", "Edward C Holmes", "George F Gao", "Guizhen Wu", "Weijun Chen", "Weifeng Shi", "Wenjie Tan"], "related_topics": ["Betacoronavirus", "Coronavirus", "Phylogenetics", "Sanger sequencing", "Genome", "Phylogenetic tree", "Virus", "Viral pneumonia", "Genetics", "Biology"]}
{"id": "3003465021", "references": ["3001118548", "3001897055", "2991491848", "3002539152", "3000413850", "3003951199", "2605343262"], "title": "First Case of 2019 Novel Coronavirus in the United States.", "abstract": "An outbreak of novel coronavirus (2019-nCoV) that began in Wuhan, China, has spread rapidly, with cases now confirmed in multiple countries. We report the first case of 2019-nCoV infection confirmed in the United States and describe the identification, diagnosis, clinical course, and management of the case, including the patient's initial mild symptoms at presentation with progression to pneumonia on day 9 of illness. This case highlights the importance of close coordination between clinicians and public health authorities at the local, state, and federal levels, as well as the need for rapid dissemination of clinical information related to the care of patients with this emerging infection.", "citation_count": "7", "reference_count": "4,801", "date": "2020", "authors": ["Michelle L Holshue", "Chas DeBolt", "Scott Lindquist", "Kathy H Lofy", "John Wiesman", "Hollianne Bruce", "Christopher Spitters", "Keith Ericson", "Sara Wilkerson", "Ahmet Tural", "George Diaz", "Amanda Cohn", "LeAnne Fox", "Anita Patel", "Susan I Gerber", "Lindsay Kim", "Suxiang Tong", "Xiaoyan Lu", "Steve Lindstrom", "Mark A Pallansch", "William C Weldon", "Holly M Biggs", "Timothy M Uyeki", "Satish K Pillai"], "related_topics": ["Public health", "Coronavirus", "Pneumonia", "Outbreak", "Intensive care medicine", "MEDLINE", "Medicine", "Clinical course", "Clinical information", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3005079553", "references": ["3001118548", "2999318660", "3001897055", "3003668884", "3002539152", "3003951199", "2999409984", "1803784511", "3000834295", "3002108456"], "title": "Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China.", "abstract": "Importance  In December 2019, novel coronavirus (2019-nCoV)\u2013infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited.  Objective  To describe the epidemiological and clinical characteristics of NCIP.  Design, Setting, and Participants  Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020.  Exposures  Documented NCIP.  Main Outcomes and Measures  Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked.  Results  Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8\u2009\u00d7\u2009109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n\u2009=\u200936), compared with patients not treated in the ICU (n\u2009=\u2009102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n\u2009=\u200947), the median hospital stay was 10 days (IQR, 7.0-14.0).  Conclusions and Relevance  In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.", "citation_count": "23", "reference_count": "15,476", "date": "2020", "authors": ["Dawei Wang", "Bo Hu", "Chang Hu", "Fangfang Zhu", "Xing Liu", "Jing Zhang", "Binbin Wang", "Hui Xiang", "Zhenshun Cheng", "Yong Xiong", "Yan Zhao", "Yirong Li", "Xinghuan Wang", "Zhiyong Peng"], "related_topics": ["Interquartile range", "Intensive care unit", "Pneumonia", "Epidemiology", "Retrospective cohort study", "ARDS", "Oxygen therapy", "Extracorporeal membrane oxygenation", "Internal medicine", "Medicine"]}
{"id": "3004239190", "references": ["3001388158", "3001897055"], "title": "Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany.", "abstract": "2019-nCoV Transmission from Asymptomatic Patient In this report, investigators in Germany detected the spread of the novel coronavirus (2019-nCoV) from a person who had recently traveled from China...", "citation_count": "2", "reference_count": "3,539", "date": "2020", "authors": ["Camilla Rothe", "Mirjam Schunk", "Peter Sothmann", "Gisela Bretzel", "Guenter Froeschl", "Claudia Wallrauch", "Thorbj\u00f6rn Zimmer", "Verena Thiel", "Christian Janke", "Wolfgang Guggemos", "Michael Seilmaier", "Christian Drosten", "Patrick Vollmar", "Katrin Zwirglmaier", "Sabine Zange", "Roman W\u00f6lfel", "Michael Hoelscher"], "related_topics": ["Asymptomatic Diseases", "Asymptomatic", "Transmission (mechanics)", "Coronavirus", "Betacoronavirus", "Pneumonia", "Pediatrics", "Medicine", "2019-20 coronavirus outbreak", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3004906315", "references": ["2112136274", "2279340859", "2102634410", "2162899218", "2800783955", "2056155046", "2080286891"], "title": "CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV).", "abstract": "In this retrospective case series, chest CT scans of 21 symptomatic patients from China infected with the 2019 novel coronavirus (2019-nCoV) were reviewed, with emphasis on identifying and characterizing the most common findings. Typical CT findings included bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. Notably, lung cavitation, discrete pulmonary nodules, pleural effusions, and lymphadenopathy were absent. Follow-up imaging in a subset of patients during the study time window often demonstrated mild or moderate progression of disease, as manifested by increasing extent and density of lung opacities.", "citation_count": "7", "reference_count": "1,879", "date": "2020", "authors": ["Michael Chung", "Adam Bernheim", "Xueyan Mei", "Ning Zhang", "Mingqian Huang", "Xianjun Zeng", "Jiufa Cui", "Wenjian Xu", "Yang Yang", "Zahi A. Fayad", "Adam Jacobi", "Kunwei Li", "Shaolin Li", "Hong Shan"], "related_topics": ["Lung", "Pneumonia", "Radiology", "Retrospective cohort study", "Tomography", "Parenchyma", "Medicine", "Ct imaging", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Time windows"]}
{"id": "3004511262", "references": ["3009992310", "3008627141", "3012211282", "3008801544", "3014051579", "3025334942", "3007497549", "3034593359", "3013468450", "3008928918"], "title": "Evolution of CT Manifestations in a Patient Recovered from 2019 Novel Coronavirus (2019-nCoV) Pneumonia in Wuhan, China.", "abstract": "", "citation_count": "0", "reference_count": "193", "date": "2020", "authors": ["Heshui Shi", "Xiaoyu Han", "Chuansheng Zheng"], "related_topics": ["Pneumonia", "Medicine", "Virology", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Lung pathology", "Patient discharge", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Tomography x ray computed"]}
{"id": "3003901880", "references": ["3002533507"], "title": "CT Imaging of the 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "", "citation_count": "1", "reference_count": "479", "date": "2020", "authors": ["Junqiang Lei", "Junfeng Li", "Xun Li", "Xiaolong Qi"], "related_topics": ["Pneumonia", "Respiratory sounds", "Tomography", "Medicine", "Radiology", "2019-20 coronavirus outbreak", "Ct imaging", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Tomography x ray computed", "X ray computed"]}
{"id": "3008818676", "references": ["3033301213", "3037451072", "3021916232", "3037552531", "3031029566", "3037851904", "3034593359", "3033453353", "3035018050"], "title": "The epidemiological characteristics of an outbreak of 2019 novel coronavirus diseases (COVID-19) in China", "abstract": "Objective\r\nAn outbreak of 2019 novel coronavirus diseases (COVID-19) in Wuhan, China has spread quickly nationwide. Here, we report results of a descriptive, exploratory analysis of all cases diagnosed as of February 11, 2020.\r\n\r\n\r\nMethods\r\nAll COVID-19 cases reported through February 11, 2020 were extracted from China\u2019s Infectious Disease Information System. Analyses included: 1) summary of patient characteristics; 2) examination of age distributions and sex ratios; 3) calculation of case fatality and mortality rates; 4) geo-temporal analysis of viral spread; 5) epidemiological curve construction; and 6) subgroup analysis.\r\n\r\n\r\nResults\r\nA total of 72 314 patient records-44 672 (61.8%) confirmed cases, 16 186 (22.4%) suspected cases, 10567 (14.6%) clinical diagnosed cases (Hubei only), and 889 asymptomatic cases (1.2%)-contributed data for the analysis. Among confirmed cases, most were aged 30-79 years (86.6%), diagnosed in Hubei (74.7%), and considered mild/mild pneumonia (80.9%). A total of 1 023 deaths occurred among confirmed cases for an overall case-fatality rate of 2.3%. The COVID-19 spread outward from Hubei sometime after December 2019 and by February 11, 2020, 1 386 counties across all 31 provinces were affected. The epidemic curve of onset of symptoms peaked in January 23-26, then began to decline leading up to February 11. A total of 1 716 health workers have become infected and 5 have died (0.3%).\r\n\r\n\r\nConclusions\r\nThe COVID-19 epidemic has spread very quickly. It only took 30 days to expand from Hubei to the rest of Mainland China. With many people returning from a long holiday, China needs to prepare for the possible rebound of the epidemic.\r\n\r\n\r\nKey words: \r\n2019 Novel Coronavirus;\u00a0Outbreak;\u00a0Epidemiological characteristics", "citation_count": "9", "reference_count": "2,805", "date": "2020", "authors": ["Novel Coronavirus Pneumonia Emergency Response Epidemiology Team"], "related_topics": ["Case fatality rate", "Outbreak", "Mortality rate", "Epidemiology", "Asymptomatic", "Subgroup analysis", "Mainland China", "Pediatrics", "Infectious disease (medical specialty)", "Medicine"]}
{"id": "3005656138", "references": ["3004668429", "3001897055"], "title": "Use of Chest CT in Combination with Negative RT-PCR Assay for the 2019 Novel Coronavirus but High Clinical Suspicion.", "abstract": "", "citation_count": "2", "reference_count": "363", "date": "2020", "authors": ["Peikai Huang", "Tianzhu Liu", "Lesheng Huang", "Hailong Liu", "Ming Lei", "Wangdong Xu", "Xiaolu Hu", "Jun Chen", "Bo Liu"], "related_topics": ["Real-time polymerase chain reaction", "Pneumonia", "False Negative Reactions", "Medicine", "Pathology", "2019-20 coronavirus outbreak", "Chest ct", "Disease progression", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "X ray computed"]}
{"id": "3006354146", "references": ["3001118548", "3001456238", "3001465255", "3002539152", "3017468735"], "title": "Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China.", "abstract": "The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia. Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient's chest CT image were recorded. Additionally, we performed imaging follow-up of these patients. CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 \u00b1 15.2 years. The mean number of affected lobes was 3.3 \u00b1 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged. Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately. \u2022 High-resolution CT (HRCT) of the chest is critical for early detection, evaluation of disease severity and follow-up of patients with the novel coronavirus pneumonia. \u2022 The manifestations of the novel coronavirus pneumonia are diverse and change rapidly. \u2022 Radiologists should be aware of the various features of the disease and temporal changes.", "citation_count": "5", "reference_count": "710", "date": "2020", "authors": ["Yueying Pan", "Hanxiong Guan", "Shuchang Zhou", "Yujin Wang", "Qian Li", "Tingting Zhu", "Qiongjie Hu", "Liming Xia"], "related_topics": ["Viral pneumonia", "Neuroradiology", "Radiology", "Interventional radiology", "Ultrasound", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Ct findings", "In patient", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3033952286", "references": ["2084576921", "2073600962", "1991420168", "2770752141", "3011969828", "2175815746", "2105275554", "3010604545", "2263084061", "3001195213"], "title": "Real-time reverse transcription loop-mediated isothermal amplification for rapid detection of SARS-CoV-2", "abstract": "Background  Highly sensitive real-time reverse transcription polymerase chain reaction (RT-qPCR) methods have been developed for the detection of SARS-CoV-2. However, they are costly. Loop-mediated isothermal amplification (LAMP) assay has emerged as a novel alternative isothermal amplification method for the detection of nucleic acid.  Methods  A rapid, sensitive and specific real-time reverse transcription LAMP (RT-LAMP) assay was developed for SARS-CoV-2 detection.  Results  This assay detected one copy/reaction of SARS-CoV-2 RNA in 30 min. Both the clinical sensitivity and specificity of this assay were 100%. The RT-LAMP showed comparable performance with RT-qPCR. Combining simplicity and cost-effectiveness, this assay is therefore recommended for use in resource resource-limited settings.", "citation_count": "14", "reference_count": "10", "date": "2020", "authors": ["Yee Ling Lau", "Ilyiana Ismail", "Nur Izati Mustapa", "Meng Yee Lai", "Tuan Suhaila Tuan Soh", "Afifah Hassan", "Kalaiarasu M Peariasamy", "Yee Leng Lee", "Yoong Min Chong", "I-Ching Sam", "Pik Pin Goh"], "related_topics": ["Reverse Transcription Loop-mediated Isothermal Amplification", "Loop-mediated isothermal amplification", "Reverse transcription polymerase chain reaction", "Reverse transcriptase", "Nucleic acid", "RNA", "Molecular biology", "Chemistry", "Rapid detection", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3036958556", "references": ["3001118548", "3002539152", "3011242477", "3007940623", "3008090866", "3008827533", "3003465021", "3010604545", "3005079553", "3002108456"], "title": "Involvement of digestive system in COVID-19: manifestations, pathology, management and challenges", "abstract": "The pandemic of novel coronavirus disease (COVID-19) has developed as a tremendous threat to global health. Although most COVID-19 patients present with respiratory symptoms, some present with gastrointestinal (GI) symptoms like diarrhoea, loss of appetite, nausea/vomiting and abdominal pain as the major complaints. These features may be attributable to the following facts: (a) COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and its receptor angiotensin converting enzyme 2 (ACE2) was found to be highly expressed in GI epithelial cells, providing a prerequisite for SARS-CoV-2 infection; (b) SARS-CoV-2 viral RNA has been found in stool specimens of infected patients, and 20% of patients showed prolonged presence of SARS-CoV-2 RNA in faecal samples after the virus converting to negative in the respiratory system. These findings suggest that SARS-CoV-2 may be able to actively infect and replicate in the GI tract. Moreover, GI infection could be the first manifestation antedating respiratory symptoms; patients suffering only digestive symptoms but no respiratory symptoms as clinical manifestation have also been reported. Thus, the implications of digestive symptoms in patients with COVID-19 is of great importance. In this review, we summarise recent findings on the epidemiology of GI tract involvement, potential mechanisms of faecal-oral transmission, GI and liver manifestation, pathological/histological features in patients with COVID-19 and the diagnosis, management of patients with pre-existing GI and liver diseases as well as precautions for preventing SARS-CoV-2 infection during GI endoscopy procedures.", "citation_count": "47", "reference_count": "20", "date": "2020", "authors": ["Song Su", "Jun Shen", "Liangru Zhu", "Yun Qiu", "Jin-Shen He", "Jin-Yu Tan", "Marietta Iacucci", "Siew C Ng", "Subrata Ghosh", "Ren Mao", "Jie Liang"], "related_topics": ["Coronavirus", "Vomiting", "Disease", "Abdominal pain", "Inflammatory bowel disease", "Nausea", "Transmission (medicine)", "Respiratory system", "Gastroenterology", "Medicine", "Internal medicine"]}
{"id": "3035464429", "references": ["3018724240", "3018334611", "2158121945", "2103503670", "3015704123", "3010449299", "2132260239", "3010604545", "3015636815", "3030968929"], "title": "Sampling and detection of corona viruses in air: A mini review.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a strain of coronaviruses that causes coronavirus disease 2019 (COVID-19). In these days, the spread of the SARS-CoV-2 virus through the air has become a controversial topic among scientists. Various organizations provide standard methods for monitoring biological agents in the air. Nevertheless, there has been no standard recommended method for sampling and determination of viruses in air. This manuscript aimed at reviewing published papers for sampling and detection of corona viruses, especially SARS-Cov-2 as a global health concern. It was found that SARS-Cov 2 was present in some air samples that were collected from patient's rooms in hospitals. This result warrants its airborne transmission potential. However, due to the fact that in the most reviewed studies, sampling was performed in the patient's room, it seems difficult to discriminate whether it is airborne or is transmitted through respiratory droplets. Moreover, some other disrupting factors such as patient distance from the sampler, using protective or oxygen masks by patients, patient activities, coughing and sneezing during sampling time, air movement, air conditioning, sampler type, sampling conditions, storage and transferring conditions, can affect the results. About the sampling methods, most of the used samplers such as PTFE filters, gelatin filers and cyclones showed suitable performance for trapping SARS-Co and MERS-Cov viruses followed by PCR analysis.", "citation_count": "41", "reference_count": "22", "date": "2020", "authors": ["Ali Reza Rahmani", "Mostafa Leili", "Ghasem Azarian", "Ali Poormohammadi"], "related_topics": ["Airborne transmission", "Sampling (statistics)", "Air conditioning", "Emergency medicine", "Environmental science", "Air movement", "Coronavirus disease 2019 (COVID-19)", "Mini review", "Pcr analysis", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3008962515", "references": ["3001118548", "2786098272", "3001897055", "3003668884", "2021442163", "3028321619", "2769543984", "3025232310", "3027541845", "3004280078"], "title": "Molecular and serological investigation of 2019-nCoV infected patients: implication of multiple shedding routes.", "abstract": "In December 2019, a novel coronavirus (2019-nCoV) caused an outbreak in Wuhan, China, and soon spread to other parts of the world. It was believed that 2019-nCoV was transmitted through respiratory tract and then induced pneumonia, thus molecular diagnosis based on oral swabs was used for confirmation of this disease. Likewise, patient will be released upon two times of negative detection from oral swabs. However, many coronaviruses can also be transmitted through oral-fecal route by infecting intestines. Whether 2019-nCoV infected patients also carry virus in other organs like intestine need to be tested. We conducted investigation on patients in a local hospital who were infected with this virus. We found the presence of 2019-nCoV in anal swabs and blood as well, and more anal swab positives than oral swab positives in a later stage of infection, suggesting shedding and thereby transmitted through oral-fecal route. We also showed serology test can improve detection positive rate thus should be used in future epidemiology. Our report provides a cautionary warning that 2019-nCoV may be shed through multiple routes.", "citation_count": "13", "reference_count": "1,382", "date": "2020", "authors": ["Wei Zhang", "Rong-Hui Du", "Bei Li", "Xiao-Shuang Zheng", "Xing-Lou Yang", "Ben Hu", "Yan-Yi Wang", "Geng-Fu Xiao", "Bing Yan", "Zheng-Li Shi", "Peng Zhou"], "related_topics": ["Viral shedding", "Coronavirus", "Serology", "Outbreak", "Pneumonia (non-human)", "Virus", "Disease", "Epidemiology", "Virology", "Medicine"]}
{"id": "3034059415", "references": ["3013967887", "3015571324", "3006642361", "3013594674", "3008443627", "3003668884", "3009885589", "3012789146", "3004397688", "3010604545"], "title": "Impact of Lockdown on the Epidemic Dynamics of COVID-19 in France", "abstract": "The COVID-19 epidemic was reported in the Hubei province in China in December 2019 and then spread around the world reaching the pandemic stage at the beginning of March 2020. Since then, several countries went into lockdown. Using a mechanistic-statistical formalism, we estimate the effect of the lockdown in France on the contact rate and the effective reproduction number Re of the COVID-19. We obtain a reduction by a factor 7 (Re=0.47, 95%-CI: 0.45-0.50), compared to the estimates carried out in France at the early stage of the epidemic. We also estimate the fraction of the population that would be infected by the beginning of May, at the official date at which the lockdown should be relaxed. We find a fraction of 3.7% (95%-CI: 3.0-4.8%) of the total French population, without taking into account the number of recovered individuals before April 1st, which is not known. This proportion is seemingly too low to reach herd immunity. Thus, even if the lockdown strongly mitigated the first epidemic wave, keeping a low value of Re is crucial to avoid an uncontrolled second wave (initiated with much more infectious cases than the first wave) and to hence avoid the saturation of hospital facilities.", "citation_count": "27", "reference_count": "37", "date": "2020", "authors": ["Lionel Roques", "Etienne K. Klein", "Julien Papa\u00efx", "Antoine Sar", "Samuel Soubeyrand"], "related_topics": ["Population", "Epidemic model", "Herd immunity", "Pandemic", "Demography", "Geography", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Epidemic dynamics", "Formalism (philosophy of mathematics)"]}
{"id": "3008452791", "references": ["3004318991", "2129542667", "3003637715"], "title": "Viral load of SARS-CoV-2 in clinical samples.", "abstract": "", "citation_count": "3", "reference_count": "850", "date": "2020", "authors": ["Yang Pan", "Daitao Zhang", "Peng Yang", "Leo L M Poon", "Quanyi Wang"], "related_topics": ["Feces analysis", "Viral load", "Pneumonia", "Pandemic", "Virology", "Real-time polymerase chain reaction", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3033453353", "references": ["3001118548", "3001897055", "3003668884", "3004239190", "3009885589", "3004318991", "3003465021", "3005079553", "3004280078", "3002108456"], "title": "Recent Understandings Toward Coronavirus Disease 2019 (COVID-19): From Bench to Bedside", "abstract": "In late December 2019, an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by SARS coronavirus 2 (SARS-CoV-2) (previously named 2019-nCoV) in Wuhan became the most challenging health emergency. Since its rapid spread in China and many other countries, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern (PHEIC) on 30th January 2020 and a pandemic on 11th March 2020. Thousands of people have died, and there are currently no vaccines or specific antiviral drugs for COVID-19. Therefore, it is critical to have a comprehensive understanding of the virus. In this review, we highlight the etiology, epidemiology, pathogenesis and pathology, clinical characteristics, diagnosis, clinical management, prognosis, infection control and prevention of COVID-19 based on recent studies.", "citation_count": "117", "reference_count": "13", "date": "2020", "authors": ["Jie Yu", "Peiwei Chai", "Shengfang Ge", "Xianqun Fan"], "related_topics": ["Pandemic", "Public health", "Epidemiology", "Outbreak", "Infection control", "Etiology", "Intensive care medicine", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3034408674", "references": ["3130405932", "3144171767", "3006961006", "3010604545"], "title": "Risk assessment of mixed and displacement ventilation (LAF) during orthopedic and trauma surgery on COVID-19 patients with increased release of infectious aerosols", "abstract": "No abstract available\r\nKeywords: Displacement ventilation; SARS-CoV-2 spread; laminar air flow; mixed ventilation; negative pressure; no ventilation.", "citation_count": "0", "reference_count": "4", "date": "2020", "authors": ["Axel Kramer", "R\u00fcdiger K\u00fclpmann", "Arnold Brunner", "Michael M\u00fcller", "Georgi Wassilew"], "related_topics": ["Ventilation (architecture)", "Displacement ventilation", "Trauma surgery", "Orthopedic surgery", "Anesthesia", "Risk assessment", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3035275617", "references": ["3013893137", "3004824173", "3009834387", "3011863580", "3010096538", "3003465021", "3006846061", "3010604545", "3004280078", "3003464757"], "title": "Recreational waters - A potential transmission route for SARS-CoV-2 to humans?", "abstract": "Coronavirus disease 2019 (COVID-19), the respiratory illness caused by the novel virus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has lead to high morbidity and mortality rates worldwide, has been causing major public health concerns since first detected in late 2019. Following identification of novel pathogens, questions in relation to dissemination of the pathogen and transmission routes begin to emerge. This rapidly spreading SARS-CoV-2 virus has been detected in both faecal and wastewater samples across the globe, highlighting the potential for faecal-oral transmission of the virus. As a result, concerns regarding the transmission of the virus in the environment and the risk associated with contracting the virus in recreational waters, particularly where inadequately treated wastewater is discharged, have been emerging in recent weeks. This paper highlights the need for further research to be carried out to investigate the presence, infectivity and viability of this newly identified SARS-CoV-2 virus in wastewater effluent and receiving recreational waters.", "citation_count": "34", "reference_count": "22", "date": "2020", "authors": ["Niamh Cahill", "Dearbh\u00e1ile Morris"], "related_topics": ["Novel virus", "Coronavirus", "Transmission (medicine)", "Virus", "Pandemic", "Infectivity", "Betacoronavirus", "Pathogen", "Environmental health", "Medicine"]}
{"id": "1901129140", "references": ["2618530766", "2148349024", "1677182931", "2147800946", "1903029394", "1948751323", "2167510172", "2962835968", "2155893237", "1893585201"], "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .", "citation_count": "13", "reference_count": "26,984", "date": "2015", "authors": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "related_topics": ["Brain segmentation", "Deep learning", "Segmentation", "Image processing", "Context (language use)", "Artificial neural network", "Image translation", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "3106250896", "references": ["2102605133", "2618530766", "2155893237", "639708223", "2097117768", "2194775991", "1903029394", "2962835968", "2117539524", "1836465849"], "title": "SSD: Single Shot MultiBox Detector", "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.", "citation_count": "29", "reference_count": "14,661", "date": "2016", "authors": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott E. Reed", "Cheng-Yang Fu", "Alexander C. Berg"], "related_topics": ["Minimum bounding box", "Convolutional neural network", "Pixel", "Algorithm", "Artificial neural network", "Image resolution", "Pascal (programming language)", "Computer science"]}
{"id": "2405756170", "references": ["2963684088", "1947481528", "2481240925", "2097117768", "1895577753", "1514535095", "2964121744", "2099471712", "2064675550", "1836465849"], "title": "Generative adversarial text to image synthesis", "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.", "citation_count": "37", "reference_count": "1,889", "date": "2016", "authors": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "related_topics": ["Recurrent neural network", "Discriminative model", "Feature (computer vision)", "Generative grammar", "Machine learning", "Computer science", "Image (mathematics)", "Bridge (nautical)", "Artificial intelligence"]}
{"id": "2738588019", "references": ["2963684088", "2963373786", "2963073614", "2108598243", "2963840672", "6908809", "1903029394", "2099471712", "1665214252", "1836465849"], "title": "Globally and locally consistent image completion", "abstract": "We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.", "citation_count": "50", "reference_count": "1,057", "date": "2017", "authors": ["Satoshi Iizuka", "Edgar Simo-Serra", "Hiroshi Ishikawa"], "related_topics": ["Real image", "Context (language use)", "Discriminator", "Local consistency", "Convolutional neural network", "Consistency (database systems)", "Computer vision", "Image (mathematics)", "Artificial neural network", "Mathematics", "Artificial intelligence"]}
{"id": "2963420272", "references": ["2102605133", "2618530766", "1536680647", "2153579005", "2964121744", "1903029394", "2099471712", "1849277567", "2155893237", "2117539524"], "title": "Context Encoders: Feature Learning by Inpainting", "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders \u2013 a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "citation_count": "39", "reference_count": "2,713", "date": "2016", "authors": ["Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A. Efros"], "related_topics": ["Inpainting", "Feature learning", "Context (language use)", "Convolutional neural network", "Initialization", "Encoder", "Segmentation", "Pattern recognition", "Visualization", "Artificial intelligence", "Computer science"]}
{"id": "2893749619", "references": ["2962974533", "3035574324", "2996035354", "2890139949", "2950893734", "3003301247", "2962883549", "2985068832"], "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.", "citation_count": "0", "reference_count": "2,654", "date": "2018", "authors": ["Andrew Brock", "Jeff Donahue", "Karen Simonyan"], "related_topics": ["Fidelity", "Generator (mathematics)", "Scale (ratio)", "High fidelity", "Pattern recognition", "Image (mathematics)", "Regularization (mathematics)", "Set (abstract data type)", "Truncation", "Computer science", "Artificial intelligence"]}
{"id": "2331128040", "references": ["2963684088", "1861492603", "2194775991", "2133665775", "2964121744", "1903029394", "2964153729", "2962835968", "2117539524", "1836465849"], "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.", "citation_count": "65", "reference_count": "5,160", "date": "2016", "authors": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "related_topics": ["Deep learning", "Image translation", "Convolutional neural network", "Optimization problem", "Image (mathematics)", "Pattern recognition", "Computer science", "Orders of magnitude (time)", "Perception", "Work (physics)", "Artificial intelligence", "Superresolution", "Texture transfer"]}
{"id": "2963800363", "references": ["2963684088", "2963373786", "1901129140", "2194775991", "2340897893", "1959608418", "1903029394", "2962793481", "2963470893", "2962835968"], "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs", "abstract": "We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 A\u2014 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.", "citation_count": "55", "reference_count": "1,693", "date": "2018", "authors": ["Ting-Chun Wang", "Ming-Yu Liu", "Jun-Yan Zhu", "Andrew Tao", "Jan Kautz", "Bryan Catanzaro"], "related_topics": ["Object (computer science)", "Image translation", "Segmentation", "Semantics", "Pattern recognition", "Generator (mathematics)", "Artificial intelligence", "Computer science", "Resolution (logic)", "Image resolution"]}
{"id": "2963836885", "references": ["2962974533", "3035574324", "2890139949", "2950893734", "2982763192", "3003301247", "2893749619", "2963841322"], "title": "Spectral Normalization for Generative Adversarial Networks", "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.", "citation_count": "0", "reference_count": "2,015", "date": "2018", "authors": ["Takeru Miyato", "Toshiki Kataoka", "Masanori Koyama", "Yuichi Yoshida"], "related_topics": ["Normalization (statistics)", "Discriminator", "Pattern recognition", "Computer science", "Generative grammar", "Adversarial system", "Artificial intelligence"]}
{"id": "2271840356", "references": ["1614298861", "2131975293", "2016053056", "2097117768", "2130942839", "2160815625", "2168231600", "2155893237", "2064675550", "1836465849"], "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.", "citation_count": "42", "reference_count": "10,044", "date": "2015", "authors": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Gregory S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian J. Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal J\u00f3zefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "related_topics": ["Interface (computing)", "Deep learning", "Information extraction", "Artificial neural network", "Mobile device", "CUDA", "Robotics", "Distributed computing", "Computer science", "Machine learning", "Inference", "Artificial intelligence"]}
{"id": "830076066", "references": ["2145094598", "2963207607", "2963382180", "1479807131", "1606347560", "2095705004", "2964121744", "2100495367", "2294059674", "1836465849"], "title": "Semi-supervised learning with Ladder networks", "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.", "citation_count": "39", "reference_count": "930", "date": "2015", "authors": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "related_topics": ["Unsupervised learning", "Semi-supervised learning", "Deep learning", "Competitive learning", "Artificial neural network", "Supervised learning", "MNIST database", "Backpropagation", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "648143168", "references": ["2125389028", "2108598243", "2118858186", "189596042", "1959608418", "3118608800", "2100495367", "2099471712", "1836465849", "2025768430"], "title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.", "citation_count": "31", "reference_count": "1,941", "date": "2015", "authors": ["Emily Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "related_topics": ["Pyramid", "Real image", "Parametric model", "Pattern recognition", "Computer vision", "Generative grammar", "Image (mathematics)", "Computer science", "Artificial intelligence", "Laplacian pyramid"]}
{"id": "1487641199", "references": ["2618530766", "1904365287", "2097117768", "2130942839", "2310919327", "1959608418", "2099471712", "2157331557", "1665214252", "2963542991"], "title": "Generative Moment Matching Networks", "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.", "citation_count": "48", "reference_count": "543", "date": "2015", "authors": ["Yujia Li", "Kevin Swersky", "Rich Zemel"], "related_topics": ["Generative model", "Generative topographic map", "Multilayer perceptron", "MNIST database", "Backpropagation", "Statistical hypothesis testing", "Matching (statistics)", "Minimax", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2963685250", "references": ["1533861849", "2962897886", "2194775991", "2963911037", "1959608418", "3118608800", "104184427", "2064675550", "1836465849", "2145339207"], "title": "Weight normalization: a simple reparameterization to accelerate training of deep neural networks", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "citation_count": "26", "reference_count": "1,093", "date": "2016", "authors": ["Tim Salimans", "Diederik P. Kingma"], "related_topics": ["Normalization (statistics)", "Artificial neural network", "Stochastic gradient descent", "Reinforcement learning", "Optimization problem", "Speedup", "Algorithm", "Computer science", "Deep neural networks"]}
{"id": "2949416428", "references": ["2335728318", "2122457239", "1676820704", "2962897886", "2158049734", "2146502635", "2407712691", "1959608418", "2136504847", "2107008379"], "title": "Semi-Supervised Learning with Deep Generative Models", "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.", "citation_count": "20", "reference_count": "1,727", "date": "2014", "authors": ["Diederik P. Kingma", "Danilo J. Rezende", "Shakir Mohamed", "Max Welling"], "related_topics": ["Semi-supervised learning", "Bayesian inference", "Generative grammar", "Machine learning", "Computer science", "Generalization", "Artificial intelligence"]}
{"id": "2964153729", "references": ["2102605133", "2618530766", "1614298861", "2150165932", "2206858481", "2108598243", "2160815625", "2120419212", "2120480077", "2072128103"], "title": "Intriguing properties of neural networks", "abstract": "Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \r\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \r\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "citation_count": "11", "reference_count": "8,634", "date": "2014", "authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "related_topics": ["Artificial neural network", "Adversarial machine learning", "Pattern recognition", "Linear combination", "Computer science", "Uninterpretable", "Backdoor", "Artificial intelligence", "Deep neural networks", "Mean squared prediction error", "Visual recognition"]}
{"id": "2133665775", "references": ["2158564760", "2142276208", "2912116903", "2115838129", "2118217749", "2124731682", "2159269332", "2153777140", "2053691921", "2107790757"], "title": "Image quality assessment: from error visibility to structural similarity", "abstract": "Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.", "citation_count": "53", "reference_count": "33,167", "date": "2004", "authors": ["Zhou Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "related_topics": ["Image quality", "Subjective video quality", "Human visual system model", "Image processing", "Visibility (geometry)", "Cyclopean image", "JPEG", "JPEG 2000", "Image compression", "Image translation", "Compression artifact", "PEVQ", "Data compression", "View synthesis", "Transform coding", "Ringing artifacts", "Computer vision", "Computer science", "Structural similarity", "Artificial intelligence", "Superresolution"]}
{"id": "2340897893", "references": ["2102605133", "2618530766", "639708223", "1536680647", "1861492603", "2919115771", "1903029394", "2168356304", "2962835968", "2117539524"], "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.", "citation_count": "81", "reference_count": "4,755", "date": "2016", "authors": ["Marius Cordts", "Mohamed Omran", "Sebastian Ramos", "Timo Rehfeld", "Markus Enzweiler", "Rodrigo Benenson", "Uwe Franke", "Stefan Roth", "Bernt Schiele"], "related_topics": ["Object detection", "Deep learning", "Machine learning", "Leverage (statistics)", "Annotation", "Computer science", "Artificial intelligence"]}
{"id": "2964015378", "references": ["2796426482", "2962711740", "3100278010", "2918342466", "2962883549", "2907492528", "2905224888", "2963224980", "2963184176", "3100848837"], "title": "Semi-Supervised Classification with Graph Convolutional Networks", "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.", "citation_count": "0", "reference_count": "7,582", "date": "2016", "authors": ["Thomas N. Kipf", "Max Welling"], "related_topics": ["Convolutional neural network", "Scalability", "Theoretical computer science", "ENCODE", "Computer science", "Graph", "Graph classification", "Graph neural networks", "Hidden layer", "Knowledge graph"]}
{"id": "2962739339", "references": ["2963748441", "2147880316", "2158899491", "2251939518", "2153579005", "2095705004", "2964121744", "2250539671", "2064675550", "2493916176"], "title": "Deep contextualized word representations", "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "citation_count": "55", "reference_count": "6,539", "date": "2018", "authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "related_topics": ["Textual entailment", "Text corpus", "Syntax", "Language model", "Polysemy", "Semantics", "Question answering", "Sentiment analysis", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1514535095", "references": ["2618530766", "2964308564", "2097117768", "2130942839", "1861492603", "2095705004", "2964121744", "2157331557", "2962835968", "2117539524"], "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.", "citation_count": "53", "reference_count": "7,190", "date": "2015", "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "related_topics": ["Object detection", "Visualization", "Machine translation", "Backpropagation", "Benchmark (computing)", "Computer vision", "Computer science", "Sequence", "Closed captioning", "Artificial intelligence"]}
{"id": "1904365287", "references": ["2108598243", "2147768505", "2310919327", "2911964244", "3118608800", "2100495367", "1993882792", "2116064496", "4919037", "2912934387"], "title": "Improving neural networks by preventing co-adaptation of feature detectors", "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.", "citation_count": "18", "reference_count": "8,061", "date": "2012", "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "related_topics": ["Dropout (neural networks)", "Feature (computer vision)", "Overfitting", "Feedforward neural network", "Artificial neural network", "Context (language use)", "Benchmark (computing)", "Cognitive neuroscience of visual object recognition", "Machine learning", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "1959608418", "references": ["2145094598", "2163922914", "2146502635", "2171490498", "2097268041", "2166851633", "2951493172", "3104819538", "2963173382"], "title": "Auto-Encoding Variational Bayes", "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.", "citation_count": "15", "reference_count": "13,487", "date": "2014", "authors": ["Diederik P Kingma", "Max Welling"], "related_topics": ["Approximate inference", "Inference", "Estimator", "Upper and lower bounds", "Bayes' theorem", "Latent variable", "Autoencoder", "Probabilistic logic", "Algorithm", "Mathematical optimization", "Computer science"]}
{"id": "1849277567", "references": ["2102605133", "2618530766", "2136922672", "1904365287", "2161381512", "2155541015", "2108598243", "2110798204", "2546302380", "2025768430"], "title": "Visualizing and Understanding Convolutional Networks", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "citation_count": "24", "reference_count": "12,388", "date": "2014", "authors": ["Matthew D. Zeiler", "Rob Fergus"], "related_topics": ["Convolutional neural network", "Softmax function", "Classifier (UML)", "Network model", "Machine learning", "Stochastic gradient descent", "Computer science", "Artificial intelligence"]}
{"id": "2168356304", "references": ["3097096317", "2152826865", "2161969291", "1576520375", "2154422044", "2151103935", "2030536784", "2145072179", "2120419212", "2115763357"], "title": "Object Detection with Discriminatively Trained Part-Based Models", "abstract": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.", "citation_count": "46", "reference_count": "10,940", "date": "2010", "authors": ["P F Felzenszwalb", "R B Girshick", "D McAllester", "D Ramanan"], "related_topics": ["Probabilistic latent semantic analysis", "Latent variable", "Viola\u2013Jones object detection framework", "Object detection", "Support vector machine", "Discriminative model", "Pascal (programming language)", "Cognitive neuroscience of visual object recognition", "Linear discriminant analysis", "Pedestrian detection", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2130325614", "references": ["2136922672", "2166049352", "2145889472", "2122922389", "2110798204", "2147800946", "2100495367", "2139427956", "2116064496", "2162915993"], "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "abstract": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.", "citation_count": "26", "reference_count": "2,986", "date": "2009", "authors": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng"], "related_topics": ["Convolutional Deep Belief Networks", "Deep belief network", "Deep learning", "Unsupervised learning", "Generative model", "Probabilistic logic", "Inference", "Pattern recognition", "Scalability", "Computer science", "Artificial intelligence"]}
{"id": "2911964244", "references": ["1605688901", "2112076978", "2113242816", "2067885219", "2152761983", "2099968818", "1975846642", "2120240539", "2912934387", "1580948147"], "title": "Random Forests", "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.", "citation_count": "12", "reference_count": "74,907", "date": "2001", "authors": ["Leo Breiman"], "related_topics": ["Random forest", "Multivariate random variable", "Random subspace method", "AdaBoost", "Alternating decision tree", "Tree (graph theory)", "Ensemble learning", "Gradient boosting", "Statistics", "Mathematics"]}
{"id": "3116298410", "references": ["3035524453", "3034978746", "602397586", "2619697695", "3101577715", "219040644", "2962865004", "2808631503", "343636949", "2963173190"], "title": "Self-Supervised Learning of Audio-Visual Objects from Video", "abstract": "Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.", "citation_count": "54", "reference_count": "22", "date": "2020", "authors": ["Triantafyllos Afouras", "Andrew Owens", "Joon Son Chung", "Andrew Zisserman"], "related_topics": ["Face detection", "Optical flow", "Object (computer science)", "Representation (mathematics)", "Computer vision", "Set (psychology)", "Computer science", "Generality", "Aggregate (data warehouse)", "Detector", "Artificial intelligence"]}
{"id": "3122924117", "references": ["1840435438", "2194775991", "2183341477", "2096733369", "3118608800", "1821462560", "1544827683", "2963341956", "2117539524", "2965373594"], "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning", "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in the few-shot learning settings, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. The new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled data.", "citation_count": "59", "reference_count": "11", "date": "2021", "authors": ["Beliz Gunel", "Jingfei Du", "Alexis Conneau", "Veselin Stoyanov"], "related_topics": ["Language model", "Generalization", "Natural language understanding", "Robustness (computer science)", "Machine learning", "Benchmark (computing)", "Class (biology)", "Memory bank", "Noise (video)", "Computer science", "Artificial intelligence"]}
{"id": "3099495704", "references": ["3161838454", "3126009523", "3143315506", "3159056052", "3136670918", "3122301478", "3139264293", "3139613640", "3119997354", "3135385999"], "title": "CrossTransformers: spatially-aware few-shot transfer", "abstract": "Given new tasks with very little data$-$such as new classes in a classification problem or a domain shift in the input$-$performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets.", "citation_count": "0", "reference_count": "18", "date": "2020", "authors": ["Carl Doersch", "Ankush Gupta", "Andrew Zisserman"], "related_topics": ["Artificial neural network", "Task (computing)", "Domain (software engineering)", "Classifier (linguistics)", "Transformer (machine learning model)", "Machine learning", "Computer science", "Transfer (computing)", "Small number", "Shot (filmmaking)", "Artificial intelligence"]}
{"id": "3145385912", "references": ["3159394092"], "title": "Spatiotemporal Contrastive Video Representation Learning", "abstract": "We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentation for video self-supervised learning and find both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on the clips that are distant in a video. On the Kinetics-600 dataset, a linear classifier trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.6% with a larger R3D-50 (4$\\times$ filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning.", "citation_count": "0", "reference_count": "29", "date": "2020", "authors": ["Rui Qian", "Tianjian Meng", "Boqing Gong", "Ming-Hsuan Yang", "Huisheng Wang", "Serge J. Belongie", "Yin Cui"], "related_topics": ["Feature learning", "Linear classifier", "Pattern recognition", "Closing (morphology)", "Frame (networking)", "Embedding", "Computer science", "Space (commercial competition)", "Sampling (signal processing)", "CLIPS", "Artificial intelligence"]}
{"id": "3125947392", "references": ["3129170303", "3139419546", "3134032827", "3162945626"], "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "citation_count": "0", "reference_count": "28", "date": "2020", "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "related_topics": ["Feature learning", "Reinforcement learning", "Bisimulation", "State space", "Domain knowledge", "Generalization", "Invariant (physics)", "Machine learning", "Causal inference", "Computer science", "Artificial intelligence"]}
{"id": "3132401450", "references": ["1997063559", "2963069010", "3097096317", "3035524453", "3034978746", "2963403868", "1821462560", "2116064496", "2963703618", "2963341956"], "title": "How to represent part-whole hierarchies in a neural network", "abstract": "This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language", "citation_count": "59", "reference_count": "5", "date": "2021", "authors": ["Geoffrey E. Hinton"], "related_topics": ["Parse tree", "Feature learning", "Artificial neural network", "Parsing", "Interpretability", "Representation (mathematics)", "Hierarchy", "Structure (mathematical logic)", "Artificial intelligence", "Computer science", "Transformer (machine learning model)"]}
{"id": "3098053103", "references": ["3131871335", "3134032827", "3131944163", "3094246835", "3134085768", "3132674603", "3137443434", "3102762742", "3107857059", "3133595589"], "title": "Reinforcement Learning with Augmented Data", "abstract": "Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at this https URL.", "citation_count": "0", "reference_count": "84", "date": "2020", "authors": ["Michael Laskin", "Kimin Lee", "Adam Stooke", "Lerrel Pinto", "Pieter Abbeel", "Aravind Srinivas"], "related_topics": ["Reinforcement learning", "Convolutional neural network", "Benchmark (computing)", "Generalization", "Machine learning", "Jitter", "Code (cryptography)", "Computer science", "State (computer science)", "Pixel", "Artificial intelligence"]}
{"id": "3154503084", "references": ["3132869322", "3154313998", "3135138557", "3134210100"], "title": "Graph Contrastive Learning with Adaptive Augmentation", "abstract": "Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes\u2014a crucial component in CL\u2014remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.", "citation_count": "0", "reference_count": "9", "date": "2021", "authors": ["Yanqiao Zhu", "Yichen Xu", "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang"], "related_topics": ["Graph (abstract data type)", "Feature learning", "Unsupervised learning", "Centrality", "Node (networking)", "Theoretical computer science", "Topology (electrical circuits)", "Two-graph", "Computer science", "Noise (video)"]}
{"id": "3108316907", "references": ["2963684088", "2963073614", "2108598243", "2183341477", "2133665775", "2964121744", "2100495367", "2099471712", "2962793481", "2962835968"], "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "abstract": "In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so \u2013 maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each \u201cdomain\u201d is only a single image.", "citation_count": "87", "reference_count": "50", "date": "2020", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "related_topics": ["Image translation", "Feature vector", "Mutual information", "Translation (geometry)", "Image (mathematics)", "Pattern recognition", "Domain (software engineering)", "Point (geometry)", "Computer science", "Rest (physics)", "Artificial intelligence"]}
{"id": "2806070179", "references": ["2102605133", "2618530766", "639708223", "1536680647", "1861492603", "2109255472", "2565639579", "2194775991", "1903029394", "2806070179"], "title": "Mask R-CNN", "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in  parallel  with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at:  https://github.com/facebookresearch/Detectron .", "citation_count": "42", "reference_count": "15,089", "date": "2020", "authors": ["Kaiming He", "Georgia Gkioxari", "Piotr Dollar", "Ross Girshick"], "related_topics": ["Object detection", "Image segmentation", "Minimum bounding box", "Pose", "Segmentation", "Feature extraction", "Object (computer science)", "Cognitive neuroscience of visual object recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1861492603", "references": ["2102605133", "2618530766", "2108598243", "2161969291", "2031489346", "3118608800", "2110158442", "2038721957", "2168356304", "2963542991"], "title": "Microsoft COCO: Common Objects in Context", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "citation_count": "46", "reference_count": "16,319", "date": "2014", "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "related_topics": ["Object detection", "Cognitive neuroscience of visual object recognition", "Minimum bounding box", "Segmentation", "User interface", "Pascal (programming language)", "Computer vision", "Spotting", "Closed captioning", "Computer science", "Artificial intelligence"]}
{"id": "2565639579", "references": ["2102605133", "2618530766", "639708223", "2161969291", "2151103935", "1901129140", "2194775991", "1903029394", "2962835968", "2117539524"], "title": "Feature Pyramid Networks for Object Detection", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.", "citation_count": "41", "reference_count": "7,283", "date": "2017", "authors": ["Tsung-Yi Lin", "Piotr Dollar", "Ross Girshick", "Kaiming He", "Bharath Hariharan", "Serge Belongie"], "related_topics": ["Feature (computer vision)", "Pyramid", "Object detection", "Feature extraction", "Semantic feature", "Cognitive neuroscience of visual object recognition", "Artificial neural network", "Robustness (computer science)", "Pattern recognition", "Computer vision", "Computer science", "Detector", "Artificial intelligence"]}
{"id": "2081580037", "references": ["1518768680", "2103318667", "2017668967", "2065157922", "2102381086", "13823885", "1483126227"], "title": "WordNet: a lexical database for English", "abstract": "Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].", "citation_count": "7", "reference_count": "21,330", "date": "1995", "authors": ["George A. Miller"], "related_topics": ["WordNet", "Lexical database", "eXtended WordNet", "Machine-readable dictionary", "Reverse dictionary", "Natural language", "VerbNet", "Noun", "Natural language processing", "Synonym", "Computer science", "Artificial intelligence", "Word-sense induction"]}
{"id": "2165225968", "references": ["1997063559", "1652505363", "1964724001", "2315016682", "2725061391", "2159080219", "2049633694", "1507849272", "2293063825", "2121407732"], "title": "Unsupervised learning of distributions on binary vectors using two layer networks", "abstract": "We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.", "citation_count": "11", "reference_count": "401", "date": "1991", "authors": ["Yoav Freund", "David Haussler"], "related_topics": ["Wake-sleep algorithm", "Feature learning", "Stability (learning theory)", "Unsupervised learning", "Online machine learning", "Boltzmann machine", "Semi-supervised learning", "Active learning (machine learning)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2096192494", "references": ["2099866409", "2136922672", "1513873506", "2135094946", "2064630666", "2100495367", "2116064496", "2169415915", "66838807", "2158164339"], "title": "On the quantitative analysis of deep belief networks", "abstract": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.", "citation_count": "14", "reference_count": "493", "date": "2008", "authors": ["Ruslan Salakhutdinov", "Iain Murray"], "related_topics": ["Deep belief network", "Restricted Boltzmann machine", "Approximate inference", "Graphical model", "Model selection", "Importance sampling", "Greedy algorithm", "Hidden variable theory", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2251939518", "references": ["1889268436", "2164019165", "2117130368", "2146502635", "71795751", "2097606805", "1423339008", "1662133657", "2132339004", "2097726431"], "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "citation_count": "42", "reference_count": "5,188", "date": "2013", "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "related_topics": ["Treebank", "Principle of compositionality", "Parsing", "Natural language processing", "Tree (data structure)", "Machine learning", "Negation", "Word (computer architecture)", "Computer science", "Scope (computer science)", "Meaning (non-linguistic)", "Artificial intelligence", "Single sentence"]}
{"id": "2131744502", "references": ["1614298861", "2158899491", "2251939518", "2117130368", "2131744502", "2153579005", "2158139315", "1423339008", "2141599568", "2132339004"], "title": "Distributed Representations of Sentences and Documents", "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "citation_count": "41", "reference_count": "7,097", "date": "2014", "authors": ["Quoc Le", "Tomas Mikolov"], "related_topics": ["Topic model", "Feature vector", "Feature (machine learning)", "Sentiment analysis", "Paragraph", "Semantics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2250539671", "references": ["1614298861", "2118020653", "2158899491", "2117130368", "2146502635", "2153579005", "2158139315", "2141599568", "2132339004", "2072128103"], "title": "Glove: Global Vectors for Word Representation", "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "citation_count": "28", "reference_count": "21,544", "date": "2014", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "related_topics": ["Word2vec", "Word embedding", "Sparse matrix", "SemEval", "Sequence labeling", "Word (computer architecture)", "Context (language use)", "Distributional semantics", "Named-entity recognition", "Sememe", "Vocabulary mismatch", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2963748441", "references": ["2964267515", "1632114991", "2962809918", "2251349042", "2108598243", "2171278097", "2962790689", "1544827683", "2125436846", "2251818205"], "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \r\nThe dataset is freely available at this https URL", "citation_count": "27", "reference_count": "2,884", "date": "2016", "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "related_topics": ["Question answering", "Reading comprehension", "Reading (process)", "Comprehension", "F1 score", "Natural language processing", "Set (abstract data type)", "Computer science", "Baseline (configuration management)", "Dependency (UML)", "Artificial intelligence"]}
{"id": "2100677568", "references": ["2154642048", "1652505363", "1596324102", "1535810436", "1569296262", "1507849272", "2895674046", "1583833196", "2075379212", "2178806388"], "title": "Learning to Predict by the Methods of Temporal Differences", "abstract": "This article introduces a class of incremental learning procedures specialized for prediction \u2013 that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.", "citation_count": "33", "reference_count": "6,387", "date": "1988", "authors": ["Richard S. Sutton"], "related_topics": ["Temporal difference learning", "Supervised learning", "Heuristic", "sort", "Connectionism", "Machine learning", "Artificial intelligence", "Class (computer programming)", "Convergence (routing)", "Computer science", "Computation"]}
{"id": "1569320505", "references": ["2121863487", "2019207321", "2148885430", "2044535354", "2131215403", "2119717200", "2095227410", "2749680651", "1481420047", "2161406034"], "title": "Adaptive filtering prediction and control", "abstract": "This unified survey focuses on linear discrete-time systems and explores the natural extensions to nonlinear systems. In keeping with the importance of computers to practical applications, the authors emphasize discrete-time systems. Their approach summarizes the theoretical and practical aspects of a large class of adaptive algorithms.1984 edition.", "citation_count": "0", "reference_count": "6,774", "date": "1984", "authors": ["Graham C. Goodwin", "Kwai Sang Sin"], "related_topics": ["Control theory", "Adaptive filter", "Nonlinear system", "Control engineering", "Computer science", "Control (linguistics)", "Natural (music)", "Large class"]}
{"id": "2154642048", "references": ["2154642048", "1652505363", "2073257493", "1505136099", "2115647291", "1535810436", "2101926813", "2021878536", "1507849272", "1490454746"], "title": "Learning internal representations by error propagation", "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion", "citation_count": "23", "reference_count": "31,447", "date": "1988", "authors": ["D. E. Rumelhart", "G. E. Hinton", "R. J. Williams"], "related_topics": ["Delta rule", "Semi-supervised learning", "Backpropagation", "Artificial neural network", "Propagation of uncertainty", "Online machine learning", "Algebra", "Stochastic gradient descent", "Descent (mathematics)", "Theoretical computer science", "Computer science"]}
{"id": "94523489", "references": ["1998442441", "2121863487", "2143956139", "2809684781", "1993717606", "2093229042", "2149723649", "1553004968", "2155399784", "2133321814"], "title": "Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks", "abstract": "Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain.", "citation_count": "0", "reference_count": "4,973", "date": "1988", "authors": ["David S. Broomhead", "David Lowe"], "related_topics": ["Interpolation", "Linear interpolation", "Bilinear interpolation", "Trilinear interpolation", "Learning rule", "Radial basis function network", "Generalization", "Hierarchical RBF", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "1603765807", "references": ["2121863487", "2164278908", "78077100", "2096544401", "2951781666", "2044212084", "2963433607", "2010630450", "2114791779"], "title": "Parallel and Distributed Computation: Numerical Methods", "abstract": "", "citation_count": "0", "reference_count": "8,514", "date": "1989", "authors": ["Dimitri P. Bertsekas", "John N. Tsitsiklis"], "related_topics": ["Distributed design patterns", "Distributed algorithm", "Computation", "Dynamic programming", "Computational science", "Computer science", "Numerical analysis"]}
{"id": "1639032689", "references": ["2165299997", "1999284878", "2167101736", "2097571405", "2168081761", "2017337590", "2165171393", "1501500081", "2106334424", "1992419399"], "title": "Genetic algorithms in search, optimization, and machine learning", "abstract": "From the Publisher:\r\nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \r\n\r\nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.", "citation_count": "0", "reference_count": "106,990", "date": "1988", "authors": ["David E. Goldberg"], "related_topics": ["Genetic representation", "Genetic programming", "Pascal (programming language)", "Evolutionary computation", "Computer programming", "Computer science", "Machine learning", "Computer techniques"]}
{"id": "1535810436", "references": ["2042264548", "2154642048", "2121863487", "2098257210", "2604319603", "2025605741", "2166851633", "1570963478", "114517082"], "title": "Adaptive switching circuits", "abstract": "", "citation_count": "0", "reference_count": "5,146", "date": "1988", "authors": ["Bernard Widrow", "Marcian E. Hoff"], "related_topics": ["Computer science", "Electronic circuit", "Electronic engineering", "Adaptive switching"]}
{"id": "3017143921", "references": ["1746819321", "1570448133", "2163352848", "2067191022", "2121647436", "2310919327", "2110158442", "2117812871", "1992419399"], "title": "Pattern classification and scene analysis", "abstract": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.", "citation_count": "0", "reference_count": "21,827", "date": "1973", "authors": ["Richard O. Duda", "Peter E. Hart"], "related_topics": ["Unsupervised learning", "Cluster analysis", "Linear discriminant analysis", "Pattern recognition (psychology)", "Perspective (graphical)", "Pattern recognition", "Nonparametric statistics", "Bayes estimator", "Computer science", "Artificial intelligence", "Scene analysis"]}
{"id": "3011120880", "references": ["2121863487", "1528676759", "2076063813", "2964043796", "2886851211", "2963923407", "2155968351", "2107726111"], "title": "Learning from delayed rewards", "abstract": "", "citation_count": "0", "reference_count": "8,126", "date": "1989", "authors": ["C. J. C. H. Watkins"], "related_topics": ["Computer science", "Cognitive psychology", "Q learning algorithm"]}
{"id": "2178806388", "references": ["2103626435", "1547925194", "1542941925", "2158091072", "1592847719", "2995201943", "2117355432", "2019003829", "2104753538"], "title": "Some studies in machine learning using the game of checkers", "abstract": "Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done by verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.", "citation_count": "0", "reference_count": "3,434", "date": "1995", "authors": ["Arthur L. Samuel"], "related_topics": ["Rote learning", "Artificial intelligence", "Period (music)", "Computer science", "Machine learning", "Course (navigation)", "Work (physics)", "Sense of direction"]}
{"id": "2145094598", "references": ["2025768430", "2136922672", "1652505363", "1479807131", "1994197834", "2110798204", "2100495367", "2116064496", "2108384452", "2072128103"], "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.", "citation_count": "55", "reference_count": "6,141", "date": "2010", "authors": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "related_topics": ["Deep belief network", "Support vector machine", "Pattern recognition", "Machine learning", "Benchmark (computing)", "Noise reduction", "Bridging (networking)", "Image (mathematics)", "Enhanced Data Rates for GSM Evolution", "Variation (game tree)", "Mathematics", "Artificial intelligence"]}
{"id": "2076063813", "references": ["2102605133", "2618530766", "2136922672", "2156909104", "2097117768", "2130942839", "2151103935", "2963542991", "1849277567", "1663973292"], "title": "Deep learning in neural networks", "abstract": "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning &amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.", "citation_count": "500", "reference_count": "14,081", "date": "2015", "authors": ["J\u00fcrgen Schmidhuber"], "related_topics": ["Deep learning", "Deep belief network", "Unsupervised learning", "Artificial neural network", "Competitive learning", "Learning classifier system", "Semi-supervised learning", "Instance-based learning", "Computational learning theory", "Reinforcement learning", "Algorithmic learning theory", "Stability (learning theory)", "Supervised learning", "Backpropagation", "Evolutionary computation", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2107941094", "references": ["2042986967", "1492640216", "1652505363", "2104670598", "2152150600", "1639032689", "2581275558", "1497256448", "3011460294", "2297395784"], "title": "Ant system: optimization by a colony of cooperating agents", "abstract": "An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS.", "citation_count": "29", "reference_count": "15,973", "date": "1996", "authors": ["M. Dorigo", "V. Maniezzo", "A. Colorni"], "related_topics": ["Metaheuristic", "Ant colony optimization algorithms", "Extremal optimization", "Greedy algorithm", "Tabu search", "Combinatorial optimization", "Parallel metaheuristic", "Travelling salesman problem", "Ant colony", "Simulated annealing", "Optimization problem", "Premature convergence", "Mathematical optimization", "Computer science", "Artificial Ants"]}
{"id": "2116064496", "references": ["1997063559", "1993845689", "1652505363", "1547224907", "2101706260", "2096175520", "2083380015", "2165225968", "2114153178", "1746680969"], "title": "Training products of experts by minimizing contrastive divergence", "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \"expert\" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \"contrastive divergence\" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.", "citation_count": "23", "reference_count": "5,201", "date": "2002", "authors": ["Geoffrey E. Hinton"], "related_topics": ["Product of experts", "Deep belief network", "Conditional independence", "Latent variable", "Inference", "Probability distribution", "Boltzmann machine", "Data type", "Machine learning", "Artificial intelligence", "Mathematics"]}
{"id": "2001141328", "references": ["2047870719", "2107636931", "2070320140", "2032647857", "2587818897", "2123977795", "2108384452", "2122538988", "2138451337", "2099741732"], "title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "abstract": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.", "citation_count": "22", "reference_count": "14,853", "date": "2000", "authors": ["J. B. Tenenbaum", "V. de Silva", "J. C. Langford"], "related_topics": ["Diffusion map", "Dimensionality reduction", "Nonlinear dimensionality reduction", "t-distributed stochastic neighbor embedding", "Isomap", "Curse of dimensionality", "Sammon mapping", "Principal component analysis", "Pattern recognition", "Artificial intelligence"]}
{"id": "2125637308", "references": ["3029645440", "1578099820", "2150134853", "2143516773", "2177274842", "2121947440", "2124351162", "2104095591", "2296319761", "2169551590"], "title": "Random Walks for Image Segmentation", "abstract": "A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs", "citation_count": "76", "reference_count": "2,817", "date": "2006", "authors": ["L. Grady"], "related_topics": ["Random walker algorithm", "Image segmentation", "Scale-space segmentation", "Pixel", "Graph theory", "Image processing", "Cut", "Discrete space", "Algorithm", "Computer vision", "Artificial intelligence", "Mathematics"]}
{"id": "2139823104", "references": ["1979711143", "200434350", "2165874743", "1511160855", "2113592823", "1585385982", "2143516773", "2122837498", "2121947440", "2154579312"], "title": "Semi-supervised learning using Gaussian fields and harmonic functions", "abstract": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.", "citation_count": "21", "reference_count": "4,318", "date": "2003", "authors": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Empirical risk minimization", "Stability (learning theory)", "Supervised learning", "Gaussian random field", "Random graph", "Online machine learning", "Belief propagation", "Transduction (machine learning)", "Spectral graph theory", "Gaussian", "Feature selection", "Random walk", "Prior probability", "Pattern recognition", "Computer science", "Artificial intelligence", "Generalization error", "Graph"]}
{"id": "2156718197", "references": ["1578099820", "2001141328", "108654854", "2053186076", "2121947440"], "title": "Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering", "abstract": "Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.", "citation_count": "5", "reference_count": "4,981", "date": "2001", "authors": ["Mikhail Belkin", "Partha Niyogi"], "related_topics": ["Spectral clustering", "Manifold alignment", "Laplacian matrix", "Nonlinear dimensionality reduction", "Laplacian smoothing", "Isomap", "Cluster analysis", "Laplace operator", "Topology", "Mathematics"]}
{"id": "2137570937", "references": ["2136922672", "2187089797", "2053186076", "2100495367", "1880262756", "1497256448", "2121947440", "2116064496", "2296319761", "2072128103"], "title": "Dimensionality Reduction: A Comparative Review", "abstract": "In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.", "citation_count": "160", "reference_count": "1,359", "date": "2009", "authors": ["Laurens van der Maaten", "Eric Postma", "Jaap van den Herik"], "related_topics": ["Dimensionality reduction", "Nonlinear dimensionality reduction", "Nonlinear system", "Machine learning", "Computer science", "Variety (cybernetics)", "Scaling", "Artificial intelligence"]}
{"id": "2157444450", "references": ["2107636931", "2106346128", "23758216", "2159174312", "2001141328", "1991848143", "2053186076", "2100659887", "2148694408"], "title": "Stochastic Neighbor Embedding", "abstract": "We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \"images\" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \"bank\", to have versions close to the images of both \"river\" and \"finance\" without forcing the images of outdoor concepts to be located close to those of corporate concepts.", "citation_count": "9", "reference_count": "1,549", "date": "2002", "authors": ["Geoffrey E. Hinton", "Sam T. Roweis"], "related_topics": ["t-distributed stochastic neighbor embedding", "Dimensionality reduction", "Object (grammar)", "Probabilistic logic", "Probability distribution", "Embedding", "Forcing (recursion theory)", "Gaussian", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2053186076", "references": ["2047870719", "2610857016", "2107636931", "2070320140", "1991848143", "2121122425", "2019020850", "1902027874", "2122538988", "1513400187"], "title": "Nonlinear dimensionality reduction by locally linear embedding.", "abstract": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.", "citation_count": "12", "reference_count": "16,334", "date": "2000", "authors": ["Sam T. Roweis", "Lawrence K. Saul"], "related_topics": ["Diffusion map", "Dimensionality reduction", "t-distributed stochastic neighbor embedding", "Nonlinear dimensionality reduction", "Isomap", "Elastic map", "Curse of dimensionality", "Local tangent space alignment", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "1742512077", "references": ["255556494", "1591018827", "2076063813", "2109574129", "2137570937", "2187089797", "2162584119", "2963460103"], "title": "Nonlinear Dimensionality Reduction", "abstract": "Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists.", "citation_count": "0", "reference_count": "1,684", "date": "2007", "authors": ["John A. Lee", "Michel Verleysen"], "related_topics": ["Dimensionality reduction", "Nonlinear dimensionality reduction", "Curse of dimensionality", "Linear model", "Metric (mathematics)", "Kernel (statistics)", "Structure (mathematical logic)", "Principal component analysis", "Theoretical computer science", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2032647857", "references": ["1993845689", "2142228262", "2089419199", "2166116275", "2063971957", "2137983211", "2079782346", "5731987", "2162604518", "2122538988"], "title": "Replicator neural networks for universal optimal source coding.", "abstract": "Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold.", "citation_count": "14", "reference_count": "178", "date": "1995", "authors": ["Robert Hecht-Nielsen"], "related_topics": ["Artificial neural network", "Data compression", "Manifold", "Backpropagation", "Minification", "Algorithm", "Source code", "Coding (social sciences)", "Essentially unique", "Computer science"]}
{"id": "2021774695", "references": ["2154642048", "1995169133", "1652505363", "2591802459", "2010581677", "1507849272", "1498436455", "2155487652"], "title": "Learning sets of filters using back-propagation", "abstract": "Abstract   A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in \u201cweight space\u201d. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.", "citation_count": "8", "reference_count": "160", "date": "1987", "authors": ["David C. Plaut", "Geoffrey E. Hinton"], "related_topics": ["Multi-task learning", "Active learning (machine learning)", "Semi-supervised learning", "Backpropagation", "Edge detection", "Noise (video)", "Task (project management)", "Set (psychology)", "Algorithm", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2121122425", "references": ["3004157836", "1634005169", "2096710051", "2017977879", "2137983211", "2913399920", "2122538988", "2140196014", "1971735090"], "title": "Dimension reduction by local principal component analysis", "abstract": "Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations.", "citation_count": "36", "reference_count": "787", "date": "1997", "authors": ["Nandakishore Kambhatla", "Todd K. Leen"], "related_topics": ["Sparse PCA", "Principal component analysis", "Dimensionality reduction", "Artificial neural network", "Redundancy (engineering)", "Representation (mathematics)", "Pattern recognition", "Nonlinear system", "Algorithm", "Image (mathematics)", "Mathematics", "Artificial intelligence"]}
{"id": "2293063825", "references": ["2076870593", "2970228278", "2086789740", "1594524188"], "title": "Neural networks and physical systems with emergent collective computational abilities", "abstract": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.", "citation_count": "4", "reference_count": "26,202", "date": "1999", "authors": ["John J. Hopfield"], "related_topics": ["Physical system", "Artificial neural network", "Asynchronous communication", "Generalization", "State (computer science)", "Parallel processing (DSP implementation)", "Error detection and correction", "Categorization", "Artificial intelligence", "Computer science"]}
{"id": "2166049352", "references": ["1746680969", "1949116567", "2567948266", "2155511848", "2154422044", "1699734612", "2124087378", "1516111018", "2164598857", "2124386111"], "title": "Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories", "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.", "citation_count": "18", "reference_count": "8,261", "date": "2007", "authors": ["Li Fei-Fei", "Rob Fergus", "Pietro Perona"], "related_topics": ["Generative model", "Learning object", "Caltech 101", "Bayesian inference", "Cognitive neuroscience of visual object recognition", "Object (computer science)", "Categorization", "Bayesian probability", "Machine learning", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2134557905", "references": ["2160225842", "2217896605", "2155511848", "2124351082", "2310919327", "2124087378", "2123977795", "2164598857", "2141376824", "2295106276"], "title": "Learning methods for generic object recognition with invariance to pose and lighting", "abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.", "citation_count": "28", "reference_count": "1,467", "date": "2004", "authors": ["Y. LeCun", "Fu Jie Huang", "L. Bottou"], "related_topics": ["Object detection", "Image segmentation", "Support vector machine", "Feature extraction", "Cognitive neuroscience of visual object recognition", "Clutter", "Grayscale", "Segmentation", "Pattern recognition", "Computer vision", "k-nearest neighbors algorithm", "Computer science", "Artificial intelligence"]}
{"id": "2161969291", "references": ["1992825118", "1576520375", "2151103935", "2172188317", "2145072179", "2177274842", "1608462934", "2156539399", "2152473410", "2295106276"], "title": "Histograms of oriented gradients for human detection", "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.", "citation_count": "22", "reference_count": "36,647", "date": "2005", "authors": ["N. Dalal", "B. Triggs"], "related_topics": ["Histogram of oriented gradients", "Local binary patterns", "GLOH", "Object detection", "Feature (computer vision)", "Pedestrian detection", "Feature extraction", "Normalization (image processing)", "Viola\u2013Jones object detection framework", "Implicit Shape Model", "Normalization (statistics)", "Bag-of-words model in computer vision", "Caltech 101", "Haar-like features", "LabelMe", "Cognitive neuroscience of visual object recognition", "Support vector machine", "Histogram", "Motion History Images", "Pattern recognition", "Traffic sign recognition", "Computer vision", "Computer science", "Artificial intelligence", "Feature descriptor", "Fisher vector"]}
{"id": "2097018403", "references": ["2166049352", "2107034620", "2113606819", "2161516371", "1624854622", "1576445103", "2153635508", "2153663612", "2162915993", "1566135517"], "title": "Linear spatial pyramid matching using sparse coding for image classification", "abstract": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.", "citation_count": "28", "reference_count": "3,793", "date": "2009", "authors": ["Jianchao Yang", "Kai Yu", "Yihong Gong", "Thomas Huang"], "related_topics": ["Support vector machine", "Kernel (image processing)", "Contextual image classification", "Vector quantization", "Caltech 101", "Neural coding", "Image segmentation", "Histogram", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2162915993", "references": ["2166049352", "2104978738", "2107034620", "2154422044", "2168002178", "2134731454", "1880262756", "1566135517", "2914885528", "2165828254"], "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories", "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u0092s \"gist\" and Lowe\u0092s SIFT descriptors.", "citation_count": "28", "reference_count": "9,939", "date": "2006", "authors": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "related_topics": ["Pyramid (image processing)", "Pyramid", "Bag-of-words model in computer vision", "Image texture", "Feature detection (computer vision)", "Image segmentation", "Caltech 101", "Visual dictionary", "Scale-invariant feature transform", "Histogram", "LabelMe", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2099866409", "references": ["2136922672", "2147152072", "2122090912", "2124914669", "1612003148", "2100495367", "205159212", "2116064496", "2158164339", "2165395308"], "title": "Restricted Boltzmann machines for collaborative filtering", "abstract": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.", "citation_count": "15", "reference_count": "2,078", "date": "2007", "authors": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey Hinton"], "related_topics": ["Restricted Boltzmann machine", "Boltzmann machine", "Collaborative filtering", "Graphical model", "Word error rate", "Inference", "Machine learning", "Data set", "Singular value decomposition", "Computer science", "Artificial intelligence"]}
{"id": "2536626143", "references": ["1989702938", "2112076978", "2033419168", "2119821739", "2098947662", "2151103935", "1782590233", "2123921160", "2155759509"], "title": "Attribute and simile classifiers for face verification", "abstract": "We present two novel methods for face verification. Our first method - \u201cattribute\u201d classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - \u201csimile\u201d classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.", "citation_count": "32", "reference_count": "1,679", "date": "2009", "authors": ["Neeraj Kumar", "Alexander C. Berg", "Peter N. Belhumeur", "Shree K. Nayar"], "related_topics": ["Feature extraction", "Facial recognition system", "Face (geometry)", "Data set", "Visualization", "Pattern recognition", "Expression (mathematics)", "Similarity (geometry)", "Computer science", "Artificial intelligence"]}
{"id": "2099587183", "references": ["1981627423", "2119409989", "3025398977", "1572038152"], "title": "General Game Playing: Overview of the AAAI Competition", "abstract": "A general game playing system is one that can accept a formal description of a game and play the game effectively without human intervention. Unlike specialized game players, such as Deep Blue, general game players do not rely on algorithms designed in advance for specific games; and, unlike Deep Blue, they are able to play different kinds of games. In order to promote work in this area, the AAAI is sponsoring an open competition at this summer's Twentieth National Conference on Artificial Intelligence. This article is an overview of the technical issues and logistics associated with this summer's competition, as well as the relevance of general game playing to the long range-goals of artificial intelligence.", "citation_count": "4", "reference_count": "618", "date": "2005", "authors": ["Michael R. Genesereth", "Nathaniel Love", "Barney Pell"], "related_topics": ["General game playing", "General video game playing", "Game design", "Video game design", "Game Developer", "Non-cooperative game", "Game mechanics", "Repeated game", "Human\u2013computer interaction", "Operations research", "Engineering"]}
{"id": "1625390266", "references": ["2077902449", "2009551863", "1512919909", "2101861158", "1551466210", "1562282139", "1863869622", "2168405694", "2135997697", "2016647253"], "title": "Bandit based monte-carlo planning", "abstract": "For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.", "citation_count": "12", "reference_count": "3,113", "date": "2006", "authors": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "related_topics": ["Monte Carlo tree search", "Monte Carlo method", "Decision problem", "Sampling (statistics)", "Markov process", "Proof-number search", "Mathematical optimization", "Computer Go", "Sample (statistics)", "Artificial intelligence", "Mathematics"]}
{"id": "2132622533", "references": ["2100677568", "2121863487", "13294968", "2149390907", "2134042548", "1515851193", "2075268401", "2062937587", "2109910161", "1491843047"], "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "abstract": "Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.", "citation_count": "24", "reference_count": "370", "date": "2011", "authors": ["Richard S. Sutton", "Joseph Modayil", "Michael Delp", "Thomas Degris", "Patrick M. Pilarski", "Adam White", "Doina Precup"], "related_topics": ["Unsupervised learning", "Robot learning", "Reinforcement learning", "Knowledge representation and reasoning", "Temporal difference learning", "Function approximation", "General knowledge", "Mobile robot", "Robot", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1515851193", "references": ["2165150801", "2076063813", "2964043796", "2168405694", "2100235918", "2155968351", "2952509347", "2072128103"], "title": "Introduction to Reinforcement Learning", "abstract": "From the Publisher:\r\nIn Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.", "citation_count": "0", "reference_count": "6,275", "date": "1998", "authors": ["Richard S. Sutton", "Andrew G. Barto"], "related_topics": ["Reinforcement learning", "Temporal difference learning", "Q-learning", "AIXI", "Cognitive science", "Field (computer science)", "Computer science", "Key (cryptography)"]}
{"id": "2126316555", "references": ["2020135152", "1500868819", "1625390266", "1714211023", "2171084228", "131069610", "1510812122", "2168405694", "1888434271", "2122410182"], "title": "A Survey of Monte Carlo Tree Search Methods", "abstract": "Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.", "citation_count": "214", "reference_count": "2,357", "date": "2012", "authors": ["C. B. Browne", "E. Powley", "D. Whitehouse", "S. M. Lucas", "P. I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "related_topics": ["Monte Carlo tree search", "Computer Go", "General game playing", "Monte Carlo method", "General video game playing", "Decision theory", "Machine learning", "Open research", "Computer science", "Game theory", "Artificial intelligence"]}
{"id": "1502916507", "references": ["1634005169", "2147152072", "2074429597", "1956559956", "2147717514", "2125148312", "2160066518", "1541459201", "2295428206", "3017143921"], "title": "Similarity Search in High Dimensions via Hashing", "abstract": "The nearestor near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \\curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should su ce for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points Supported by NAVY N00014-96-1-1221 grant and NSF Grant IIS-9811904. Supported by Stanford Graduate Fellowship and NSF NYI Award CCR-9357849. Supported by ARO MURI Grant DAAH04-96-1-0007, NSF Grant IIS-9811904, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 25th VLDB Conference, Edinburgh, Scotland, 1999. from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives signi cant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).", "citation_count": "39", "reference_count": "4,154", "date": "1999", "authors": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "related_topics": ["Nearest neighbor search", "Locality-sensitive hashing", "Very large database", "Open addressing", "Dynamic perfect hashing", "Cover tree", "Hash function", "Ball tree", "Feature hashing", "Linear search", "Universal hashing", "K-independent hashing", "Hopscotch hashing", "Metric (mathematics)", "k-nearest neighbors algorithm", "Information retrieval", "Computer science", "Tree decomposition", "Machine learning", "Artificial intelligence"]}
{"id": "2101355568", "references": ["2121863487", "1625390266", "1506806321", "1515851193", "2168359464", "2168405694", "2119567691", "1576452626", "2107726111", "2122410182"], "title": "An object-oriented representation for efficient reinforcement learning", "abstract": "Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.", "citation_count": "102", "reference_count": "271", "date": "2008", "authors": ["Carlos Diuk", "Andre Cohen", "Michael L. Littman"], "related_topics": ["Learning classifier system", "Reinforcement learning", "Robot learning", "Representation (mathematics)", "Generalization", "Polynomial", "Artificial intelligence", "Domain (software engineering)", "Computer science", "Markov process"]}
{"id": "2013391942", "references": ["2076063813", "2952509347", "2034806191", "2099397840", "2792315573", "1564034482", "2073384958", "2101493843", "2072128103"], "title": "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability", "abstract": "This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environment, the latter is suited for passive prediction in unknown environment. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational problems. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.", "citation_count": "0", "reference_count": "707", "date": "2004", "authors": ["Marcus Hutter"], "related_topics": ["Algorithmic learning theory", "Reinforcement learning", "Algorithmic probability", "AIXI", "Algorithmic information theory", "Solomonoff's theory of inductive inference", "Kolmogorov complexity", "Computational problem", "Artificial intelligence", "Computer science"]}
{"id": "2156909104", "references": ["1746819321", "1570448133", "2164278908", "2140190241", "2076063813", "2139212933", "2310919327", "2148603752", "2129812935", "2072128103"], "title": "The Nature of Statistical Learning Theory", "abstract": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.", "citation_count": "0", "reference_count": "67,802", "date": "1995", "authors": ["Vladimir N. Vapnik"], "related_topics": ["Algorithmic learning theory", "Statistical learning theory", "Computational learning theory", "Instance-based learning", "Proactive learning", "Stability (learning theory)", "Unsupervised learning", "Probably approximately correct learning", "Machine learning", "Mathematics"]}
{"id": "2119821739", "references": ["2154642048", "2168228682", "1530699444", "1568787085", "2322002063", "2087347434", "5594912", "1498436455", "2504871398", "2154579312"], "title": "Support-Vector Networks", "abstract": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.\r\n\r\nHigh generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.", "citation_count": "14", "reference_count": "48,716", "date": "1995", "authors": ["Corinna Cortes", "Vladimir Vapnik"], "related_topics": ["Feature learning", "Active learning (machine learning)", "Feature vector", "Computational learning theory", "Online machine learning", "Semi-supervised learning", "Linear classifier", "Relevance vector machine", "Artificial intelligence", "Computer science"]}
{"id": "2129131372", "references": ["2145096794", "2136235822", "2099641086", "2050834445", "2296616510", "2078204800", "2154332973", "2296319761", "2129638195", "2116148865"], "title": "Decoding by linear programming", "abstract": "This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/&gt;0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.", "citation_count": "29", "reference_count": "7,881", "date": "2005", "authors": ["E.J. Candes", "T. Tao"], "related_topics": ["Underdetermined system", "Linear code", "Sigma", "Restricted isometry property", "Linear system", "Convex optimization", "Discrete mathematics", "Combinatorics", "Matching pursuit", "System of linear equations", "Mathematics"]}
{"id": "2296616510", "references": ["2145096794", "2097323375", "2136235822", "2099641086", "2050880896", "2147656689", "2096613063", "2078204800", "2012365979", "2116148865"], "title": "Compressed sensing", "abstract": "Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0&lt;ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0&lt;ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces", "citation_count": "20", "reference_count": "17,733", "date": "2004", "authors": ["D.L. Donoho"], "related_topics": ["Basis pursuit", "Orthonormal basis", "Linear combination", "Matching pursuit", "Basis pursuit denoising", "Wavelet", "Linear subspace", "Restricted isometry property", "Discrete mathematics", "Combinatorics", "Mathematics"]}
{"id": "2163455955", "references": ["2053463056", "3146306708", "2156909104", "1576520375", "2143516773", "2166706824", "1964357740", "2115023510", "2114524997"], "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales", "abstract": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.", "citation_count": "26", "reference_count": "2,414", "date": "2005", "authors": ["Bo Pang", "Lillian Lee"], "related_topics": ["Semantic similarity", "Class (philosophy)", "Sentiment analysis", "Categorization", "Similarity measure", "Classifier (linguistics)", "Similarity (psychology)", "Metric (mathematics)", "Support vector machine", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2151048449", "references": ["2096152098", "2120779048", "1548663377", "2123142779", "2131357087", "2165897980", "1960027552", "158057341", "86887328", "2100341149"], "title": "Local and Global Algorithms for Disambiguation to Wikipedia", "abstract": "Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.", "citation_count": "17", "reference_count": "793", "date": "2011", "authors": ["Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "related_topics": ["Entity linking", "Online encyclopedia", "Natural language processing", "Information retrieval", "Computer science", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "2097606805", "references": ["2161204834", "2153439141", "1859173823", "2092654472", "1567570606", "1551104980", "1535015163", "2170716495", "2110882317", "2155693943"], "title": "Accurate Unlexicalized Parsing", "abstract": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.", "citation_count": "18", "reference_count": "3,795", "date": "2003", "authors": ["Dan Klein", "Christopher D. Manning"], "related_topics": ["Statistical parsing", "Parsing", "Treebank", "Natural language processing", "Independence (mathematical logic)", "Grammar", "Computer science", "Artificial intelligence"]}
{"id": "71795751", "references": ["3146306708", "2022204871", "2117130368", "2114524997", "2158139315", "2166706824", "1423339008", "1880262756", "2132339004", "2097726431"], "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "citation_count": "35", "reference_count": "1,484", "date": "2011", "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "related_topics": ["Sentiment analysis", "Machine learning", "Multinomial distribution", "Computer science", "Artificial intelligence"]}
{"id": "1984052055", "references": ["1652505363", "2156909104", "2147152072", "1587094587", "1662133657", "2038721957", "1880262756", "2132339004", "1498436455", "1631260214"], "title": "Composition in distributional models of semantics.", "abstract": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.", "citation_count": "216", "reference_count": "999", "date": "2010", "authors": ["Jeff Mitchell", "Mirella Lapata"], "related_topics": ["Distributional semantics", "Semantic similarity", "Semantics", "Phrase", "Similarity (psychology)", "Priming (psychology)", "Principle of compositionality", "Meaning (non-linguistic)", "Natural language processing", "Linguistics", "Mathematics", "Artificial intelligence"]}
{"id": "2103305545", "references": ["2140833774", "2117130368", "1566018662", "2097606805", "71795751", "2095739681", "2158139315", "1423339008", "2132339004", "2296073425"], "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.", "citation_count": "26", "reference_count": "978", "date": "2011", "authors": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y. Ng"], "related_topics": ["Paraphrase", "Feature vector", "Pooling", "Classifier (UML)", "Natural language processing", "Pattern recognition", "Computer science", "Matrix (mathematics)", "Artificial intelligence"]}
{"id": "3104097132", "references": ["2618530766", "1614298861", "2163922914", "2117130368", "2122646361", "2118585731", "2147768505", "2153579005", "2141599568", "2168231600"], "title": "DeepWalk: online learning of social representations", "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "citation_count": "50", "reference_count": "4,992", "date": "2014", "authors": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "related_topics": ["Feature learning", "Deep learning", "Language model", "Anomaly detection", "Statistical model", "Machine learning", "Class (biology)", "Graph embedding", "Computer science", "Artificial intelligence"]}
{"id": "2100664567", "references": ["1614298861", "2964308564", "2101105183", "2130942839", "1606347560", "1753482797", "2157331557", "2964199361", "2118434577", "2153653739"], "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.", "citation_count": "20", "reference_count": "870", "date": "2015", "authors": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "related_topics": ["Transfer-based machine translation", "Rule-based machine translation", "Machine translation", "Vocabulary", "Artificial neural network", "Translation (geometry)", "Machine learning", "Speech recognition", "Decoding methods", "Importance sampling", "Computer science", "Artificial intelligence"]}
{"id": "1895577753", "references": ["1614298861", "2964308564", "2155541015", "2097117768", "2130942839", "2157331557", "2117539524", "2064675550", "1836465849", "2963542991"], "title": "Show and tell: A neural image caption generator", "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.", "citation_count": "36", "reference_count": "4,638", "date": "2015", "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "related_topics": ["Generative model", "Machine translation", "Pascal (programming language)", "Sentence", "Pattern recognition", "Machine learning", "Artificial intelligence", "Fluency", "Computer science", "Closed captioning"]}
{"id": "1888005072", "references": ["1614298861", "2001141328", "2187089797", "1854214752", "3104097132", "2156718197", "2131744502", "2153579005", "2053186076", "1532325895"], "title": "LINE: Large-scale Information Network Embedding", "abstract": "This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\footnote{\\url{https://github.com/tangjianpku/LINE}}.", "citation_count": "23", "reference_count": "3,345", "date": "2015", "authors": ["Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "related_topics": ["Graph embedding", "Embedding", "Node (networking)", "Stochastic gradient descent", "Feature learning", "Dimensionality reduction", "Scalability", "Theoretical computer science", "Computer science", "Social network"]}
{"id": "2964321699", "references": ["1614298861", "2101491865", "2116341502", "2310919327", "2964121744", "2158787690", "2121947440", "603908379", "2132914434", "2070232376"], "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "abstract": "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.", "citation_count": "26", "reference_count": "3,141", "date": "2016", "authors": ["Micha\u00ebl Defferrard", "Xavier Bresson", "Pierre Vandergheynst"], "related_topics": ["Deep learning", "Convolutional neural network", "Spectral graph theory", "MNIST database", "Computational complexity theory", "Embedding", "Theoretical computer science", "Computer science", "Connectome", "Artificial intelligence", "Graph"]}
{"id": "2123024445", "references": ["2618530766", "1614298861", "1904365287", "2108598243", "2187089797", "2146502635", "2153579005", "2132339004", "2117539524", "2168231600"], "title": "DeViSE: A Deep Visual-Semantic Embedding Model", "abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.", "citation_count": "21", "reference_count": "2,004", "date": "2013", "authors": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc'Aurelio Ranzato", "Tomas Mikolov"], "related_topics": ["Cognitive neuroscience of visual object recognition", "Visual Objects", "Semantic memory", "Machine learning", "Leverage (statistics)", "Embedding", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "1486649854", "references": ["1614298861", "2964308564", "2130942839", "2187089797", "1861492603", "1832693441", "2964121744", "2157331557", "2962835968", "2064675550"], "title": "Skip-thought vectors", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.", "citation_count": "38", "reference_count": "1,938", "date": "2015", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "related_topics": ["Sentence", "Semantic similarity", "Vocabulary", "Unsupervised learning", "Ranking (information retrieval)", "Encoder", "Paraphrase", "Natural language processing", "Ranking", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2158823144", "references": ["1632114991", "2147880316", "2008652694", "1934019294", "2156515921", "2125838338", "1574901103", "2096175520", "2116064496", "2169415915"], "title": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data", "abstract": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.", "citation_count": "57", "reference_count": "1,063", "date": "2004", "authors": ["Charles Sutton", "Khashayar Rohanimanesh", "Andrew McCallum"], "related_topics": ["Approximate inference", "Variable elimination", "Bayesian network", "Belief propagation", "Graphical model", "Conditional random field", "Dynamic Bayesian network", "Inference", "Probabilistic logic", "Pattern recognition", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2107008379", "references": ["2964015378", "2118020653", "2165698076", "2158899491", "2117130368", "1479807131", "2150102617", "1995903777", "2104290444"], "title": "Transductive Inference for Text Classification using Support Vector Machines", "abstract": "", "citation_count": "0", "reference_count": "3,787", "date": "1999", "authors": ["Thorsten Joachims"], "related_topics": ["Relevance vector machine", "Transduction (machine learning)", "Structured support vector machine", "Support vector machine", "Co-training", "Machine learning", "Computer science", "Pattern recognition", "Artificial intelligence"]}
{"id": "2914746235", "references": ["2165698076", "2117130368", "2076063813", "1536680647", "1512387364", "2150341604", "2114315281", "2098411764", "2184188583", "2550821151"], "title": "Multitask learning", "abstract": "Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in artificial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many different kinds of domains, and can be used with different learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.", "citation_count": "0", "reference_count": "8,169", "date": "1998", "authors": ["Rich Caruana"], "related_topics": ["Multi-task learning", "Inductive transfer", "Artificial neural network", "Decision tree", "Task (project management)", "Generalization (learning)", "Machine learning", "Computer science", "Sketch", "Representation (mathematics)", "Artificial intelligence"]}
{"id": "2173629880", "references": ["2217896605", "3115898234", "3007502375", "2098694627", "2973127116", "3016138882", "2982413405", "2970350205", "1606209288"], "title": "Phoneme recognition using time-delay neural networks", "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. &gt;", "citation_count": "0", "reference_count": "4,232", "date": "1995", "authors": ["Alexander Waibel", "Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J. Lang"], "related_topics": ["Time delay neural network", "Hidden Markov model", "Artificial neural network", "Backpropagation", "Speech recognition", "Task (computing)", "Hierarchy (mathematics)", "Position (vector)", "Computer science", "Nonlinear system"]}
{"id": "2163568299", "references": ["1632114991", "2158195707", "1567570606", "2729906263", "2037894654", "2048679005", "3021452258", "1535015163", "2098379588", "2125712079"], "title": "Effective Self-Training for Parsing", "abstract": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.", "citation_count": "20", "reference_count": "608", "date": "2006", "authors": ["David McClosky", "Eugene Charniak", "Mark Johnson"], "related_topics": ["Parsing", "Discriminative model", "Bootstrapping", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence", "Error reduction", "Self training"]}
{"id": "2158847908", "references": ["1632114991", "2151170651", "2154626406", "2092654472", "1567277581", "2126851059", "3021452258", "1535015163", "2039217078", "2115792525"], "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles", "abstract": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.\r\n\r\nWe describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.", "citation_count": "57", "reference_count": "2,695", "date": "2005", "authors": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury"], "related_topics": ["Treebank", "Semantic role labeling", "Explicit semantic analysis", "Semantic computing", "Semantic similarity", "PropBank", "FrameNet", "VerbNet", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2130903752", "references": ["1480376833", "2154455818", "2139823104", "2914746235", "2010353172", "2048679005", "2148603752", "2097089247", "2107008379", "2101210369"], "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "citation_count": "26", "reference_count": "1,518", "date": "2005", "authors": ["Rie Kubota Ando", "Tong Zhang"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Instance-based learning", "Active learning (machine learning)", "Multi-task learning", "Stability (learning theory)", "Online machine learning", "Learning classifier system", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2885050925", "references": ["2150203234", "2151170651", "1520377376", "2145310422", "2040909025", "2092654472", "2166776180", "1988995507", "2093647425", "2155693943"], "title": "Shallow Semantic Parsing using Support Vector Machines.", "abstract": "", "citation_count": "19", "reference_count": "499", "date": "2004", "authors": ["Sameer S. Pradhan", "Wayne H. Ward", "Kadri Hacioglu", "James H. Martin", "Daniel Jurafsky"], "related_topics": ["Parsing", "Support vector machine", "Natural language processing", "PropBank", "Computer science", "Artificial intelligence"]}
{"id": "2127314673", "references": ["2099111195", "2123084125", "2049633694", "2059800182", "2099247782", "3017143921", "1982944197", "2121227244", "2016001305", "2025887562"], "title": "DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS", "abstract": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.", "citation_count": "11", "reference_count": "1,464", "date": "1993", "authors": ["Fernando Pereira", "Naftali Tishby", "Lillian Lee"], "related_topics": ["Single-linkage clustering", "Complete-linkage clustering", "Correlation clustering", "Brown clustering", "Cluster analysis", "k-medians clustering", "CURE data clustering algorithm", "Consensus clustering", "Fuzzy clustering", "Similarity measure", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2158195707", "references": ["2611071497", "2075201173", "2097333193", "2170120409", "1966812932", "2158195707", "2099247782", "2166637769", "2121227244", "2134237567"], "title": "An empirical study of smoothing techniques for language modeling", "abstract": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser?Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.", "citation_count": "23", "reference_count": "3,717", "date": "1999", "authors": ["Stanley F. Chen", "Joshua Goodman"], "related_topics": ["Kneser\u2013Ney smoothing", "Smoothing", "Bigram", "Trigram", "Language model", "Test data", "Speech recognition", "Empirical research", "Gram", "Computer science"]}
{"id": "2111305191", "references": ["2127314673", "2097333193", "2170120409", "2158195707", "2595741664", "2099247782", "47415966", "2121227244", "2134237567", "1996764654"], "title": "A bit of progress in language modeling", "abstract": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.", "citation_count": "50", "reference_count": "630", "date": "2001", "authors": ["Joshua T. Goodman"], "related_topics": ["Perplexity", "Trigram", "Word error rate", "Smoothing", "Language model", "Mixture model", "Cluster analysis", "Entropy (information theory)", "Statistics", "Speech recognition", "Mathematics"]}
{"id": "2056590938", "references": ["2016243284", "82490022", "1554663460", "17500809", "2132339004"], "title": "Connectionist language modeling for large vocabulary continuous speech recognition", "abstract": "This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate.", "citation_count": "6", "reference_count": "175", "date": "2002", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "related_topics": ["Cache language model", "Language model", "Vocabulary", "Perplexity", "Word error rate", "Text corpus", "Connectionism", "Artificial neural network", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2091812280", "references": ["2136922672", "145476170", "2437096199", "2158195707", "2147010501", "2116064496", "2132339004", "36903255", "1631260214", "1558797106"], "title": "Three new graphical models for statistical language modelling", "abstract": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.", "citation_count": "13", "reference_count": "702", "date": "2007", "authors": ["Andriy Mnih", "Geoffrey Hinton"], "related_topics": ["Language model", "Graphical model", "Word (computer architecture)", "Probabilistic logic", "Parametric model", "Theoretical computer science", "Sequence", "Natural language processing", "Computer science", "Large set (Ramsey theory)", "Artificial intelligence"]}
{"id": "2121227244", "references": ["2097333193", "2751862591", "2016871293", "1966812932", "2142901448", "1628850721", "2049633694", "2007780422", "1575431606", "1597533204"], "title": "Class-based n -gram models of natural language", "abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "citation_count": "12", "reference_count": "4,038", "date": "1992", "authors": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "related_topics": ["n-gram", "Natural language", "Word (group theory)", "Natural language processing", "Class (biology)", "Sample (statistics)", "Computer science", "Artificial intelligence"]}
{"id": "36903255", "references": ["2127314673", "2147152072", "2110485445", "1978394996", "2096175520", "2038721957", "2116064496", "2121227244", "2132339004", "1631260214"], "title": "Hierarchical Probabilistic Neural Network Language Model.", "abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.", "citation_count": "27", "reference_count": "1,120", "date": "2005", "authors": ["Frederic Morin", "Yoshua Bengio"], "related_topics": ["Hierarchical network model", "Language model", "Time delay neural network", "Hierarchical clustering", "Probabilistic neural network", "WordNet", "Nervous system network models", "Component (UML)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1558797106", "references": ["145476170", "1989705153", "1574901103", "2092654472", "2096175520", "1535015163", "2116064496", "2134237567", "2132339004", "1746680969"], "title": "Quick Training of Probabilistic Neural Nets by Importance Sampling.", "abstract": "", "citation_count": "16", "reference_count": "206", "date": "2003", "authors": ["Yoshua Bengio", "Jean-S\u00e9bastien Senecal"], "related_topics": ["Time delay neural network", "Probabilistic neural network", "Artificial neural network", "Probabilistic logic", "Importance sampling", "Machine learning", "Computer science", "Training (meteorology)", "Artificial intelligence"]}
{"id": "2158997610", "references": ["1996650435", "2147152072", "2056029990", "2072773380", "2928502135", "1674947250", "2059086756", "2092919341", "1981617416", "1983578042"], "title": "An introduction to latent semantic analysis", "abstract": "Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual\u2010usage meaning of words by statistical computations applied to a large corpus of text (Landauer &amp; Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word\u2010word and passage\u2010word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.", "citation_count": "33", "reference_count": "6,475", "date": "1998", "authors": ["Thomas K Landauer", "Peter W. Foltz", "Darrell Laham"], "related_topics": ["Latent semantic analysis", "Probabilistic latent semantic analysis", "Vocabulary", "Similarity (psychology)", "Semantics", "Learnability", "Priming (psychology)", "Automated essay scoring", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "168564468", "references": ["2147152072", "2063392856", "2170120409", "2334889010", "2143017621", "2159426623", "1880262756", "2001082470", "2158266063", "2047804403"], "title": "Software Framework for Topic Modelling with Large Corpora", "abstract": "Large corpora are ubiquitous in today's world and memory\nquickly becomes the limiting factor in practical applications\nof the Vector Space Model (VSM). We identify gap in existing\nVSM implementations, which is their scalability and ease of\nuse. We describe a Natural Language Processing software\nframework which is based on the idea of document streaming,\ni.e. processing corpora document after document, in a memory\nindependent fashion. In this framework, we implement several\npopular algorithms for topical inference, including Latent\nSemantic Analysis and Latent Dirichlet Allocation, in a way\nthat makes them completely independent of the training corpus\nsize. Particular emphasis is placed on straightforward and\nintuitive framework design, so that modifications and\nextensions of the methods and/or their application by\ninterested practitioners are effortless. We demonstrate the\nusefulness of our approach on a real-world scenario of\ncomputing document similarities within an existing digital\nlibrary DML-CZ.", "citation_count": "25", "reference_count": "3,281", "date": "2010", "authors": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "related_topics": ["Latent Dirichlet allocation", "Topic model", "Software framework", "Latent semantic analysis", "Vector space model", "Scalability", "Software", "Python (programming language)", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "2156515921", "references": ["2147880316", "2008652694", "1934019294", "2117400858", "2160842254", "1520377376", "2962735828", "1995945562", "2096175520", "1773803948"], "title": "Shallow parsing with conditional random fields", "abstract": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.", "citation_count": "33", "reference_count": "1,926", "date": "2003", "authors": ["Fei Sha", "Fernando Pereira"], "related_topics": ["Conditional random field", "Shallow parsing", "Sequence labeling", "Chunking (psychology)", "Machine learning", "Natural language processing", "Sequence", "Computer science", "Task (computing)", "Generative grammar", "Artificial intelligence"]}
{"id": "1880262756", "references": ["1746680969", "1508165687", "2045656233", "2020842694", "2147152072", "2063392856", "1956559956", "1516111018", "2097089247", "2107743791"], "title": "Latent dirichlet allocation", "abstract": "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.", "citation_count": "28", "reference_count": "38,976", "date": "2003", "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "related_topics": ["Latent Dirichlet allocation", "Dynamic topic model", "Hierarchical Dirichlet process", "Topic model", "Pachinko allocation", "Probabilistic latent semantic analysis", "Variational message passing", "Dirichlet-multinomial distribution", "Data mining", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2004763266", "references": ["1979711143", "2144578941", "2147880316", "2128634885", "2008652694", "2096765155", "2148540243", "2125838338", "1996430422", "2121227244"], "title": "Design Challenges and Misconceptions in Named Entity Recognition", "abstract": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.", "citation_count": "32", "reference_count": "1,573", "date": "2009", "authors": ["Lev Ratinov", "Dan Roth"], "related_topics": ["Named-entity recognition", "Inference", "F1 score", "Process (engineering)", "Machine learning", "Task (project management)", "Computer science", "Artificial intelligence", "Representation (mathematics)"]}
{"id": "2296073425", "references": ["2099866409", "2025768430", "2136922672", "2117130368", "1994197834", "2110798204", "2100495367", "205159212", "2172174689", "2072128103"], "title": "Curriculum learning", "abstract": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).", "citation_count": "31", "reference_count": "2,888", "date": "2009", "authors": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "related_topics": ["Active learning", "Stability (learning theory)", "Instance-based learning", "Curriculum", "Context (language use)", "Generalization (learning)", "Stochastic neural network", "Process (engineering)", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2536208356", "references": ["2116877738", "3097096317", "2110764733", "2161969291", "2067191022", "1528789833", "2033819227", "2132947399", "2112301665", "2125310925"], "title": "Decomposing a scene into geometric and semantically consistent regions", "abstract": "High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.", "citation_count": "22", "reference_count": "815", "date": "2009", "authors": ["Stephen Gould", "Richard Fulton", "Daphne Koller"], "related_topics": ["Orientation (computer vision)", "Image segmentation", "Object (computer science)", "Solid modeling", "3D reconstruction", "Representation (mathematics)", "Pixel", "Pattern recognition", "Inference", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2067191022", "references": ["2159128898", "2999729612", "2099244020", "2150134853", "2129905273", "2140235142", "1971784203", "2132549764", "2798766386", "2482402870"], "title": "Mean shift: a robust approach toward feature space analysis", "abstract": "A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.", "citation_count": "69", "reference_count": "14,727", "date": "2002", "authors": ["D. Comaniciu", "P. Meer"], "related_topics": ["Mean-shift", "Smoothing", "Estimator", "Kernel (statistics)", "Feature vector", "Image segmentation", "Estimation theory", "Kernel regression", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "1574901103", "references": ["182831726", "1508165687", "1736036918", "2108321481", "1549026077", "1795234945", "1746620543", "1994851566", "2949237929"], "title": "Foundations of Statistical Natural Language Processing", "abstract": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.", "citation_count": "9", "reference_count": "15,340", "date": "1999", "authors": ["Christopher D. Manning", "Hinrich Sch\u00fctze"], "related_topics": ["Computational linguistics", "Natural language", "Parsing", "Collocation extraction", "Data-oriented parsing", "Probabilistic logic", "Collocation", "Natural language processing", "Construct (python library)", "Computer science", "Artificial intelligence"]}
{"id": "1566135517", "references": ["2130259898", "1524408959", "2156406284", "2142796031", "2117812871", "2104825706", "2128716185", "2012352340", "2167034998", "2180838288"], "title": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope", "abstract": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.", "citation_count": "45", "reference_count": "7,535", "date": "2001", "authors": ["Aude Oliva", "Antonio Torralba"], "related_topics": ["Scene statistics", "Representation (systemics)", "Envelope (motion)", "Naturalness", "Pattern recognition (psychology)", "Categorization", "LabelMe", "Segmentation", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1632114991", "references": ["2076526090", "2439178139", "2334801970", "2012837062", "2099247782", "2110190189", "900993354", "2166675302", "1483126227", "2121407024"], "title": "Building a large annotated corpus of English: the penn treebank", "abstract": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.", "citation_count": "12", "reference_count": "9,188", "date": "1993", "authors": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "related_topics": ["Corpus linguistics", "Brown Corpus", "Treebank", "Text corpus", "PropBank", "Trigram tagger", "Computational linguistics", "Shallow parsing", "Natural language processing", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2147152072", "references": ["1964262399", "2096411881", "2024683548", "2000215628", "3012395598", "1984565341", "1965061793", "1956559956", "2114804204", "2151561903"], "title": "Indexing by Latent Semantic Analysis", "abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.", "citation_count": "26", "reference_count": "17,287", "date": "1990", "authors": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "related_topics": ["Document-term matrix", "Latent semantic analysis", "Vector space model", "Automatic indexing", "Probabilistic latent semantic analysis", "Explicit semantic analysis", "Random indexing", "Document retrieval", "Information retrieval", "Mathematics"]}
{"id": "179875071", "references": ["2107878631", "2292896937", "2110485445", "2437096199", "2027499299", "2152808281", "2468573742", "2132339004", "36903255", "2096072088"], "title": "Recurrent neural network based language model", "abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition", "citation_count": "14", "reference_count": "5,581", "date": "2010", "authors": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "related_topics": ["Time delay neural network", "Language model", "Recurrent neural network", "Word error rate", "Perplexity", "Connectionism", "Reduction (complexity)", "Speech recognition", "NIST", "Computer science"]}
{"id": "1970689298", "references": ["2147152072", "2110485445", "2158195707", "2096175520", "1554663460", "2148603752", "2132339004", "36903255", "2912934387", "1631260214"], "title": "Continuous space language models", "abstract": "This paper describes the use of a neural network language model for large vocabulary continuous speech recognition. The underlying idea of this approach is to attack the data sparseness problem by performing the language model probability estimation in a continuous space. Highly efficient learning algorithms are described that enable the use of training corpora of several hundred million words. It is also shown that this approach can be incorporated into a large vocabulary continuous speech recognizer using a lattice rescoring framework at a very low additional processing time. The neural network language model was thoroughly evaluated in a state-of-the-art large vocabulary continuous speech recognizer for several international benchmark tasks, in particular the Nist evaluations on broadcast news and conversational speech recognition. The new approach is compared to four-gram back-off language models trained with modified Kneser-Ney smoothing which has often been reported to be the best known smoothing method. Usually the neural network language model is interpolated with the back-off language model. In that way, consistent word error rate reductions for all considered tasks and languages were achieved, ranging from 0.4% to almost 1% absolute.", "citation_count": "63", "reference_count": "606", "date": "2007", "authors": ["Holger Schwenk"], "related_topics": ["Cache language model", "Language model", "Vocabulary", "Time delay neural network", "Computational linguistics", "n-gram", "Word error rate", "Factored language model", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2173213060", "references": ["2109722477", "2045271686", "2173213060", "1510543252", "2148317584", "2073965851", "2044534358", "1988243929", "2104644701", "2119565742"], "title": "MapReduce: simplified data processing on large clusters", "abstract": "MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.", "citation_count": "19", "reference_count": "30,307", "date": "2008", "authors": ["Jeffrey Dean", "Sanjay Ghemawat"], "related_topics": ["Data-intensive computing", "Runtime system", "Many-task computing", "Programming with Big Data in R", "Jaql", "Programming paradigm", "NewSQL", "Data center network architectures", "Parallel computing", "Petabyte", "Distributed computing", "Computer science", "Massively parallel computation"]}
{"id": "2166706824", "references": ["1550206324", "3146306708", "1576520375", "2140785063", "2149684865", "2199803028", "2160842254", "1924689489", "2096175520"], "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques", "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.", "citation_count": "32", "reference_count": "10,538", "date": "2002", "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "related_topics": ["Sentiment analysis", "Multiclass classification", "Relevance vector machine", "Structured support vector machine", "Naive Bayes classifier", "Support vector machine", "Categorization", "Machine learning", "Principle of maximum entropy", "Computer science", "Artificial intelligence"]}
{"id": "2024165284", "references": ["2132267493", "2147152072", "2752853835", "2072773380", "2090208105", "2013912476", "2113722075", "1902027874", "2798909945", "2099741732"], "title": "Tensor Decompositions and Applications", "abstract": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.", "citation_count": "223", "reference_count": "7,947", "date": "2009", "authors": ["Tamara G. Kolda", "Brett W. Bader"], "related_topics": ["Tensor contraction", "Cartesian tensor", "Symmetric tensor", "Tensor", "Matricization", "Tensor product of Hilbert spaces", "Tucker decomposition", "Tensor product", "Algebra", "Combinatorics", "Mathematics"]}
{"id": "1532325895", "references": ["2164019165", "1521626219", "2122369144", "2150341604", "1662133657", "2248026759", "1888005072", "2962965405", "1736726159"], "title": "Introduction to Information Retrieval", "abstract": "Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.", "citation_count": "0", "reference_count": "19,593", "date": "2005", "authors": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "related_topics": ["Human\u2013computer information retrieval", "Cognitive models of information retrieval", "Concept search", "Relevance (information retrieval)", "Information science", "Document retrieval", "Adversarial information retrieval", "Search engine indexing", "Multimedia", "Information retrieval", "Computer science"]}
{"id": "3013264884", "references": ["2113112851", "2142827986", "2017333496", "1984137208", "1984250866", "2089554624", "2048562911", "2144629587", "2108278206", "2007541434"], "title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine.", "abstract": "In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The  prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search  engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search  engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.", "citation_count": "0", "reference_count": "20,969", "date": "1998", "authors": ["Sergey Brin", "Lawrence Page"], "related_topics": ["Web search engine", "Web page", "Hypertext", "Hyperlink", "Index (publishing)", "Search engine", "World Wide Web", "Exploit", "Public Description", "Computer science"]}
{"id": "1660390307", "references": ["2037959956", "166263196", "1499900670", "2122962290", "2107745473"], "title": "Modern Information Retrieval", "abstract": "From the Publisher:\r\nThis is a rigorous and complete textbook for a first course on information retrieval from the computer science (as opposed to a user-centred) perspective. The advent of the Internet and the enormous increase in volume of electronically stored information generally has led to substantial work on IR from the computer science perspective - this book provides an up-to-date student oriented treatment of the subject.", "citation_count": "5", "reference_count": "19,760", "date": "1999", "authors": ["Ricardo A. Baeza-Yates", "Berthier Ribeiro-Neto"], "related_topics": ["Human\u2013computer information retrieval", "Information science", "Electronically stored information", "Adversarial information retrieval", "The Internet", "Okapi BM25", "Subject (documents)", "World Wide Web", "Multimedia", "Perspective (graphical)", "Information retrieval", "Computer science"]}
{"id": "1992419399", "references": ["2138745909", "2095897464", "2152150600", "2049633694", "1639032689", "1971784203", "2581275558", "1497256448", "2912565176", "2133671888"], "title": "Data clustering: a review", "abstract": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.", "citation_count": "191", "reference_count": "18,126", "date": "1999", "authors": ["A. K. Jain", "M. N. Murty", "P. J. Flynn"], "related_topics": ["Cluster analysis", "Correlation clustering", "Single-linkage clustering", "Conceptual clustering", "Constrained clustering", "Consensus clustering", "Brown clustering", "Data stream clustering", "Information retrieval", "Data mining", "Computer science"]}
{"id": "2110485445", "references": ["2154642048", "2046432185", "1652505363", "3036751298", "2173629880", "2122988375", "2118373646", "2094249282", "2170716495", "2016589492"], "title": "Finding Structure in Time", "abstract": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.", "citation_count": "39", "reference_count": "12,895", "date": "1990", "authors": ["Jeffrey L. Elman"], "related_topics": ["Task (project management)", "Context (language use)", "Semantics", "Connectionism", "Set (psychology)", "Artificial grammar learning", "Structure (mathematical logic)", "Prediction in language comprehension", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2914484425", "references": ["2158899491", "2163922914", "2022508996", "2194775991", "1677182931", "1903029394", "2168231600", "1836465849", "2072128103"], "title": "Efficient BackProp", "abstract": "", "citation_count": "0", "reference_count": "4,271", "date": "1998", "authors": ["Yann LeCun", "L\u00e9on Bottou", "Genevieve B. Orr", "Klaus-Robert M\u00fcller"], "related_topics": ["Computer science"]}
{"id": "1575350781", "references": ["1964564149", "2083200599", "2090683636", "2010269868", "2010542899", "1978513924", "1521571223", "2294265735", "1480928214", "1843937266"], "title": "MPI: A Message-Passing Interface Standard", "abstract": "The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.", "citation_count": "21", "reference_count": "4,601", "date": "1994", "authors": ["Message P Forum"], "related_topics": ["Message passing", "Message Passing Interface", "Interface (Java)", "Standards organization", "MPICH", "Set (abstract data type)", "World Wide Web", "Computer science", "iWarp", "Message passing interface standard"]}
{"id": "2096175520", "references": ["2097333193", "2099111195", "2099345940", "2160842254", "1976241232", "2006969979", "2167434254", "2049633694", "2121227244", "1597533204"], "title": "A maximum entropy approach to natural language processing", "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.", "citation_count": "25", "reference_count": "4,846", "date": "1996", "authors": ["Adam L. Berger", "Vincent J. Della Pietra", "Stephen A. Della Pietra"], "related_topics": ["Maximum-entropy Markov model", "Principle of maximum entropy", "Statistical model", "Generalized iterative scaling", "Algorithm", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "1631260214", "references": ["1549285799", "1904457459", "2158195707", "1528470941", "2594610113", "2097978681", "1797288984", "2121227244", "2127836646", "2100506586"], "title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "abstract": "", "citation_count": "19", "reference_count": "5,423", "date": "2002", "authors": ["Andreas Stolcke"], "related_topics": ["Modeling language", "High-level programming language", "Computer science", "Pivot language", "Language model", "Programming language", "Natural language processing", "Artificial intelligence", "Inversion transduction grammars", "Machine translation system", "Speech transcription", "System combination"]}
{"id": "2322002063", "references": ["2155653793", "2119821739", "2076063813", "2002016471", "2148138104", "1570963478", "1873332500", "2144499799", "1498436455"], "title": "Principles of neurodynamics", "abstract": "", "citation_count": "0", "reference_count": "2,747", "date": "1962", "authors": ["A. A. Mullin", "Frank Rosenblatt"], "related_topics": ["Computer science"]}
{"id": "2153663612", "references": ["2158940042", "2132680427", "2151693816", "2097323375", "2131686571", "2079724595", "2160547390", "2113945798", "2078204800", "2146842127"], "title": "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries", "abstract": "We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods", "citation_count": "41", "reference_count": "5,601", "date": "2006", "authors": ["M. Elad", "M. Aharon"], "related_topics": ["Non-local means", "Video denoising", "Feature detection (computer vision)", "Image processing", "K-SVD", "Image quality", "Sparse approximation", "Matching pursuit", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2567948266", "references": ["2117853077", "581152777", "2024476015", "1991278573", "2049633694", "1580495158"], "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "abstract": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.", "citation_count": "6", "reference_count": "3,202", "date": "1998", "authors": ["Radford M. Neal", "Geoffrey E. Hinton"], "related_topics": ["Expectation\u2013maximization algorithm", "Conditional probability distribution", "Function (mathematics)", "Range (statistics)", "Convergence (routing)", "Standard algorithms", "Distribution (mathematics)", "Algorithm", "Energy (signal processing)", "Mathematical optimization", "Computer science"]}
{"id": "2131686571", "references": ["1997063559", "2105464873", "2116013899", "2295936755", "2149760002", "2133665775", "2113945798", "2121927366", "2116064496", "2108384452"], "title": "Fields of Experts: a framework for learning image priors", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.", "citation_count": "25", "reference_count": "1,180", "date": "2005", "authors": ["S. Roth", "M.J. Black"], "related_topics": ["Markov random field", "Approximate inference", "Inpainting", "Machine vision", "Field (computer science)", "Pixel", "Markov process", "Prior probability", "Machine learning", "Pattern recognition", "Iterative reconstruction", "Computer science", "Artificial intelligence"]}
{"id": "2124914669", "references": ["2138448681", "1813659000", "2147152072", "2140124448", "145818128", "1612003148", "2116064496", "1880262756", "2109720450", "1934021597"], "title": "Exponential Family Harmoniums with an Application to Information Retrieval", "abstract": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.", "citation_count": "14", "reference_count": "573", "date": "2004", "authors": ["Max Welling", "Michal Rosen-zvi", "Geoffrey E. Hinton"], "related_topics": ["Exponential random graph models", "Divergence-from-randomness model", "Graphical model", "Probabilistic latent semantic analysis", "Exponential family", "Posterior probability", "Statistical model", "Document retrieval", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2057175746", "references": ["2119821739", "2146766088", "2310919327", "2124087378", "2123977795", "2117812871", "2038952578", "2101522199", "2138451337", "2124386111"], "title": "Shape matching and object recognition using shape contexts", "abstract": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.", "citation_count": "58", "reference_count": "8,348", "date": "2002", "authors": ["S. Belongie", "J. Malik", "J. Puzicha"], "related_topics": ["Shape analysis (digital geometry)", "Heat kernel signature", "Shape context", "Similarity (geometry)", "Correspondence problem", "Feature extraction", "GLOH", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2159080219", "references": ["1593793857", "1997063559", "158727920", "2142901448", "2108309071", "2797148637", "2581275558", "2138162238", "2155322595", "1986808060"], "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "abstract": "From the Publisher:\r\nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.\r\nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.", "citation_count": "235", "reference_count": "24,370", "date": "1988", "authors": ["Judea Pearl"], "related_topics": ["Reasoning system", "Intelligent decision support system", "Probabilistic logic network", "Probabilistic argumentation", "Non-monotonic logic", "Probabilistic logic", "Uncertain inference", "Decision support system", "Artificial intelligence", "Computer science", "Machine learning"]}
{"id": "2159737176", "references": ["2156909104", "2119821739", "2310919327", "1512098439", "1604938182", "2147800946", "2148603752", "2087347434", "2140095548", "2151040995"], "title": "Training Invariant Support Vector Machines", "abstract": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.", "citation_count": "38", "reference_count": "775", "date": "2002", "authors": ["Dennis Decoste", "Bernhard Sch\u00f6lkopf"], "related_topics": ["Sequential minimal optimization", "Structured support vector machine", "Least squares support vector machine", "MNIST database", "Relevance vector machine", "Support vector machine", "Contextual image classification", "Machine learning", "Pattern recognition", "Invariant (physics)", "Computer science", "Artificial intelligence"]}
{"id": "2153635508", "references": ["2104978738", "2172000360", "2132870739", "2119821739", "1576520375", "2124351082", "2109943925", "1512098439", "2148603752", "2087347434"], "title": "LIBSVM: A library for support vector machines", "abstract": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.", "citation_count": "52", "reference_count": "43,942", "date": "2011", "authors": ["Chih-Chung Chang", "Chih-Jen Lin"], "related_topics": ["Sequential minimal optimization", "Structured support vector machine", "Support vector machine", "Relevance vector machine", "Multiclass classification", "Radial basis function kernel", "Graph kernel", "String kernel", "Machine learning", "Computer science"]}
{"id": "2147800946", "references": ["169539560", "56903235", "2154642048", "1965770722", "19621276", "2606594511", "2165758113", "2101926813", "2116360511", "2157475639"], "title": "Backpropagation applied to handwritten zip code recognition", "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.", "citation_count": "14", "reference_count": "8,794", "date": "1989", "authors": ["Y. LeCun", "B. Boser", "J. S. Denker", "D. Henderson", "R. E. Howard", "W. Hubbard", "L. D. Jackel"], "related_topics": ["Backpropagation", "Character (computing)", "Pattern recognition", "Domain (software engineering)", "Computer science", "Image (mathematics)", "Task (project management)", "Artificial intelligence", "Postal service", "Zip code"]}
{"id": "2613634265", "references": ["2136922672", "2119821739", "2001141328", "2110798204", "2310919327", "2057175746", "2053186076", "2148603752", "2116064496", "2140095548"], "title": "Scaling learning algorithms towards AI", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.", "citation_count": "47", "reference_count": "1,419", "date": "2007", "authors": ["Yoshua Bengio", "Yann Lecun"], "related_topics": ["Kernel method", "Kernel (statistics)", "Nonlinear dimensionality reduction", "Intelligent control", "Simple (abstract algebra)", "Curse of dimensionality", "Machine learning", "Invariant (computer science)", "Computer science", "Layer (object-oriented design)", "Algorithm", "Artificial intelligence"]}
{"id": "1993845689", "references": ["1533169541", "94647076", "2044875682", "2177040213", "2740373864"], "title": "The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks", "abstract": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.", "citation_count": "5", "reference_count": "1,284", "date": "1995", "authors": ["Geoffrey E. Hinton", "Peter Dayan", "Brendan J. Frey", "Radford M. Neal"], "related_topics": ["Helmholtz machine", "Wake-sleep algorithm", "Artificial neural network", "Layer (object-oriented design)", "Pattern recognition (psychology)", "Connectionism", "Stochastic process", "Pattern recognition", "Representation (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "2109779438", "references": ["2154642048", "2127385318", "1652505363", "50076749", "19621276", "2169163929", "2160208155", "3121126077", "2160699933"], "title": "The Cascade-Correlation Learning Architecture", "abstract": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.", "citation_count": "12", "reference_count": "4,321", "date": "1989", "authors": ["Scott E. Fahlman", "Christian Lebiere"], "related_topics": ["Network topology", "Network simulation", "Artificial neural network", "Topology (electrical circuits)", "Artificial intelligence", "Computer science", "Structure (mathematical logic)", "Detector", "Training set"]}
{"id": "2103626435", "references": ["2100677568", "2154642048", "1652505363", "2137983211", "1569296262", "2154952480", "2159047538", "1583833196", "2178806388"], "title": "Practical Issues in Temporal Difference Learning", "abstract": "This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(\u03bb) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(\u03bb) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.", "citation_count": "28", "reference_count": "1,569", "date": "1992", "authors": ["Gerald Tesauro"], "related_topics": ["Temporal difference learning", "Outcome (game theory)", "Context (language use)", "Artificial neural network", "Connectionism", "Artificial intelligence", "Machine learning", "Task (project management)", "Computer science", "Zero-knowledge proof", "Perspective (graphical)"]}
{"id": "2167967601", "references": ["2151693816", "2108263314", "2075887074", "1678356000", "2109405055", "2135046866", "2091886411", "2504871398", "1498436455", "3124955340"], "title": "Convex Neural Networks", "abstract": "Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.", "citation_count": "16", "reference_count": "190", "date": "2005", "authors": ["Yoshua Bengio", "Nicolas L. Roux", "Pascal Vincent", "Olivier Delalleau", "Patrice Marcotte"], "related_topics": ["Deep learning", "Convex optimization", "Types of artificial neural networks", "Artificial neural network", "Recurrent neural network", "Competitive learning", "Feedforward neural network", "Time delay neural network", "Artificial intelligence", "Mathematical optimization", "Mathematics"]}
{"id": "2125569215", "references": ["2154455818", "2119821739", "2001141328", "2139823104", "2160167256", "1604938182", "2053186076", "2087347434", "2140095548", "3017143921"], "title": "The Curse of Highly Variable Functions for Local Kernel Machines", "abstract": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.", "citation_count": "19", "reference_count": "224", "date": "2005", "authors": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas L. Roux"], "related_topics": ["Semi-supervised learning", "Kernel method", "Kernel embedding of distributions", "Radial basis function kernel", "Unsupervised learning", "Polynomial kernel", "Curse of dimensionality", "Tree kernel", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2102409316", "references": ["2063089147", "2151907713", "1578739277", "2110553242", "2078626246", "2176028050"], "title": "Autoencoders, Minimum Description Length and Helmholtz Free Energy", "abstract": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.", "citation_count": "6", "reference_count": "1,061", "date": "1993", "authors": ["Geoffrey E. Hinton", "Richard S. Zemel"], "related_topics": ["Autoencoder", "Minimum description length", "Upper and lower bounds", "Code (cryptography)", "Boltzmann distribution", "Helmholtz free energy", "Lyapunov function", "Set (abstract data type)", "Algorithm", "Theoretical computer science", "Mathematics"]}
{"id": "2105464873", "references": ["2133069808", "2132984323", "2145889472", "3022628558", "1536929369", "2145012779", "2167034998", "2107790757", "2108384452", "2099741732"], "title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1 ?", "abstract": "The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.", "citation_count": "38", "reference_count": "4,209", "date": "1997", "authors": ["Bruno A. Olshausen", "David J. Field"], "related_topics": ["Basis function", "Efficient coding hypothesis", "Neural coding", "K-SVD", "Linear independence", "Gabor wavelet", "Function (mathematics)", "Wavelet transform", "Algorithm", "Computer science", "Communication"]}
{"id": "1802356529", "references": ["2154642048", "2145889472", "1652505363", "2151693816", "2105464873", "2146474141", "2078204800", "2116064496", "2108384452", "2099741732"], "title": "Energy-based models for sparse overcomplete representations", "abstract": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.", "citation_count": "31", "reference_count": "206", "date": "2003", "authors": ["Yee Whye Teh", "Max Welling", "Simon Osindero", "Geoffrey E. Hinton"], "related_topics": ["Independent component analysis", "Independence (probability theory)", "Conditional independence", "Blind signal separation", "Probability distribution", "Density estimation", "Free parameter", "Contrast (statistics)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "11828546", "references": ["2132984323", "2145889472", "2105464873", "2109863423", "2113945798", "2134929491", "2053691921", "2137234026", "2127006916", "2103504761"], "title": "4.7 \u2013 Statistical Modeling of Photographic Images", "abstract": "", "citation_count": "61", "reference_count": "118", "date": "2005", "authors": ["Eero P. Simoncelli"], "related_topics": ["Statistical model", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2075187489", "references": ["2042422091", "43284170", "2110208125", "1993303421", "1991233288", "2096519870", "1976738367", "1907121963", "1603661052", "2003739479"], "title": "The Cost of Cortical Computation", "abstract": "Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.", "citation_count": "37", "reference_count": "1,164", "date": "2003", "authors": ["Peter Lennie"], "related_topics": ["Functional magnetic resonance imaging", "Cortex (anatomy)", "Brain mapping", "Electrophysiology", "Task (project management)", "Neuroscience", "Energy consumption", "Constraint (information theory)", "Computation", "Biology"]}
{"id": "1902027874", "references": ["1993845689", "2145889472", "1996355918", "1956559956", "2049633694", "2156406284", "2180838288", "2108384452", "1983578042", "2138451337"], "title": "Learning the parts of objects by non-negative matrix factorization", "abstract": "Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.", "citation_count": "23", "reference_count": "12,805", "date": "1999", "authors": ["Daniel D. Lee", "H. Sebastian Seung"], "related_topics": ["Non-negative matrix factorization", "Matrix decomposition", "Document-term matrix", "Factorization", "Representation (mathematics)", "Sign (mathematics)", "Cognitive neuroscience of visual object recognition", "Vector quantization", "Pattern recognition", "Physics", "Bioinformatics", "Artificial intelligence"]}
{"id": "3023071679", "references": ["2963012544", "2102113734", "2076063813", "2143612262", "1810943226", "2963674932", "1689711448", "2285660444", "2127141656"], "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "abstract": "In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it'.", "citation_count": "0", "reference_count": "3,037", "date": "2005", "authors": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "related_topics": ["Artificial neural network", "Perceptron", "Speech processing", "Benchmark (computing)", "Network architecture", "Speech recognition", "Computer science", "Recurrent neural nets"]}
{"id": "2143612262", "references": ["2155273149", "3023071679", "2144499799", "2184045248", "2108677974", "2160815625", "1828163288", "1993882792", "2064675550", "2127141656"], "title": "Speech recognition with deep recurrent neural networks", "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "citation_count": "26", "reference_count": "7,601", "date": "2013", "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "related_topics": ["Recurrent neural network", "Deep learning", "Time delay neural network", "TIMIT", "Context (language use)", "Connectionism", "Speech recognition", "Test set", "Machine learning", "Benchmark (computing)", "Computer science", "Artificial intelligence"]}
{"id": "44815768", "references": ["2099866409", "2136922672", "2096192494", "2124914669", "2116825644", "2029949252", "2116064496", "1665214252", "2293063825", "2158164339"], "title": "A Practical Guide to Training Restricted Boltzmann Machines", "abstract": "Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.", "citation_count": "25", "reference_count": "3,330", "date": "2012", "authors": ["Geoffrey E. Hinton"], "related_topics": ["Boltzmann machine", "Restricted Boltzmann machine", "Deep belief network", "Artificial intelligence", "Data type", "Computer science", "Set (abstract data type)", "Generative grammar", "Training (civil)", "Contrastive divergence"]}
{"id": "2108677974", "references": ["2150218618", "2170942820", "2103359087", "2129652681", "3161062409", "2114766824", "2054658115", "1498436455", "2064675550", "2127141656"], "title": "Practical Variational Inference for Neural Networks", "abstract": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.", "citation_count": "25", "reference_count": "909", "date": "2011", "authors": ["Alex Graves"], "related_topics": ["Stochastic neural network", "Recurrent neural network", "Artificial neural network", "Minimum description length", "Bayesian inference", "TIMIT", "Inference", "Variational method", "Artificial intelligence", "Mathematics"]}
{"id": "1554663460", "references": ["1746819321", "1570448133", "2140190241", "2097998348", "2139212933", "1810943226", "2194775991", "2117812871", "1964357740"], "title": "Neural networks for pattern recognition", "abstract": "From the Publisher:\r\nThis is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.", "citation_count": "0", "reference_count": "33,991", "date": "1995", "authors": ["Christopher M. Bishop"], "related_topics": ["Time delay neural network", "Types of artificial neural networks", "Feature (machine learning)", "Recurrent neural network", "Artificial neural network", "Neural gas", "Feedforward neural network", "Rectifier (neural networks)", "Artificial intelligence", "Machine learning", "Pattern recognition", "Computer science"]}
{"id": "2120861206", "references": ["2117130368", "1521626219", "179875071", "2138204974", "2131462252", "2158139315", "1423339008", "2096175520", "2132339004", "1631260214"], "title": "A fast and simple algorithm for training neural probabilistic language models", "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.\r\n\r\nWe propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well.\r\n\r\nWe demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.", "citation_count": "21", "reference_count": "497", "date": "2012", "authors": ["Andriy Mnih", "Yee W. Teh"], "related_topics": ["Probabilistic logic", "Language model", "Vocabulary", "Importance sampling", "Treebank", "SIMPLE algorithm", "Machine learning", "Noise (video)", "Normalization (statistics)", "Computer science", "Sentence completion tests", "Artificial intelligence"]}
{"id": "196214544", "references": ["2107878631", "2170942820", "179875071", "196761320", "2131462252", "2118706537", "2110575115", "1408639475", "2064675550", "1498436455"], "title": "Generating Text with Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.", "citation_count": "23", "reference_count": "1,441", "date": "2011", "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E. Hinton"], "related_topics": ["Recurrent neural network", "Language model", "Artificial intelligence", "Machine learning", "State vector", "Sequence", "Computer science", "Character (mathematics)", "Stochastic matrix"]}
{"id": "2118706537", "references": ["2103179919", "1943433854", "2058580716", "1543237449", "2143879519", "2094631910", "2045182040", "2166322089", "2134514463", "2016589492"], "title": "Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication", "abstract": "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.", "citation_count": "12", "reference_count": "2,662", "date": "2004", "authors": ["Herbert Jaeger", "Harald Haas"], "related_topics": ["Recurrent neural network", "Echo state network", "Reservoir computing", "Chaotic", "Benchmark (computing)", "Wireless", "Word error rate", "Energy (signal processing)", "Computer engineering", "Computer science"]}
{"id": "1606347560", "references": ["2061939373", "2154642048", "2110114082", "2006903949", "2254715784", "3005347330", "1408639475", "753012316", "2185726469", "2011301426"], "title": "Theano: new features and speed improvements", "abstract": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.", "citation_count": "10", "reference_count": "1,557", "date": "2012", "authors": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "related_topics": ["Theano", "Compiler", "Recurrent neural network", "Linear algebra", "Programming language", "Theoretical computer science", "Computation", "Implementation", "Mathematics"]}
{"id": "2122585011", "references": ["2107878631", "1578856370", "2147568880", "3023071679", "2079735306", "2125838338", "2131774270", "2142069714", "2064675550", "2127141656"], "title": "A Novel Connectionist System for Unconstrained Handwriting Recognition", "abstract": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.", "citation_count": "54", "reference_count": "1,884", "date": "2009", "authors": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "related_topics": ["Intelligent character recognition", "Handwriting recognition", "Language model", "Recurrent neural network", "Sequence labeling", "Handwriting", "Hidden Markov model", "Word recognition", "Artificial neural network", "Cursive", "Speech recognition", "Machine learning", "Image segmentation", "Computer science", "Artificial intelligence"]}
{"id": "2171865010", "references": ["2154642048", "2118706537", "1659842140", "2100495367", "2112090702", "1497256448", "2293063825", "2008620264", "2108384452", "2064675550"], "title": "Survey: Reservoir computing approaches to recurrent neural network training", "abstract": "Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ''brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ''map'' of it.", "citation_count": "144", "reference_count": "1,862", "date": "2009", "authors": ["Mantas Luko\u0161evi\u010dius", "Herbert Jaeger"], "related_topics": ["Reservoir computing", "Echo state network", "Recurrent neural network", "Liquid state machine", "Field (computer science)", "Artificial intelligence", "Machine learning", "Computer science", "Adaptation (computer science)", "State (computer science)", "Natural (music)"]}
{"id": "2120420045", "references": ["1533861849", "2113651538", "2146502635", "2914484425", "2156779765", "1598497354", "3118608800", "2137515395", "2130984546", "1568229137"], "title": "No more pesky learning rates", "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "citation_count": "21", "reference_count": "406", "date": "2013", "authors": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "related_topics": ["Online machine learning", "Stochastic gradient descent", "Artificial intelligence", "Machine learning", "Computer science", "Generalization error"]}
{"id": "19621276", "references": ["1526055535", "2160699933"], "title": "Improving the convergence of back-propagation learning with second-order methods", "abstract": "", "citation_count": "2", "reference_count": "507", "date": "1989", "authors": ["S. Becker", "Yann Lecun"], "related_topics": ["Convergence (routing)", "Order (business)", "Backpropagation", "Mathematical optimization", "Computer science"]}
{"id": "1994616650", "references": ["2225156818", "1491468723", "607505555", "2523246573", "6908809", "189596042", "2166851633"], "title": "A Stochastic Approximation Method", "abstract": "Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = \u03b8 of the equation M(x) = \u03b1, where a is a given constant. We give a method for making successive experiments at levels x1, x2, \u00b7\u00b7\u00b7 in such a way that xn will tend to \u03b8 in probability.", "citation_count": "0", "reference_count": "10,467", "date": "1951", "authors": ["Herbert Robbins", "Sutton Monro"], "related_topics": ["Minimax approximation algorithm", "Stochastic approximation", "Constant (mathematics)", "Continuous-time stochastic process", "Approximation error", "Stochastic optimization", "Stochastic gradient descent", "Expected value", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2251222643", "references": ["2251098065", "2143719855", "2250379827", "2146574666", "1970689298", "2103078213", "2109664771", "2156985047", "2132339004"], "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "abstract": "This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed.", "citation_count": "17", "reference_count": "142", "date": "2012", "authors": ["Holger Schwenk"], "related_topics": ["Phrase", "Example-based machine translation", "Machine translation", "Synchronous context-free grammar", "Phrase search", "Transfer-based machine translation", "Rule-based machine translation", "Evaluation of machine translation", "Machine translation software usability", "Natural language processing", "Speech recognition", "Computer science", "BLEU", "Artificial intelligence"]}
{"id": "2171928131", "references": ["2107878631", "2110485445", "179875071", "36903255", "2152808281", "2111305191", "2132339004", "2613634265", "1498436455", "2096072088"], "title": "Extensions of recurrent neural network language model", "abstract": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.", "citation_count": "20", "reference_count": "5,360", "date": "2011", "authors": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "related_topics": ["Backpropagation through time", "Recurrent neural network", "Language model", "Artificial neural network", "Backpropagation", "Speedup", "Computational complexity theory", "Feed forward", "Artificial intelligence", "Machine learning", "Probability distribution", "Computer science", "Recurrent neural nets"]}
{"id": "2006969979", "references": ["2138584836", "1489181569", "2048390999", "2097333193", "2154384676", "2129139611", "2117652747", "2049633694", "1575431606", "2121227244"], "title": "The mathematics of statistical machine translation: parameter estimation", "abstract": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.", "citation_count": "15", "reference_count": "5,827", "date": "1993", "authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "related_topics": ["Hybrid machine translation", "Example-based machine translation", "Synchronous context-free grammar", "Machine translation", "Interactive machine translation", "Transfer-based machine translation", "Computer-assisted translation", "Machine translation software usability", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2395935897", "references": ["2107878631", "2162911105", "2154642048", "2136922672", "2147768505", "2184045248", "1828163288", "2108563286", "2962968839", "2072128103"], "title": "Audio Chord Recognition with Recurrent Neural Networks.", "abstract": "In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.", "citation_count": "25", "reference_count": "185", "date": "2013", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "related_topics": ["Time delay neural network", "Chord (music)", "Recurrent neural network", "Artificial neural network", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence", "Chord recognition", "Efficient algorithm"]}
{"id": "2130942839", "references": ["2618530766", "2964308564", "2101105183", "179875071", "2147768505", "2310919327", "1753482797", "2157331557", "2132339004", "2064675550"], "title": "Sequence to Sequence Learning with Neural Networks", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "citation_count": "29", "reference_count": "12,919", "date": "2014", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "related_topics": ["Sequence learning", "Phrase", "Sentence", "Artificial neural network", "Test set", "Speech recognition", "Sequence", "Natural language processing", "Word order", "Term (logic)", "Computer science", "BLEU", "Artificial intelligence"]}
{"id": "1905522558", "references": ["2117278770", "2130450156", "2146574666", "2158195707", "2148861208", "2132001515", "2124807415", "2137387514", "2156985047", "1631260214"], "title": "Domain Adaptation via Pseudo In-Domain Data Selection", "abstract": "We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora -- 1% the size of the original -- can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.", "citation_count": "18", "reference_count": "535", "date": "2011", "authors": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao"], "related_topics": ["Rule-based machine translation", "Machine translation", "Domain (software engineering)", "Natural language processing", "Speech recognition", "Task (computing)", "Computer science", "Simple (abstract algebra)", "Artificial intelligence", "Data selection", "Domain adaptation"]}
{"id": "1828163288", "references": ["2170942820", "2147880316", "3023071679", "2079735306", "179875071", "2131774270", "2310919327", "2064675550", "2127141656", "196214544"], "title": "Sequence Transduction with Recurrent Neural Networks", "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.", "citation_count": "19", "reference_count": "1,057", "date": "2012", "authors": ["Alex Graves"], "related_topics": ["Sequence learning", "Recurrent neural network", "Transduction (machine learning)", "TIMIT", "Speech corpus", "Probabilistic logic", "Machine translation", "Speech recognition", "Invariant (mathematics)", "Computer science"]}
{"id": "2341457423", "references": ["2123301721", "2078861931", "2101105183", "222053410", "2146574666", "1489525520", "2087735403", "2124807415", "2115081467", "2895810819"], "title": "BLEU Deconstructed: Designing a Better MT Evaluation Metric", "abstract": "BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.", "citation_count": "29", "reference_count": "20", "date": "2013", "authors": ["Xingyi Song", "Trevor Cohn", "Lucia Specia"], "related_topics": ["Evaluation of machine translation", "Metric (mathematics)", "BLEU", "Machine translation", "De facto standard", "Discriminative model", "Natural language processing", "Sentence", "Theoretical computer science", "Computer science", "Artificial intelligence"]}
{"id": "2335728318", "references": ["2145094598", "2161969291", "2147768505", "2118858186", "2145607950", "2097018403", "2144161366", "2132424367", "1998042868", "2122410182"], "title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.", "citation_count": "28", "reference_count": "2,814", "date": "2011", "authors": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y. Ng"], "related_topics": ["Feature learning", "Reading (process)", "Pattern recognition", "Speech recognition", "Benchmark (computing)", "Computer science", "Task (project management)", "Natural (music)", "Variety (linguistics)", "Artificial intelligence", "Character recognition", "Research use"]}
{"id": "2131241448", "references": ["1746819321", "60686164", "2106411961", "2097998348", "2099201756", "2951665052", "1973333099", "3118608800", "2165599843", "2141125852"], "title": "Practical Bayesian Optimization of Machine Learning Algorithms", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "citation_count": "23", "reference_count": "3,926", "date": "2012", "authors": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "related_topics": ["Weighted Majority Algorithm", "Wake-sleep algorithm", "Bayesian optimization", "Hyperparameter optimization", "Support vector machine", "Convolutional neural network", "Latent Dirichlet allocation", "Hyperparameter", "Gaussian process", "Machine learning", "Artificial intelligence", "Computer science", "Algorithm", "Generalization error"]}
{"id": "189596042", "references": ["2136922672", "2134557905", "2567948266", "2096192494", "2124914669", "2116825644", "2100495367", "2159737176", "2116064496", "2613634265"], "title": "Deep Boltzmann machines", "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.", "citation_count": "21", "reference_count": "1,549", "date": "2009", "authors": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "related_topics": ["Boltzmann machine", "Restricted Boltzmann machine", "MNIST database", "Markov chain", "Hidden variable theory", "Boltzmann constant", "Cognitive neuroscience of visual object recognition", "Inference", "Theoretical computer science", "Computer science"]}
{"id": "2912934387", "references": ["1570448133", "2112076978", "1904365287", "2140190241", "2076063813", "1983364832", "2911964244", "2117812871", "2063978378", "2294059674"], "title": "Bagging predictors", "abstract": "", "citation_count": "0", "reference_count": "27,552", "date": "1996", "authors": ["Leo Breiman"], "related_topics": ["Computer science", "Bootstrap aggregating", "Classifier fusion", "Diversity measure", "Ensemble diversity", "Ensemble selection", "Multiple classifier", "Rotation forest"]}
{"id": "1508165687", "references": ["1980862600"], "title": "Statistical methods for speech recognition", "abstract": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method.", "citation_count": "1", "reference_count": "2,864", "date": "1997", "authors": ["Frederick Jelinek"], "related_topics": ["Cache language model", "Language model", "Principle of maximum entropy", "Hidden Markov model", "Acoustic model", "Information theory", "Decision tree", "Tree (data structure)", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1517947178", "references": ["2294072136", "2006969979", "2107551411", "2113106066", "2158164089", "2038698865", "2196555355"], "title": "Improved Alignment Models for Statistical Machine Translation", "abstract": "", "citation_count": "7", "reference_count": "878", "date": "1999", "authors": ["Franz Josef Och", "Christoph Tillmann", "Hermann Ney"], "related_topics": ["Interactive machine translation", "Machine translation", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1549285799", "references": ["1916559533", "2016243284", "2134800885", "2116316001", "2153653739", "2158195707", "2142069714", "2159981908", "2143017621", "1631260214"], "title": "Statistical Language Modeling using the CMU-Cambridge Toolkit", "abstract": "", "citation_count": "0", "reference_count": "971", "date": "1997", "authors": ["Philip Clarkson", "Ronald Rosenfeld"], "related_topics": ["Modeling language", "Language model", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "2116316001", "references": ["1916559533", "2101105183", "2117400858", "1574901103", "2092654472", "2006969979", "2049633694", "2135843243", "2101210369", "2122410182"], "title": "A Syntax-based Statistical Translation Model", "abstract": "We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.", "citation_count": "315", "reference_count": "1,044", "date": "2001", "authors": ["Kenji Yamada", "Kevin Knight"], "related_topics": ["Parse tree", "Word error rate", "Word (computer architecture)", "String (computer science)", "Syntax (programming languages)", "Syntax", "Time complexity", "Word order", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2101105183", "references": ["2732923061", "2001810881", "3037252522"], "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "citation_count": "3", "reference_count": "15,340", "date": "2002", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "related_topics": ["Evaluation of machine translation", "Interactive machine translation", "Hybrid machine translation", "Postediting", "Example-based machine translation", "Computer-assisted translation", "Machine translation software usability", "Machine translation", "Transfer-based machine translation", "Translation memory", "Synchronous context-free grammar", "Speech translation", "Arabic machine translation", "Text simplification", "Pivot language", "Natural language generation", "Machine learning", "Closed captioning", "Computer science", "Artificial intelligence", "BLEU"]}
{"id": "2161792612", "references": ["2129765547", "1916559533", "1517947178", "1549285799", "2116316001", "2006969979", "2049633694", "133045130", "2001810881", "2139403546"], "title": "A Phrase-Based,Joint Probability Model for Statistical Machine Translation", "abstract": "We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.", "citation_count": "15", "reference_count": "635", "date": "2002", "authors": ["Daniel Marcu", "Daniel Wong"], "related_topics": ["Machine translation", "Phrase", "Word (computer architecture)", "Joint probability distribution", "Speech recognition", "Natural language processing", "Joint (audio engineering)", "Computer science", "Artificial intelligence"]}
{"id": "1973923101", "references": ["3104029765", "2030750105", "2006969979", "1811404221", "1525706028", "1979102019", "136130055", "1575431606", "2038698865"], "title": "Improved statistical alignment models", "abstract": "In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.", "citation_count": "9", "reference_count": "1,323", "date": "2000", "authors": ["Franz Josef Och", "Hermann Ney"], "related_topics": ["Viterbi algorithm", "Machine translation", "Algorithm", "Machine learning", "Computer science", "Quality (business)", "Artificial intelligence", "Statistical alignment"]}
{"id": "1986543644", "references": ["1632114991", "2153439141", "1972573551", "2052449326", "2069912724", "2087165009", "2093647425", "2162455891", "2110882317", "1773803948"], "title": "Three Generative, Lexicalised Models for Statistical Parsing", "abstract": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).", "citation_count": "15", "reference_count": "1,113", "date": "1997", "authors": ["Michael Collins"], "related_topics": ["Parser combinator", "Top-down parsing", "Statistical parsing", "Parsing", "Data-oriented parsing", "Generative model", "Generative grammar", "Grammar", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2068730032", "references": ["2102605133", "2618530766", "2017691720", "2129305389", "2130306094", "2122146326", "2031489346", "2088049833", "2168356304", "2128715914"], "title": "Scalable Object Detection Using Deep Neural Networks", "abstract": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.", "citation_count": "16", "reference_count": "1,074", "date": "2014", "authors": ["Dumitru Erhan", "Christian Szegedy", "Alexander Toshev", "Dragomir Anguelov"], "related_topics": ["Time delay neural network", "Object detection", "Minimum bounding box", "Convolutional neural network", "3D single-object recognition", "Artificial neural network", "Object (computer science)", "Context (language use)", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2158388102", "references": ["2138584836", "1489181569", "1991133427", "2097333193", "2439178139", "2117652747", "2006969979", "2110190189", "201288405", "1513168562"], "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "abstract": "We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.", "citation_count": "48", "reference_count": "1,120", "date": "1997", "authors": ["Dekai Wu"], "related_topics": ["S-attributed grammar", "Parsing", "Language model", "Computational linguistics", "Rule-based machine translation", "Probabilistic logic", "Transduction (machine learning)", "Natural language processing", "Computer science", "Segmentation", "Artificial intelligence"]}
{"id": "2963911037", "references": ["2963446712", "2302255633", "2097117768", "1536680647", "2109255472", "2096733369", "2963037989", "2962835968", "2117539524"], "title": "Network In Network", "abstract": "Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "citation_count": "0", "reference_count": "4,847", "date": "2014", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "related_topics": ["Multilayer perceptron", "Activation function", "Artificial neural network", "MNIST database", "Overfitting", "Feature (machine learning)", "Linear filter", "Layer (object-oriented design)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2963542991", "references": ["2155893237", "639708223", "2097117768", "2194775991", "1903029394", "2962835968"], "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "abstract": "Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "citation_count": "0", "reference_count": "4,487", "date": "2014", "authors": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michael Mathieu", "Rob Fergus", "Yann LeCun"], "related_topics": ["Deep learning", "Feature (computer vision)", "Sliding window protocol", "Pattern recognition", "Task (project management)", "Bounding overwatch", "Object (computer science)", "Computer science", "State (computer science)", "Scale (ratio)", "Artificial intelligence"]}
{"id": "2016053056", "references": ["2102605133", "2618530766", "2022508996", "2131846894", "2108598243", "2161969291", "2310919327", "2062118960", "2168231600", "2963542991"], "title": "Large-Scale Video Classification with Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).", "citation_count": "28", "reference_count": "5,802", "date": "2014", "authors": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "related_topics": ["Convolutional neural network", "Feature (machine learning)", "Feature extraction", "Machine learning", "Generalization", "Class (biology)", "Computer science", "Scale (map)", "Artificial intelligence"]}
{"id": "2096733369", "references": ["2097117768", "2146502635", "2145287260", "1498436455", "2963911037", "1782590233", "1849277567", "2294059674", "2296073425", "2168231600"], "title": "FaceNet: A unified embedding for face recognition and clustering", "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.", "citation_count": "23", "reference_count": "7,775", "date": "2015", "authors": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "related_topics": ["Three-dimensional face recognition", "Face detection", "Object-class detection", "Facial recognition system", "Cluster analysis", "Feature vector", "Face (geometry)", "Similarity (geometry)", "Embedding", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2135046866", "references": ["2158940042", "2106706098", "2954064014", "2102201073", "2797583072", "2117897510", "1995945562", "2007069447", "191129667", "1594031697"], "title": "Regression Shrinkage and Selection via the Lasso", "abstract": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.", "citation_count": "19", "reference_count": "40,310", "date": "1996", "authors": ["Robert Tibshirani"], "related_topics": ["Lasso (statistics)", "Elastic net regularization", "Residual sum of squares", "Least-angle regression", "Linear model", "g-prior", "Design matrix", "Shrinkage estimator", "Applied mathematics", "Statistics", "Mathematics"]}
{"id": "2155541015", "references": ["2618530766", "1904365287", "2108598243", "2161969291", "2187089797", "2310919327", "2100495367", "2546302380", "2168356304", "1677409904"], "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.", "citation_count": "46", "reference_count": "4,386", "date": "2014", "authors": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "related_topics": ["Feature (machine learning)", "Cognitive neuroscience of visual object recognition", "Machine learning", "Set (psychology)", "Pattern recognition", "Concept learning", "Computer science", "Range (mathematics)", "Variety (cybernetics)", "Artificial intelligence", "Visual recognition"]}
{"id": "2109255472", "references": ["2102605133", "2618530766", "2097117768", "2108598243", "2161969291", "2151103935", "2153635508", "2168356304", "2962835968", "2117539524"], "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224  $\\times$      224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102   $\\times$       faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "citation_count": "45", "reference_count": "5,746", "date": "2015", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "related_topics": ["Convolutional neural network", "Contextual image classification", "Object detection", "Feature extraction", "Pooling", "Pascal (programming language)", "Pattern recognition", "Image resolution", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1663973292", "references": ["1496357020", "2117812871"], "title": "Pattern Recognition and Machine Learning", "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.", "citation_count": "2", "reference_count": "52,518", "date": "2006", "authors": ["Christopher M. Bishop"], "related_topics": ["Kernel method", "Kernel (statistics)", "Graphical model", "Variational Bayesian methods", "Approximate inference", "Artificial neural network", "Mixture model", "Linear model", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2963846996", "references": ["1632114991", "2618530766", "2155541015", "2963918774", "2095705004", "2964121744", "1849277567", "2250539671", "2064675550", "1840435438"], "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.", "citation_count": "29", "reference_count": "1,340", "date": "2018", "authors": ["Adina Williams", "Nikita Nangia", "Samuel R. Bowman"], "related_topics": ["Textual entailment", "Inference", "Sentence", "Natural language processing", "Task (project management)", "Resource (project management)", "Computer science", "Agreement", "Artificial intelligence"]}
{"id": "2962736243", "references": ["2963969878", "2963846996", "2962809918", "2963918774", "1544827683", "1933349210", "2963626623", "2963748441", "2413794162", "1840435438"], "title": "Annotation Artifacts in Natural Language Inference Data", "abstract": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.", "citation_count": "27", "reference_count": "512", "date": "2018", "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R. Bowman", "Noah A. Smith"], "related_topics": ["Inference", "Premise", "Sentence", "Negation", "Vagueness", "Natural language processing", "Annotation", "Task (project management)", "Protocol (object-oriented programming)", "Computer science", "Artificial intelligence"]}
{"id": "2963918774", "references": ["2158899491", "2108598243", "2130942839", "2145287260", "2194775991", "2153579005", "2131744502", "2964121744", "2250539671", "2064675550"], "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.", "citation_count": "37", "reference_count": "1,313", "date": "2017", "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Lo\u00efc Barrault", "Antoine Bordes"], "related_topics": ["Supervised learning", "Sentence", "Transfer of learning", "Natural language processing", "Word (computer architecture)", "Computer science", "Encoder", "Range (mathematics)", "Base (topology)", "Artificial intelligence", "Natural language inference"]}
{"id": "2525127255", "references": ["2963846996", "2923014074", "1970381522", "2185175083", "1840435438", "2996428491", "2965373594"], "title": "The PASCAL Recognising Textual Entailment Challenge", "abstract": "This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.", "citation_count": "0", "reference_count": "1,185", "date": "2006", "authors": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "related_topics": ["Textual entailment", "Pascal (programming language)", "Semantics", "Natural language processing", "Inference", "Computer science", "Artificial intelligence"]}
{"id": "2130158090", "references": ["2396767181", "2182572585", "2134061542", "1990524510", "2102065370", "137514618", "2525127255", "2912565176", "2115792525", "2002664886"], "title": "The Third PASCAL Recognizing Textual Entailment Challenge", "abstract": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.", "citation_count": "28", "reference_count": "767", "date": "2007", "authors": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan"], "related_topics": ["Textual entailment", "Pascal (programming language)", "Question answering", "Natural language processing", "Machine translation", "Information extraction", "Logical consequence", "Computer science", "Reading comprehension", "Artificial intelligence"]}
{"id": "3104033643", "references": ["1614298861", "2123442489", "2131744502", "2153579005", "2038721957", "2250539671", "2963626623", "2064675550", "1486649854", "1840435438"], "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "citation_count": "79", "reference_count": "610", "date": "2017", "authors": ["Daniel M. Cer", "Mona T. Diab", "Eneko Agirre", "I\u00f1igo Lopez-Gazpio", "Lucia Specia"], "related_topics": ["SemEval", "Semantic search", "Automatic summarization", "Machine translation", "Semantics", "Question answering", "Natural language understanding", "Natural language processing", "Dialog box", "Computer science", "Artificial intelligence"]}
{"id": "3127550471", "references": ["2963012544", "2923014074", "2963403868", "2525778437", "2963026768", "2970597249", "2964121744", "2963341956", "2113459411", "2965373594"], "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today\u2019s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task\u2019s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.", "citation_count": "60", "reference_count": "283", "date": "2020", "authors": ["Suchin Gururangan", "Ana Marasovi\u0107", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith"], "related_topics": ["Task (project management)", "Language model", "Machine learning", "Computer science", "Variety (linguistics)", "Domain (software engineering)", "Artificial intelligence"]}
{"id": "3100307207", "references": ["2618530766", "2962739339", "2117130368", "2963403868", "2153579005", "2250539671", "1880262756", "2963341956", "2132339004", "2117539524"], "title": "Experience Grounds Language", "abstract": "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle  tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful.  Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.", "citation_count": "181", "reference_count": "65", "date": "2020", "authors": ["Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "Joyce Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "Nicolas Pinto", "Joseph P. Turian"], "related_topics": ["Cognitive science", "Feature learning", "Social environment", "Field (computer science)", "Computer science", "Language research", "Language understanding"]}
{"id": "3105966348", "references": ["3037029945", "3092111011", "3146365155", "3098576111", "3064953855", "3101045333", "3138154797"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.", "citation_count": "0", "reference_count": "282", "date": "2020", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "related_topics": ["Transformer (machine learning model)", "Language model", "Natural language understanding", "Inference", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2996035354", "references": ["1614298861", "2963684088", "2962739339", "2158899491", "2251939518", "2963403868", "2964121744", "2099471712", "2250539671", "2963341956"], "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "abstract": "While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.", "citation_count": "48", "reference_count": "389", "date": "2020", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "related_topics": ["Security token", "Language model", "Discriminative model", "Feature learning", "Transformer (machine learning model)", "Speech recognition", "Encoder", "Computer science", "Natural language understanding"]}
{"id": "3100345210", "references": ["3109684201", "3122924117", "3128741952", "3095121901", "3103215803", "3124635886", "3106428938"], "title": "Supervised Contrastive Learning", "abstract": "Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.", "citation_count": "0", "reference_count": "181", "date": "2020", "authors": ["Prannay Khosla", "Piotr Teterwak", "Chen Wang", "Aaron Sarna", "Yonglong Tian", "Phillip Isola", "Aaron Maschinot", "Ce Liu", "Dilip Krishnan"], "related_topics": ["Supervised learning", "Cross entropy", "Hyperparameter", "Leverage (statistics)", "Robustness (computer science)", "Contextual image classification", "Machine learning", "Calibration (statistics)", "Embedding", "Computer science", "Artificial intelligence"]}
{"id": "2990704537", "references": ["3127550471", "3100307207", "3007332492", "3098824823", "2965373594", "2970986510", "2980282514", "3098903812"], "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.", "citation_count": "0", "reference_count": "377", "date": "2019", "authors": ["Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "related_topics": ["General-purpose language", "Benchmark (computing)", "Transfer of learning", "Software engineering", "Computer science", "Set (psychology)", "Software", "Metric (mathematics)"]}
{"id": "3099342932", "references": ["1975879668", "2125980212", "3024622987", "2164777277", "2970597249", "3104186312", "3099077829", "2963626623", "2963341956", "2965373594"], "title": "WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets", "abstract": "In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.", "citation_count": "12", "reference_count": "83", "date": "2020", "authors": ["Dat Quoc Nguyen", "Thanh Vu", "Afshin Rahimi", "Mai Hoang Dao", "Long Doan"], "related_topics": ["Task (project management)", "Language model", "F1 score", "Identification (information)", "Natural language processing", "Construct (philosophy)", "Computer science", "Baseline (configuration management)", "Artificial intelligence", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "2525778437", "references": ["2121879602", "2964121744", "1968594024", "2251743902"], "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "citation_count": "4", "reference_count": "4,725", "date": "2016", "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "\u0141ukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "related_topics": ["Example-based machine translation", "Rule-based machine translation", "Transfer-based machine translation", "Machine translation", "Deep learning", "Sentence", "Beam search", "Phrase", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2963756346", "references": ["2102605133", "2618530766", "1614298861", "2964308564", "2108598243", "2130942839", "2194775991", "2250539671", "2962835968", "1836465849"], "title": "Learned in translation: contextualized word vectors", "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "citation_count": "64", "reference_count": "655", "date": "2017", "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher"], "related_topics": ["Machine translation", "Sentiment analysis", "Context (language use)", "Question answering", "Word (computer architecture)", "Natural language processing", "Initialization", "Character (computing)", "Computer science", "Artificial intelligence"]}
{"id": "2962784628", "references": ["2964308564", "1902237438", "2130942839", "1815076433", "6908809", "2100664567", "1753482797", "2251012068", "2157331557", "2124807415"], "title": "Neural Machine Translation of Rare Words with Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.", "citation_count": "34", "reference_count": "3,854", "date": "2016", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "related_topics": ["Machine translation", "Text segmentation", "Vocabulary", "Natural language processing", "Transliteration", "Data compression", "Computer science", "Transformer (machine learning model)", "BLEU", "Artificial intelligence"]}
{"id": "2963026768", "references": ["2962739339", "2165698076", "2963446712", "2155541015", "2194775991", "2153579005", "1903029394", "1948751323", "2062118960", "1836465849"], "title": "Universal Language Model Fine-tuning for Text Classification", "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.", "citation_count": "49", "reference_count": "1,951", "date": "2018", "authors": ["Jeremy Howard", "Sebastian Ruder"], "related_topics": ["Inductive transfer", "Language model", "Transfer of learning", "Machine learning", "Computer science", "Task (project management)", "Code (cryptography)", "Key (cryptography)", "Universal language", "Artificial intelligence"]}
{"id": "2899771611", "references": ["2754835109", "1585773866", "1579027943"], "title": "Automatic differentiation in PyTorch", "abstract": "", "citation_count": "3", "reference_count": "7,688", "date": "2017", "authors": ["Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer"], "related_topics": ["Automatic differentiation", "Computer science", "Pattern recognition", "Artificial intelligence"]}
{"id": "1840435438", "references": ["2251939518", "2097606805", "6908809", "2154359981", "2095705004", "2081580037", "2250539671", "2584341106", "2185175083", "2064675550"], "title": "A large annotated corpus for learning natural language inference", "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "citation_count": "32", "reference_count": "1,988", "date": "2015", "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "related_topics": ["Textual entailment", "Inference", "Natural language", "Sentence", "Logical consequence", "Natural language processing", "Closed captioning", "Artificial neural network", "Machine learning", "Computer science", "Task (project management)", "Artificial intelligence"]}
{"id": "2031489346", "references": ["2104974755", "3097096317", "2110764733", "2161969291", "2151103935", "1576445103", "2038721957", "2162915993", "1565746575", "2131846894"], "title": "The Pascal Visual Object Classes (VOC) Challenge", "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.\r\n\r\nThis paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "citation_count": "57", "reference_count": "11,341", "date": "2010", "authors": ["Mark Everingham", "Luc Gool", "Christopher K. Williams", "John Winn", "Andrew Zisserman"], "related_topics": ["Object detection", "Cognitive neuroscience of visual object recognition", "Pascal (programming language)", "Natural language processing", "Artificial intelligence", "Computer science", "Image processing", "Annotation", "Object category recognition", "Semantic image segmentation", "Statistical analysis"]}
{"id": "2088049833", "references": ["3097096317", "2163352848", "2161969291", "2151103935", "2031489346", "2110158442", "2164598857", "2168356304", "2162915993", "2121947440"], "title": "Selective Search for Object Recognition", "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).", "citation_count": "37", "reference_count": "5,522", "date": "2013", "authors": ["J. R. Uijlings", "K. E. Sande", "T. Gevers", "A. W. Smeulders"], "related_topics": ["Beam search", "Brute-force search", "Cognitive neuroscience of visual object recognition", "Pattern recognition (psychology)", "Object (computer science)", "Segmentation", "Software", "Pattern recognition", "Image (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "1825604117", "references": ["1889268436", "2066134726", "2088049833", "2128017662", "2141362318", "1618905105", "21006490", "1897761818", "2094728533", "2131846894"], "title": "Open-vocabulary Object Retrieval", "abstract": "", "citation_count": "34", "reference_count": "81", "date": "2014", "authors": ["Sergio Guadarrama", "Erik Rodner", "Kate Saenko", "Ning Zhang", "Ryan Farrell", "Jeff Donahue", "Trevor Darrell"], "related_topics": ["Visual Word", "Computer science", "Natural language processing", "Deep-sky object", "Artificial intelligence", "Vocabulary Object"]}
{"id": "1872489089", "references": ["2335728318", "2618530766", "1904365287", "2119821739", "2101234009", "2310919327", "3118608800", "2116064496", "2168231600", "2025768430"], "title": "Pylearn2: a machine learning research library", "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.", "citation_count": "44", "reference_count": "307", "date": "2013", "authors": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "related_topics": ["Flexibility (engineering)", "Computer science", "Machine learning", "Order (business)", "Artificial intelligence"]}
{"id": "2147414309", "references": ["2618530766", "2253807446", "2155541015", "2536626143", "2310919327", "2546302380", "2168356304", "2162915993", "2098411764", "1498436455"], "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling", "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.", "citation_count": "29", "reference_count": "513", "date": "2014", "authors": ["Ning Zhang", "Manohar Paluri", "Marc'Aurelio Ranzato", "Trevor Darrell", "Lubomir Bourdev"], "related_topics": ["3D pose estimation", "Deep learning", "Context (language use)", "Minimum bounding box", "Artificial neural network", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Machine learning", "Expression (mathematics)", "Variation (game tree)", "Computer science", "Artificial intelligence"]}
{"id": "2962883796", "references": ["2618530766", "2075456404", "2155541015", "2108598243", "2146502635", "2078807908", "1566135517", "1511924373", "2135957164", "2157462866"], "title": "Recognizing Image Style.", "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.", "citation_count": "22", "reference_count": "396", "date": "2014", "authors": ["Sergey Karayev", "Matthew Trentacoste", "Helen Han", "Aseem Agarwala", "Trevor Darrell", "Aaron Hertzmann", "Holger Winnemoeller"], "related_topics": ["Style (sociolinguistics)", "Natural language processing", "Computer science", "Image (mathematics)", "Painting", "Artificial intelligence", "Learning methods", "Object Class"]}
{"id": "753012316", "references": ["2606594511"], "title": "Torch7: A Matlab-like Environment for Machine Learning", "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.", "citation_count": "1", "reference_count": "1,762", "date": "2011", "authors": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "related_topics": ["Scripting language", "CUDA", "Interface (Java)", "MATLAB", "Software", "Theano", "Computer science", "Flexibility (engineering)", "Machine learning", "Implementation", "Artificial intelligence"]}
{"id": "2164598857", "references": ["2217896605", "2155511848", "2124351082", "2159686933", "2128272608", "1975846642", "3146003712", "2115763357", "2101522199", "3124955340"], "title": "Rapid object detection using a boosted cascade of simple features", "abstract": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.", "citation_count": "18", "reference_count": "23,874", "date": "2001", "authors": ["P. Viola", "M. Jones"], "related_topics": ["Object detection", "Object-class detection", "Viola\u2013Jones object detection framework", "Haar-like features", "Face detection", "Image differencing", "Cascading classifiers", "Image processing", "Feature extraction", "Contextual image classification", "Pedestrian detection", "Facial motion capture", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1647075334", "references": ["2032579724", "1594381468", "1992194142", "1944127002", "2022846624", "1989952069", "2151068726", "1581412826", "2034831897", "2149871164"], "title": "Potent and specific genetic interference by double-stranded RNA in Caenorhabditis elegans", "abstract": "Experimental introduction of RNA into cells can be used in certain biological systems to interfere with the function of an endogenous gene. Such effects have been proposed to result from a simple antisense mechanism that depends on hybridization between the injected RNA and endogenous messenger RNA transcripts. RNA interference has been used in the nematode Caenorhabditis elegans to manipulate gene expression. Here we investigate the requirements for structure and delivery of the interfering RNA. To our surprise, we found that double-stranded RNA was substantially more effective at producing interference than was either strand individually. After injection into adult animals, purified single strands had at most a modest effect, whereas double-stranded mixtures caused potent and specific interference. The effects of this interference were evident in both the injected animals and their progeny. Only a few molecules of injected double-stranded RNA were required per affected cell, arguing against stochiometric interference with endogenous mRNA and suggesting that there could be a catalytic or amplification component in the interference process.", "citation_count": "26", "reference_count": "24,546", "date": "1998", "authors": ["Andrew Fire", "SiQun Xu", "Mary K. Montgomery", "Steven A. Kostas", "Samuel E. Driver", "Craig C. Mello"], "related_topics": ["RNA silencing", "RNA", "Antisense RNA", "DNA-directed RNA interference", "RNA interference", "RNA-induced transcriptional silencing", "RNA activation", "Small interfering RNA", "Cell biology", "Molecular biology", "Biology"]}
{"id": "2119823327", "references": ["2145023731", "2032210760", "2153635508", "2111308925", "2121927366", "2141376824", "2093191240", "2135705692", "1490632837", "2025653905"], "title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "abstract": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.", "citation_count": "37", "reference_count": "2,776", "date": "2004", "authors": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "related_topics": ["Image texture", "Edge detection", "Brightness", "Ground truth", "Supervised learning", "Posterior probability", "Classifier (UML)", "Pattern recognition", "Computer vision", "Detector", "Computer science", "Artificial intelligence", "Boundary detection"]}
{"id": "1991848143", "references": ["2186428165", "2121601095", "2076063813", "2115689562", "2053186076", "2153791616", "2161160262", "2141125852", "1992419399"], "title": "Self-Organization and Associative Memory", "abstract": "1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References.", "citation_count": "0", "reference_count": "12,946", "date": "1984", "authors": ["Teuvo Kohonen"], "related_topics": ["Holographic associative memory", "Content-addressable memory", "Autoassociative memory", "Artificial neural network", "Memory model", "Perceptron", "Learning vector quantization", "Associative property", "Algorithm", "Computer science"]}
{"id": "2121927366", "references": ["2124592837", "2124731682", "1524408959", "2120838001", "2139643804", "2101933716", "2121947440", "1537519384", "2150117517", "2126326837"], "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "abstract": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.", "citation_count": "15", "reference_count": "6,606", "date": "2001", "authors": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "related_topics": ["Image segmentation", "Segmentation", "Ground truth", "Consistency (database systems)", "Pattern recognition", "Probability distribution", "Data mining", "Algorithm", "Measure (data warehouse)", "Image (mathematics)", "Computer science", "Database", "Statistics", "Artificial intelligence"]}
{"id": "2104095591", "references": ["2107198582", "1977699267", "2003370853", "1995756857", "2109863423", "1531060698", "2582614493", "1631253743", "2139762693", "2045798786"], "title": "Snakes : Active Contour Models", "abstract": "A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.", "citation_count": "23", "reference_count": "25,285", "date": "1988", "authors": ["Michael Kass", "Andrew P. Witkin", "Demetri Terzopoulos"], "related_topics": ["Active contour model", "Active shape model", "Match moving", "Mumford\u2013Shah functional", "Spline (mathematics)", "Level set method", "Computer vision", "Vector flow", "Mathematics", "Artificial intelligence", "Stereo matching"]}
{"id": "1578099820", "references": ["2053631808", "2901284226", "2027808858"], "title": "Spectral Graph Theory", "abstract": "Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.", "citation_count": "3", "reference_count": "5,980", "date": "1996", "authors": ["Fan R K Chung"], "related_topics": ["Integral graph", "Spectral graph theory", "Laplacian matrix", "Resistance distance", "Algebraic connectivity", "Isoperimetric inequality", "Sobolev inequality", "Random walk", "Combinatorics", "Computer science"]}
{"id": "1999192586", "references": ["2136922672", "2130325614", "2161969291", "2151103935", "3118608800", "2177274842", "2119605622", "1677409904", "2142194269", "2124386111"], "title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "abstract": "Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\u223cwzou/", "citation_count": "47", "reference_count": "1,239", "date": "2011", "authors": ["Quoc V. Le", "Will Y. Zou", "Serena Y. Yeung", "Andrew Y. Ng"], "related_topics": ["Feature learning", "Unsupervised learning", "Feature extraction", "Deep learning", "Contextual image classification", "Scale-invariant feature transform", "Subspace topology", "Pattern recognition", "Machine learning", "Invariant (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "2962820688", "references": ["2963574257", "2964311892", "2016053056", "2963446712", "2152839228", "2095705004", "2162741153", "2607333215", "2294059674", "2308045930"], "title": "Convolutional neural networks applied to house numbers digit classification", "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.", "citation_count": "0", "reference_count": "498", "date": "2012", "authors": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"], "related_topics": ["Convolutional neural network", "Feature learning", "Artificial neural network", "Pooling", "Feature extraction", "Convolutional code", "Pattern recognition (psychology)", "Pattern recognition", "Artificial intelligence", "Computer science"]}
{"id": "1554944419", "references": ["2122825543", "2131975293", "2164278908", "2140190241", "2087681821", "2109574129", "2117756735", "2063978378", "2179438025"], "title": "The elements of statistical learning : data mining, inference,and prediction", "abstract": "During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.\n\nThis major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.", "citation_count": "0", "reference_count": "28,721", "date": "2005", "authors": ["Trevor Hastie", "Robert J. Tibshirani", "Jerome Friedman"], "related_topics": ["Least-angle regression", "Lasso (statistics)", "Ensemble learning", "Unsupervised learning", "Gradient boosting", "Supervised learning", "Generalized additive model", "Random forest", "Data mining", "Mathematics"]}
{"id": "2147860648", "references": ["2136922672", "2117130368", "2118585731", "2110798204", "2310919327", "3118608800", "2100495367", "2546302380", "1548802052", "2072128103"], "title": "Tiled convolutional neural networks", "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.", "citation_count": "31", "reference_count": "368", "date": "2010", "authors": ["Jiquan Ngiam", "Zhenghao Chen", "Daniel Chia", "Pang W. Koh", "Quoc V. Le", "Andrew Y. Ng"], "related_topics": ["Convolutional neural network", "Artificial neural network", "Cognitive neuroscience of visual object recognition", "Convolution", "Invariant (mathematics)", "Theoretical computer science", "Invariant (physics)", "Pooling", "Computer science"]}
{"id": "2132914434", "references": ["1480376833", "1578099820", "2165874743", "1479807131", "2097308346", "2071949631", "2798707604", "2121947440", "2011832962", "2798909945"], "title": "A tutorial on spectral clustering", "abstract": "In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.", "citation_count": "72", "reference_count": "9,299", "date": "2007", "authors": ["Ulrike Luxburg"], "related_topics": ["Cluster analysis", "Correlation clustering", "Canopy clustering algorithm", "Fuzzy clustering", "CURE data clustering algorithm", "Biclustering", "Brown clustering", "DBSCAN", "Theoretical computer science", "Computer science"]}
{"id": "3035743198", "references": ["2618530766", "2963207607", "2963446712", "2097117768", "2194775991", "2183341477", "2964253222", "2962835968", "2964153729", "1836465849"], "title": "Adversarial Examples Improve Image Recognition", "abstract": "Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more parameters. Code and models will be made publicly available.", "citation_count": "49", "reference_count": "121", "date": "2020", "authors": ["Cihang Xie", "Mingxing Tan", "Boqing Gong", "Jiang Wang", "Alan L. Yuille", "Quoc V. Le"], "related_topics": ["Convolutional neural network", "Overfitting", "Supervised learning", "Robustness (computer science)", "Range (mathematics)", "Code (cryptography)", "Computer science", "Key (cryptography)", "Adversarial system", "Computer vision", "Artificial intelligence"]}
{"id": "2971149989", "references": ["3117548433", "3061525482", "3019371514", "3100971012"], "title": "How to Initialize your Network? Robust Initialization for WeightNorm &amp; ResNets", "abstract": "Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.", "citation_count": "0", "reference_count": "4", "date": "2019", "authors": ["Devansh Arpit", "V\u00edctor Campos", "Yoshua Bengio"], "related_topics": ["Initialization", "Robustness (computer science)", "Deep learning", "Residual", "Normalization (statistics)", "Algorithm", "Computer science", "Artificial intelligence", "Residual neural network"]}
{"id": "3129973872", "references": ["2963399829", "2604763608", "1901129140", "3137695714", "2732026016", "2964253222", "2963466845", "2963749936", "2296073425", "3149085914"], "title": "Teaching with Commentaries", "abstract": "Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process.", "citation_count": "28", "reference_count": "2", "date": "2021", "authors": ["Aniruddh Raghu", "Maithra Raghu", "Simon Kornblith", "David Duvenaud", "Geoffrey Hinton"], "related_topics": ["Metalearning", "Data science", "Process (engineering)", "Artificial neural network", "Task (project management)", "Computer science", "Scalability", "Salient", "Scope (project management)", "Training (civil)"]}
{"id": "2963855133", "references": ["1533861849", "2618530766", "2963420686", "639708223", "2194775991", "2183341477", "1903029394", "2962835968", "2560023338", "2963911037"], "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks", "abstract": "Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.", "citation_count": "22", "reference_count": "384", "date": "2019", "authors": ["Tong He", "Zhi Zhang", "Hang Zhang", "Zhongyue Zhang", "Junyuan Xie", "Mu Li"], "related_topics": ["Deep learning", "Convolutional neural network", "Contextual image classification", "Object detection", "Transfer of learning", "Segmentation", "Machine learning", "Artificial intelligence", "Categorization", "Computer science"]}
{"id": "2966415767", "references": ["2976016473", "2987875759", "3035687950", "2996501936", "2962369866", "3100859887", "3035160371", "3001197829"], "title": "Interpolation Consistency Training for Semi-supervised Learning", "abstract": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values.", "citation_count": "0", "reference_count": "179", "date": "2019", "authors": ["Vikas Verma", "Alex Lamb", "Juho Kannala", "Yoshua Bengio", "David Lopez-Paz"], "related_topics": ["Interpolation", "Semi-supervised learning", "Overfitting", "Artificial neural network", "Decision boundary", "Benchmark (computing)", "Regularization (mathematics)", "Machine learning", "Consistency (statistics)", "Computer science", "Artificial intelligence"]}
{"id": "2970902013", "references": ["3162734203", "3125645205", "3108796939", "3118146262", "3130223764", "3107669106", "3137863028", "3093382707", "3019676419"], "title": "On Adversarial Mixup Resynthesis", "abstract": "In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.", "citation_count": "0", "reference_count": "23", "date": "2019", "authors": ["Christopher Beckham", "Sina Honari", "Vikas Verma", "Alex M. Lamb", "Farnoosh Ghadiri", "R Devon Hjelm", "Yoshua Bengio", "Chris Pal"], "related_topics": ["Context (language use)", "Machine learning", "Class (computer programming)", "Function (engineering)", "Computer science", "Adversarial system", "Mixing (mathematics)", "Artificial intelligence", "Qualitative evidence"]}
{"id": "3098350627", "references": ["2963420686", "2963446712", "2531409750", "2108598243", "2101234009", "1901129140", "2099471712", "2155893237", "2011301426", "2899771611"], "title": "Fastai: A Layered API for Deep Learning", "abstract": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4\u20135 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.", "citation_count": "34", "reference_count": "245", "date": "2020", "authors": ["Jeremy Howard", "Sylvain Gugger"], "related_topics": ["Python (programming language)", "Callback", "Source lines of code", "Deep learning", "Multitier architecture", "Programming language", "Computer science", "Usability", "Data processing", "Dynamism", "Artificial intelligence"]}
{"id": "2964274690", "references": ["2963096987", "2963207607", "2963399829", "2302255633", "2073459066", "3137695714", "2194775991", "3118608800", "2136504847", "2964292098"], "title": "Joint Optimization Framework for Learning with Noisy Labels", "abstract": "Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.", "citation_count": "21", "reference_count": "250", "date": "2018", "authors": ["Daiki Tanaka", "Daiki Ikami", "Toshihiko Yamasaki", "Kiyoharu Aizawa"], "related_topics": ["Overfitting", "Artificial neural network", "Contextual image classification", "Pattern recognition", "Artificial intelligence", "Entropy (information theory)", "Computer science"]}
{"id": "1934019294", "references": ["1520377376", "2125838338", "2117400858", "2160842254", "1597379537", "1528056001", "2049633694", "1592796124", "1557074680", "2158873310"], "title": "Maximum Entropy Markov Models for Information Extraction and Segmentation", "abstract": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s.", "citation_count": "21", "reference_count": "1,914", "date": "2000", "authors": ["Andrew McCallum", "Dayne Freitag", "Fernando C. N. Pereira"], "related_topics": ["Maximum-entropy Markov model", "Variable-order Markov model", "Principle of maximum entropy", "Markov chain", "Hidden Markov model", "Markov model", "Markov property", "Markov process", "Markov kernel", "Conditional probability", "Probabilistic logic", "Viterbi algorithm", "Multinomial distribution", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2117400858", "references": ["1632114991", "1489181569", "2081687495", "2097333193", "2149706766", "2099247782", "2102381086", "2170381724", "2046224275", "1594031697"], "title": "Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging", "abstract": "Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.", "citation_count": "40", "reference_count": "2,315", "date": "1995", "authors": ["Eric Brill"], "related_topics": ["Deep linguistic processing", "Language identification", "Brill tagger", "Rule-based machine translation", "Coh-Metrix", "Natural language processing", "Field (computer science)", "Computer science", "Encoding (memory)", "Trigram tagger", "Artificial intelligence"]}
{"id": "2160842254", "references": ["1997063559", "2097333193", "2167837909", "107938046", "2049633694", "2096175520", "2035301451", "2089969354", "2121227244", "1594031697"], "title": "Inducing features of random fields", "abstract": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.", "citation_count": "20", "reference_count": "1,626", "date": "1997", "authors": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "related_topics": ["Random field", "Generalized iterative scaling", "Greedy algorithm", "Feature extraction", "Stochastic process", "Expectation\u2013maximization algorithm", "Cluster analysis", "Decision tree", "Iterative method", "Kullback\u2013Leibler divergence", "Empirical distribution function", "Principle of maximum entropy", "Theoretical computer science", "Computer science", "Training set"]}
{"id": "3021452258", "references": ["2147880316", "607505555", "2168029744", "2092654472", "2168020168", "2160218441", "2158847908", "2107890099", "2105842272"], "title": "Discriminative Reranking for Natural Language Parsing", "abstract": "", "citation_count": "0", "reference_count": "716", "date": "2000", "authors": ["Michael Collins"], "related_topics": ["Statistical parsing", "Data-oriented parsing", "Discriminative model", "Natural language processing", "Computer science", "Artificial intelligence", "Natural language parsing"]}
{"id": "1773803948", "references": ["1632114991", "2153439141", "1718065290", "2160842254", "2015042937", "2112861996", "2096175520", "2069912724", "2121227244", "2170381724"], "title": "A Maximum Entropy Model for Part-Of-Speech Tagging", "abstract": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems", "citation_count": "12", "reference_count": "2,392", "date": "1996", "authors": ["Adwait Ratnaparkhi"], "related_topics": ["Maximum-entropy Markov model", "Principle of maximum entropy", "Statistical model", "Part of speech", "Trigram tagger", "Natural language processing", "Consistency (database systems)", "Computer science", "State (computer science)", "Artificial intelligence", "Part-of-speech tagging"]}
{"id": "2009570821", "references": ["2162315106", "2147880316", "2168133698", "2153233077", "2140190241", "2110575115", "2138122982", "2145191876", "2158266063", "2045843097"], "title": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids", "abstract": "Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.", "citation_count": "0", "reference_count": "7,702", "date": "1998", "authors": ["Richard Durbin", "Sean Eddy", "Anders St\u00e6rmose Krogh", "Graeme Mitchison"], "related_topics": ["Probabilistic logic", "Probabilistic method", "Hidden Markov model", "Genomics", "Field (computer science)", "Bayesian probability", "Sequence analysis", "Machine learning", "Data mining", "Human genome", "Biology", "Artificial intelligence"]}
{"id": "3124955340", "references": ["2132511032", "2115781554", "2109826612", "2122005484", "2031910487", "2120520366", "2132827378", "1984194948", "2114839088", "2070277296"], "title": "A Decision Theoretic Generalization of On-Line Learning and an Application to Boosting", "abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.", "citation_count": "0", "reference_count": "27,437", "date": "2010", "authors": ["Y. Freund", "R. Schapire"], "related_topics": ["Boosting (machine learning)", "Generalization", "Bounded function", "Range (mathematics)", "Set (abstract data type)", "Finite set", "Multiplicative function", "Repeated game", "Theoretical computer science", "Computer science"]}
{"id": "2068017609", "references": ["2130851608", "2119966617", "1519443010", "2118996379", "1648417313", "2156202195", "1953828586", "2097089247", "1977182536", "1988842251"], "title": "Mitigating the paucity-of-data problem: exploring the effect of training corpus size on classifier performance for natural language processing", "abstract": "In this paper, we discuss experiments applying machine learning techniques to the task of confusion set disambiguation, using three orders of magnitude more training data than has previously been used for any disambiguation-in-string-context problem. In an attempt to determine when current learning methods will cease to benefit from additional training data, we analyze residual errors made by learners when issues of sparse data have been significantly mitigated. Finally, in the context of our results, we discuss possible directions for the empirical natural language research community.", "citation_count": "12", "reference_count": "132", "date": "2001", "authors": ["Michele Banko", "Eric Brill"], "related_topics": ["Online machine learning", "Language identification", "Semi-supervised learning", "Instance-based learning", "Algorithmic learning theory", "Natural language", "Learning curve", "Classifier (UML)", "Machine learning", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2166469100", "references": ["2111494971", "2137291015", "2607313294", "1980501707", "2140539590", "2099070536", "2150884987", "2055075080", "2170541567", "2124229187"], "title": "Effective Training of a Neural Network Character Classifier for Word Recognition", "abstract": "We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton Message Pad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed.", "citation_count": "13", "reference_count": "106", "date": "1996", "authors": ["Larry S. Yaeger", "Richard F. Lyon", "Brandyn J. Webb"], "related_topics": ["Intelligent character recognition", "Intelligent word recognition", "Word error rate", "Word recognition", "Time delay neural network", "Text segmentation", "Classifier (linguistics)", "Character (mathematics)", "Artificial neural network", "Speech recognition", "Pattern recognition", "Computer science", "Normalization (statistics)", "Image warping", "Artificial intelligence"]}
{"id": "2027197837", "references": ["2416739038", "2090270852", "1613359937", "2137983211", "2056099894", "1490039160", "2090248140", "1537887709", "1971735090"], "title": "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks", "abstract": "Abstract   We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.", "citation_count": "17", "reference_count": "2,324", "date": "1990", "authors": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "related_topics": ["Piecewise", "Differentiable function", "Activation function", "Function (mathematics)", "Artificial neural network", "Function space", "Sobolev space", "Domain (mathematical analysis)", "Applied mathematics", "Mathematical analysis", "Mathematics"]}
{"id": "51975515", "references": ["2107600630", "2227933188", "49742075", "2151513848", "2126727781", "2124776405", "2072181047", "1519534430", "2122827492", "2256679588"], "title": "An improved recognition module for the identification of handwritten digits", "abstract": "", "citation_count": "16", "reference_count": "12", "date": "1999", "authors": ["Anshu Sinha"], "related_topics": ["Intelligent character recognition", "Intelligent word recognition", "Document processing", "Optical character recognition", "Identification (information)", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2147345686", "references": ["2125838338", "2064838583", "2149597185", "2310919327", "2113292028", "183625566", "2142069714", "2148295954", "2077863651", "101240229"], "title": "An offline cursive handwritten word recognition system", "abstract": "This paper describes an offline cursive handwritten word recognition system that combines hidden Markov models (HMM) and neural networks (NN). Using a fast left-right slicing method, we generate a segmentation graph that describes all possible ways to segment a word into letters. The NN computes the observation probabilities for each letter hypothesis in the segmentation graph. Then, the HMM compute the likelihood for each word in the lexicon by summing the probabilities over all possible paths through the graph. We present the preprocessing and the recognition process as well as the training procedure for the NN-HMM hybrid system. Another recognition system based on discrete HMM is also presented for performance comparison. The latter is also used for bootstrapping the NN-HMM hybrid system. Recognition performances of the two recognition systems using two image databases of French isolated words are presented. This paper is one of the first publications using the IRONOFF database, and thus can be used as a reference for future work on this database.", "citation_count": "11", "reference_count": "32", "date": "2001", "authors": ["Yong Haur Tay", "P.M. Lallican", "M. Khalid", "C. Viard-Gaudin", "S. Kneer"], "related_topics": ["Intelligent word recognition", "Handwriting recognition", "Word recognition", "Hidden Markov model", "Graph (abstract data type)", "Cursive", "Image segmentation", "Artificial neural network", "Speech recognition", "Pattern recognition", "Lexicon", "Computer science", "Artificial intelligence"]}
{"id": "1677409904", "references": ["2124404372", "1980911747", "2151103935", "2012778485", "2172188317", "2128017662", "2177274842", "2145072179", "2164598857", "2124386111"], "title": "SURF: speeded up robust features", "abstract": "In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.\r\n\r\nThis is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.", "citation_count": "36", "reference_count": "19,564", "date": "2006", "authors": ["Herbert Bay", "Tinne Tuytelaars", "Luc Van Gool"], "related_topics": ["Scale-invariant feature transform", "GLOH", "Interest point detection", "Robustness (computer science)", "Hessian affine region detector", "Principal curvature-based region detector", "Image processing", "Implicit Shape Model", "Cognitive neuroscience of visual object recognition", "Computer vision", "Visual odometry", "Computer science", "Artificial intelligence"]}
{"id": "2019575783", "references": ["1570448133", "2143331230", "2140190241", "1510526001", "1602492977", "2104269704", "2159680539", "2101276256", "2133864802"], "title": "Classification by pairwise coupling", "abstract": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated data sets. Classifiers used include linear discriminants, nearest neighbors, adaptive nonlinear methods and the support vector machine.", "citation_count": "0", "reference_count": "2,215", "date": "1998", "authors": ["Trevor Hastie", "Robert Tibshirani"], "related_topics": ["Bradley\u2013Terry model", "Support vector machine", "Pairwise comparison", "Coupling (probability)", "Class (biology)", "Algorithm", "Mathematics", "Nonlinear methods", "Pairwise coupling", "Simulated data"]}
{"id": "2084812512", "references": ["2026131661", "2172000360", "2011430131", "2017337590", "2148143831", "2111072639", "2212660284", "1565746575"], "title": "UCI Repository of machine learning databases", "abstract": "", "citation_count": "0", "reference_count": "9,260", "date": "1998", "authors": ["C. L. Blake"], "related_topics": ["BrownBoost", "Computer science", "LPBoost", "Machine learning", "Artificial intelligence"]}
{"id": "2087347434", "references": ["2086472796", "2111494971", "1965770722", "2171277043", "1530699444", "2266946488", "2165758113", "3017143921", "2076118331", "2154579312"], "title": "A training algorithm for optimal margin classifiers", "abstract": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.", "citation_count": "20", "reference_count": "13,591", "date": "1992", "authors": ["Bernhard E. Boser", "Isabelle M. Guyon", "Vladimir N. Vapnik"], "related_topics": ["Margin (machine learning)", "Decision boundary", "Stability (learning theory)", "Perceptron", "Generalization", "Linear combination", "Radial basis function", "Optical character recognition", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "1618905105", "references": ["1825604117", "9657784", "2122646361", "2115252128", "2153635508", "2964212410", "1648445109", "2115150266", "2963466845", "1999954155"], "title": "Probabilistic Outputs for Support vector Machines and Comparisons to Regularized Likelihood Methods", "abstract": "", "citation_count": "0", "reference_count": "6,451", "date": "1999", "authors": ["John C. Platt"], "related_topics": ["Relevance vector machine", "Margin classifier", "Structured support vector machine", "Support vector machine", "Probabilistic logic", "Platt scaling", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2101276256", "references": ["2024046085", "2154642048", "2125055259", "2156909104", "2119821739", "2032210760", "1975846642", "2161920802", "1594031697", "3124955340"], "title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "abstract": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.", "citation_count": "25", "reference_count": "2,587", "date": "2001", "authors": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer"], "related_topics": ["Multiclass classification", "AdaBoost", "Structured support vector machine", "Support vector machine", "Margin (machine learning)", "Binary number", "Machine learning", "Pattern recognition", "Categorization", "Class (biology)", "Mathematics", "Artificial intelligence"]}
{"id": "1988195734", "references": ["2119479037", "2067885219", "2096729078", "1678356000", "2911964244", "1975846642", "1840338487", "2005685204", "2133199783", "3124955340"], "title": "Random forest: a classification and regression tool for compound classification and QSAR modeling.", "abstract": "A new classification and regression tool, Random Forest, is introduced and investigated for predicting a compound's quantitative or categorical biological activity based on a quantitative description of the compound's molecular structure. Random Forest is an ensemble of unpruned classification or regression trees created by using bootstrap samples of the training data and random feature selection in tree induction. Prediction is made by aggregating (majority vote or averaging) the predictions of the ensemble. We built predictive models for six cheminformatics data sets. Our analysis demonstrates that Random Forest is a powerful tool capable of delivering performance that is among the most accurate methods to date. We also present three additional features of Random Forest:\u2009 built-in performance assessment, a measure of relative importance of descriptors, and a measure of compound similarity that is weighted by the relative importance of descriptors. It is the combination of relatively high prediction accu...", "citation_count": "21", "reference_count": "2,196", "date": "2003", "authors": ["Vladimir Svetnik", "Andy Liaw", "Christopher Tong", "J. Christopher Culberson", "Robert P. Sheridan", "Bradley P. Feuston"], "related_topics": ["Random forest", "Categorical variable", "Feature selection", "Quantitative structure\u2013activity relationship", "Cheminformatics", "Regression", "Tree (data structure)", "Similarity (network science)", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2107369107", "references": ["2217896605", "2033419168", "2121601095", "2156909104", "2121647436", "2049633694", "2148603752", "2132549764", "2914885528", "2138451337"], "title": "Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class", "abstract": "The classical way of attempting to solve the face (or object) recognition problem is by using large and representative data sets. In many applications, though, only one sample per class is available to the system. In this contribution, we describe a probabilistic approach that is able to compensate for imprecisely localized, partially occluded, and expression-variant faces even when only one single training sample per class is available to the system. To solve the localization problem, we find the subspace (within the feature space, e.g., eigenspace) that represents this error for each of the training images. To resolve the occlusion problem, each face is divided into k local regions which are analyzed in isolation. In contrast with other approaches where a simple voting space is used, we present a probabilistic method that analyzes how \"good\" a local match is. To make the recognition system less sensitive to the differences between the facial expression displayed on the training and the testing images, we weight the results obtained on each local area on the basis of how much of this local area is affected by the expression displayed on the current test image.", "citation_count": "79", "reference_count": "1,001", "date": "2002", "authors": ["A.M. Martinez"], "related_topics": ["Facial recognition system", "Feature vector", "Standard test image", "Pattern recognition (psychology)", "Face (geometry)", "Probabilistic logic", "Subspace topology", "Expression (mathematics)", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2994340921", "references": ["2134262590", "2121601095", "1963932623", "1782590233", "3102431071", "2102544846", "2157364932", "2912990735", "2129812935", "2132467081"], "title": "The AR face databasae", "abstract": "", "citation_count": "0", "reference_count": "4,961", "date": "1998", "authors": ["A. M. Martinez"], "related_topics": ["Face (sociological concept)", "Computer science", "Computer vision", "Artificial intelligence"]}
{"id": "2144354855", "references": ["2046079134", "2098947662", "2115689562", "2113341759", "1770825568", "2124776405", "2095757522", "2012352340", "1679913846", "2138451337"], "title": "Face recognition: a convolutional neural-network approach", "abstract": "We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.", "citation_count": "43", "reference_count": "3,323", "date": "1997", "authors": ["S. Lawrence", "C.L. Giles", "Ah Chung Tsoi", "A.D. Back"], "related_topics": ["Convolutional neural network", "Multilayer perceptron", "Self-organizing map", "Artificial neural network", "Facial recognition system", "Feature extraction", "Quantization (image processing)", "Dimensionality reduction", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2121647436", "references": ["2098947662", "2115689562", "2113341759", "2098693229", "2130259898", "2123977795", "3017143921", "2740373864", "2138451337", "2159173611"], "title": "Eigenfaces vs. Fisherfaces: recognition using class specific linear projection", "abstract": "We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher's linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases.", "citation_count": "31", "reference_count": "16,894", "date": "1997", "authors": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "related_topics": ["Eigenface", "Three-dimensional face recognition", "Face Recognition Grand Challenge", "Face hallucination", "Facial recognition system", "Multilinear subspace learning", "Linear subspace", "Linear discriminant analysis", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2095757522", "references": ["2051719061", "2135463994", "2079948225", "23758216", "1991848143", "2130259898", "2167034998", "3144368627", "2011039300", "1914401667"], "title": "Distortion invariant object recognition in the dynamic link architecture", "abstract": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. &gt;", "citation_count": "31", "reference_count": "2,702", "date": "1993", "authors": ["M. Lades", "J.C. Vorbruggen", "J. Buhmann", "J. Lange", "C. von der Malsburg", "R.P. Wurtz", "W. Konen"], "related_topics": ["Dynamic link matching", "3D single-object recognition", "Facial recognition system", "Pattern recognition (psychology)", "Cognitive neuroscience of visual object recognition", "Matching (graph theory)", "Artificial neural network", "Image processing", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "10021998", "references": ["2147880316", "2134557905", "2008652694", "1802356529", "2105644991", "2148099973", "2310919327", "2175582831", "2132339004", "2137813581"], "title": "Loss Functions for Discriminative Training of Energy-Based Models.", "abstract": "", "citation_count": "12", "reference_count": "142", "date": "2005", "authors": ["Yann LeCun", "Fu Jie Huang"], "related_topics": ["Discriminative model", "Speech recognition", "Pattern recognition", "Mathematics", "Training (meteorology)", "Artificial intelligence", "Energy based"]}
{"id": "2138451337", "references": ["2135463994", "2032361618", "1509703770", "2130259898", "2125848778", "2125999363", "1526492552", "1507699566", "2055712799", "1986450498"], "title": "Eigenfaces for recognition", "abstract": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.", "citation_count": "23", "reference_count": "20,218", "date": "1991", "authors": ["Matthew Turk", "Alex Pentland"], "related_topics": ["Eigenface", "Three-dimensional face recognition", "Face Recognition Grand Challenge", "Face detection", "Face hallucination", "Facial recognition system", "Face perception", "Pattern recognition (psychology)", "Pattern recognition", "Psychology", "Artificial intelligence"]}
{"id": "2141125852", "references": ["2154642048", "2134557905", "2156163116", "2148461049", "2110798204", "2310919327", "3118608800", "2144982973", "2138857742", "2139427956"], "title": "Multi-column deep neural networks for image classification", "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.", "citation_count": "39", "reference_count": "4,696", "date": "2012", "authors": ["Dan Cire\u015fan", "Ueli Meier", "Juergen Schmidhuber"], "related_topics": ["Convolutional neural network", "Deep learning", "Artificial neural network", "MNIST database", "Traffic sign recognition", "Contextual image classification", "Pattern recognition", "Receptive field", "Machine learning", "Visual cortex", "Computer science", "Artificial intelligence"]}
{"id": "2097726431", "references": ["2118020653", "2147880316", "3146306708", "2138621811", "2166706824", "2038721957", "1880262756", "3013264884", "2160660844", "2114524997"], "title": "Opinion Mining and Sentiment Analysis", "abstract": "An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.\r\n\r\nThis survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.", "citation_count": "314", "reference_count": "8,720", "date": "2008", "authors": ["Bo Pang", "Lillian Lee"], "related_topics": ["Sentiment analysis", "Automatic summarization", "Information technology", "Popularity", "Data science", "Economic impact analysis", "Information extraction", "Object (philosophy)", "Subjectivity", "Computer science"]}
{"id": "1996741810", "references": ["2032164462", "2047515372", "2110485445", "2140539590", "2133656308", "2007431958", "2121553911", "2094096029", "2143503258", "2016589492"], "title": "Local feedback multilayered networks", "abstract": "In this paper, we investigate the capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves. In this class, learning can be accomplished by an algorithm that is local in both space and time. We describe the limits and properties of these networks and give some insights on their use for solving practical problems.", "citation_count": "14", "reference_count": "272", "date": "1992", "authors": ["Paolo Frasconi", "Marco Gori", "Giovanni Soda"], "related_topics": ["Class (computer programming)", "Artificial intelligence", "Computer science", "Spacetime"]}
{"id": "2088978850", "references": ["2171074980", "2029673686", "2065606540", "1652464192", "2022772618", "2056760934", "2152710595", "2581275558", "2012231377", "2026258334"], "title": "Minimizing multimodal functions of continuous variables with the \u201csimulated annealing\u201d algorithm\u2014Corrigenda for this article is available here", "abstract": "A new global optimization algorithm for functions of continuous variables is presented, derived from the \u201cSimulated Annealing\u201d algorithm recently introduced in combinatorial optimization.The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered.The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions.The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.", "citation_count": "15", "reference_count": "2,079", "date": "1987", "authors": ["A. Corana", "M. Marchesi", "C. Martini", "S. Ridella"], "related_topics": ["Adaptive simulated annealing", "Simulated annealing", "Hill climbing", "Simplex algorithm", "Random search", "Maxima and minima", "Function (mathematics)", "Probabilistic logic", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2148099973", "references": ["169539560", "2154642048", "2086699924", "2125838338", "2077804127", "2140539590", "2169415433", "2140766383", "2125610452", "2135622428"], "title": "Global optimization of a neural network-hidden Markov model hybrid", "abstract": "The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed. ANNs are suitable for approximating functions that compute new acoustic parameters, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported. &gt;", "citation_count": "30", "reference_count": "351", "date": "1992", "authors": ["Y. Bengio", "R. De Mori", "G. Flammia", "R. Kompe"], "related_topics": ["Hidden Markov model", "Markov model", "Artificial neural network", "TIMIT", "Markov process", "Global optimization", "Estimation theory", "Pattern recognition", "Sequence", "Computer science", "Artificial intelligence"]}
{"id": "2581275558", "references": ["2042986967", "2114552889", "2148673189", "2022494241", "2014068360", "2056760934", "2014952973", "86906884", "2022820481", "2143037347"], "title": "Optimization by simulated annealing", "abstract": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.", "citation_count": "14", "reference_count": "52,083", "date": "1987", "authors": ["S. Kirkpatrick", "C. D. Gelatt", "M. P. Vecchi"], "related_topics": ["Optimization problem", "Combinatorial optimization", "Simulated annealing", "Degrees of freedom (physics and chemistry)", "Statistical mechanics", "Complex system", "Function (mathematics)", "Connection (mathematics)", "Mathematical optimization", "Mathematics"]}
{"id": "1527772862", "references": ["2190154063", "2293185259", "3046044791", "2106725254", "2569140349", "2754791538", "2124592697", "2083703071", "2783190965"], "title": "A focused backpropagation algorithm for temporal pattern recognition", "abstract": "", "citation_count": "0", "reference_count": "308", "date": "1995", "authors": ["Michael C. Mozer"], "related_topics": ["Pattern recognition (psychology)", "Backpropagation", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2125329357", "references": ["2154642048", "2003357516", "2165758113", "1529808766", "2010029425", "1507849272", "2084544490", "2181111061", "2176028050", "2016589492"], "title": "Using random weights to train multilayer networks of hard-limiting units", "abstract": "A gradient descent algorithm suitable for training multilayer feedforward networks of processing units with hard-limiting output functions is presented. The conventional backpropagation algorithm cannot be applied in this case because the required derivatives are not available. However, if the network weights are random variables with smooth distribution functions, the probability of a hard-limiting unit taking one of its two possible values is a continuously differentiable function. In the paper, this is used to develop an algorithm similar to backpropagation, but for the hard-limiting case. It is shown that the computational framework of this algorithm is similar to standard backpropagation, but there is an additional computational expense involved in the estimation of gradients. Upper bounds on this estimation penalty are given. Two examples which indicate that, when this algorithm is used to train networks of hard-limiting units, its performance is similar to that of conventional backpropagation applied to networks of units with sigmoidal characteristics are presented. &gt;", "citation_count": "15", "reference_count": "57", "date": "1992", "authors": ["P.L. Barlett", "T. Downs"], "related_topics": ["Backpropagation", "Gradient descent", "Artificial neural network", "Random variable", "Sigmoid function", "Function (mathematics)", "Feed forward", "Mathematical optimization", "Computer science"]}
{"id": "2016589492", "references": ["1959983357", "2154642048", "1984375561", "2110485445", "1881179843", "1984205520", "1527772862", "2119796132", "2293063825", "2143503258"], "title": "A learning algorithm for continually running fully recurrent neural networks", "abstract": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.", "citation_count": "14", "reference_count": "5,334", "date": "1989", "authors": ["Ronald J. Williams", "David Zipser"], "related_topics": ["Deep learning", "Recurrent neural network", "Backpropagation through time", "Artificial neural network", "Supervised learning", "Artificial intelligence", "Machine learning", "Interval (mathematics)", "Computer science", "Algorithm", "Basis (linear algebra)", "Recurrent neural nets"]}
{"id": "1959983357", "references": ["2128499899", "2058791286", "2999879503", "1934184906", "2007800656", "2155041441", "2019385508", "1996605950", "2143503258", "2546314413"], "title": "Attractor dynamics and parallelism in a connectionist sequential machine", "abstract": "", "citation_count": "0", "reference_count": "2,539", "date": "1990", "authors": ["Michael I. Jordan"], "related_topics": ["Parallelism (grammar)", "Attractor", "Connectionism", "Theoretical computer science", "Computer science", "Dynamics (music)", "Sequential machine"]}
{"id": "2053127376", "references": ["2045597501", "2106654511", "2135255848", "2098205603", "2094493170", "2147311265", "1502139053", "1529340823", "1784695092", "1967670055"], "title": "On the time relations of mental processes: An examination of systems of processes in cascade.", "abstract": "", "citation_count": "50", "reference_count": "1,838", "date": "1979", "authors": ["James L. McClelland"], "related_topics": ["Cognition", "Perception", "Cognitive science", "Cognitive psychology", "Computer science", "Cascade", "Abstract reasoning", "Research methodology"]}
{"id": "2167607759", "references": ["3004157836", "2154642048", "2796837256", "1984375561", "1984205520", "2077658674", "2325850497", "2143503258", "2016589492"], "title": "The \"Moving Targets\" Training Algorithm", "abstract": "A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.", "citation_count": "19", "reference_count": "63", "date": "1989", "authors": ["Richard Rohwer"], "related_topics": ["State space", "Artificial neural network", "Error function", "Simple (abstract algebra)", "Algorithm", "Signal", "Training (meteorology)", "Computer science"]}
{"id": "1026270304", "references": ["1533861849", "2618530766", "2136922672", "2097117768", "2141125852", "1677182931", "104184427", "2962835968", "2155893237", "2064675550"], "title": "Training very deep networks", "abstract": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.", "citation_count": "36", "reference_count": "1,123", "date": "2015", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "related_topics": ["Artificial neural network", "Gradient descent", "Distributed computing", "Machine learning", "Computer science", "Training (civil)", "Artificial intelligence"]}
{"id": "1924770834", "references": ["2964308564", "2097998348", "2130942839", "2143612262", "1810943226", "2144499799", "1606347560", "2108677974", "2964199361", "2064675550"], "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "citation_count": "16", "reference_count": "7,976", "date": "2014", "authors": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "related_topics": ["Recurrent neural network", "Speech recognition", "Computer science", "Sequence modeling"]}
{"id": "1991133427", "references": ["1527096151", "2034274945", "2087362480", "2005530146", "1527268325", "1976797517", "1993944611"], "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "abstract": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.", "citation_count": "7", "reference_count": "9,842", "date": "1967", "authors": ["A. Viterbi"], "related_topics": ["Sequential decoding", "Serial concatenated convolutional codes", "List decoding", "Convolutional code", "Concatenated error correction code", "Linear code", "Turbo code", "Low-density parity-check code", "Discrete mathematics", "Combinatorics", "Algorithm", "Mathematics"]}
{"id": "1966812932", "references": ["2341171179", "2137095888", "2142384583", "2157477135", "1989226853", "2163929346", "1575431606", "2029491572", "1597533204", "2035227369"], "title": "A Maximum Likelihood Approach to Continuous Speech Recognition", "abstract": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.", "citation_count": "17", "reference_count": "1,965", "date": "1983", "authors": ["Lalit R. Bahl", "Frederick Jelinek", "Robert L. Mercer"], "related_topics": ["Speech processing", "Sequential decoding", "Decoding methods", "Estimation theory", "Statistical model", "Speech production", "Markov model", "Natural language", "Speech recognition", "Computer science", "Maximum likelihood"]}
{"id": "2048330959", "references": ["2052810501", "1989544735", "1997494543", "1975068880", "2089840306", "1992476998", "2087895317", "1981520343", "2170716495", "2001963156"], "title": "Cooperative computation of stereo disparity", "abstract": "Perhaps one of the most striking differences between a brain and today\u2019s computers is the amount of \u201cwiring.\u201d In a digital computer the ratio of connections to components is about 3, whereas for the mammalian cortex it lies between 10 and 10,000 (1).", "citation_count": "13", "reference_count": "2,307", "date": "1988", "authors": ["D. Marr", "T. Poggio"], "related_topics": ["Computation", "Information processing", "Cortex (anatomy)", "Correspondence problem", "Image processing", "Computer vision", "Perception", "Computer science", "Artificial intelligence", "Combinatorial analysis", "Digital computer"]}
{"id": "2101926813", "references": ["1594551768", "1588340522", "2053120614", "22889343", "2324189819", "2322002063", "2010315761", "2091546412", "2116360511", "2272360941"], "title": "Neocognitron: A Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position", "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.", "citation_count": "12", "reference_count": "4,995", "date": "1980", "authors": ["Kunihiko Fukushima"], "related_topics": ["Neocognitron", "Form perception", "Stimulus (physiology)", "Artificial neural network", "Unsupervised learning", "Hypercomplex number", "Gestalt psychology", "Pattern recognition", "Cascade", "Computer science", "Artificial intelligence"]}
{"id": "3121926921", "references": ["2618530766", "2170973209", "2019207321", "2076063813", "2002016471", "3123753580", "2141125852", "2132424367"], "title": "Beyond regression : new fools for prediction and analysis in the behavioral sciences", "abstract": "", "citation_count": "0", "reference_count": "4,576", "date": "1974", "authors": ["P. Werbos"], "related_topics": ["Regression", "Behavioural sciences", "Computer science", "Cognitive psychology"]}
{"id": "2176028050", "references": ["1597286183", "1997063559", "2154642048", "2049633694", "2581275558", "1497256448", "2293063825", "2895674046", "22297218", "1498436455"], "title": "Connectionist learning procedures", "abstract": "A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.", "citation_count": "85", "reference_count": "3,990", "date": "1990", "authors": ["Geoffrey E. Hinton"], "related_topics": ["Artificial neural network", "Connectionism", "Task (computing)", "Generalization", "Artificial intelligence", "Machine learning", "Computer science", "Domain (software engineering)", "Rate of convergence", "Construct (python library)", "Connection (mathematics)"]}
{"id": "2138484437", "references": ["1597286183", "1572161815", "3036751298", "2174984063", "2122136962", "2137983211", "2095425517", "2007431958", "2293063825"], "title": "Identification and control of dynamical systems using neural networks", "abstract": "It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described. &gt;", "citation_count": "25", "reference_count": "11,680", "date": "1990", "authors": ["K.S. Narendra", "K. Parthasarathy"], "related_topics": ["Nonlinear system identification", "Adaptive control", "Dynamical systems theory", "Artificial neural network", "Backpropagation", "Identification (information)", "Control system", "Linear system", "Nonlinear system", "Control engineering", "Control theory", "Computer science"]}
{"id": "2798813531", "references": ["2134673975", "1521785144", "1588998206", "2065297540", "2912155302", "2149072817", "2012445782", "1986922155", "2099839128", "1506119886"], "title": "Linear systems", "abstract": "", "citation_count": "0", "reference_count": "11,071", "date": "1980", "authors": ["Thomas Kailath"], "related_topics": ["Linear system", "Computer science", "Applied mathematics", "Rational matrices"]}
{"id": "2098398123", "references": ["2154642048", "1603277681", "1652505363", "2103496339", "2102380305", "1540723801", "1535689967", "1650765400", "1971735090", "2125812768"], "title": "Non-linear system identification using neural networks", "abstract": "Multi-layered neural networks offer an exciting alternative for modelling complex non-linear systems. This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer. New parameter estimation algorithms are derived for the neural network model based on a prediction error formulation and the application to both simulated and real data is included to demonstrate the effectiveness of the neural network approach.", "citation_count": "20", "reference_count": "1,327", "date": "1990", "authors": ["S. Chen", "S. A. Billings", "Peter Grant"], "related_topics": ["Time delay neural network", "Recurrent neural network", "Types of artificial neural networks", "Stochastic neural network", "Cellular neural network", "Probabilistic neural network", "Nervous system network models", "Artificial neural network", "Artificial intelligence", "Computer science"]}
{"id": "2147568880", "references": ["2107878631", "194249466", "2154890045", "2121029939", "1674799117", "2914484425", "1525783482", "2064675550", "2136848157", "2016589492"], "title": "Learning precise timing with lstm recurrent networks", "abstract": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.", "citation_count": "23", "reference_count": "1,395", "date": "2003", "authors": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber"], "related_topics": ["Recurrent neural network", "Hidden Markov model", "Machine learning", "Forcing (recursion theory)", "Motor control", "Computer science", "Focus (optics)", "Rhythm", "Artificial intelligence", "Computational Science and Engineering", "Temporal distance"]}
{"id": "3099873379", "references": ["2618530766", "2025768430", "2136922672", "2124386111", "2154642048", "2130942839", "1810943226", "2310919327", "2064675550", "2145339207"], "title": "Neural NILM: Deep Neural Networks Applied to Energy Disaggregation", "abstract": "Energy disaggregation estimates appliance-by-appliance electricity consumption from a single meter that measures the whole home's electricity demand. Recently, deep neural networks have driven remarkable improvements in classification performance in neighbouring machine learning fields such as image classification and automatic speech recognition. In this paper, we adapt three deep neural network architectures to energy disaggregation: 1) a form of recurrent neural network called `long short-term memory' (LSTM); 2) denoising autoencoders; and 3) a network which regresses the start time, end time and average power demand of each appliance activation. We use seven metrics to test the performance of these algorithms on real aggregate power data from five appliances. Tests are performed against a house not seen during training and against houses seen during training. We find that all three neural nets achieve better F1 scores (averaged over all five appliances) than either combinatorial optimisation or factorial hidden Markov models and that our neural net algorithms generalise well to an unseen house.", "citation_count": "31", "reference_count": "445", "date": "2015", "authors": ["Jack Kelly", "William Knottenbelt"], "related_topics": ["Deep learning", "Recurrent neural network", "Artificial neural network", "Feature learning", "Contextual image classification", "Machine learning", "Energy (signal processing)", "Energy conservation", "Computer science", "Aggregate (data warehouse)", "Artificial intelligence"]}
{"id": "1735317348", "references": ["2145094598", "2618530766", "2130942839", "2113325037", "1810943226", "1895577753", "2136391815", "2100495367", "2064675550", "196214544"], "title": "Recurrent Network Models for Human Dynamics", "abstract": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units [31].", "citation_count": "49", "reference_count": "495", "date": "2015", "authors": ["Katerina Fragkiadaki", "Sergey Levine", "Panna Felsen", "Jitendra Malik"], "related_topics": ["Recurrent neural network", "Feature learning", "Optical flow", "Motion capture", "Feature (machine learning)", "Network model", "Encoder", "Motion (physics)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2144499799", "references": ["2136922672", "2147880316", "2156909104", "2125838338", "2110798204", "2310919327", "1554663460", "1993882792", "2064675550", "1663973292"], "title": "Supervised Sequence Labelling with Recurrent Neural Networks", "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences.", "citation_count": "137", "reference_count": "1,803", "date": "2012", "authors": ["Alexander Graves"], "related_topics": ["Recurrent neural network", "Context (language use)", "Sequence", "Network architecture", "Machine learning", "Labelling", "Layer (object-oriented design)", "Computer science", "Extension (predicate logic)", "Artificial intelligence", "Multidimensional data"]}
{"id": "2136848157", "references": ["2107878631", "1959983357", "194249466", "2121029939", "2103452139", "1674799117", "1971129545", "2057653135", "2064675550", "2154890045"], "title": "Learning to Forget: Continual Prediction with LSTM", "abstract": "Long short-term memory (LSTM; Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \"forget gate\" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.", "citation_count": "21", "reference_count": "7,623", "date": "2000", "authors": ["Felix A. Gers", "J\u00fcrgen A. Schmidhuber", "Fred A. Cummins"], "related_topics": ["Recurrent neural network", "Artificial neural network", "Reset (computing)", "Benchmark (computing)", "Artificial intelligence", "State (computer science)", "A priori and a posteriori", "Psychology", "Long short term memory"]}
{"id": "2075510082", "references": ["2082628174", "2159116087", "1995030605", "2072972096", "2159187100", "2060666586", "1968014724"], "title": "Characteristics of Random Nets of Analog Neuron-Like Elements", "abstract": "The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net?in contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.", "citation_count": "7", "reference_count": "388", "date": "1972", "authors": ["Shun-Ichi Amarimber"], "related_topics": ["Stochastic process", "Stability (probability)", "Bistability", "Statistical parameter", "Oscillation", "Net (mathematics)", "State (functional analysis)", "Statistical physics", "Contrast (statistics)", "Control theory", "Mathematics"]}
{"id": "2177721432", "references": ["582196039", "1973108021"], "title": "Neurons with graded response have collective computational properties like those of two-state neurons", "abstract": "A model for a large network of \"neurons\" with a graded response (or sigmoid input--output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch--Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.", "citation_count": "2", "reference_count": "9,598", "date": "1988", "authors": ["J. J. Hopfield"], "related_topics": ["Deterministic system", "Electrical network", "Sigmoid function", "Stochastic modelling", "Connection (algebraic framework)", "Function (mathematics)", "Simple (abstract algebra)", "Action (physics)", "Statistical physics", "Artificial intelligence", "Mathematics"]}
{"id": "2150355110", "references": ["2154642048", "1881179843", "1971129545", "3148194443", "2068484625", "2090248140", "3121926921", "2143503258", "1487148666"], "title": "Backpropagation through time: what it does and how to do it", "abstract": "Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users. &gt;", "citation_count": "23", "reference_count": "5,227", "date": "1990", "authors": ["P.J. Werbos"], "related_topics": ["Backpropagation through time", "Backpropagation", "Recurrent neural network", "Artificial neural network", "Deep learning", "Time delay neural network", "Nonlinear system identification", "Types of artificial neural networks", "Artificial intelligence", "Computer science"]}
{"id": "1529008516", "references": ["2118415523", "2993446282", "2167224731", "2115072676", "2156562940", "2152503618", "2188233853", "2056655352", "2123716044"], "title": "Supervised learning and systems with excess degrees of freedom", "abstract": "WHEN DISTINCT OUTPUTS OF AN ADAPTIVE SYSTEM HAVE EQUIVALENT EFFECTS ON THE ENVIRONMENT, THE PROBLEM OF FINDING APPROPRIATE ACTIONS GIVEN DESIRED RESULTS IS ILL-POSED. FOR SUPERVISED LEARNING ALGORITHMS, THE ILL-POSEDNESS OF SUCH \"INVERSE LEARNING PROBLEMS\" IMPLIES A CERTAIN FLEXIBILITY---DURING TRAINING, THERE ARE IN GENERAL MANY POSSIBLE TARGET VECTORS CORRESPONDING TO EACH INPUT VECTOR. TO ALLOW SUPERVISED LEARNING ALGORITHMS TO MAKE USE OF THIS FLEXIBILITY, THE CURRENT PAPER CONSIDERS HOW TO SPECIFY TARGETS BY SETS OF CONSTRAINTS, RATHER THAN AS PARTICULAR VECTORS. TWO CLASSES OF CONSTRAINTS ARE DISTINGUISHED---`CONFIGURATIONAL'' CONSTRAINTS, WHICH DEFINE REGIONS OF OUTPUT SPACE IN WHICH AN OUTPUT VECTOR MUST LIE, AND `TEMPORAL'' CONSTRAINTS, WHICH DEFINE RELATIONSHIPS BETWEEN OUTPUTS PRODUCED AT DIFFER- ENT POINTS IN TIME. LEARNING ALGORITHMS MINIMIZE A COST FUNCTION THAT CON- TAINS TERMS FOR BOTH KINDS OF CONSTRAINTS. THIS APPROACH TO INVERSE LEARN- ING IS ILLUSTRATED BY A ROBOTICS APPLICATION IN WHICH A NETWORK FINDS TRA- JECTORIES OF INVERSE KINEMATIC SOLUTIONS FOR MANIPULATORS WITH EXCESS DEGREES OF FREEDOM.", "citation_count": "0", "reference_count": "238", "date": "1988", "authors": ["Michael I. Jordan"], "related_topics": ["Semi-supervised learning", "Supervised learning", "Degrees of freedom", "Function (mathematics)", "Adaptive system", "Inverse", "Kinematics", "Robotics", "Mathematical optimization", "Mathematics", "Artificial intelligence"]}
{"id": "2132152975", "references": ["2102496649", "2077493113", "2122568838", "3147404844", "2112462566", "2138484437", "1761621746", "107400462"], "title": "Decoupled extended Kalman filter training of feedforward layered networks", "abstract": "Presents a training algorithm for feedforward layered networks based on a decoupled extended Kalman filter (DEKF). The authors present an artificial process noise extension to DEKF that increases its convergence rate and assists in the avoidance of local minima. Computationally efficient formulations for two particularly natural and useful cases of DEKF are given. Through a series of pattern classification and function approximation experiments, three members of DEKF are compared with one another and with standard backpropagation (SBP). These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures. &gt;", "citation_count": "8", "reference_count": "316", "date": "1991", "authors": ["G.V. Puskorius", "L.A. Feldkamp"], "related_topics": ["Extended Kalman filter", "Kalman filter", "Backpropagation", "Artificial neural network", "Function approximation", "Rate of convergence", "Maxima and minima", "Feed forward", "Control theory", "Computer science"]}
{"id": "2143787696", "references": ["2076086013", "2150355110", "2016261381", "2062870975", "2138484437", "2016589492"], "title": "Gradient methods for the optimization of dynamical systems containing neural networks", "abstract": "An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function. &gt;", "citation_count": "6", "reference_count": "873", "date": "1991", "authors": ["K.S. Narendra", "K. Parthasarathy"], "related_topics": ["Backpropagation", "Linear dynamical system", "Gradient method", "Artificial neural network", "Dynamical systems theory", "Linear system", "Nonlinear system", "Algorithm", "Artificial intelligence", "Computer science"]}
{"id": "2112462566", "references": ["2154642048", "2051992922", "2105934661", "1613359937", "2293807537", "2173629880", "304861154"], "title": "Training Multilayer Perceptrons with the Extended Kalman Algorithm", "abstract": "A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.", "citation_count": "7", "reference_count": "498", "date": "1988", "authors": ["Sharad Singhal", "Lance Wu"], "related_topics": ["Perceptron", "Artificial neural network", "Parameter identification problem", "Nonlinear system", "Artificial intelligence", "Fraction (mathematics)", "Computer science", "Training (meteorology)", "Kalman algorithm"]}
{"id": "2057653135", "references": ["2154642048", "2047515372", "1881179843", "1984205520", "2139273175", "2007431958", "2121553911", "2143503258", "2044422789", "2016589492"], "title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "abstract": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time.", "citation_count": "10", "reference_count": "737", "date": "1990", "authors": ["Ronald J. Williams", "Jing Peng"], "related_topics": ["Backpropagation through time", "Reset (computing)", "State (computer science)", "Artificial intelligence", "Line (geometry)", "Computer science", "Training (meteorology)", "Gradient based algorithm"]}
{"id": "2090248140", "references": ["1967377907", "1892385946", "1969166509", "2150367199", "1981297107", "1507849272", "2895674046", "2046329526", "1885639605", "2016589492"], "title": "Generic constraints on underspecified target trajectories", "abstract": "Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning. &gt;", "citation_count": "31", "reference_count": "222", "date": "1989", "authors": ["Jordan"], "related_topics": ["Learning rule", "Instance-based learning", "Learning classifier system", "Stability (learning theory)", "Semi-supervised learning", "Unsupervised learning", "Algorithmic learning theory", "Leabra", "Online machine learning", "Active learning (machine learning)", "Competitive learning", "Inductive transfer", "Multi-task learning", "Computational learning theory", "Robot learning", "Wake-sleep algorithm", "Artificial neural network", "Motor learning", "Artificial intelligence", "Computer science", "Generalization error"]}
{"id": "1583833196", "references": ["2170227253", "1481405077", "1602079996", "1971728746", "2834852173", "2293846015", "2001185203", "212314082", "2038584310", "1504806788"], "title": "Neuronlike adaptive elements that can solve difficult learning control problems", "abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.", "citation_count": "0", "reference_count": "4,306", "date": "1990", "authors": ["Andrew G. Barto", "Richard S. Sutton", "Charles W. Anderson"], "related_topics": ["Evaluation function", "Relation (database)", "Machine learning", "Computer science", "Task (project management)", "Control (management)", "Element (category theory)", "Reinforcement", "Base (topology)", "Operant conditioning", "Artificial intelligence"]}
{"id": "1597286183", "references": ["1666015432", "2042986967", "1543738661", "2112325651", "2032533296", "2177721432", "2581275558", "2293063825", "2011039300", "307896644"], "title": "Neural computation of decisions in optimization problems", "abstract": "Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.", "citation_count": "28", "reference_count": "8,064", "date": "1985", "authors": ["J. J. Hopfield", "D. W. Tank"], "related_topics": ["Optimization problem", "Models of neural computation", "Hopfield network", "Computation", "Complex system", "Cybernetics", "Nonlinear system", "Mathematical optimization", "Decision theory", "Algorithm", "Computer science"]}
{"id": "1971129545", "references": ["134309601", "2176028050", "1969166509", "3022423118", "2010526455", "22297218", "3150413596", "2068484625", "3121926921", "1583833196"], "title": "Generalization of backpropagation with application to a recurrent gas market model", "abstract": "Abstract   Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research.  This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.", "citation_count": "20", "reference_count": "1,031", "date": "1988", "authors": ["Paul J. Werbos"], "related_topics": ["Backpropagation", "Catastrophic interference", "Artificial neural network", "Delta rule", "Hopfield network", "Generalization", "Reinforcement learning", "Least squares", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1507849272", "references": ["1997063559", "1980658026", "2112325651", "2098205603", "2056760934", "2157629899", "807785616", "2581275558", "2293063825", "1597474747"], "title": "A learning algorithm for Boltzmann machines", "abstract": "The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.", "citation_count": "10", "reference_count": "4,821", "date": "1988", "authors": ["David H. Ackley", "Geoffrey E. Hinton", "Terrence J. Sejnowski"], "related_topics": ["Massively parallel", "Learning rule", "Constraint satisfaction", "Domain (software engineering)", "Simple (abstract algebra)", "Encoding (memory)", "Connection (vector bundle)", "Theoretical computer science", "Computation", "Computer science", "Algorithm"]}
{"id": "2895674046", "references": ["2118776392", "2148982591", "2091881639", "1544329015", "2100677568", "2019207321", "2158518777", "2148138104", "2071707134", "2098674248"], "title": "Adaptive Signal Processing", "abstract": "GENERAL INTRODUCTION. Adaptive Systems. The Adaptive Linear Combiner. THEORY OF ADAPTATION WITH STATIONARY SIGNALS. Properties of the Quadratic Performance Surface. Searching the Performance Surface. Gradient Estimation and Its Effects on Adaptation. ADAPTIVE ALGORITHMS AND STRUCTURES. The LMS Algorithm. The Z-Transform in Adaptive Signal Processing. Other Adaptive Algorithms and Structures. Adaptive Lattice Filters. APPLICATIONS. Adaptive Modeling and System Identification. Inverse Adaptive Modeling, Deconvolution, and Equalization. Adaptive Control Systems. Adaptive Interference Cancelling. Introduction to Adaptive Arrays and Adaptive Beamforming. Analysis of Adaptive Beamformers.", "citation_count": "0", "reference_count": "10,639", "date": "1985", "authors": ["Bernard Widrow", "Samuel D. Stearns"], "related_topics": ["Adaptive filter", "Adaptive beamformer", "Adaptive control", "Adaptive system", "Multidelay block frequency domain adaptive filter", "Signal processing", "Least mean squares filter", "Linear system", "Algorithm", "Computer science"]}
{"id": "2610857016", "references": ["2151795416", "2160643434", "2146502635", "2139212933", "2107396783", "2118040894", "2053186076", "2146890818", "2165744313"], "title": "Matrix Analysis", "abstract": "Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.", "citation_count": "0", "reference_count": "45,346", "date": "1985", "authors": ["Roger A. Horn", "Charles R. Johnson"], "related_topics": ["Weyr canonical form", "Canonical form", "Matrix (mathematics)", "Adjugate matrix", "Commuting matrices", "Involutory matrix", "Matrix analysis", "Hermitian matrix", "Algebra", "Pure mathematics", "Mathematics"]}
{"id": "1992208280", "references": ["2000257769", "2169713291", "2038669746", "2000953623", "2064076655", "2086161653", "1983916623", "2090359754", "1490324987", "203276351"], "title": "Robust Stochastic Approximation Approach to Stochastic Programming", "abstract": "In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.", "citation_count": "26", "reference_count": "1,886", "date": "2008", "authors": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "related_topics": ["Stochastic optimization", "Stochastic approximation", "Stochastic programming", "Subgradient method", "Optimization problem", "Monte Carlo method", "Mathematical optimization", "Saddle point", "Structure (category theory)", "Mathematics"]}
{"id": "1978394996", "references": ["1557757161", "2068632118", "2075006521", "3090556797", "1956559956", "2043909051", "11171803", "2083605078", "2095396650", "3091372544"], "title": "Term Weighting Approaches in Automatic Text Retrieval", "abstract": "The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.", "citation_count": "48", "reference_count": "11,546", "date": "1988", "authors": ["Gerard Salton", "Christopher Buckley"], "related_topics": ["Term Discrimination", "Weighting", "Term (time)", "tf\u2013idf", "Term indexing", "Automatic indexing", "Information retrieval", "Computer science", "Probability distribution", "Baseline (configuration management)"]}
{"id": "2150102617", "references": ["2114535528", "2118020653", "2005422315", "2118202495", "2149684865", "2098162425", "1620204465", "2000672666", "2435251607", "2107008379"], "title": "RCV1: A New Benchmark Collection for Text Categorization Research", "abstract": "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.", "citation_count": "35", "reference_count": "3,020", "date": "2004", "authors": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "related_topics": ["Documentation", "Supervised learning", "Information retrieval", "Coding (social sciences)", "Computer science", "Quality control", "Future studies", "Original data", "Text categorization"]}
{"id": "2167732364", "references": ["1669104078", "2008164266", "2015263936", "1568307856", "2124541940", "1568288633", "2150126561", "3141595720", "2969945254", "1553702074"], "title": "Smooth minimization of non-smooth functions", "abstract": "In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from ** keeping basically the complexity of each iteration unchanged.", "citation_count": "10", "reference_count": "2,967", "date": "2005", "authors": ["Yu Nesterov"], "related_topics": ["Convex optimization", "Proximal Gradient Methods", "Random coordinate descent", "Smoothing", "Functional decomposition", "Minification", "Mathematical optimization", "Numerical analysis", "Mathematics", "Non smooth"]}
{"id": "2160218441", "references": ["1601740268", "1563088657", "1978394996", "1560724230", "2032210760", "2157791002", "2148603752", "2101276256", "2053463056", "2296319761"], "title": "Online Passive-Aggressive Algorithms", "abstract": "We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.", "citation_count": "35", "reference_count": "2,132", "date": "2006", "authors": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "related_topics": ["Margin (machine learning)", "Decision problem", "Lemma (mathematics)", "Algorithm", "Mathematical optimization", "Series (mathematics)", "Binary number", "Categorization", "Simple (abstract algebra)", "Regression", "Mathematics"]}
{"id": "2798766386", "references": ["2030723843", "2164278908", "607505555", "2132870739", "2067191022", "2146502635", "2120340025", "2109449402", "1964357740", "2100556411"], "title": "Nonlinear Programming", "abstract": "", "citation_count": "0", "reference_count": "17,017", "date": "1995", "authors": ["Dimitri Bertsekas"], "related_topics": ["Nonlinear programming", "Fritz John conditions", "Computer science", "Mathematical optimization", "Random coordinate descent"]}
{"id": "3120740533", "references": ["2963977107", "2963399829", "2106525823", "2099419573", "2146502635", "2786672974", "2170505850", "3100515187", "2964318098"], "title": "UCI Machine Learning Repository", "abstract": "", "citation_count": "0", "reference_count": "32,061", "date": "2007", "authors": ["A. Asuncion"], "related_topics": ["Ensembles of classifiers", "LPBoost", "Computer science", "Machine learning", "Artificial intelligence", "Associative classifier", "Ensemble diversity", "Ensemble selection", "Instance selection"]}
{"id": "2296319761", "references": ["2030723843", "2611147814", "2000296233", "2020830442", "2006980285", "82689443"], "title": "Convex Optimization", "abstract": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.", "citation_count": "6", "reference_count": "59,323", "date": "2004", "authors": ["Stephen Boyd", "Lieven Vandenberghe"], "related_topics": ["Conic optimization", "Convex optimization", "Nonlinear programming", "Engineering optimization", "Multi-objective optimization", "Drift plus penalty", "Semidefinite programming", "Geometric programming", "Management science", "Mathematics"]}
{"id": "2184045248", "references": ["1533861849", "2145094598", "2136922672", "2147768505", "44815768", "2159080219", "2100495367", "1993882792", "2116064496", "1498436455"], "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "abstract": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.", "citation_count": "64", "reference_count": "5,942", "date": "2012", "authors": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "related_topics": ["Time delay neural network", "Hidden Markov model", "Artificial neural network", "Mixture model", "Speech recognition", "Margin (machine learning)", "Posterior probability", "Frame (networking)", "Computer science", "State (computer science)"]}
{"id": "3141595720", "references": ["2913535645", "2107438106", "607505555", "2167732364", "2622263826", "2473418344", "104184427", "2963433607", "1992371516"], "title": "Introductory Lectures on Convex Optimization: A Basic Course", "abstract": "It was in the middle of the 1980s, when the seminal paper by Kar- markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op- timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre- diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc- tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop- ing field, which got the name \"polynomial-time interior-point methods\", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].", "citation_count": "0", "reference_count": "5,590", "date": "2014", "authors": ["I\ufe20u\ufe21. E. Nesterov"], "related_topics": ["Nonlinear programming", "Convex optimization", "Linear programming", "Proximal Gradient Methods", "Random coordinate descent", "Diction", "Field (computer science)", "Calculus", "Computer science", "Epoch (reference date)"]}
{"id": "2118858186", "references": ["2136922672", "2130325614", "2107034620", "2097018403", "3118608800", "1625255723", "2546302380", "2116064496", "2162915993", "2025768430"], "title": "An analysis of single-layer networks in unsupervised feature learning", "abstract": "A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).", "citation_count": "34", "reference_count": "2,602", "date": "2011", "authors": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "related_topics": ["Unsupervised learning", "Feature learning", "Cluster analysis", "Feature extraction", "Benchmark (computing)", "Pattern recognition", "Machine learning", "Hyperparameter", "Gaussian", "Computer science", "Structure (mathematical logic)", "Artificial intelligence"]}
{"id": "1703839189", "references": ["2129542667", "1593955729", "2161315652", "2105870155", "2069961370", "1975169783", "2100516702", "2132260239", "2167080692", "2145810580"], "title": "Detection of a novel human coronavirus by real-time reverse-transcription polymerase chain reaction", "abstract": "We present two real-time reverse-transcription polymerase chain reaction assays for a novel human coronavirus (CoV), targeting regions upstream of the E gene (upE) or within open reading frame (ORF)1b, respectively. Sensitivity for upE is 3.4 copies per reaction (95% confidence interval (CI): 2.5-6.9 copies) or 291 copies/mL of sample. No cross-reactivity was observed with coronaviruses OC43, NL63, 229E, SARS-CoV, nor with 92 clinical specimens containing common human respiratory viruses. We recommend using upE for screening and ORF1b for confirmation.", "citation_count": "12", "reference_count": "636", "date": "2012", "authors": ["V M Corman", "I Eckerle", "T Bleicker", "A Zaki", "O Landt", "M Eschbach-Bludau", "S van Boheemen", "R Gopal", "M Ballhause", "T M Bestebroer", "D Muth", "M A M\u00fcller", "J F Drexler", "M Zambon", "A D Osterhaus", "R M Fouchier", "C Drosten"], "related_topics": ["Coronavirus", "Reverse transcription polymerase chain reaction", "Real-time polymerase chain reaction", "Polymerase chain reaction", "Respiratory virus", "Open reading frame", "Viral replication", "Middle East respiratory syndrome coronavirus", "Virology", "Molecular biology", "Biology"]}
{"id": "1852588318", "references": ["2129542667", "1593955729", "1703839189", "1690366459", "2122612816", "2166867592", "2136039989", "2082755732", "2167080692", "2145810580"], "title": "Assays for laboratory confirmation of novel human coronavirus (hCoV-EMC) infections.", "abstract": "We present a rigorously validated and highly sensitive confirmatory real-time RT-PCR assay (1A assay) that can be used in combination with the previously reported upE assay. Two additional RT-PCR assays for sequencing are described, targeting the RdRp gene (RdRpSeq assay) and N gene (NSeq assay), where an insertion/deletion polymorphism might exist among different hCoV-EMC strains. Finally, a simplified and biologically safe protocol for detection of antibody response by immunofluorescence microscopy was developed using convalescent patient serum.", "citation_count": "12", "reference_count": "415", "date": "2012", "authors": ["Victor Corman", "Marcel M\u00fcller", "U. Costabel", "J. Timm", "Tabea Binger", "Bernhard Meyer", "P. Kreher", "Erik Lattwein", "Monika Eschbach-Bludau", "A. Nitsche", "T. Bleicker", "O. Landt", "Brunhilde Schweiger", "Jan-Felix Drexler", "Albert Osterhaus", "Bart Haagmans", "U. Dittmer", "F. Bonin", "Thorsten Wolff", "Christian Drosten"], "related_topics": ["Sequence analysis", "Virology", "Gene", "Molecular biology", "Polymorphism (computer science)", "DNA", "Biology", "Antibody response", "Highly sensitive", "Human coronavirus", "Immunofluorescence Microscopy"]}
{"id": "2140143765", "references": ["1703839189", "1690366459", "1852588318", "2103546861", "2166867592", "2119111857", "2112147913", "2045002682", "2132260239", "2119775949"], "title": "Clinical features and virological analysis of a case of Middle East respiratory syndrome coronavirus infection", "abstract": "Summary Background The Middle East respiratory syndrome coronavirus (MERS-CoV) is an emerging virus involved in cases and case clusters of severe acute respiratory infection in the Arabian Peninsula, T unisia, Morocco, France, Italy, Germany, and the UK. We provide a full description of a fatal case of MERS-CoV infection and associated phylogenetic analyses. Methods We report data for a patient who was admitted to the Klinikum Schwabing (Munich, Germany) for severe acute respiratory infection. We did diagnostic RT -PCR and indirect immunofl uorescence. From time of diagnosis, respiratory, faecal, and urine samples were obtained for virus quantifi cation. We constructed a maximum likelihood tree of the fi ve available complete MERS-CoV genomes.", "citation_count": "29", "reference_count": "442", "date": "2013", "authors": ["Christian Drosten", "Michael Seilmaier", "Victor M. Corman", "Wulf Hartmann", "Gregor Scheible", "Stefan Sack", "Wolfgang Guggemos", "Rene Kallies", "Doreen Muth", "Sandra Junglen", "Marcel A. M\u00fcller", "Walter Haas", "Hana Guberina", "Tim R\u00f6hnisch", "Monika Schmid-Wendtner", "Souhaib Aldabbagh", "Ulf Dittmer", "Hermann Gold", "Petra Graf", "Frank Bonin", "Andrew Rambaut", "Clemens Martin Wendtner"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Viral load", "Respiratory system", "Virus", "Virology", "Biology", "Fatal outcome", "Maximum likelihood tree", "Severe acute respiratory infection"]}
{"id": "2107053896", "references": ["2100820722", "1703839189", "2113457186", "2166867592", "2058144955", "2116586125", "2045002682", "2132260239", "2111412754", "2147166346"], "title": "Hospital Outbreak of Middle East Respiratory Syndrome Coronavirus", "abstract": "Background In September 2012, the World Health Organization reported the first cases of pneumonia caused by the novel Middle East respiratory syndrome coronavirus (MERS-CoV). We describe a cluster of health care\u2013acquired MERS-CoV infections. Methods Medical records were reviewed for clinical and demographic information and determination of potential contacts and exposures. Case patients and contacts were interviewed. The incubation period and serial interval (the time between the successive onset of symptoms in a chain of transmission) were estimated. Viral RNA was sequenced. Results Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV infection were reported in the eastern province of Saudi Arabia. Symptoms included fever in 20 patients (87%), cough in 20 (87%), shortness of breath in 11 (48%), and gastrointestinal symptoms in 8 (35%); 20 patients (87%) presented with abnormal chest radiographs. As of June 12, a total of 15 patients (65%) had died, 6 (26%) had recovered, and 2 (9%) remained ...", "citation_count": "30", "reference_count": "1,270", "date": "2013", "authors": ["Abdullah Assiri", "Allison McGeer", "Trish M. Perl", "Connie S. Price", "Abdullah A. Al Rabeeah", "Derek A.T. Cummings", "Zaki N. Alabdullatif", "Maher Assad", "Abdulmohsen Almulhim", "Hatem Makhdoom", "Hossam Madani", "Rafat Alhakeem", "Jaffar A. Al-Tawfiq", "Matthew Cotten", "Simon J. Watson", "Paul Kellam", "Alimuddin I. Zumla", "Ziad A. Memish"], "related_topics": ["Middle East respiratory syndrome", "Middle East respiratory syndrome coronavirus", "Pneumonia", "Coronavirus", "Incubation period", "Serial interval", "Outbreak", "Disease cluster", "Internal medicine", "Medicine", "Immunology"]}
{"id": "2112147913", "references": ["2102229939", "2113457186", "1690366459", "1035662337", "2166867592", "2066347985", "2105558355", "2060720058", "2079206979"], "title": "Middle East respiratory syndrome coronavirus (MERS-CoV): announcement of the Coronavirus Study Group.", "abstract": "During the summer of 2012, in Jeddah, Saudi Arabia, a hitherto unknown coronavirus (CoV) was isolated from the sputum of a patient with acute pneumonia and renal failure ([1][1], [2][2]). The isolate was provisionally called human coronavirus Erasmus Medical Center (EMC) ([3][3]). Shortly thereafter", "citation_count": "9", "reference_count": "1,166", "date": "2013", "authors": ["R. J. de Groot", "S. C. Baker", "R. S. Baric", "C. S. Brown", "C. Drosten", "L. Enjuanes", "R. A. M. Fouchier", "M. Galiano", "A. E. Gorbalenya", "Z. A. Memish", "S. Perlman", "L. L. M. Poon", "E. J. Snijder", "G. M. Stephens", "P. C. Y. Woo", "A. M. Zaki", "M. Zambon", "J. Ziebuhr"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Coronavirus", "Sputum", "Virology", "Biology", "Acute pneumonia", "Human coronavirus"]}
{"id": "2163627712", "references": ["2100820722", "2104548316", "2463755683", "2158075347", "2136166622", "1966714873", "2131262274", "2136883754", "2125251240", "2115709314"], "title": "Clinical features and short-term outcomes of 144 patients with SARS in the greater Toronto area.", "abstract": "ContextSevere acute respiratory syndrome (SARS) is an emerging infectious disease\nthat first manifested in humans in China in November 2002 and has subsequently\nspread worldwide.ObjectivesTo describe the clinical characteristics and short-term outcomes of\nSARS in the first large group of patients in North America; to describe how\nthese patients were treated and the variables associated with poor outcome.Design, Setting, and PatientsRetrospective case series involving 144 adult patients admitted to 10\nacademic and community hospitals in the greater Toronto, Ontario, area between\nMarch 7 and April 10, 2003, with a diagnosis of suspected or probable SARS.\nPatients were included if they had fever, a known exposure to SARS, and respiratory\nsymptoms or infiltrates observed on chest radiograph. Patients were excluded\nif an alternative diagnosis was determined.Main Outcome MeasuresLocation of exposure to SARS; features of the history, physical examination,\nand laboratory tests at admission to the hospital; and 21-day outcomes such\nas death or intensive care unit (ICU) admission with or without mechanical\nventilation.ResultsOf the 144 patients, 111 (77%) were exposed to SARS in the hospital\nsetting. Features of the clinical examination most commonly found in these\npatients at admission were self-reported fever (99%), documented elevated\ntemperature (85%), nonproductive cough (69%), myalgia (49%), and dyspnea (42%).\nCommon laboratory features included elevated lactate dehydrogenase (87%),\nhypocalcemia (60%), and lymphopenia (54%). Only 2% of patients had rhinorrhea.\nA total of 126 patients (88%) were treated with ribavirin, although its use\nwas associated with significant toxicity, including hemolysis (in 76%) and\ndecrease in hemoglobin of 2 g/dL (in 49%). Twenty-nine patients (20%) were\nadmitted to the ICU with or without mechanical ventilation, and 8 patients\ndied (21-day mortality, 6.5%; 95% confidence interval [CI], 1.9%-11.8%). Multivariable\nanalysis showed that the presence of diabetes (relative risk [RR], 3.1; 95%\nCI, 1.4-7.2; P = .01) or other comorbid conditions\n(RR, 2.5; 95% CI, 1.1-5.8; P = .03) were independently\nassociated with poor outcome (death, ICU admission, or mechanical ventilation).ConclusionsThe majority of cases in the SARS outbreak in the greater Toronto area\nwere related to hospital exposure. In the event that contact history becomes\nunreliable, several features of the clinical presentation will be useful in\nraising the suspicion of SARS. Although SARS is associated with significant\nmorbidity and mortality, especially in patients with diabetes or other comorbid\nconditions, the vast majority (93.5%) of patients in our cohort survived.Published online May 6, 2003 (doi:10.1001/jama.289.21.JOC30885).", "citation_count": "14", "reference_count": "1,620", "date": "2003", "authors": ["Christopher M. Booth", "Larissa M. Matukas", "George A. Tomlinson", "Anita R. Rachlis", "David B. Rose", "Hy A. Dwosh", "Sharon L. Walmsley", "Tony Mazzulli", "Monica Avendano", "Peter Derkach", "Issa E. Ephtimios", "Ian Kitai", "Barbara D. Mederski", "Steven B. Shadowitz", "Wayne L. Gold", "Laura A. Hawryluck", "Elizabeth Rea", "Jordan S. Chenkin", "David W. Cescon", "Susan M. Poutanen", "Allan S. Detsky"], "related_topics": ["Severe acute respiratory syndrome", "Intensive care unit", "Relative risk", "Retrospective cohort study", "Risk factor", "Physical examination", "myalgia", "Mechanical ventilation", "Internal medicine", "Medicine", "Surgery"]}
{"id": "2045002682", "references": ["2129542667", "1703839189", "2088479029", "2166867592", "2158773042", "2130450914", "297155885"], "title": "Family Cluster of Middle East Respiratory Syndrome Coronavirus Infections", "abstract": "A human coronavirus, called the Middle East respiratory syndrome coronavirus (MERS-CoV), was first identified in September 2012 in samples obtained from a Saudi Arabian businessman who died from acute respiratory failure. Since then, 49 cases of infections caused by MERS-CoV (previously called a novel coronavirus) with 26 deaths have been reported to date. In this report, we describe a family case cluster of MERS-CoV infection, including the clinical presentation, treatment outcomes, and household relationships of three young men who became ill with MERS-CoV infection after the hospitalization of an elderly male relative, who died of the disease. Twenty-four other family members living in the same household and 124 attending staff members at the hospitals did not become ill. MERS-CoV infection may cause a spectrum of clinical illness. Although an animal reservoir is suspected, none has been discovered. Meanwhile, global concern rests on the ability of MERS-CoV to cause major illness in close contacts of patients.", "citation_count": "7", "reference_count": "522", "date": "2013", "authors": ["Ziad A. Memish", "Alimuddin I. Zumla", "Rafat F. Al-Hakeem", "Abdullah A. Al-Rabeeah", "Gwen M. Stephens"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Coronavirus", "Pneumonia", "Disease cluster", "Disease", "Pediatrics", "Medicine", "Immunology", "Family cluster", "Human coronavirus", "Treatment outcome"]}
{"id": "2119775949", "references": ["2129542667", "1703839189", "2113457186", "1690366459", "1852588318", "2166867592", "2112147913", "2163627712", "2046153984", "1998725525"], "title": "Clinical features and viral diagnosis of two cases of infection with Middle East Respiratory Syndrome coronavirus: a report of nosocomial transmission", "abstract": "Summary  Background  Human infection with a novel coronavirus named Middle East Respiratory Syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia and the Middle East in September, 2012, with 44 laboratory-confirmed cases as of May 23, 2013. We report detailed clinical and virological data for two related cases of MERS-CoV disease, after nosocomial transmission of the virus from one patient to another in a French hospital.  Methods  Patient 1 visited Dubai in April, 2013; patient 2 lives in France and did not travel abroad. Both patients had underlying immunosuppressive disorders. We tested specimens from the upper (nasopharyngeal swabs) or the lower (bronchoalveolar lavage, sputum) respiratory tract and whole blood, plasma, and serum specimens for MERS-CoV by real-time RT-PCR targeting the upE and Orf1A genes of MERS-CoV.  Findings  Initial clinical presentation included fever, chills, and myalgia in both patients, and for patient 1, diarrhoea. Respiratory symptoms rapidly became predominant with acute respiratory failure leading to mechanical ventilation and extracorporeal membrane oxygenation (ECMO). Both patients developed acute renal failure. MERS-CoV was detected in lower respiratory tract specimens with high viral load (eg, cycle threshold [Ct] values of 22\u00b79 for upE and 24 for Orf1a for a bronchoalveolar lavage sample from patient 1; Ct values of 22\u00b75 for upE and 23\u00b79 for Orf1a for an induced sputum sample from patient 2), whereas nasopharyngeal specimens were weakly positive or inconclusive. The two patients shared the same room for 3 days. The incubation period was estimated at 9\u201312 days for the second case. No secondary transmission was documented in hospital staff despite the absence of specific protective measures before the diagnosis of MERS-CoV was suspected. Patient 1 died on May 28, due to refractory multiple organ failure.  Interpretation  Patients with respiratory symptoms returning from the Middle East or exposed to a confirmed case should be isolated and investigated for MERS-CoV with lower respiratory tract sample analysis and an assumed incubation period of 12 days. Immunosuppression should also be taken into account as a risk factor.  Funding  French Institute for Public Health Surveillance, ANR grant Labex Integrative Biology of Emerging Infectious Diseases, and the European Community's Seventh Framework Programme projects EMPERIE and PREDEMICS.", "citation_count": "22", "reference_count": "476", "date": "2013", "authors": ["Benoit Guery", "Julien Poissy", "Loubna El Mansouf", "Caroline S\u00e9journ\u00e9", "Nicolas Ettahar", "Xavier Lemaire", "Fanny Vuotto", "Anne Goffard", "Sylvie Behillil", "Vincent Enouf", "Val\u00e9rie Caro", "Alexandra Mailles", "Didier Che", "Jean Claude Manuguerra", "Daniel Mathieu", "Arnaud Fontanet", "Sylvie Van Der Werf"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Sputum", "Coronavirus", "Chills", "Extracorporeal membrane oxygenation", "Viral load", "Respiratory tract", "Bronchoalveolar lavage", "Internal medicine", "Medicine", "Intensive care medicine"]}
{"id": "2100820722", "references": ["2597070792", "2161328469", "2152552492", "2135259291", "2097665403", "2463755683", "2170881661", "2136166622", "1898899939", "2093852073"], "title": "Identification of Severe Acute Respiratory Syndrome in Canada", "abstract": "background Severe acute respiratory syndrome (SARS) is a condition of unknown cause that has recently been recognized in patients in Asia, North America, and Europe. This report summarizes the initial epidemiologic findings, clinical description, and diagnostic findings that followed the identification of SARS in Canada. methods SARS was first identified in Canada in early March 2003. We collected epidemiologic, clinical, and diagnostic data from each of the first 10 cases prospectively as they were identified. Specimens from all cases were sent to local, provincial, national, and international laboratories for studies to identify an etiologic agent. results The patients ranged from 24 to 78 years old; 60 percent were men. Transmission occurred only after close contact. The most common presenting symptoms were fever (in 100 percent of cases) and malaise (in 70 percent), followed by nonproductive cough (in 100 percent) and dyspnea (in 80 percent) associated with infiltrates on chest radiography (in 100 percent). Lymphopenia (in 89 percent of those for whom data were available), elevated lactate dehydrogenase levels (in 80 percent), elevated aspartate aminotransferase levels (in 78 percent), and elevated creatinine kinase levels (in 56 percent) were common. Empirical therapy most commonly included antibiotics, oseltamivir, and intravenous ribavirin. Mechanical ventilation was required in five patients. Three patients died, and five have had clinical improvement. The results of laboratory investigations were negative or not clinically significant except for the amplification of human metapneumovirus from respiratory specimens from five of nine patients and the isolation and amplification of a novel coronavirus from five of nine patients. In four cases both pathogens were isolated. conclusions SARS is a condition associated with substantial morbidity and mortality. It appears to be of viral origin, with patterns suggesting droplet or contact transmission. The role of human metapneumovirus, a novel coronavirus, or both requires further investigation.", "citation_count": "22", "reference_count": "1,775", "date": "2003", "authors": ["Susan M. Poutanen", "Donald E. Low", "Bonnie Henry", "Sandy Finkelstein", "David Rose", "Karen Green", "Raymond Tellier", "Ryan Draker", "Dena Adachi", "Melissa Ayers", "Adrienne K. Chan", "Danuta M. Skowronski", "Irving Salit", "Andrew E. Simor", "Arthur S. Slutsky", "Patrick W. Doyle", "Mel Krajden", "Martin Petric", "Robert C. Brunham", "Allison J. McGeer"], "related_topics": ["Severe acute respiratory syndrome", "Human metapneumovirus", "Epidemiology", "Respiratory disease", "Coronavirus", "Contact tracing", "Ribavirin", "Oseltamivir", "Internal medicine", "Medicine", "Surgery"]}
{"id": "2463755683", "references": ["2089784797"], "title": "Update: Outbreak of severe acute respiratory syndrome - Worldwide, 2003", "abstract": "", "citation_count": "1", "reference_count": "345", "date": "2003", "authors": ["T. Tsang", "L. Pak-Yin", "M. Lee", "J.-S. Wu", "Y.-C. Wu", "I.-H. Chiang", "K.-T. Chen", "K.-H. Hsu", "T.-J. Chen", "H.-T. Lee", "S.-J. Twu", "S. Chunsuttiwat", "P. Sawanpanyalert", "K. Ungchusak", "A. Chaovavanich"], "related_topics": ["Outbreak", "Respiratory system", "Medicine", "Emergency medicine"]}
{"id": "2127949919", "references": ["1699123896", "1981329413", "2147898590", "2040567164", "367527820", "2029131064", "1983097458", "2028665290", "2130324980", "2094644220"], "title": "Nipah Virus: A Recently Emergent Deadly Paramyxovirus", "abstract": "A paramyxovirus virus termed Nipah virus has been identified as the etiologic agent of an outbreak of severe encephalitis in people with close contact exposure to pigs in Malaysia and Singapore. The outbreak was first noted in late September 1998 and by mid-June 1999, more than 265 encephalitis cases, including 105 deaths, had been reported in Malaysia, and 11 cases of encephalitis or respiratory illness with one death had been reported in Singapore. Electron microscopic, serologic, and genetic studies indicate that this virus belongs to the family Paramyxoviridae and is most closely related to the recently discovered Hendra virus. We suggest that these two viruses are representative of a new genus within the family Paramyxoviridae. Like Hendra virus, Nipah virus is unusual among the paramyxoviruses in its ability to infect and cause potentially fatal disease in a number of host species, including humans.", "citation_count": "13", "reference_count": "1,316", "date": "2000", "authors": ["K. B. Chua", "W. J. Bellini", "P. A. Rota", "B. H. Harcourt", "A. Tamin", "S. K. Lam", "T. G. Ksiazek", "P. E. Rollin", "S. R. Zaki", "W.-J. Shieh", "C. S. Goldsmith", "D. J. Gubler", "J. T. Roehrig", "B. Eaton", "A. R. Gould", "J. Olson", "P. Daniels", "A. E. Ling", "C. J. Peters", "L. J. Anderson", "B. W. J. Mahy"], "related_topics": ["Hendra Virus", "Tioman virus", "Henipavirus", "Veterinary virology", "Menangle virus", "Virus", "Encephalitis", "Henipavirus Infections", "Virology", "Biology"]}
{"id": "2398786667", "references": ["2104548316", "2126707939", "2119629693", "1981665776", "2556073860", "2127435093", "2101273146", "2098459942", "2023092952"], "title": "Microbial Threats to Health: Emergence, Detection, and Response", "abstract": "Infectious diseases are a global hazard that puts every nation and every person at risk. The recent SARS outbreak is a prime example. Knowing neither geographic nor political borders, often arriving silently and lethally, microbial pathogens constitute a grave threat to the health of humans. Indeed, a majority of countries recently identified the spread of infectious disease as the greatest global problem they confront. Throughout history, humans have struggled to control both the causes and consequences of infectious diseases and we will continue to do so into the foreseeable future.Following up on a high-profile 1992 report from the Institute of Medicine, Microbial Threats to Health examines the current state of knowledge and policy pertaining to emerging and re-emerging infectious diseases from around the globe. It examines the spectrum of microbial threats, factors in disease emergence, and the ultimate capacity of the United States to meet the challenges posed by microbial threats to human health. From the impact of war or technology on disease emergence to the development of enhanced disease surveillance and vaccine strategies, Microbial Threats to Health contains valuable information for researchers, students, health care providers, policymakers, public health officials. and the interested public.", "citation_count": "0", "reference_count": "659", "date": "2003", "authors": ["Mark S. Smolinski", "Margaret A. Hamburg", "Joshua Lederberg"], "related_topics": ["Public health", "Infectious disease (medical specialty)", "Disease surveillance", "Health care", "Hazard", "Outbreak", "Disease", "Economic growth", "Politics", "Political science"]}
{"id": "1576737979", "references": ["1549647993", "2033380151", "2099369211", "2133247102", "1502936039", "2037142940", "2137089963", "2028318125", "2169391021", "2055043387"], "title": "Microarray-based detection and genotyping of viral pathogens", "abstract": "The detection of viral pathogens is of critical importance in biology, medicine, and agriculture. Unfortunately, existing techniques to screen for a broad spectrum of viruses suffer from severe limitations. To facilitate the comprehensive and unbiased analysis of viral prevalence in a given biological setting, we have developed a genomic strategy for highly parallel viral screening. The cornerstone of this approach is a long oligonucleotide (70-mer) DNA microarray capable of simultaneously detecting hundreds of viruses. Using virally infected cell cultures, we were able to efficiently detect and identify many diverse viruses. Related viral serotypes could be distinguished by the unique pattern of hybridization generated by each virus. Furthermore, by selecting microarray elements derived from highly conserved regions within viral families, individual viruses that were not explicitly represented on the microarray were still detected, raising the possibility that this approach could be used for virus discovery. Finally, by using a random PCR amplification strategy in conjunction with the microarray, we were able to detect multiple viruses in human respiratory specimens without the use of sequence-specific or degenerate primers. This method is versatile and greatly expands the spectrum of detectable viruses in a single assay while simultaneously providing the capability to discriminate among viral subtypes.", "citation_count": "21", "reference_count": "931", "date": "2002", "authors": ["David Wang", "Laurent Coscoy", "Maxine Zylberberg", "Pedro C. Avila", "Homer A. Boushey", "Don Ganem", "Joseph L. DeRisi"], "related_topics": ["DNA microarray", "Genotyping", "Virus", "Genome", "Polymerase chain reaction", "Microarray", "Genotype", "Human genetics", "Genetics", "Biology"]}
{"id": "2106882534", "references": ["2143210482", "2045391589", "2065461553", "2097706568", "2102122585", "2149208773", "2009310436", "2008708467", "2015292449", "1998300401"], "title": "CLUSTAL W: IMPROVING THE SENSITIVITY OF PROGRESSIVE MULTIPLE SEQUENCE ALIGNMENT THROUGH SEQUENCE WEIGHTING, POSITION-SPECIFIC GAP PENALTIES AND WEIGHT MATRIX CHOICE", "abstract": "The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available.", "citation_count": "34", "reference_count": "67,974", "date": "1994", "authors": ["Julie D. Thompson", "Desmond G. Higgins", "Toby J. Gibson"], "related_topics": ["Gap penalty", "Multiple sequence alignment", "Structural alignment", "Substitution matrix", "MUSCLE", "Alignment-free sequence analysis", "BLOSUM", "Sequence logo", "Genetics", "Biology"]}
{"id": "2128788856", "references": ["2065996927", "2152552492", "1589490904", "2170881661", "2076770315", "2079378048", "2048618475", "2165127900", "2110198546", "2068094897"], "title": "Human Metapneumovirus Infections in Young and Elderly Adults", "abstract": "Human metapneumovirus virus (hMPV) is a newly discovered respiratory pathogen with limited epidemiological data available. Cohorts of young and older adults were prospectively evaluated for hMPV infection during 2 winter seasons. Patients hospitalized for cardiopulmonary conditions during that period were also studied. Overall, 44 (4.5%) of 984 illnesses were associated with hMPV infection, and 9 (4.1%) of 217 asymptomatic subjects were infected. There was a significant difference in rates of hMPV illnesses between years 1 and 2 (7/452 [1.5%] vs. 37/532 [7.0%]; P&lt;.0001). In the second year, 11% of hospitalized patients had evidence of hMPV infection. Infections occurred in all age groups but were most common among young adults. Frail elderly people with hMPV infection frequently sought medical attention. In conclusion, hMPV infection occurs in adults of all ages and may account for a significant portion of persons hospitalized with respiratory infections during some years.", "citation_count": "10", "reference_count": "696", "date": "2003", "authors": ["Ann R Falsey", "Dean Erdman", "Larry J Anderson", "Edward E Walsh"], "related_topics": ["Human metapneumovirus", "Asymptomatic", "Young adult", "Epidemiology", "Metapneumovirus", "Viral disease", "Pediatrics", "Paramyxoviridae Infections", "Respiratory disease", "Medicine", "Immunology"]}
{"id": "2076620790", "references": ["1582863396", "2106255335", "2034148010", "2148205933", "2002849005", "2123427059", "2078406556", "2020526182", "1997387663", "1972759043"], "title": "A morbillivirus that caused fatal disease in horses and humans", "abstract": "A morbillivirus has been isolated and added to an increasing list of emerging viral diseases. This virus caused an outbreak of fatal respiratory disease in horses and humans. Genetic analyses show it to be only distantly related to the classic morbilliviruses rinderpest, measles, and canine distemper. When seen by electron microscopy, viruses had 10- and 18-nanometer surface projections that gave them a \"double-fringed\" appearance. The virus induced syncytia that developed in the endothelium of blood vessels, particularly the lungs.", "citation_count": "18", "reference_count": "800", "date": "1995", "authors": ["K Murray", "P Selleck", "P Hooper", "A Hyatt", "A Gould", "L Gleeson", "H Westbury", "L Hiley", "L Selvey", "B Rodwell"], "related_topics": ["Morbillivirus", "Rinderpest virus", "Phocine distemper virus", "Paramyxoviridae", "Canine distemper", "Rinderpest", "Hendra Virus", "Menangle virus", "Virology", "Biology"]}
{"id": "2125251240", "references": ["1982444609", "2132293969", "1607298558", "2041775285"], "title": "A cluster of cases of severe acute respiratory syndrome in Hong Kong.", "abstract": "Background Information on the clinical features of the severe acute respiratory syndrome (SARS) will be of value to physicians caring for patients suspected of having this disorder. Methods We abstracted data on the clinical presentation and course of disease in 10 epidemiologically linked Chinese patients (5 men and 5 women 38 to 72 years old) in whom SARS was diagnosed between February 22, 2003, and March 22, 2003, at our hospitals in Hong Kong, China. Results Exposure between the source patient and subsequent patients ranged from minimal to that between patient and health care provider. The incubation period ranged from 2 to 11 days. All patients presented with fever (temperature, &gt;38\u00b0C for over 24 hours), and most presented with rigor, dry cough, dyspnea, malaise, headache, and hypoxemia. Physical examination of the chest revealed crackles and percussion dullness. Lymphopenia was observed in nine patients, and most patients had mildly elevated aminotransferase levels but normal serum creatinine levels...", "citation_count": "4", "reference_count": "1,600", "date": "2003", "authors": ["Kenneth W. Tsang", "Pak L. Ho", "Gaik C. Ooi", "Wilson K. Yee", "Teresa Wang", "Moira Chan-Yeung", "Wah K. Lam", "Wing H. Seto", "Loretta Y. Yam", "Thomas M. Cheung", "Poon C. Wong", "Bing Lam", "Mary S. Ip", "Jane Chan", "Kwok Y. Yuen", "Kar N. Lai"], "related_topics": ["Severe acute respiratory syndrome", "Crackles", "Physical examination", "Hypoxemia", "Respiratory disease", "Malaise", "Pediatrics", "Pharmacotherapy", "Medicine", "Creatinine", "Surgery"]}
{"id": "2129542667", "references": ["2161328469", "2100820722", "2104548316", "2061759246", "1675164605", "2131262274", "2155583106", "2107978811", "2025170735", "2125251240"], "title": "Clinical progression and viral load in a community outbreak of coronavirus-associated SARS pneumonia: a prospective study.", "abstract": "Summary Background We investigated the temporal progression of the clinical, radiological, and virological changes in a community outbreak of severe acute respiratory syndrome (SARS). Methods We followed up 75 patients for 3 weeks managed with a standard treatment protocol of ribavirin and corticosteroids, and assessed the pattern of clinical disease, viral load, risk factors for poor clinical outcome, and the usefulness of virological diagnostic methods. Findings Fever and pneumonia initially improved but 64 (85%) patients developed recurrent fever after a mean of 8\u00b79 (SD 3\u00b71) days, 55 (73%) had watery diarrhoea after 7\u00b75 (2\u00b73) days, 60 (80%) had radiological worsening after 7\u00b74 (2\u00b72) days, and respiratory symptoms worsened in 34 (45%) after 8\u00b76 (3\u00b70) days. In 34 (45%) patients, improvement of initial pulmonary lesions was associated with appearance of new radiological lesions at other sites. Nine (12%) patients developed spontaneous pneumomediastinum and 15 (20%) developed acute respiratory distress syndrome (ARDS) in week 3. Quantitative reverse-transcriptase (RT) PCR of nasopharyngeal aspirates in 14 patients (four with ARDS) showed peak viral load at day 10, and at day 15 a load lower than at admission. Age and chronic hepatitis B virus infection treated with lamivudine were independent significant risk factors for progression to ARDS (p=0\u00b7001). SARS-associated coronavirus in faeces was seen on RT-PCR in 65 (97%) of 67 patients at day 14. The mean time to seroconversion was 20 days. Interpretation The consistent clinical progression, shifting radiological infiltrates, and an inverted V viral-load profile suggest that worsening in week 2 is unrelated to uncontrolled viral replication but may be related to immunopathological damage.", "citation_count": "15", "reference_count": "2,486", "date": "2003", "authors": ["J S M Peiris", "C M Chu", "V C C Cheng", "K S Chan", "I F N Hung", "L L M Poon", "K I Law", "B S F Tang", "T Y W Hon", "C S Chan", "K H Chan", "J S C Ng", "B J Zheng", "W L Ng", "R W M Lai", "Y Guan", "Kwok-Yung Yuen"], "related_topics": ["Severe acute respiratory syndrome", "Viral load", "ARDS", "Pneumonia", "Respiratory disease", "Ribavirin", "Standard treatment", "Coronavirus", "Internal medicine", "Immunology", "Medicine"]}
{"id": "2126707939", "references": ["2129542667", "2104548316", "2100820722", "2131262274", "2116586125", "2132260239", "2147166346", "2025170735", "2125251240", "1966238900"], "title": "Severe acute respiratory syndrome", "abstract": "Severe acute respiratory syndrome (SARS) was caused by a previously unrecognized animal coronavirus that exploited opportunities provided by 'wet markets' in southern China to adapt to become a virus readily transmissible between humans. Hospitals and international travel proved to be 'amplifiers' that permitted a local outbreak to achieve global dimensions. In this review we will discuss the substantial scientific progress that has been made towards understanding the virus-SARS coronavirus (SARS-CoV)-and the disease. We will also highlight the progress that has been made towards developing vaccines and therapies The concerted and coordinated response that contained SARS is a triumph for global public health and provides a new paradigm for the detection and control of future emerging infectious disease threats.", "citation_count": "146", "reference_count": "1,006", "date": "2004", "authors": ["J S M Peiris", "Y Guan", "K Y Yuen"], "related_topics": ["Emerging infectious disease", "Coronavirus", "Global health", "Disease", "Outbreak", "Public health", "Vaccination", "Scientific progress", "Intensive care medicine", "Medicine", "Immunology"]}
{"id": "1987783718", "references": ["2096238447", "2563391002", "2610582798", "2105123265", "2166867592", "2094826575", "2037145800", "3011611380", "2048608755", "3012186724"], "title": "Extracorporeal membrane oxygenation for 2009 Influenza A (H1N1) Acute Respiratory Distress Syndrome", "abstract": "CONTEXT The novel influenza A(H1N1) pandemic affected Australia and New Zealand during the 2009 southern hemisphere winter. It caused an epidemic of critical illness and some patients developed severe acute respiratory distress syndrome (ARDS) and were treated with extracorporeal membrane oxygenation (ECMO). OBJECTIVES To describe the characteristics of all patients with 2009 influenza A(H1N1)-associated ARDS treated with ECMO and to report incidence, resource utilization, and patient outcomes. DESIGN, SETTING, AND PATIENTS An observational study of all patients (n = 68) with 2009 influenza A(H1N1)-associated ARDS treated with ECMO in 15 intensive care units (ICUs) in Australia and New Zealand between June 1 and August 31, 2009. MAIN OUTCOME MEASURES Incidence, clinical features, degree of pulmonary dysfunction, technical characteristics, duration of ECMO, complications, and survival. RESULTS Sixty-eight patients with severe influenza-associated ARDS were treated with ECMO, of whom 61 had either confirmed 2009 influenza A(H1N1) (n = 53) or influenza A not subtyped (n = 8), representing an incidence rate of 2.6 ECMO cases per million population. An additional 133 patients with influenza A received mechanical ventilation but no ECMO in the same ICUs. The 68 patients who received ECMO had a median (interquartile range [IQR]) age of 34.4 (26.6-43.1) years and 34 patients (50%) were men. Before ECMO, patients had severe respiratory failure despite advanced mechanical ventilatory support with a median (IQR) Pao(2)/fraction of inspired oxygen (Fio(2)) ratio of 56 (48-63), positive end-expiratory pressure of 18 (15-20) cm H(2)O, and an acute lung injury score of 3.8 (3.5-4.0). The median (IQR) duration of ECMO support was 10 (7-15) days. At the time of reporting, 48 of the 68 patients (71%; 95% confidence interval [CI], 60%-82%) had survived to ICU discharge, of whom 32 had survived to hospital discharge and 16 remained as hospital inpatients. Fourteen patients (21%; 95% CI, 11%-30%) had died and 6 remained in the ICU, 2 of whom were still receiving ECMO. CONCLUSIONS During June to August 2009 in Australia and New Zealand, the ICUs at regional referral centers provided mechanical ventilation for many patients with 2009 influenza A(H1N1)-associated respiratory failure, one-third of whom received ECMO. These ECMO-treated patients were often young adults with severe hypoxemia and had a 21% mortality rate at the end of the study period.", "citation_count": "0", "reference_count": "1,842", "date": "2009", "authors": ["Andrew Davies", "Daryl Jones", "Michael Bailey", "John Beca", "Rinaldo Bellomo", "Nikki Blackwell", "Paul Forrest", "David Gattas", "Emily Granger", "Robert Herkes", "Andrew Jackson", "Shay McGuinness", "Priya Nair", "Vincent Pellegrino", "Ville Yrjo Olavi Pettila", "Brian Plunkett", "Roger Pye", "Paul Torzillo", "Steven Webb", "Michael Wilson", "Marc Ziegenfuss"], "related_topics": ["Extracorporeal membrane oxygenation", "Intensive care", "Fraction of inspired oxygen", "Respiratory failure", "Lung injury", "ARDS", "Population", "Mechanical ventilation", "Emergency medicine", "Surgery", "Medicine"]}
{"id": "2116586125", "references": ["2048093932", "2027659163", "2126980050", "2152215476", "72835126", "2014171176", "2103854602", "2163400707", "2025170735", "2169198329"], "title": "Characterization of a novel coronavirus associated with severe acute respiratory syndrome", "abstract": "In March 2003, a novel coronavirus (SARS-CoV) was discovered in association with cases of severe acute respiratory syndrome (SARS). The sequence of the complete genome of SARS-CoV was determined, and the initial characterization of the viral genome is presented in this report. The genome of SARS-CoV is 29,727 nucleotides in length and has 11 open reading frames, and its genome organization is similar to that of other coronaviruses. Phylogenetic analyses and sequence comparisons showed that SARS-CoV is not closely related to any of the previously characterized coronaviruses.", "citation_count": "13", "reference_count": "3,067", "date": "2003", "authors": ["Paul A. Rota", "M. Steven Oberste", "Stephan S. Monroe", "W. Allan Nix", "Ray Campagnoli", "Joseph P. Icenogle", "Silvia Pe\u00f1aranda", "Bettina Bankamp", "Kaija Maher", "Min hsin Chen", "Suxiong Tong", "Azaibi Tamin", "Luis Lowe", "Michael Frace", "Joseph L. DeRisi", "Qi Chen", "David Wang", "Dean D. Erdman", "Teresa C.T. Peret", "Cara Burns", "Thomas G. Ksiazek", "Pierre E. Rollin", "Anthony Sanchez", "Stephanie Liffick", "Brian Holloway", "Josef Limor", "Karen McCaustland", "Mellissa Olsen-Rasmussen", "Ron Fouchier", "Stephan G\u00fcnther", "Albert D.H.E. Osterhaus", "Christian Drosten", "Mark A. Pallansch", "Larry J. Anderson", "William J. Bellini"], "related_topics": ["Coronavirus", "Severe acute respiratory syndrome", "Human coronavirus NL63", "Genome", "Coronaviridae", "Genomic organization", "Nucleic acid sequence", "Sequence analysis", "Virology", "Genetics", "Biology"]}
{"id": "2170933940", "references": ["2141885858", "2129542667", "2170881661", "2171091522", "2116586125", "2141877163", "2134061616", "2111412754", "2025170735", "2169198329"], "title": "Characterization and Complete Genome Sequence of a Novel Coronavirus, Coronavirus HKU1, from Patients with Pneumonia", "abstract": "Despite extensive laboratory investigations in patients with respiratory tract infections, no microbiological cause can be identified in a significant proportion of patients. In the past 3 years, several novel respiratory viruses, including human metapneumovirus, severe acute respiratory syndrome (SARS) coronavirus (SARSCoV), and human coronavirus NL63, were discovered. Here we report the discovery of another novel coronavirus, coronavirus HKU1 (CoV-HKU1), from a 71-year-old man with pneumonia who had just returned from Shenzhen, China. Quantitative reverse transcription-PCR showed that the amount of CoV-HKU1 RNA was 8.5 to 9.6 10 6 copies per ml in his nasopharyngeal aspirates (NPAs) during the first week of the illness and dropped progressively to undetectable levels in subsequent weeks. He developed increasing serum levels of specific antibodies against the recombinant nucleocapsid protein of CoV-HKU1, with immunoglobulin M (IgM) titers of 1:20, 1:40, and 1:80 and IgG titers of &lt;1:1,000, 1:2,000, and 1:8,000 in the first, second and fourth weeks of the illness, respectively. Isolation of the virus by using various cell lines, mixed neuron-glia culture, and intracerebral inoculation of suckling mice was unsuccessful. The complete genome sequence of CoV-HKU1 is a 29,926-nucleotide, polyadenylated RNA, with GC content of 32%, the lowest among all known coronaviruses with available genome sequence. Phylogenetic analysis reveals that CoV-HKU1 is a new group 2 coronavirus. Screening of 400 NPAs, negative for SARS-CoV, from patients with respiratory illness during the SARS period identified the presence of CoV-HKU1 RNA in an additional specimen, with a viral load of 1.13 10 6 copies per ml, from a 35-year-old woman with pneumonia. Our data support the existence of a novel group 2 coronavirus associated with pneumonia in humans.", "citation_count": "52", "reference_count": "1,484", "date": "2005", "authors": ["Patrick C. Y. Woo", "Susanna K. P. Lau", "Chung-ming Chu", "Kwok-hung Chan", "Hoi-wah Tsoi", "Yi Huang", "Beatrice H. L. Wong", "Rosana W. S. Poon", "James J. Cai", "Wei-kwang Luk", "Leo L. M. Poon", "Samson S. Y. Wong", "Yi Guan", "J. S. Malik Peiris", "Kwok-yung Yuen"], "related_topics": ["Human coronavirus OC43", "Coronavirus", "Human coronavirus HKU1", "Human coronavirus NL63", "Human metapneumovirus", "Respiratory tract infections", "Pneumonia", "Coronaviridae", "Virology", "Biology"]}
{"id": "2111412754", "references": ["2129542667", "2104548316", "2029293367", "2116586125", "2159857626", "2166229810", "2132260239", "2163400707", "2134061616", "2169198329"], "title": "Identification of a new human coronavirus", "abstract": "Three human coronaviruses are known to exist: human coronavirus 229E (HCoV-229E), HCoV-OC43 and severe acute respiratory syndrome (SARS)-associated coronavirus (SARS-CoV). Here we report the identification of a fourth human coronavirus, HCoV-NL63, using a new method of virus discovery. The virus was isolated from a 7-month-old child suffering from bronchiolitis and conjunctivitis. The complete genome sequence indicates that this virus is not a recombinant, but rather a new group 1 coronavirus. The in vitro host cell range of HCoV-NL63 is notable because it replicates on tertiary monkey kidney cells and the monkey kidney LLC-MK2 cell line. The viral genome contains distinctive features, including a unique N-terminal fragment within the spike protein. Screening of clinical specimens from individuals suffering from respiratory illness identified seven additional HCoV-NL63-infected individuals, indicating that the virus was widely spread within the human population.", "citation_count": "39", "reference_count": "1,831", "date": "2004", "authors": ["Lia van der Hoek", "Krzysztof Pyrc", "Maarten F Jebbink", "Wilma Vermeulen-Oost", "Ron J M Berkhout", "Katja C Wolthers", "Pauline M E Wertheim-van Dillen", "Jos Kaandorp", "Joke Spaargaren", "Ben Berkhout"], "related_topics": ["Human coronavirus OC43", "Human coronavirus 229E", "Coronavirus", "Human coronavirus NL63", "Human coronavirus HKU1", "Virus", "Population", "Alphacoronavirus", "Virology", "Biology"]}
{"id": "2025170735", "references": ["2081510963", "2104730345", "2154664055", "2141291230", "2336133541", "1970720481", "576359727", "2116682907", "2122399224", "2239493136"], "title": "Coronavirus as a possible cause of severe acute respiratory syndrome", "abstract": "Summary  Background  An outbreak of severe acute respiratory syndrome (SARS) has been reported in Hong Kong. We investigated the viral cause and clinical presentation among 50 patients.  Methods  We analysed case notes and microbiological findings for 50 patients with SARS, representing more than five separate epidemiologically linked transmission clusters. We defined the clinical presentation and risk factors associated with severe disease and investigated the causal agents by chest radiography and laboratory testing of nasopharyngeal aspirates and sera samples. We compared the laboratory findings with those submitted for microbiological investigation of other diseases from patients whose identity was masked.  Findings  Patients' age ranged from 23 to 74 years. Fever, chills, myalgia, and cough were the most frequent complaints. When compared with chest radiographic changes, respiratory symptoms and auscultatory findings were disproportionally mild. Patients who were household contacts of other infected people and had older age, lymphopenia, and liver dysfunction were associated with severe disease. A virus belonging to the family  Coronaviridae  was isolated from two patients. By use of serological and reverse-transcriptase PCR specific for this virus, 45 of 50 patients with SARS, but no controls, had evidence of infection with this virus.  Interpretation  A coronavirus was isolated from patients with SARS that might be the primary agent associated with this disease. Serological and molecular tests specific for the virus permitted a definitive laboratory diagnosis to be made and allowed further investigation to define whether other cofactors play a part in disease progression.", "citation_count": "11", "reference_count": "3,689", "date": "2003", "authors": ["Jsm Peiris", "ST Lai", "Llm Poon", "Y Guan", "Lyc Yam", "W Lim", "J Nicholls", "Wks Yee", "WW Yan", "MT Cheung", "Vcc Cheng", "KH Chan", "Dnc Tsang", "Rwh Yung", "TK Ng", "KY Yuen"], "related_topics": ["Severe acute respiratory syndrome", "Human coronavirus NL63", "Coronavirus", "Human coronavirus HKU1", "Pneumonia", "myalgia", "Viral disease", "Chills", "Internal medicine", "Immunology", "Medicine"]}
{"id": "1963953102", "references": ["2113457186", "2170881661", "2166867592", "2145525392", "1986937402", "589702940", "2029197798", "2078917493", "2037210253", "2141047744"], "title": "Fingerprinting genomes using PCR with arbitrary primers", "abstract": "Simple and reproducible fingerprints of complex genomes can be generated using single arbitrarily chosen primers and the polymerase chain reaction (PCR). No prior sequence information is required. The method, arbitrarily primed PCR (AP-PCR), involves two cycles of low stringency amplification followed by PCR at higher stringency. We show that strains can be distinguished by comparing polymorphisms in genomic fingerprints. The generality of the method is demonstrated by application to twenty four strains from five species of Staphylococcus, eleven strains of Streptococcus pyogenes and three varieties of Oryza sativa (rice).", "citation_count": "0", "reference_count": "8,329", "date": "1990", "authors": ["John Welsh", "Michael McClelland"], "related_topics": ["RAPD", "DNA profiling", "Polymerase chain reaction", "Genomic organization", "Primer (molecular biology)", "Genome", "Molecular probe", "Oryza sativa", "Genetics", "Biology"]}
{"id": "2793022939", "references": ["2104548316", "2195009776", "2166867592", "311927316", "2292021561", "2586093485", "2255243349", "2298153446", "2115555188", "2725497285"], "title": "Coronavirus Susceptibility to the Antiviral Remdesivir (GS-5734) Is Mediated by the Viral Polymerase and the Proofreading Exoribonuclease", "abstract": "Emerging coronaviruses (CoVs) cause severe disease in humans, but no approved therapeutics are available. The CoV nsp14 exoribonuclease (ExoN) has complicated development of antiviral nucleosides due to its proofreading activity. We recently reported that the nucleoside analogue GS-5734 (remdesivir) potently inhibits human and zoonotic CoVs in vitro and in a severe acute respiratory syndrome coronavirus (SARS-CoV) mouse model. However, studies with GS-5734 have not reported resistance associated with GS-5734, nor do we understand the action of GS-5734 in wild-type (WT) proofreading CoVs. Here, we show that GS-5734 inhibits murine hepatitis virus (MHV) with similar 50% effective concentration values (EC50) as SARS-CoV and Middle East respiratory syndrome coronavirus (MERS-CoV). Passage of WT MHV in the presence of the GS-5734 parent nucleoside selected two mutations in the nsp12 polymerase at residues conserved across all CoVs that conferred up to 5.6-fold resistance to GS-5734, as determined by EC50 The resistant viruses were unable to compete with WT in direct coinfection passage in the absence of GS-5734. Introduction of the MHV resistance mutations into SARS-CoV resulted in the same in vitro resistance phenotype and attenuated SARS-CoV pathogenesis in a mouse model. Finally, we demonstrate that an MHV mutant lacking ExoN proofreading was significantly more sensitive to GS-5734. Combined, the results indicate that GS-5734 interferes with the nsp12 polymerase even in the setting of intact ExoN proofreading activity and that resistance can be overcome with increased, nontoxic concentrations of GS-5734, further supporting the development of GS-5734 as a broad-spectrum therapeutic to protect against contemporary and emerging CoVs.IMPORTANCE Coronaviruses (CoVs) cause severe human infections, but there are no approved antivirals to treat these infections. Development of nucleoside-based therapeutics for CoV infections has been hampered by the presence of a proofreading exoribonuclease. Here, we expand the known efficacy of the nucleotide prodrug remdesivir (GS-5734) to include a group \u03b2-2a CoV. Further, GS-5734 potently inhibits CoVs with intact proofreading. Following selection with the GS-5734 parent nucleoside, 2 amino acid substitutions in the nsp12 polymerase at residues that are identical across CoVs provide low-level resistance to GS-5734. The resistance mutations decrease viral fitness of MHV in vitro and attenuate pathogenesis in a SARS-CoV animal model of infection. Together, these studies define the target of GS-5734 activity and demonstrate that resistance is difficult to select, only partial, and impairs fitness and virulence of MHV and SARS-CoV, supporting further development of GS-5734 as a potential effective pan-CoV antiviral.", "citation_count": "58", "reference_count": "781", "date": "2018", "authors": ["Maria L. Agostini", "Erica L. Andres", "Amy C Sims", "Rachel Lauren Graham", "Timothy Patrick Sheahan", "Xiaotao Lu", "Everett Clinton Smith", "James Brett Case", "Joy Y. Feng", "Robert Jordan", "Adrian S. Ray", "Tomas Cihlar", "Dustin Siegel", "Richard L. Mackman", "Michael O. Clarke", "Ralph S Baric", "Mark R. Denison"], "related_topics": ["Coronavirus", "Proofreading", "Nucleoside analogue", "Polymerase", "Viral replication", "Virus", "Mutation", "Exoribonuclease", "Virology", "Biology"]}
{"id": "2470646526", "references": ["2006434809", "2129542667", "2104548316", "2138324310", "2131262274", "2166867592", "1993577573", "2132260239", "1909499787", "2025170735"], "title": "SARS and MERS: recent insights into emerging coronaviruses", "abstract": "The emergence of Middle East respiratory syndrome coronavirus (MERS-CoV) in 2012 marked the second introduction of a highly pathogenic coronavirus into the human population in the twenty-first century. The continuing introductions of MERS-CoV from dromedary camels, the subsequent travel-related viral spread, the unprecedented nosocomial outbreaks and the high case-fatality rates highlight the need for prophylactic and therapeutic measures. Scientific advancements since the 2002-2003 severe acute respiratory syndrome coronavirus (SARS-CoV) pandemic allowed for rapid progress in our understanding of the epidemiology and pathogenesis of MERS-CoV and the development of therapeutics. In this Review, we detail our present understanding of the transmission and pathogenesis of SARS-CoV and MERS-CoV, and discuss the current state of development of measures to combat emerging coronaviruses.", "citation_count": "156", "reference_count": "2,421", "date": "2016", "authors": ["Emmie de Wit", "Neeltje van Doremalen", "Darryl Falzarano", "Vincent J. Munster"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome coronavirus", "Population", "Viral pathogenesis", "Pandemic", "Transmission (medicine)", "Virology", "Biology", "Highly pathogenic", "Severe acute respiratory syndrome coronavirus"]}
{"id": "1993577573", "references": ["2049975503", "2104548316", "2103503670", "2166867592", "2119111857", "1990059132", "2141008678", "2140338292", "1966238900", "2101063972"], "title": "Isolation and characterization of a bat SARS-like coronavirus that uses the ACE2 receptor", "abstract": "The 2002-3 pandemic caused by severe acute respiratory syndrome coronavirus (SARS-CoV) was one of the most significant public health events in recent history. An ongoing outbreak of Middle East respiratory syndrome coronavirus suggests that this group of viruses remains a key threat and that their distribution is wider than previously recognized. Although bats have been suggested to be the natural reservoirs of both viruses, attempts to isolate the progenitor virus of SARS-CoV from bats have been unsuccessful. Diverse SARS-like coronaviruses (SL-CoVs) have now been reported from bats in China, Europe and Africa, but none is considered a direct progenitor of SARS-CoV because of their phylogenetic disparity from this virus and the inability of their spike proteins to use the SARS-CoV cellular receptor molecule, the human angiotensin converting enzyme II (ACE2). Here we report whole-genome sequences of two novel bat coronaviruses from Chinese horseshoe bats (family: Rhinolophidae) in Yunnan, China: RsSHC014 and Rs3367. These viruses are far more closely related to SARS-CoV than any previously identified bat coronaviruses, particularly in the receptor binding domain of the spike protein. Most importantly, we report the first recorded isolation of a live SL-CoV (bat SL-CoV-WIV1) from bat faecal samples in Vero E6 cells, which has typical coronavirus morphology, 99.9% sequence identity to Rs3367 and uses ACE2 from humans, civets and Chinese horseshoe bats for cell entry. Preliminary in vitro testing indicates that WIV1 also has a broad species tropism. Our results provide the strongest evidence to date that Chinese horseshoe bats are natural reservoirs of SARS-CoV, and that intermediate hosts may not be necessary for direct human infection by some bat SL-CoVs. They also highlight the importance of pathogen-discovery programs targeting high-risk wildlife groups in emerging disease hotspots as a strategy for pandemic preparedness.", "citation_count": "30", "reference_count": "1,361", "date": "2013", "authors": ["Xing Yi Ge", "Jia Lu Li", "Xing Lou Yang", "Aleksei A. Chmura", "Guangjian Zhu", "Jonathan H. Epstein", "Jonna A Mazet", "Ben Hu", "Wei Zhang", "Cheng Peng", "Yu Ji Zhang", "Chu Ming Luo", "Bing Tan", "Ning Wang", "Yan Zhu", "Gary Crameri", "Shu Yi Zhang", "Lin Fa Wang", "Peter Daszak", "Zheng Li Shi"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome coronavirus", "Virus genetics", "Alphacoronavirus", "Virus", "Outbreak", "Tropism", "Rhinolophus sinicus", "Virology", "Biology"]}
{"id": "2292021561", "references": ["2072202424", "277041599", "2094993149", "2326558924", "2138192885", "1975876487", "1971292277", "2030160453", "2117671399", "2314681741"], "title": "Therapeutic efficacy of the small molecule GS-5734 against Ebola virus in rhesus monkeys", "abstract": "The most recent Ebola virus outbreak in West Africa, which was unprecedented in the number of cases and fatalities, geographic distribution, and number of nations affected, highlights the need for safe, effective, and readily available antiviral agents for treatment and prevention of acute Ebola virus (EBOV) disease (EVD) or sequelae. No antiviral therapeutics have yet received regulatory approval or demonstrated clinical efficacy. Here we report the discovery of a novel small molecule GS-5734, a monophosphoramidate prodrug of an adenosine analogue, with antiviral activity against EBOV. GS-5734 exhibits antiviral activity against multiple variants of EBOV and other filoviruses in cell-based assays. The pharmacologically active nucleoside triphosphate (NTP) is efficiently formed in multiple human cell types incubated with GS-5734 in vitro, and the NTP acts as an alternative substrate and RNA-chain terminator in primer-extension assays using a surrogate respiratory syncytial virus RNA polymerase. Intravenous administration of GS-5734 to nonhuman primates resulted in persistent NTP levels in peripheral blood mononuclear cells (half-life, 14\u2009h) and distribution to sanctuary sites for viral replication including testes, eyes, and brain. In a rhesus monkey model of EVD, once-daily intravenous administration of 10\u2009mg\u2009kg(-1) GS-5734 for 12 days resulted in profound suppression of EBOV replication and protected 100% of EBOV-infected animals against lethal disease, ameliorating clinical disease signs and pathophysiological markers, even when treatments were initiated three days after virus exposure when systemic viral RNA was detected in two out of six treated animals. These results show the first substantive post-exposure protection by a small-molecule antiviral compound against EBOV in nonhuman primates. The broad-spectrum antiviral activity of GS-5734 in vitro against other pathogenic RNA viruses, including filoviruses, arenaviruses, and coronaviruses, suggests the potential for wider medical use. GS-5734 is amenable to large-scale manufacturing, and clinical studies investigating the drug safety and pharmacokinetics are ongoing.", "citation_count": "31", "reference_count": "940", "date": "2016", "authors": ["Travis K. Warren", "Robert Jordan", "Michael K. Lo", "Adrian S. Ray", "Richard L. Mackman", "Veronica Soloveva", "Dustin Siegel", "Michel Perron", "Roy Bannister", "Hon C. Hui", "Nate Larson", "Robert Strickley", "Jay Wells", "Kelly S. Stuthman", "Sean A. Van Tongeren", "Nicole L. Garza", "Ginger Donnelly", "Amy C. Shurtleff", "Cary J. Retterer", "Dima Gharaibeh", "Rouzbeh Zamani", "Tara Kenny", "Brett P. Eaton", "Elizabeth Grimes", "Lisa S. Welch", "Laura Gomba", "Catherine L. Wilhelmsen", "Donald K. Nichols", "Jonathan E. Nuss", "Elyse R. Nagle", "Jeffrey R. Kugelman", "Gustavo Palacios", "Edward Doerffler", "Sean Neville", "Ernest Carra", "Michael O. Clarke", "Lijun Zhang", "Willard Lew", "Bruce Ross", "Queenie Wang", "Kwon Chun", "Lydia Wolfe", "Darius Babusis", "Yeojin Park", "Kirsten M. Stray", "Iva Trancheva", "Joy Y. Feng", "Ona Barauskas", "Yili Xu", "Pamela Wong"], "related_topics": ["Ebola virus", "Virus", "Viral pathogenesis", "Viral replication", "Ebolavirus", "Drug development", "Virology", "Peripheral blood mononuclear cell", "RNA", "Immunology", "Biology"]}
{"id": "2255243349", "references": ["2006434809", "2129542667", "2104548316", "2138324310", "2107053896", "2166867592", "2131262274", "1993577573", "2132260239", "2025170735"], "title": "Coronaviruses - drug discovery and therapeutic options.", "abstract": "Severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which are caused by coronaviruses, have attracted substantial attention owing to their high mortality rates and potential to cause epidemics. Yuen and colleagues discuss progress with treatment options for these syndromes, including virus- and host-targeted drugs, and the challenges that need to be overcome in their further development. In humans, infections with the human coronavirus (HCoV) strains HCoV-229E, HCoV-OC43, HCoV-NL63 and HCoV-HKU1 usually result in mild, self-limiting upper respiratory tract infections, such as the common cold. By contrast, the CoVs responsible for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which were discovered in Hong Kong, China, in 2003, and in Saudi Arabia in 2012, respectively, have received global attention over the past 12 years owing to their ability to cause community and health-care-associated outbreaks of severe infections in human populations. These two viruses pose major challenges to clinical management because there are no specific antiviral drugs available. In this Review, we summarize the epidemiology, virology, clinical features and current treatment strategies of SARS and MERS, and discuss the discovery and development of new virus-based and host-based therapeutic options for CoV infections.", "citation_count": "299", "reference_count": "1,223", "date": "2016", "authors": ["Alimuddin Zumla", "Jasper F. W. Chan", "Esam I. Azhar", "David S. C. Hui", "Kwok-Yung Yuen"], "related_topics": ["Middle East respiratory syndrome", "Respiratory tract infections", "Common cold", "Outbreak", "Virus", "Epidemiology", "Virology", "Drug discovery", "Biology", "High mortality"]}
{"id": "2290466312", "references": ["2080335269", "2024938615", "2034194552", "2057666951", "2160483062", "2065093669", "2100521244", "2142366069"], "title": "A simple practice guide for dose conversion between animals and human.", "abstract": "Understanding the concept of extrapolation of dose between species is important for pharmaceutical researchers when initiating new animal or human experiments. Interspecies allometric scaling for dose conversion from animal to human studies is one of the most controversial areas in clinical pharmacology. Allometric approach considers the differences in body surface area, which is associated with animal weight while extrapolating the doses of therapeutic agents among the species. This review provides basic information about translation of doses between species and estimation of starting dose for clinical trials using allometric scaling. The method of calculation of injection volume for parenteral formulation based on human equivalent dose is also briefed.", "citation_count": "8", "reference_count": "1,887", "date": "2016", "authors": ["Anroop B Nair", "Shery Jacob"], "related_topics": ["Clinical pharmacology", "Body surface area", "Clinical trial", "Pharmacology", "Biology", "Dose conversion", "Human studies", "Injection volume"]}
{"id": "2565805236", "references": ["3001118548", "2138324310", "3007866917", "1843578997", "3014604938", "2610356532", "3000413850", "3015696390", "3007537254", "3158868742"], "title": "Middle East Respiratory Syndrome Coronavirus (MERS-CoV)", "abstract": "", "citation_count": "0", "reference_count": "654", "date": "2016", "authors": ["Geethamma Jolly"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Virology", "Medicine"]}
{"id": "2107277218", "references": ["2123325948", "2069943574", "2145879504", "2164578725", "2109970232", "2134343377", "1756433044", "2128088040", "1566892773", "1983241347"], "title": "ANALYSIS OF RELATIVE GENE EXPRESSION DATA USING REAL-TIME QUANTITATIVE PCR AND THE 2(-DELTA DELTA C(T)) METHOD", "abstract": "The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.", "citation_count": "11", "reference_count": "135,572", "date": "2001", "authors": ["Kenneth J. Livak", "Thomas D. Schmittgen"], "related_topics": ["Cell wall organization", "Protein kinase B signaling", "Lupeol synthase", "Endosperm cellularization", "Cell wall modification", "Female sex determination", "Floral organ abscission", "Isoflavonoid biosynthesis", "Molecular biology", "Biology", "Bioinformatics"]}
{"id": "2791599184", "references": ["1977050884", "1703839189", "2150120685", "2591646177", "2345375456", "2139937737", "2166867592", "2586093485", "2034462612", "2145758369"], "title": "Treatment of Middle East Respiratory Syndrome with a combination of lopinavir-ritonavir and interferon-\u03b21b (MIRACLE trial): study protocol for a randomized controlled trial.", "abstract": "It had been more than 5\u00a0years since the first case of Middle East Respiratory Syndrome coronavirus infection (MERS-CoV) was recorded, but no specific treatment has been investigated in randomized clinical trials. Results from in vitro and animal studies suggest that a combination of lopinavir/ritonavir and interferon-\u03b21b (IFN-\u03b21b) may be effective against MERS-CoV. The aim of this study is to investigate the efficacy of treatment with a combination of lopinavir/ritonavir and recombinant IFN-\u03b21b provided with standard supportive care, compared to treatment with placebo provided with standard supportive care in patients with laboratory-confirmed MERS requiring hospital admission. The protocol is prepared in accordance with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) guidelines. Hospitalized adult patients with laboratory-confirmed MERS will be enrolled in this recursive, two-stage, group sequential, multicenter, placebo-controlled, double-blind randomized controlled trial. The trial is initially designed to include 2 two-stage components. The first two-stage component is designed to adjust sample size and determine futility stopping, but not efficacy stopping. The second two-stage component is designed to determine efficacy stopping and possibly readjustment of sample size. The primary outcome is 90-day mortality. This will be the first randomized controlled trial of a potential treatment for MERS. The study is sponsored by King Abdullah International Medical Research Center, Riyadh, Saudi Arabia. Enrollment for this study began in November 2016, and has enrolled thirteen patients as of Jan 24-2018. ClinicalTrials.gov, ID: NCT02845843. Registered on 27 July 2016.", "citation_count": "36", "reference_count": "275", "date": "2018", "authors": ["Yaseen M. Arabi", "Adel Alothman", "Hanan H. Balkhy", "Abdulaziz Al-Dawood", "Sameera AlJohani", "Shmeylan Al Harbi", "Suleiman Kojan", "Majed Al Jeraisy", "Ahmad M. Deeb", "Abdullah M. Assiri", "Fahad Al-Hameed", "Asim AlSaedi", "Yasser Mandourah", "Ghaleb A. Almekhlafi", "Nisreen Murad Sherbeeni", "Fatehi Elnour Elzein", "Javed Memon", "Yusri Taha", "Abdullah Almotairi", "Khalid A. Maghrabi", "Ismael Qushmaq", "Ali Al Bshabshe", "Ayman Kharaba", "Sarah Shalhoub", "Jesna Jose", "Robert A. Fowler", "Frederick G. Hayden", "Mohamed A. Hussein"], "related_topics": ["Lopinavir/ritonavir", "Randomized controlled trial", "Lopinavir", "Clinical trial", "Ritonavir", "Placebo", "Middle East respiratory syndrome", "Middle East respiratory syndrome coronavirus", "Internal medicine", "Medicine"]}
{"id": "2130141864", "references": ["2112302527", "2037712857", "2109779439", "2409448230", "2336287850", "1833207062", "2202941334", "2103828083", "2320270386", "3140139051"], "title": "Practice Guidelines for the Management of Community-Acquired Pneumonia in Adults", "abstract": "John G. Bartlett,1 Scott F Dowell,2 Lionel A. Mandell,6 Thomas M. File, Jr.,3 Daniel M. Musher,4 and Michael J. Fine5 'Johns Hopkins University School of Medicine, Baltimore, Maryland, 2Centers for Disease Control and Prevention, Atlanta, Georgia, 3Northeastern Ohio Universities College of Medicine, Cleveland, Ohio, 4Baylor College of Medicine and Veterans Affairs Medical Center, Houston, Texas, and 5University of Pittsburgh, Pennsylvania, USA; and 6McMaster University, Toronto, Canada", "citation_count": "204", "reference_count": "3,220", "date": "2000", "authors": ["John G Bartlett", "Scott F Dowell", "Lionel A Mandell", "Thomas M File", "Daniel M Musher", "Michael J Fine"], "related_topics": ["Antibacterial agent", "Veterans Affairs", "Atlanta", "Epidemiology", "Library science", "Community-acquired pneumonia", "Medicine", "Intensive care medicine", "Disease control", "Lung disease", "Medical treatment"]}
{"id": "1991467275", "references": ["2626588662", "2076477487", "1971694872", "3025576394", "2037346751", "2006351935", "2331116046", "2030046096", "1981681632", "1976530706"], "title": "Bronchiolitis Obliterans Organizing Pneumonia", "abstract": "Bronchiolar disorders can be divided into 2 general categories: (1) airway disorders (cellular bronchiolitis and obliterative bronchiolitis) and (2) parenchymal disorders (respiratory bronchiolitis-interstitial lung disease, which occurs in smokers and is treatable with smoking cessation or corticosteroid therapy, and bronchiolitis obliterans organizing pneumonia, an inflammatory lung disease simultaneously involving the terminal bronchioles and alveoli). This article reviews the clinical findings and therapeutic management of bronchiolitis obliterans organizing pneumonia.", "citation_count": "82", "reference_count": "337", "date": "2001", "authors": ["Gary R. Epler"], "related_topics": ["Bronchiolitis obliterans organizing pneumonia", "Bronchiolitis", "Respiratory system", "Airway", "Smoking cessation", "Pathology", "Parenchyma", "Medicine", "Lung disease", "Terminal Bronchioles"]}
{"id": "1982444609", "references": ["2149661971", "2111742711", "2131262274", "2416914730", "2103258766", "2114857071", "2151996610", "2034656398", "2163026870", "2125251240"], "title": "Bronchiolitis obliterans organizing pneumonia: CT features in 14 patients.", "abstract": "Bronchiolitis obliterans organizing pneumonia is a disease characterized by the presence of granulation tissue within small airways and the presence of areas of organizing pneumonia. We retrospectively reviewed the chest radiographs, CT scans, and biopsy specimens in 14 consecutive patients with proved bronchiolitis obliterans organizing pneumonia. Six patients were immunocompromised because of leukemia or bone-marrow transplantation. In all patients, 10-mm collimation CT scans were available. In 11 of the 14 patients, select 1.5-mm scans were obtained. The CT findings included patchy unilateral (n = 1) or bilateral air-space consolidation (n = 9), small nodular opacities (n = 7), irregular linear opacities (n = 2), bronchial wall thickening and dilatation (n = 6), and small pleural effusions (n = 4). All patients had areas of air-space consolidation, small nodules, or both. A predominantly subpleural distribution of the air-space consolidation was apparent on the radiographs of two patients and on CT scans of six. Pathologically, the nodules and the consolidation represented different degrees of inflammation in bronchioles, alveolar ducts, and alveoli. Although most of the findings were apparent on the radiographs, the CT scans depicted the anatomic distribution and extent of bronchiolitis obliterans organizing pneumonia more accurately than did the plain chest radiographs.", "citation_count": "0", "reference_count": "271", "date": "1990", "authors": ["N L M\u00fcller", "C A Staples", "R R Miller"], "related_topics": ["Bronchiolitis obliterans organizing pneumonia", "Bronchiolitis obliterans", "Pneumonia", "Transplantation", "Lung", "Respiratory disease", "Thorax", "Biopsy", "Radiology", "Medicine"]}
{"id": "2123324969", "references": ["2130141864", "1988729025", "2115071132", "2158075347", "2339696769", "2035760184", "2004554957", "2336287850", "2320270386", "2027162482"], "title": "Guidelines for the management of adults with community-acquired pneumonia. Diagnosis, assessment of severity, antimicrobial therapy, and prevention.", "abstract": "", "citation_count": "143", "reference_count": "2,491", "date": "2001", "authors": ["M. S. Niederman", "L. A. Mandell", "A. Anzueto", "J. B. Bass", "W. A. Broughton", "G. D. Campbell", "N. Dean", "T. File", "M. J. Fine", "P. A. Gross", "F. Martinez", "T. J. Marrie", "J. F. Plouffe", "J. Ramirez", "G. A. Sarosi", "A. Torres", "R. Wilson", "V. L. Yu"], "related_topics": ["Pneumonia severity index", "Pneumonia", "CURB-65", "Community-acquired pneumonia", "MEDLINE", "Antimicrobial", "Intensive care medicine", "Medicine", "Viral therapy"]}
{"id": "3009739970", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3004318991", "3008827533", "3003465021", "3005079553", "3004280078", "3002108456"], "title": "The deadly coronaviruses: The 2003 SARS pandemic and the 2020 novel coronavirus epidemic in China.", "abstract": "The 2019-nCoV is officially called SARS-CoV-2 and the disease is named COVID-19. This viral epidemic in China has led to the deaths of over 1800 people, mostly elderly or those with an underlying chronic disease or immunosuppressed state. This is the third serious Coronavirus outbreak in less than 20 years, following SARS in 2002-2003 and MERS in 2012. While human strains of Coronavirus are associated with about 15% of cases of the common cold, the SARS-CoV-2 may present with varying degrees of severity, from flu-like symptoms to death. It is currently believed that this deadly Coronavirus strain originated from wild animals at the Huanan market in Wuhan, a city in Hubei province. Bats, snakes and pangolins have been cited as potential carriers based on the sequence homology of CoV isolated from these animals and the viral nucleic acids of the virus isolated from SARS-CoV-2 infected patients. Extreme quarantine measures, including sealing off large cities, closing borders and confining people to their homes, were instituted in January 2020 to prevent spread of the virus, but by that time much of the damage had been done, as human-human transmission became evident. While these quarantine measures are necessary and have prevented a historical disaster along the lines of the Spanish flu, earlier recognition and earlier implementation of quarantine measures may have been even more effective. Lessons learned from SARS resulted in faster determination of the nucleic acid sequence and a more robust quarantine strategy. However, it is clear that finding an effective antiviral and developing a vaccine are still significant challenges. The costs of the epidemic are not limited to medical aspects, as the virus has led to significant sociological, psychological and economic effects globally. Unfortunately, emergence of SARS-CoV-2 has led to numerous reports of Asians being subjected to racist behavior and hate crimes across the world.", "citation_count": "113", "reference_count": "518", "date": "2020", "authors": ["Yongshi Yang", "Fujun Peng", "Runsheng Wang", "Kai Guan", "Taijiao Jiang", "Guogang Xu", "Jinlyu Sun", "Christopher Chang"], "related_topics": ["Pandemic", "Coronavirus", "Quarantine", "Outbreak", "Transmission (medicine)", "Common cold", "Disease", "Virus", "Virology"]}
{"id": "3005655936", "references": ["3001118548", "2999364275", "1964982019", "3001897055", "3003668884", "2005357317", "3002539152", "3017468735", "2091671824", "2025672718"], "title": "Clinical and biochemical indexes from 2019-nCoV infected patients linked to viral loads and lung injury.", "abstract": "The outbreak of the 2019-nCoV infection began in December 2019 in Wuhan, Hubei province, and rapidly spread to many provinces in China as well as other countries. Here we report the epidemiological, clinical, laboratory, and radiological characteristics, as well as potential biomarkers for predicting disease severity in 2019-nCoV-infected patients in Shenzhen, China. All 12 cases of the 2019-nCoV-infected patients developed pneumonia and half of them developed acute respiratory distress syndrome (ARDS). The most common laboratory abnormalities were hypoalbuminemia, lymphopenia, decreased percentage of lymphocytes (LYM) and neutrophils (NEU), elevated C-reactive protein (CRP) and lactate dehydrogenase (LDH), and decreased CD8 count. The viral load of 2019-nCoV detected from patient respiratory tracts was positively linked to lung disease severity. ALB, LYM, LYM (%), LDH, NEU (%), and CRP were highly correlated to the acute lung injury. Age, viral load, lung injury score, and blood biochemistry indexes, albumin (ALB), CRP, LDH, LYM (%), LYM, and NEU (%), may be predictors of disease severity. Moreover, the Angiotensin II level in the plasma sample from 2019-nCoV infected patients was markedly elevated and linearly associated to viral load and lung injury. Our results suggest a number of potential diagnosis biomarkers and angiotensin receptor blocker (ARB) drugs for potential repurposing treatment of 2019-nCoV infection.", "citation_count": "35", "reference_count": "1,378", "date": "2020", "authors": ["Yingxia Liu", "Yang Yang", "Cong Zhang", "Fengming Huang", "Fuxiang Wang", "Jing Yuan", "Zhaoqin Wang", "Jinxiu Li", "Jianming Li", "Cheng Feng", "Zheng Zhang", "Lifei Wang", "Ling Peng", "Li Chen", "Yuhao Qin", "Dandan Zhao", "Shuguang Tan", "Lu Yin", "Jun Xu", "Congzhao Zhou", "Chengyu Jiang", "Lei Liu"], "related_topics": ["Lung injury", "Angiotensin II", "Viral load", "ARDS", "Hypoalbuminemia", "Pneumonia", "Severity of illness", "Respiratory system", "Gastroenterology", "Medicine", "Internal medicine"]}
{"id": "3002764620", "references": ["3001118548", "2999612210", "3001343166", "2582743722", "3002539152", "3017468735", "3002533591", "2147166346", "1998725525", "3026046290"], "title": "Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions", "abstract": "Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39-4.13); 58-76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6-7.4); 21022 (11090-33490) total infections in Wuhan 1 to 22 January.", "citation_count": "15", "reference_count": "679", "date": "2020", "authors": ["Read Jm", "Bridgen", "Cummings Da", "Ho A", "Jewell Cp"], "related_topics": ["Coronavirus", "Basic reproduction number", "Transmission (mechanics)", "Epidemiology", "Estimation", "Environmental health", "Biology", "Case ascertainment", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3015190630", "references": ["3007643904", "2152207030", "3001897055", "3003217347", "3001465255", "1539796472", "2166867592", "3009912996", "2114622716", "3004280078"], "title": "Structural and Functional Basis of SARS-CoV-2 Entry by Using Human ACE2.", "abstract": "The recent emergence of a novel coronavirus (SARS-CoV-2) in China has caused significant public health concerns. Recently, ACE2 was reported as an entry receptor for SARS-CoV-2. In this study, we present the crystal structure of the C-terminal domain of SARS-CoV-2 (SARS-CoV-2-CTD) spike (S) protein in complex with human ACE2 (hACE2), which reveals a hACE2-binding mode similar overall to that observed for SARS-CoV. However, atomic details at the binding interface demonstrate that key residue substitutions in SARS-CoV-2-CTD slightly strengthen the interaction and lead to higher affinity for receptor binding than SARS-RBD. Additionally, a panel of murine monoclonal antibodies (mAbs) and polyclonal antibodies (pAbs) against SARS-CoV-S1/receptor-binding domain (RBD) were unable to interact with the SARS-CoV-2\u00a0S protein, indicating notable differences in antigenicity between SARS-CoV and SARS-CoV-2. These findings shed light on the viral pathogenesis and provide important structural information regarding development of therapeutic countermeasures against the emerging virus.", "citation_count": "58", "reference_count": "1,250", "date": "2020", "authors": ["Qihui Wang", "Yanfang Zhang", "Lili Wu", "Sheng Niu", "Chunli Song", "Zengyuan Zhang", "Guangwen Lu", "Chengpeng Qiao", "Yu Hu", "Kwok Yung Yuen", "Qisheng Wang", "Huan Zhou", "Jinghua Yan", "Jianxun Qi"], "related_topics": ["Coronavirus", "Protein domain", "Viral protein", "Antigenicity", "Monoclonal antibody", "Viral pathogenesis", "Polyclonal antibodies", "Epitope", "Virology", "Biology"]}
{"id": "3010819577", "references": ["3001118548", "3007643904", "3001897055", "3003668884", "3002539152", "3003951199", "3004318991", "3000834295", "3005079553", "3004280078"], "title": "Epidemiology of COVID-19 Among Children in China.", "abstract": "OBJECTIVE:  To identify the epidemiological characteristics and transmission patterns of pediatric patients with the 2019 novel coronavirus disease (COVID-19) in China.  METHODS:  Nationwide case series of 2135 pediatric patients with COVID-19 reported to the Chinese Center for Disease Control and Prevention from January 16, 2020, to February 8, 2020, were included. The epidemic curves were constructed by key dates of disease onset and case diagnosis. Onset-to-diagnosis curves were constructed by fitting a log-normal distribution to data on both onset and diagnosis dates.  RESULTS:  There were 728 (34.1%) laboratory-confirmed cases and 1407 (65.9%) suspected cases. The median age of all patients was 7 years (interquartile range: 2\u201313 years), and 1208 case patients (56.6%) were boys. More than 90% of all patients had asymptomatic, mild, or moderate cases. The median time from illness onset to diagnoses was 2 days (range: 0\u201342 days). There was a rapid increase of disease at the early stage of the epidemic, and then there was a gradual and steady decrease. The disease rapidly spread from Hubei province to surrounding provinces over time. More children were infected in Hubei province than any other province.  CONCLUSIONS:  Children of all ages appeared susceptible to COVID-19, and there was no significant sex difference. Although clinical manifestations of children\u2019s COVID-19 cases were generally less severe than those of adult patients, young children, particularly infants, were vulnerable to infection. The distribution of children\u2019s COVID-19 cases varied with time and space, and most of the cases were concentrated in Hubei province and surrounding areas. Furthermore, this study provides strong evidence of human-to-human transmission.", "citation_count": "15", "reference_count": "2,514", "date": "2020", "authors": ["Yuanyuan Dong", "Xi Mo", "Yabin Hu", "Xin Qi", "Fang Jiang", "Zhongyi Jiang", "Shilu Tong"], "related_topics": ["Epidemiology", "Interquartile range", "Asymptomatic", "Cohort study", "Severity of illness", "Retrospective cohort study", "Pediatrics", "Survival analysis", "Disease", "Medicine"]}
{"id": "3007814559", "references": ["3001118548", "2999364275", "3003668884", "3002539152", "3002812395", "3017468735", "2163627712", "3008827533", "3005079553", "3002108456"], "title": "Clinical characteristics of 140 patients infected with SARS-CoV-2 in Wuhan, China.", "abstract": "Background  Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection has been widely spread. We aim to investigate the clinical characteristic and allergy status of patients infected with SARS-CoV-2.  Methods  Electronic medical records including demographics, clinical manifestation, comorbidities, laboratory data, and radiological materials of 140 hospitalized COVID-19 patients, with confirmed result of SARS-CoV-2 viral infection, were extracted and analyzed.  Results  An approximately 1:1 ratio of male (50.7%) and female COVID-19 patients was found, with an overall median age of 57.0 years. All patients were community-acquired cases. Fever (91.7%), cough (75.0%), fatigue (75.0%), and gastrointestinal symptoms (39.6%) were the most common clinical manifestations, whereas hypertension (30.0%) and diabetes mellitus (12.1%) were the most common comorbidities. Drug hypersensitivity (11.4%) and urticaria (1.4%) were self-reported by several patients. Asthma or other allergic diseases were not reported by any of the patients. Chronic obstructive pulmonary disease (COPD, 1.4%) patients and current smokers (1.4%) were rare. Bilateral ground-glass or patchy opacity (89.6%) was the most common sign of radiological finding. Lymphopenia (75.4%) and eosinopenia (52.9%) were observed in most patients. Blood eosinophil counts correlate positively with lymphocyte counts in severe (r = .486, P   Conclusion  Detailed clinical investigation of 140 hospitalized COVID-19 cases suggests eosinopenia together with lymphopenia may be a potential indicator for diagnosis. Allergic diseases, asthma, and COPD are not risk factors for SARS-CoV-2 infection. Older age, high number of comorbidities, and more prominent laboratory abnormalities were associated with severe patients.", "citation_count": "40", "reference_count": "2,500", "date": "2020", "authors": ["Jin-jin Zhang", "Xiang Dong", "Yi-yuan Cao", "Ya-dong Yuan", "Yi-bin Yang", "You-qin Yan", "Cezmi A. Akdis", "Ya-dong Gao"], "related_topics": ["Eosinopenia", "Asthma", "COPD", "Risk factor", "Allergy", "Comorbidity", "Severity of illness", "Diabetes mellitus", "Internal medicine", "Medicine"]}
{"id": "3007273493", "references": ["2006434809", "3001118548", "2104548316", "2999364275", "2149661971", "2131262274", "2166867592", "3017468735", "2112147913", "2125251240"], "title": "Radiological findings from 81 patients with COVID-19 pneumonia in Wuhan, China: a descriptive study.", "abstract": "Summary  Background  A cluster of patients with coronavirus disease 2019 (COVID-19) pneumonia caused by infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) were successively reported in Wuhan, China. We aimed to describe the CT findings across different timepoints throughout the disease course.  Methods  Patients with COVID-19 pneumonia (confirmed by next-generation sequencing or RT-PCR) who were admitted to one of two hospitals in Wuhan and who underwent serial chest CT scans were retrospectively enrolled. Patients were grouped on the basis of the interval between symptom onset and the first CT scan: group 1 (subclinical patients; scans done before symptom onset), group 2 (scans done \u22641 week after symptom onset), group 3 (&gt;1 week to 2 weeks), and group 4 (&gt;2 weeks to 3 weeks). Imaging features and their distribution were analysed and compared across the four groups.  Findings  81 patients admitted to hospital between Dec 20, 2019, and Jan 23, 2020, were retrospectively enrolled. The cohort included 42 (52%) men and 39 (48%) women, and the mean age was 49\u00b75 years (SD 11\u00b70). The mean number of involved lung segments was 10\u00b75 (SD 6\u00b74) overall, 2\u00b78 (3\u00b73) in group 1, 11\u00b71 (5\u00b74) in group 2, 13\u00b70 (5\u00b77) in group 3, and 12\u00b71 (5\u00b79) in group 4. The predominant pattern of abnormality observed was bilateral (64 [79%] patients), peripheral (44 [54%]), ill-defined (66 [81%]), and ground-glass opacification (53 [65%]), mainly involving the right lower lobes (225 [27%] of 849 affected segments). In group 1 (n=15), the predominant pattern was unilateral (nine [60%]) and multifocal (eight [53%]) ground-glass opacities (14 [93%]). Lesions quickly evolved to bilateral (19 [90%]), diffuse (11 [52%]) ground-glass opacity predominance (17 [81%]) in group 2 (n=21). Thereafter, the prevalence of ground-glass opacities continued to decrease (17 [57%] of 30 patients in group 3, and five [33%] of 15 in group 4), and consolidation and mixed patterns became more frequent (12 [40%] in group 3, eight [53%] in group 4).  Interpretation  COVID-19 pneumonia manifests with chest CT imaging abnormalities, even in asymptomatic patients, with rapid evolution from focal unilateral to diffuse bilateral ground-glass opacities that progressed to or co-existed with consolidations within 1\u20133 weeks. Combining assessment of imaging features with clinical and laboratory findings could facilitate early diagnosis of COVID-19 pneumonia.  Funding  None.", "citation_count": "27", "reference_count": "2,664", "date": "2020", "authors": ["Heshui Shi", "Xiaoyu Han", "Nanchuan Jiang", "Yukun Cao", "Osamah Alwalid", "Jin Gu", "Yanqing Fan", "Chuansheng Zheng"], "related_topics": ["Pneumonia", "Asymptomatic", "Subclinical infection", "Cohort", "Lung", "Internal medicine", "Medicine", "Radiological weapon", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2775086803", "references": ["2103503670", "2195009776", "1993577573", "2141008678", "2111211467", "2141877163", "2140338292", "2298153446", "2134061616", "2169198329"], "title": "Discovery of a rich gene pool of bat SARS-related coronaviruses provides new insights into the origin of SARS coronavirus.", "abstract": "A large number of SARS-related coronaviruses (SARSr-CoV) have been detected in horseshoe bats since 2005 in different areas of China. However, these bat SARSr-CoVs show sequence differences from SARS coronavirus (SARS-CoV) in different genes (S, ORF8, ORF3, etc) and are considered unlikely to represent the direct progenitor of SARS-CoV. Herein, we report the findings of our 5-year surveillance of SARSr-CoVs in a cave inhabited by multiple species of horseshoe bats in Yunnan Province, China. The full-length genomes of 11 newly discovered SARSr-CoV strains, together with our previous findings, reveals that the SARSr-CoVs circulating in this single location are highly diverse in the S gene, ORF3 and ORF8. Importantly, strains with high genetic similarity to SARS-CoV in the hypervariable N-terminal domain (NTD) and receptor-binding domain (RBD) of the S1 gene, the ORF3 and ORF8 region, respectively, were all discovered in this cave. In addition, we report the first discovery of bat SARSr-CoVs highly similar to human SARS-CoV in ORF3b and in the split ORF8a and 8b. Moreover, SARSr-CoV strains from this cave were more closely related to SARS-CoV in the non-structural protein genes ORF1a and 1b compared with those detected elsewhere. Recombination analysis shows evidence of frequent recombination events within the S gene and around the ORF8 between these SARSr-CoVs. We hypothesize that the direct progenitor of SARS-CoV may have originated after sequential recombination events between the precursors of these SARSr-CoVs. Cell entry studies demonstrated that three newly identified SARSr-CoVs with different S protein sequences are all able to use human ACE2 as the receptor, further exhibiting the close relationship between strains in this cave and SARS-CoV. This work provides new insights into the origin and evolution of SARS-CoV and highlights the necessity of preparedness for future emergence of SARS-like diseases.", "citation_count": "43", "reference_count": "602", "date": "2017", "authors": ["Ben Hu", "Lei Ping Zeng", "Xing Lou Yang", "Xing Yi Ge", "Wei Zhang", "Bei Li", "Jia Zheng Xie", "Xu Rui Shen", "Yun Zhi Zhang", "Ning Wang", "Dong Sheng Luo", "Xiao Shuang Zheng", "Mei Niang Wang", "Peter Daszak", "Lin Fa Wang", "Jie Cui", "Zheng Li Shi"], "related_topics": ["Gene pool", "Sequence analysis", "Genome", "Cave", "Gene", "Genomics", "Sequence alignment", "Evolutionary biology", "Polymerase chain reaction", "Biology"]}
{"id": "2119111857", "references": ["1964982019", "2113457186", "1690366459", "2103503670", "2126707939", "2166867592", "2091671824", "1982533785", "1966238900", "2101063972"], "title": "Dipeptidyl peptidase 4 is a functional receptor for the emerging human coronavirus-EMC", "abstract": "Most human coronaviruses cause mild upper respiratory tract disease but may be associated with more severe pulmonary disease in immunocompromised individuals. However, SARS coronavirus caused severe lower respiratory disease with nearly 10% mortality and evidence of systemic spread. Recently, another coronavirus (human coronavirus-Erasmus Medical Center (hCoV-EMC)) was identified in patients with severe and sometimes lethal lower respiratory tract infection. Viral genome analysis revealed close relatedness to coronaviruses found in bats. Here we identify dipeptidyl peptidase 4 (DPP4; also known as CD26) as a functional receptor for hCoV-EMC. DPP4 specifically co-purified with the receptor-binding S1 domain of the hCoV-EMC spike protein from lysates of susceptible Huh-7 cells. Antibodies directed against DPP4 inhibited hCoV-EMC infection of primary human bronchial epithelial cells and Huh-7 cells. Expression of human and bat (Pipistrellus pipistrellus) DPP4 in non-susceptible COS-7 cells enabled infection by hCoV-EMC. The use of the evolutionarily conserved DPP4 protein from different species as a functional receptor provides clues about the host range potential of hCoV-EMC. In addition, it will contribute critically to our understanding of the pathogenesis and epidemiology of this emerging human coronavirus, and may facilitate the development of intervention strategies.", "citation_count": "25", "reference_count": "1,602", "date": "2013", "authors": ["V. Stalin Raj", "Huihui Mou", "Saskia L. Smits", "Dick H. W. Dekkers", "Marcel A. M\u00fcller", "Ronald Dijkman", "Doreen Muth", "Jeroen A. A. Demmers", "Ali Zaki", "Ron A. M. Fouchier", "Volker Thiel", "Christian Drosten", "Peter J. M. Rottier", "Albert D. M. E. Osterhaus", "Berend Jan Bosch", "Bart L. Haagmans"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome", "Middle East respiratory syndrome coronavirus", "Dipeptidyl peptidase-4", "Viral pathogenesis", "Lower respiratory tract infection", "Virus genetics", "Respiratory disease", "Virology", "Biology"]}
{"id": "2144081223", "references": ["2130479394", "1969222787", "2135839939", "1986830449", "2013083986"], "title": "Coot: model-building tools for molecular graphics.", "abstract": "CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as `Coot'.", "citation_count": "5", "reference_count": "26,438", "date": "2004", "authors": ["Paul Emsley", "Kevin Cowtan"], "related_topics": ["Computer graphics", "Molecular graphics", "Enzyme structure", "Graphics", "Oxidoreductase inhibitor", "Model building", "Software engineering", "Computer science", "Software", "Structure (mathematical logic)", "Bioinformatics"]}
{"id": "2306794997", "references": ["2006434809", "2129542667", "2138324310", "2166867592", "1993577573", "2134061616", "2160011624", "2115555188", "2025170735", "2125251240"], "title": "Epidemiology, Genetic Recombination, and Pathogenesis of Coronaviruses", "abstract": "Human coronaviruses (HCoVs) were first described in the 1960s for patients with the common cold. Since then, more HCoVs have been discovered, including those that cause severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), two pathogens that, upon infection, can cause fatal respiratory disease in humans. It was recently discovered that dromedary camels in Saudi Arabia harbor three different HCoV species, including a dominant MERS HCoV lineage that was responsible for the outbreaks in the Middle East and South Korea during 2015. In this review we aim to compare and contrast the different HCoVs with regard to epidemiology and pathogenesis, in addition to the virus evolution and recombination events which have, on occasion, resulted in outbreaks amongst humans.", "citation_count": "94", "reference_count": "1,936", "date": "2016", "authors": ["Shuo Su", "Gary Wong", "Weifeng Shi", "Jun Liu", "Alexander C.K. Lai", "Jiyong Zhou", "Wenjun Liu", "Yuhai Bi", "George F. Gao"], "related_topics": ["Middle East respiratory syndrome", "Coronavirus", "Outbreak", "Common cold", "Viral evolution", "Pathogenesis", "Epidemiology", "Respiratory disease", "Virology", "Immunology", "Biology"]}
{"id": "2111211467", "references": ["2031611770", "2103546861", "2146058063", "2163627198", "2030966943", "2127847431", "2105926960", "2098448352", "2103088017", "2168696662"], "title": "New Algorithms and Methods to Estimate Maximum-Likelihood Phylogenies: Assessing the Performance of PhyML 3.0", "abstract": "PhyML is a phylogeny software based on the maximum-likelihood principle. Early PhyML versions used a fast algorithm performing nearest neighbor interchanges to improve a reasonable starting tree topology. Since the original publication (Guindon S., Gascuel O. 2003. A simple, fast and accurate algorithm to estimate large phylogenies by maximum likelihood. Syst. Biol. 52:696-704), PhyML has been widely used (&gt;2500 citations in ISI Web of Science) because of its simplicity and a fair compromise between accuracy and speed. In the meantime, research around PhyML has continued, and this article describes the new algorithms and methods implemented in the program. First, we introduce a new algorithm to search the tree space with user-defined intensity using subtree pruning and regrafting topological moves. The parsimony criterion is used here to filter out the least promising topology modifications with respect to the likelihood function. The analysis of a large collection of real nucleotide and amino acid data sets of various sizes demonstrates the good performance of this method. Second, we describe a new test to assess the support of the data for internal branches of a phylogeny. This approach extends the recently proposed approximate likelihood-ratio test and relies on a nonparametric, Shimodaira-Hasegawa-like procedure. A detailed analysis of real alignments sheds light on the links between this new approach and the more classical nonparametric bootstrap method. Overall, our tests show that the last version (3.0) of PhyML is fast, accurate, stable, and ready to use. A Web server and binary files are available from http://www.atgc-montpellier.fr/phyml/.", "citation_count": "39", "reference_count": "12,605", "date": "2010", "authors": ["St\u00e9phane Guindon", "Jean-Fran\u00e7ois Dufayard", "Vincent Lefort", "Maria Anisimova", "Wim Hordijk", "Olivier Gascuel"], "related_topics": ["Likelihood function", "Tree (data structure)", "Pruning (decision trees)", "Algorithm", "k-nearest neighbors algorithm", "Nonparametric statistics", "Network topology", "Data mining", "Filter (signal processing)", "Web server", "Biology"]}
{"id": "2127062009", "references": ["163073849", "2018812376", "2032842024", "1856165804", "2146133178", "2337555053", "1830634530", "2098388207", "2055750915", "1699035432"], "title": "Viruses and Bacteria in the Etiology of the Common Cold", "abstract": "Two hundred young adults with common colds were studied during a 10-month period. Virus culture, antigen detection, PCR, and serology with paired samples were used to identify the infection. Viral etiology was established for 138 of the 200 patients (69%). Rhinoviruses were detected in 105 patients, coronavirus OC43 or 229E infection was detected in 17, influenza A or B virus was detected in 12, and single infections with parainfluenza virus, respiratory syncytial virus, adenovirus, and enterovirus were found in 14 patients. Evidence for bacterial infection was found in seven patients. Four patients had a rise in antibodies against Chlamydia pneumoniae, one had a rise in antibodies against Haemophilus influenzae, one had a rise in antibodies against Streptococcus pneumoniae, and one had immunoglobulin M antibodies against Mycoplasma pneumoniae. The results show that although approximately 50% of episodes of the common cold were caused by rhinoviruses, the etiology can vary depending on the epidemiological situation with regard to circulating viruses. Bacterial infections were rare, supporting the concept that the common cold is almost exclusively a viral disease.", "citation_count": "31", "reference_count": "890", "date": "1998", "authors": ["Mika J. M\u00e4kel\u00e4", "Tuomo Puhakka", "Olli Ruuskanen", "Maija Leinonen", "Pekka Saikku", "Marko Kimpim\u00e4ki", "Soile Blomqvist", "Timo Hyypi\u00e4", "Pertti Arstila"], "related_topics": ["Viral culture", "Serology", "Rhinovirus", "Coronavirus", "Mycoplasma pneumoniae", "Common cold", "Virus", "Enterovirus", "Virology", "Microbiology", "Biology"]}
{"id": "2090060897", "references": ["1963591796", "1987051173", "2409643934", "2302897180", "2406151794", "2045765248", "2072381230", "50597173", "1979854169", "2032346601"], "title": "Evaluation of concurrent shedding of bovine coronavirus via the respiratory tract and enteric route in feedlot cattle.", "abstract": "Objective\u2014To assess the relationship between shedding of bovine coronavirus (BCV) via the respiratory tract and enteric routes and the association with weight gain in feedlot cattle. Animals\u201456 crossbred steers. Procedures\u2014Paired fecal samples and nasal swab specimens were obtained and were tested for BCV, using antigen-capture ELISA. Paired serum samples obtained were tested for antibodies to BCV, using antibody-detection ELISA. Information was collected on weight gain, clinical signs, and treatments for enteric and respiratory tract disease during the study period. Results\u2014Number of samples positive for bovine respiratory coronavirus (BRCV) or bovine enteric coro navirus (BECV) was 37/224 (17%) and 48/223 (22%), respectively. Some cattle (25/46, 45%) shed BECV and BRCV. There were 25/29 (86%) cattle positive for BECV that shed BRCV, but only 1/27 (4%) cattle negative to BECV shed BRCV. Twenty-seven of 48 (56%) paired nasal swab specimens and fecal samples positive for BECV were positive for BRCV. In con...", "citation_count": "31", "reference_count": "104", "date": "2001", "authors": ["Kyoung Oh Cho", "Armando E. Hoet", "Steven C. Loerch", "Thomas E. Wittum", "Linda J. Saif"], "related_topics": ["Bovine coronavirus", "Coronavirus", "Feces", "Nasal Swab", "Respiratory tract", "Respiratory system", "Antibody", "Weight gain", "Veterinary medicine", "Virology", "Biology"]}
{"id": "2004869546", "references": ["2038264706", "2328399749", "2089551619", "2022277835", "2028973331", "2016137045"], "title": "TTV a common virus, but pathogenic?", "abstract": "", "citation_count": "6", "reference_count": "60", "date": "1998", "authors": ["Yvonne Cossart"], "related_topics": ["Virus", "Virology", "Medicine"]}
{"id": "2030133843", "references": ["2083266836", "2023962288", "1984200234", "2075432722", "2313004219", "2155517838"], "title": "HGV: hepatitis G virus or harmless G virus?", "abstract": "The discovery of the hepatitis G virus (HGV) has given hepatologists a new lease on life. Just when they were becoming frustrated with the slow rate of progress in unravell ing the pathobiological consequences of hepatitis B and C virus infections, along comes another candidate virus. HGV, a single-stranded ribonucleic acid (RNA) virus that belongs to the Flaviviridae family, has a global distribution. The virus is present in 1-2% of blood donors in the USA, a frequency higher than that of either HCV or hepatitis B virus (HBV) (Alter). Even more striking is the seroprevalence of 15\"2% reported in West African residents (JMed Virol 1996; 50: 97). HGVexis t s in a chronic carrier state. The virus is transmitted parenterally and is often present in patients who have received multiple transfusions or who are on haemodialysis (N Engl J Med 1996; 334: 1485), and in intravenous drug users. There is preliminary evidence for perinatal transmission (Lancet 1996; 347: 615). HGV RNA sequences have been identified in serum from patients with non-A-E acute and chronic hepatitis and cirrhosis. Impressive data comes from Brescia, Italy, where 35% of patients with acute hepatitis and 39% of those with chronic hepatitis were positive for HGV RNA (Fiordalisi). Among blood donors the virus is more common in those with raised serum aminotransferase concentrations (3\"9%) than in those with normal concentrations (0\"8%) (ff Med Virol 1996; 50: 97). These findings imply that HGV is a human pathogen, but is it? Other information is more consistent with HGV being an innocent passenger. The great majority of individuals who become HGV-RNA positive after blood transfusion have normal serum aminotronsperase concentrations and neither they, nor those found positive for HGV RNA in other circumstances, develop liver disease during prolonged follow-up (Alter). Moreover, when serum enzyme concentrations are raised they seldom accord with levels of viraemia. HGV and HCV are often, and HGV and HBV less often, found together in serum. In those coinfected with HGV and HCV, aminotransterases run parallel to HCV rather than HGV, and the presence of the latter seems to have no effect on outcome. HGV usually accounts for only a minority of cases of acute non-A-E hepatitis, and there is no evidence yet of progression over time to chronic hepatitis, cirrhosis, or hepatocellular", "citation_count": "6", "reference_count": "20", "date": "1996", "authors": ["Michael C Kew", "Chris Kassianides"], "related_topics": ["Hepatitis B", "Hepatitis B virus", "Hepatitis", "Flaviviridae", "Liver disease", "Virus", "Cirrhosis", "Blood transfusion", "Virology", "Medicine"]}
{"id": "2084994773", "references": ["132455992", "2087363345", "2134812217", "2329318335", "1994193749", "3011200155", "1582561043", "2009310436", "2156596665", "2149495938"], "title": "Phylogenetic analysis of a highly conserved region of the polymerase gene from 11 coronaviruses and development of a consensus polymerase chain reaction assay.", "abstract": "Viruses in the genus Coronavirus are currently placed in three groups based on antigenic cross-reactivity and sequence analysis of structural protein genes. Consensus polymerase chain reaction (PCR) primers were used to obtain cDNA, then cloned and sequenced a highly conserved 922 nucleotide region in open reading frame (ORF) 1b of the polymerase (pol) gene from eight coronaviruses. These sequences were compared with published sequences for three additional coronaviruses. In this comparison, it was found that nucleotide substitution frequencies (per 100 nucleotides) varied from 46.40 to 50.13 when viruses were compared among the traditional coronavirus groups and, with one exception (the human coronavirus (HCV) 229E), varied from 2.54 to 15.89 when compared within these groups. (The substitution frequency for 229E, as compared to other members of the same group, varied from 35.37 to 35.72.) Phylogenetic analysis of these pol gene sequences resulted in groupings which correspond closely with the previously described groupings, including recent data which places the two avian coronaviruses--infectious bronchitis virus (IBV) of chickens and turkey coronavirus (TCV)--in the same group [Guy, J.S., Barnes, H.J., Smith L.G., Breslin, J., 1997. Avian Dis. 41:583-590]. A single pair of degenerate primers was identified which amplify a 251 bp region from coronaviruses of all three groups using the same reaction conditions. This consensus PCR assay for the genus Coronavirus may be useful in identifying as yet unknown coronaviruses.", "citation_count": "25", "reference_count": "230", "date": "1999", "authors": ["Charles B. Stephensen", "Donald B. Casebolt", "Nupur N. Gangopadhyay"], "related_topics": ["Coronavirus", "Consensus PCR", "Turkey coronavirus", "Sequence analysis", "Polymerase Gene", "Conserved sequence", "Polymerase", "Sequence alignment", "Genetics", "Virology", "Biology"]}
{"id": "2149579937", "references": ["2154128645", "1967150940", "2044967254", "1845818816", "2326037208", "1509033271", "2109638242", "2163443142", "2171229191", "1941129807"], "title": "Imported Lassa fever in Germany: molecular characterization of a new Lassa virus strain.", "abstract": "We describe the isolation and characterization of a new Lassa virus strain imported into Germany by a traveler who had visited Ghana, Cote D'Ivoire, and Burkina Faso. This strain, designated \"AV,\" originated from a region in West Africa where Lassa fever has not been reported. Viral S RNA isolated from the patient's serum was amplified and sequenced. A long-range reverse transcription polymerase chain reaction allowed amplification of the full-length (3.4 kb) S RNA. The coding sequences of strain AV differed from those of all known Lassa prototype strains (Josiah, Nigeria, and LP) by approximately 20%, mainly at third codon positions. Phylogenetically, strain AV appears to be most closely related to strain Josiah from Sierra Leone. Lassa viruses comprise a group of genetically highly diverse strains, which has implications for vaccine development. The new method for full-length S RNA amplification may facilitate identification and molecular analysis of new arenaviruses or arenavirus strains.", "citation_count": "40", "reference_count": "211", "date": "2000", "authors": ["S G\u00fcnther", "P Emmerich", "T Laue", "O K\u00fchle", "M Asper", "A Jung", "T Grewing", "J ter Meulen", "H Schmitz"], "related_topics": ["Lassa fever", "Lassa virus", "Arenavirus", "Sierra leone", "Strain (biology)", "RNA", "Virology", "Reverse transcription polymerase chain reaction", "Vero cell", "Genetics", "Biology"]}
{"id": "2107922358", "references": ["2070721758", "2171308211", "2134971582", "2798078005", "1994091239", "2163760194", "2047480444", "2149579937", "2137089963", "2131589770"], "title": "Rapid detection and quantification of RNA of Ebola and Marburg viruses, Lassa virus, Crimean-Congo hemorrhagic fever virus, Rift Valley fever virus, dengue virus, and yellow fever virus by real-time reverse transcription-PCR.", "abstract": "Viral hemorrhagic fevers (VHFs) are acute infections with high case fatality rates. Important VHF agents are Ebola and Marburg viruses (MBGV/EBOV), Lassa virus (LASV), Crimean-Congo hemorrhagic fever virus (CCHFV), Rift Valley fever virus (RVFV), dengue virus (DENV), and yellow fever virus (YFV). VHFs are clinically difficult to diagnose and to distinguish; a rapid and reliable laboratory diagnosis is required in suspected cases. We have established six one-step, real-time reverse transcription-PCR assays for these pathogens based on the Superscript reverse transcriptase-Platinum Taq polymerase enzyme mixture. Novel primers and/or 5\u2032-nuclease detection probes were designed for RVFV, DENV, YFV, and CCHFV by using the latest DNA database entries. PCR products were detected in real time on a LightCycler instrument by using 5\u2032-nuclease technology (RVFV, DENV, and YFV) or SybrGreen dye intercalation (MBGV/EBOV, LASV, and CCHFV). The inhibitory effect of SybrGreen on reverse transcription was overcome by initial immobilization of the dye in the reaction capillaries. Universal cycling conditions for SybrGreen and 5\u2032-nuclease probe detection were established. Thus, up to three assays could be performed in parallel, facilitating rapid testing for several pathogens. All assays were thoroughly optimized and validated in terms of analytical sensitivity by using in vitro-transcribed RNA. The \u226595% detection limits as determined by probit regression analysis ranged from 1,545 to 2,835 viral genome equivalents/ml of serum (8.6 to 16 RNA copies per assay). The suitability of the assays was exemplified by detection and quantification of viral RNA in serum samples of VHF patients.", "citation_count": "39", "reference_count": "784", "date": "2002", "authors": ["Christian Drosten", "Stephan G\u00f6ttig", "Stefan Schilling", "Marcel Asper", "Marcus Panning", "Herbert Schmitz", "Stephan G\u00fcnther"], "related_topics": ["Lassa virus", "Phlebovirus", "Dengue virus", "Ebola virus", "Flavivirus", "Arenavirus", "Marburgvirus", "Crimean Congo hemorrhagic fever virus", "Virology", "Microbiology", "Biology"]}
{"id": "1945961678", "references": ["2006434809", "1977050884", "2096238447", "2150120685", "1947409115", "2166867592", "2078121682", "2034462612", "2115555188", "2017248106"], "title": "Treatment With Lopinavir/Ritonavir or Interferon-\u03b21b Improves Outcome of MERS-CoV Infection in a Nonhuman Primate Model of Common Marmoset", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) causes severe disease in human with an overall case-fatality rate of &gt;35%. Effective antivirals are crucial for improving the clinical outcome of MERS. Although a number of repurposed drugs, convalescent-phase plasma, antiviral peptides, and neutralizing antibodies exhibit anti-MERS-CoV activity in vitro, most are not readily available or have not been evaluated in nonhuman primates. We assessed 3 repurposed drugs with potent in vitro anti-MERS-CoV activity (mycophenolate mofetil [MMF], lopinavir/ritonavir, and interferon-\u03b21b) in common marmosets with severe disease resembling MERS in humans. The lopinavir/ritonavir-treated and interferon-\u03b21b-treated animals had better outcome than the untreated animals, with improved clinical (mean clinical scores \u219350.9%-95.0% and \u2193weight loss than the untreated animals), radiological (minimal pulmonary infiltrates), and pathological (mild bronchointerstitial pneumonia) findings, and lower mean viral loads in necropsied lung (\u21930.59-1.06 log10 copies/glyceraldehyde 3-phosphate dehydrogenase [GAPDH]; P &lt; .050) and extrapulmonary (\u21930.11-1.29 log10 copies/GAPDH; P &lt; .050 in kidney) tissues. In contrast, all MMF-treated animals developed severe and/or fatal disease with higher mean viral loads (\u21910.15-0.54 log10 copies/GAPDH) than the untreated animals. The mortality rate at 36 hours postinoculation was 67% (untreated and MMF-treated) versus 0-33% (lopinavir/ritonavir-treated and interferon-\u03b21b-treated). Lopinavir/ritonavir and interferon-\u03b21b alone or in combination should be evaluated in clinical trials. MMF alone may worsen MERS and should not be used.", "citation_count": "47", "reference_count": "587", "date": "2015", "authors": ["Jasper Fuk Woo Chan", "Yanfeng Yao", "Man Lung Yeung", "Wei Deng", "Linlin Bao", "Lilong Jia", "Fengdi Li", "Chong Xiao", "Hong Gao", "Pin Yu", "Jian Piao Cai", "Hin Chu", "Jie Zhou", "Honglin Chen", "Chuan Qin", "Kwok Yung Yuen"], "related_topics": ["Lopinavir/ritonavir", "Lopinavir", "Ritonavir", "Viral load", "Middle East respiratory syndrome", "Coronavirus", "Middle East respiratory syndrome coronavirus", "Pneumonia", "Immunology", "Virology", "Biology"]}
{"id": "2099941783", "references": ["1852588318", "2107053896", "2128886090", "2130227690", "2119111857", "2108756402", "2160011624", "1968393246", "2119837294", "2145441153"], "title": "Presence of Middle East respiratory syndrome coronavirus antibodies in Saudi Arabia: a nationwide, cross-sectional, serological study", "abstract": "Summary  Background  Scientific evidence suggests that dromedary camels are the intermediary host for the Middle East respiratory syndrome coronavirus (MERS-CoV). However, the actual number of infections in people who have had contact with camels is unknown and most index patients cannot recall any such contact. We aimed to do a nationwide serosurvey in Saudi Arabia to establish the prevalence of MERS-CoV antibodies, both in the general population and in populations of individuals who have maximum exposure to camels.  Methods  In the cross-sectional serosurvey, we tested human serum samples obtained from healthy individuals older than 15 years who attended primary health-care centres or participated in a national burden-of-disease study in all 13 provinces of Saudi Arabia. Additionally, we tested serum samples from shepherds and abattoir workers with occupational exposure to camels. Samples were screened by recombinant ELISA and MERS-CoV seropositivity was confirmed by recombinant immunofluorescence and plaque reduction neutralisation tests. We used two-tailed Mann Whitney  U  exact tests, \u03c7 2 , and Fisher's exact tests to analyse the data.  Findings  Between Dec 1, 2012, and Dec 1, 2013, we obtained individual serum samples from 10\u2008009 individuals. Anti-MERS-CoV antibodies were confirmed in 15 (0\u00b715%; 95% CI 0\u00b709\u20130\u00b724) of 10\u2008009 people in six of the 13 provinces. The mean age of seropositive individuals was significantly younger than that of patients with reported, laboratory-confirmed, primary Middle Eastern respiratory syndrome (43\u00b75 years [SD 17\u00b73]  vs  53\u00b78 years [17\u00b75]; p=0\u00b7008). Men had a higher antibody prevalence than did women (11 [0\u00b725%] of 4341  vs  two [0\u00b705%] of 4378; p=0\u00b7028) and antibody prevalence was significantly higher in central versus coastal provinces (14 [0\u00b726%] of 5479  vs  one [0\u00b702%] of 4529; p=0\u00b7003). Compared with the general population, seroprevalence of MERS-CoV antibodies was significantly increased by 15 times in shepherds (two [2\u00b73%] of 87, p=0\u00b70004) and by 23 times in slaughterhouse workers (five [3\u00b76%] of 140; p  Interpretation  Seroprevalence of MERS-CoV antibodies was significantly higher in camel-exposed individuals than in the general population. By simple multiplication, a projected 44\u2008951 (95% CI 26\u2008971\u201371\u2008922) individuals older than 15 years might be seropositive for MERS-CoV in Saudi Arabia. These individuals might be the source of infection for patients with confirmed MERS who had no previous exposure to camels.  Funding  European Union, German Centre for Infection Research, Federal Ministry of Education and Research, German Research Council, and Ministry of Health of Saudi Arabia.", "citation_count": "21", "reference_count": "270", "date": "2015", "authors": ["Marcel A. M\u00fcller", "Benjamin Meyer", "Victor M. Corman", "Malak Al-Masri", "Abdulhafeez Turkestani", "Daniel Ritz", "Andrea Sieberg", "Souhaib Aldabbagh", "Berend J. Bosch", "Erik Lattwein", "Raafat F. Alhakeem", "Abdullah M. Assiri", "Ali M. Albarrak", "Ali M. Al-Shangiti", "Jaffar A. Al-Tawfiq", "Paul Wikramaratna", "Abdullah A. Alrabeeah", "Christian Drosten", "Ziad A. Memish"], "related_topics": ["Seroprevalence", "European union", "Population", "Middle East respiratory syndrome coronavirus", "Serology", "Cross-sectional study", "Seroepidemiologic Studies", "Demography", "Antibody", "Veterinary medicine", "Medicine"]}
{"id": "2195009776", "references": ["2092969802", "1963580683", "2098037373", "2126707939", "1995367098", "2094993149", "2074618762", "1993577573", "2143230291", "2152528032"], "title": "A SARS-like cluster of circulating bat coronaviruses shows potential for human emergence", "abstract": "The emergence of severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome (MERS)-CoV underscores the threat of cross-species transmission events leading to outbreaks in humans. Here we examine the disease potential of a SARS-like virus, SHC014-CoV, which is currently circulating in Chinese horseshoe bat populations. Using the SARS-CoV reverse genetics system, we generated and characterized a chimeric virus expressing the spike of bat coronavirus SHC014 in a mouse-adapted SARS-CoV backbone. The results indicate that group 2b viruses encoding the SHC014 spike in a wild-type backbone can efficiently use multiple orthologs of the SARS receptor human angiotensin converting enzyme II (ACE2), replicate efficiently in primary human airway cells and achieve in vitro titers equivalent to epidemic strains of SARS-CoV. Additionally, in vivo experiments demonstrate replication of the chimeric virus in mouse lung with notable pathogenesis. Evaluation of available SARS-based immune-therapeutic and prophylactic modalities revealed poor efficacy; both monoclonal antibody and vaccine approaches failed to neutralize and protect from infection with CoVs using the novel spike protein. On the basis of these findings, we synthetically re-derived an infectious full-length SHC014 recombinant virus and demonstrate robust viral replication both in vitro and in vivo. Our work suggests a potential risk of SARS-CoV re-emergence from viruses currently circulating in bat populations.", "citation_count": "25", "reference_count": "635", "date": "2015", "authors": ["Vineet D. Menachery", "Boyd L. Yount", "Kari Debbink", "Sudhakar Agnihothram", "Lisa E. Gralinski", "Jessica A. Plante", "Rachel L. Graham", "Trevor Scobey", "Xing Yi Ge", "Eric F. Donaldson", "Scott H. Randell", "Antonio Lanzavecchia", "Wayne A. Marasco", "Zhengli Li Shi", "Ralph S. Baric"], "related_topics": ["Virus", "Middle East respiratory syndrome", "Viral replication", "Recombinant virus", "Antibody", "Reverse genetics", "Monoclonal antibody", "Virology", "Horseshoe bat", "Biology"]}
{"id": "2525468044", "references": ["2103118479", "2134527559", "2405185968", "2126707939"], "title": "Viral Load Kinetics of MERS Coronavirus Infection.", "abstract": "Middle East respiratory syndrome coronavirus continues to circulate in the Middle East. During a recent outbreak in Korea, changes in MERS coronavirus viral load were determined during the course of illness in 17 patients.", "citation_count": "4", "reference_count": "174", "date": "2016", "authors": ["Myoung Don Oh", "Wan Beom Park", "Pyoeng Gyun Choe", "Su Jin Choi", "Jong I.I. Kim", "Jeesoo Chae", "Sung Sup Park", "Eui Chong Kim", "Hong Sang Oh", "Eun Jung Kim", "Eun Young Nam", "Sun Hee Na", "Dong Ki Kim", "Sang Min Lee", "Kyoung Ho Song", "Ji Hwan Bang", "Eu Suk Kim", "Hong Bin Kim", "Sang Won Park", "Nam Joong Kim"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Viral load", "Outbreak", "Virology", "Severity of illness", "Medicine", "Coronavirus Infections", "Course of illness"]}
{"id": "2298153446", "references": ["2092969802", "2101176145", "1963580683", "2126707939", "1995367098", "2094993149", "1998383538", "1993577573", "2143230291", "2152528032"], "title": "SARS-like WIV1-CoV poised for human emergence", "abstract": "Outbreaks from zoonotic sources represent a threat to both human disease as well as the global economy. Despite a wealth of metagenomics studies, methods to leverage these datasets to identify future threats are underdeveloped. In this study, we describe an approach that combines existing metagenomics data with reverse genetics to engineer reagents to evaluate emergence and pathogenic potential of circulating zoonotic viruses. Focusing on the severe acute respiratory syndrome (SARS)-like viruses, the results indicate that the WIV1-coronavirus (CoV) cluster has the ability to directly infect and may undergo limited transmission in human populations. However, in vivo attenuation suggests additional adaptation is required for epidemic disease. Importantly, available SARS monoclonal antibodies offered success in limiting viral infection absent from available vaccine approaches. Together, the data highlight the utility of a platform to identify and prioritize prepandemic strains harbored in animal reservoirs and document the threat posed by WIV1-CoV for emergence in human populations.", "citation_count": "27", "reference_count": "276", "date": "2016", "authors": ["Vineet D. Menachery", "Boyd L. Yount", "Amy C Sims", "Kari Debbink", "Sudhakar S. Agnihothram", "Lisa E. Gralinski", "Rachel Lauren Graham", "Trevor Scobey", "Jessica A. Plante", "Scott R. Royal", "Jesica Swanstrom", "Timothy Patrick Sheahan", "Raymond J Pickles", "Davide Corti", "Scott H Randell", "Antonio Lanzavecchia", "Wayne A. Marasco", "Ralph S Baric"], "related_topics": ["Transmission (medicine)", "Metagenomics", "Outbreak", "Disease cluster", "Virology", "Biology", "Epidemic disease", "Human disease", "Limiting", "Viral infection"]}
{"id": "2115555188", "references": ["2006434809", "2129542667", "1703839189", "2107053896", "2166867592", "2119111857", "2112147913", "1993577573", "2565805236", "2025170735"], "title": "Middle East Respiratory Syndrome Coronavirus: Another Zoonotic Betacoronavirus Causing SARS-Like Disease", "abstract": "SUMMARY  The source of the severe acute respiratory syndrome (SARS) epidemic was traced to wildlife market civets and ultimately to bats. Subsequent hunting for novel coronaviruses (CoVs) led to the discovery of two additional human and over 40 animal CoVs, including the prototype lineage C betacoronaviruses, Tylonycteris bat CoV HKU4 and Pipistrellus bat CoV HKU5; these are phylogenetically closely related to the Middle East respiratory syndrome (MERS) CoV, which has affected more than 1,000 patients with over 35% fatality since its emergence in 2012. All primary cases of MERS are epidemiologically linked to the Middle East. Some of these patients had contacted camels which shed virus and/or had positive serology. Most secondary cases are related to health care-associated clusters. The disease is especially severe in elderly men with comorbidities. Clinical severity may be related to MERS-CoV9s ability to infect a broad range of cells with DPP4 expression, evade the host innate immune response, and induce cytokine dysregulation. Reverse transcription-PCR on respiratory and/or extrapulmonary specimens rapidly establishes diagnosis. Supportive treatment with extracorporeal membrane oxygenation and dialysis is often required in patients with organ failure. Antivirals with potent in vitro activities include neutralizing monoclonal antibodies, antiviral peptides, interferons, mycophenolic acid, and lopinavir. They should be evaluated in suitable animal models before clinical trials. Developing an effective camel MERS-CoV vaccine and implementing appropriate infection control measures may control the continuing epidemic.", "citation_count": "351", "reference_count": "729", "date": "2015", "authors": ["Jasper F. W. Chan", "Susanna K. P. Lau", "Kelvin K. W. To", "Vincent C. C. Cheng", "Patrick C. Y. Woo", "Kwok-Yung Yuen"], "related_topics": ["Middle East respiratory syndrome", "Middle East respiratory syndrome coronavirus", "Betacoronavirus", "Virus", "Disease", "Innate immune system", "Infection control", "Virology", "Immunology", "Respiratory system", "Biology"]}
{"id": "2107034620", "references": ["2045656233", "2094414211", "2104924585", "1699734612", "1880262756", "2171188998", "2127006916", "1566135517", "1484228140", "2124386111"], "title": "A Bayesian hierarchical model for learning natural scene categories", "abstract": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.", "citation_count": "19", "reference_count": "4,729", "date": "2005", "authors": ["L. Fei-Fei", "P. Perona"], "related_topics": ["Unsupervised learning", "Categorization", "Bag-of-words model in computer vision", "Caltech 101", "Theme (narrative)", "Visual dictionary", "Dynamic topic model", "LabelMe", "Contextual image classification", "Natural language processing", "Machine learning", "Artificial intelligence", "Computer science", "Training set"]}
{"id": "2154422044", "references": ["1949116567", "2160225842", "2217896605", "2155511848", "2159686933", "2049633694", "1699734612", "2119747362", "2164598857", "2109200236"], "title": "Object class recognition by unsupervised scale-invariant learning", "abstract": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).", "citation_count": "19", "reference_count": "3,001", "date": "2003", "authors": ["R. Fergus", "P. Perona", "A. Zisserman"], "related_topics": ["Object model", "Implicit Shape Model", "Constellation model", "Feature extraction", "Cognitive neuroscience of visual object recognition", "Caltech 101", "Contextual image classification", "Naive Bayes classifier", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2156598602", "references": ["3029645440", "2063366997", "2110764733", "1980911747", "2151103935", "2141282920", "2085261163", "2033819227", "2119781527", "2131846894"], "title": "Photo tourism: exploring photo collections in 3D", "abstract": "We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.", "citation_count": "43", "reference_count": "4,029", "date": "2006", "authors": ["Noah Snavely", "Steven M. Seitz", "Richard Szeliski"], "related_topics": ["Digital photo frame", "Rendering (computer graphics)", "Rephotography", "Computer graphics (images)", "Computer vision", "Computer science", "Tourism", "Artificial intelligence"]}
{"id": "2970081408", "references": ["2113283043", "2156539109", "2151185985", "2141282920", "2080787847", "2126345904", "3035582413", "2116254741", "2559617320"], "title": "Language Change", "abstract": "", "citation_count": "0", "reference_count": "295", "date": "2004", "authors": ["Adrian Beard"], "related_topics": ["Language change", "Linguistics", "Psychology"]}
{"id": "2050457084", "references": ["2152027810", "2141282920", "2252966386", "2001574834", "2273590290"], "title": "Categories, Photographs &amp; Predicaments: Exploratory Research on Representing Pictures for Access", "abstract": "", "citation_count": "0", "reference_count": "5", "date": "2005", "authors": ["Brian C. O'Connor", "Mary Keeney O'Connor"], "related_topics": ["Exploratory research", "Applied psychology", "Psychology", "Multimedia"]}
{"id": "2055225264", "references": ["2006119904", "2008297189", "1987777228", "2138621811", "2089199911", "2079672501", "2140350208", "2099251025", "2117086609", "3013264884"], "title": "PicASHOW: pictorial authority search by hyperlinks on the web.", "abstract": "We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page p displays (or links to) an image when the author of p considers the image to be of value to the viewers of the page. We thus extend some well known link-based WWW page retrieval schemes to the context of image retrieval. PicASHOW\u2019s analysis of the link structure enables it to retrieve relevant images even when those are stored in files with meaningless names. The same analysis also allows it to identify image containers and image hubs. We define these as Web pages that are rich in relevant images, or from which many images are readily accessible. PicASHOW requires no image analysis whatsoever and no creation of taxonomies for preclassification of the Web\u2019s images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines. Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a proven effective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its definition to include the retrieval of image hubs and containers.", "citation_count": "18", "reference_count": "121", "date": "2002", "authors": ["Ronny Lempel", "Aya Soffer"], "related_topics": ["Image retrieval", "Static web page", "Web page", "Web search engine", "Printer-friendly", "Web search query", "Web crawler", "Hyperlink", "Information retrieval", "World Wide Web", "Computer science"]}
{"id": "1587328194", "references": ["2037732452", "2008297189", "2145023731", "2102475035", "2069266228", "2068272887", "2053197265", "2123977795", "1530454533", "2093191240"], "title": "Finding Naked People", "abstract": "This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources.", "citation_count": "20", "reference_count": "687", "date": "1996", "authors": ["Margaret M. Fleck", "David A. Forsyth", "Chris Bregler"], "related_topics": ["Object model", "Cognitive neuroscience of visual object recognition", "Texture (music)", "Computer vision", "Test set", "Structure (mathematical logic)", "Image (mathematics)", "Range (mathematics)", "Computer science", "World Wide Web", "Artificial intelligence"]}
{"id": "2166770390", "references": ["2217896605", "2119821739", "2140785063", "2124351082", "2159686933", "2032210760", "1658679052", "2164598857", "2914885528", "3124955340"], "title": "Object Detection Using the Statistics of Parts", "abstract": "In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively.\r\n\r\nEach classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation\u2014labeling the input as non-object\u2014or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.", "citation_count": "31", "reference_count": "496", "date": "2004", "authors": ["Henry Schneiderman", "Takeo Kanade"], "related_topics": ["Margin classifier", "Quadratic classifier", "Object detection", "Classifier (UML)", "AdaBoost", "Face detection", "Cognitive neuroscience of visual object recognition", "Likelihood-ratio test", "Pattern recognition", "Computer vision", "Statistics", "Mathematics", "Artificial intelligence"]}
{"id": "2612148268", "references": ["2141282920", "2152027810"], "title": "Categories, photographs &amp; predicaments : Exploratory research on representing pictures for access : Theory and practice in the organization of images and other visuo-spatial data for retrieval", "abstract": "", "citation_count": "0", "reference_count": "2", "date": "1999", "authors": ["B. C. O'connor", "M. K. O'connor"], "related_topics": ["Exploratory research", "Spatial analysis", "Information retrieval", "Computer science", "Artificial intelligence"]}
{"id": "1934863104", "references": ["2062270497", "2125101937", "2155099190", "2011549082", "2049633694", "1587328194", "2160066518", "2099251025", "2117086609", "2135705692"], "title": "Learning the semantics of words and pictures", "abstract": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.", "citation_count": "23", "reference_count": "771", "date": "2001", "authors": ["K. Barnard", "D. Forsyth"], "related_topics": ["Visual Word", "Image retrieval", "Automatic image annotation", "Unsupervised learning", "Feature (computer vision)", "Human\u2013computer information retrieval", "Semantics", "Cognitive neuroscience of visual object recognition", "Information retrieval", "Statistical model", "Computer science"]}
{"id": "1666447063", "references": ["2129765547", "1540386283", "1574901103", "1579838312", "2006969979", "1508960934", "1585814348", "1934863104", "2121947440", "2293605478"], "title": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary", "abstract": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.", "citation_count": "11", "reference_count": "2,202", "date": "2002", "authors": ["P. Duygulu", "Kobus Barnard", "J. F. G. de Freitas", "David A. Forsyth"], "related_topics": ["Vocabulary", "Lexicon", "Machine translation", "Test set", "Cognitive neuroscience of visual object recognition", "Set (abstract data type)", "Natural language processing", "Pattern recognition", "Noun", "Expectation\u2013maximization algorithm", "Computer science", "Artificial intelligence"]}
{"id": "181417509", "references": ["2849837516", "2852735096", "2141282920", "2086174602", "2816429464", "1978530472", "2153166546", "2079855258", "2134814621"], "title": "Storage and Retrieval of Feature Data for a Very Large Online Image Collection.", "abstract": "", "citation_count": "0", "reference_count": "84", "date": "1996", "authors": ["T. K. Rengarajan", "Lucien A. Dimino", "Dwayne Chung"], "related_topics": ["Visual Word", "Image retrieval", "Automatic image annotation", "Feature detection (computer vision)", "Image texture", "Feature data", "Image processing", "Digital image processing", "Information retrieval", "Computer science"]}
{"id": "2293605478", "references": ["2081687495", "1504061712", "2166447979", "2049633694", "1934863104", "2102381086", "2121947440", "2117086609", "1972812142", "2004690028"], "title": "Clustering art", "abstract": "We extend a recently developed method (K. Barnard and D. Forsyth, 2001) for learning the semantics of image databases using text and pictures. We incorporate statistical natural language processing in order to deal with free text. We demonstrate the current system on a difficult dataset, namely 10000 images of work from the Fine Arts Museum of San Francisco. The images include line drawings, paintings, and pictures of sculpture and ceramics. Many of the images have associated free text which varies greatly from physical description to interpretation and mood. We use WordNet to provide semantic grouping information and to help disambiguate word senses, as well as emphasize the hierarchical nature of semantic relationships. This allows us to impose a natural structure on the image collection that reflects semantics to a considerable degree. Our method produces a joint probability distribution for words and picture elements. We demonstrate that this distribution can be used: (a) to provide illustrations for given captions, and (b) to generate words for images outside the training set. Results from this annotation process yield a quantitative study of our method. Finally, the annotation process can be seen as a form of object recognizer that has been learned through a partially supervised process.", "citation_count": "10", "reference_count": "226", "date": "2001", "authors": ["K. Barnard", "P. Duygulu", "D. Forsyth"], "related_topics": ["WordNet", "Semantics", "Natural language", "Computational linguistics", "Cluster analysis", "Object (computer science)", "Annotation", "Natural language processing", "Structure (mathematical logic)", "Computer science", "Text mining", "Artificial intelligence"]}
{"id": "1949116567", "references": ["2125791971", "2049633694", "2095757522", "2029727948", "3017143921", "1958762911", "2117138270", "1628541567", "2124722975", "1564419782"], "title": "Unsupervised Learning of Models for Recognition", "abstract": "We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.", "citation_count": "16", "reference_count": "947", "date": "2000", "authors": ["Markus Weber", "Max Welling", "Pietro Perona"], "related_topics": ["Constellation model", "Unsupervised learning", "Cluster analysis", "Pattern recognition (psychology)", "Cognitive neuroscience of visual object recognition", "Contextual image classification", "Expectation\u2013maximization algorithm", "Focus (optics)", "Computer vision", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2124386111", "references": ["2096077837", "2124087378", "2011891945", "2096600681", "2123977795", "2042243448", "1553558465", "22745672", "2914885528", "2131806657"], "title": "Object recognition from local scale-invariant features", "abstract": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.", "citation_count": "23", "reference_count": "21,546", "date": "1999", "authors": ["D.G. Lowe"], "related_topics": ["3D single-object recognition", "Haar-like features", "Scale space", "Scale-invariant feature transform", "Cognitive neuroscience of visual object recognition", "Implicit Shape Model", "Feature extraction", "Bag-of-words model in computer vision", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2012778485", "references": ["2103504761", "2124087378", "2011891945", "22745672", "2005433550", "2119747362", "2109200236", "2165497495", "1505641881", "2124386111"], "title": "Invariant Features from Interest Point Groups", "abstract": "This paper approaches the problem of \u00afnding correspondences between images in which there are large changes in viewpoint, scale and illumi- nation. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Further- more, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descrip- tors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image rel- ative to canonical frames de\u00afned by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough trans- form. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.", "citation_count": "13", "reference_count": "1,141", "date": "2002", "authors": ["Matthew Brown", "David G. Lowe"], "related_topics": ["RANSAC", "Epipolar geometry", "Invariant (mathematics)", "Discriminative model", "Pattern recognition", "Outlier", "Resampling", "Computer vision", "Computation", "Pentium", "Mathematics", "Artificial intelligence"]}
{"id": "2124087378", "references": ["2022735534", "2112328181", "2098693229", "2160835070", "2011891945", "2109863423", "2095757522", "2123977795", "2111308925", "2914885528"], "title": "Local grayvalue invariants for image retrieval", "abstract": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.", "citation_count": "36", "reference_count": "2,336", "date": "1997", "authors": ["C. Schmid", "R. Mohr"], "related_topics": ["Visual Word", "Image retrieval", "Search engine indexing", "Computer vision", "Robustness (computer science)", "Autocorrelation", "Mathematics", "Artificial intelligence", "Image matching", "Voting algorithm"]}
{"id": "2033819227", "references": ["2104974755", "2171740948", "1980911747", "2151103935", "2151290401", "2123487311", "2177274842", "2141362318"], "title": "Multiple view geometry in computer vision", "abstract": "From the Publisher:\r\nA basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.", "citation_count": "0", "reference_count": "28,613", "date": "2000", "authors": ["Richard Hartley", "Andrew Zisserman"], "related_topics": ["Structure from motion", "Epipolar geometry", "Computer graphics", "Trifocal tensor", "RANSAC", "Bundle adjustment", "Visual odometry", "Eight-point algorithm", "Computer science", "Computer vision", "Artificial intelligence"]}
{"id": "2111308925", "references": ["2997169974", "1756736144", "2039106392", "2063599328", "1639227073", "2048192053"], "title": "A COMBINED CORNER AND EDGE DETECTOR", "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.", "citation_count": "6", "reference_count": "19,238", "date": "1988", "authors": ["Christopher G. Harris", "Mike Stephens"], "related_topics": ["Motion analysis", "Corner detection", "Interest point detection", "Alvey", "Principal curvature-based region detector", "Harris affine region detector", "GLOH", "Hessian affine region detector", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2165497495", "references": ["1970269179", "3022352042", "2112328181", "1549739843", "2124087378", "2085261163", "2111308925", "2143753158", "2130103520", "2124386111"], "title": "Reliable feature matching across widely separated views", "abstract": "We present a robust method for automatically matching features in images corresponding to the same physical point on an object seen from two arbitrary viewpoints. Unlike conventional stereo matching approaches we assume no prior knowledge about the relative camera positions and orientations. In fact in our application this is the information we wish to determine from the image feature matches. Features are detected in two or more images and characterised using affine texture invariants. The problem of window effects is explicitly addressed by our method-our feature characterisation is invariant to linear transformations of the image data including rotation, stretch and skew. The feature matching process is optimised for a structure-from-motion application where we wish to ignore unreliable matches at the expense of reducing the number of feature matches.", "citation_count": "20", "reference_count": "915", "date": "2000", "authors": ["A. Baumberg"], "related_topics": ["Feature extraction", "Affine transformation", "Hessian affine region detector", "Motion estimation", "Invariant (mathematics)", "Skew", "Pattern recognition", "Computer vision", "Electrical capacitance tomography", "Linear map", "Mathematics", "Artificial intelligence"]}
{"id": "1676552347", "references": ["1970269179", "2112328181", "2124087378", "2005433550", "2111308925", "2119747362", "2109200236", "2165497495", "1991605728", "2124386111"], "title": "An Affine Invariant Interest Point Detector", "abstract": "This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.", "citation_count": "17", "reference_count": "2,127", "date": "2002", "authors": ["K. Mikolajczyk", "C. Schmid"], "related_topics": ["Affine shape adaptation", "Harris affine region detector", "Affine transformation", "Affine combination", "Affine coordinate system", "Affine hull", "Hessian affine region detector", "Affine group", "Topology", "Mathematics"]}
{"id": "2045656233", "references": ["2159080219", "2126163471", "1533179050", "2156273867"], "title": "Bayesian Data Analysis", "abstract": "FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter.", "citation_count": "4", "reference_count": "30,525", "date": "1995", "authors": ["Andrew Gelman", "John B. Carlin", "Hal S. Stern", "David B. Dunson", "Aki Vehtari", "Donald B. Rubin"], "related_topics": ["Variable-order Bayesian network", "Bayesian statistics", "Dynamic Bayesian network", "Bayesian inference", "Dirichlet process", "Probabilistic programming language", "Markov chain", "Bayesian probability", "Applied mathematics", "Data mining", "Computer science"]}
{"id": "2217896605", "references": ["2124351082", "2098947662", "2139212933", "2173629880", "2147800946", "2981264952", "1997011019", "2133671888", "2042371054", "2159173611"], "title": "Neural network-based face detection", "abstract": "We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.", "citation_count": "46", "reference_count": "6,478", "date": "1998", "authors": ["H.A. Rowley", "S. Baluja", "T. Kanade"], "related_topics": ["Face detection", "Object-class detection", "Facial recognition system", "Face (geometry)", "Artificial neural network", "Pattern recognition (psychology)", "Pattern recognition", "Computer vision", "Pixel", "Computer science", "Artificial intelligence"]}
{"id": "2030536784", "references": ["1997063559", "3145128584", "301824129", "2159080219", "2143516773", "2085261163", "2123977795", "1560013842", "2138451337"], "title": "Pictorial Structures for Object Recognition", "abstract": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.", "citation_count": "43", "reference_count": "2,824", "date": "2005", "authors": ["Pedro F. Felzenszwalb", "Daniel P. Huttenlocher"], "related_topics": ["3D single-object recognition", "Object model", "Method", "Visual appearance", "Cognitive neuroscience of visual object recognition", "Object (computer science)", "Pattern recognition (psychology)", "Articulated body pose estimation", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2130416410", "references": ["2163738067", "2108207895"], "title": "Markov Chain Monte Carlo in Practice", "abstract": "INTRODUCING MARKOV CHAIN MONTE CARLO Introduction The Problem Markov Chain Monte Carlo Implementation Discussion HEPATITIS B: A CASE STUDY IN MCMC METHODS Introduction Hepatitis B Immunization Modelling Fitting a Model Using Gibbs Sampling Model Elaboration Conclusion MARKOV CHAIN CONCEPTS RELATED TO SAMPLING ALGORITHMS Markov Chains Rates of Convergence Estimation The Gibbs Sampler and Metropolis-Hastings Algorithm INTRODUCTION TO GENERAL STATE-SPACE MARKOV CHAIN THEORY Introduction Notation and Definitions Irreducibility, Recurrence, and Convergence Harris Recurrence Mixing Rates and Central Limit Theorems Regeneration Discussion FULL CONDITIONAL DISTRIBUTIONS Introduction Deriving Full Conditional Distributions Sampling from Full Conditional Distributions Discussion STRATEGIES FOR IMPROVING MCMC Introduction Reparameterization Random and Adaptive Direction Sampling Modifying the Stationary Distribution Methods Based on Continuous-Time Processes Discussion IMPLEMENTING MCMC Introduction Determining the Number of Iterations Software and Implementation Output Analysis Generic Metropolis Algorithms Discussion INFERENCE AND MONITORING CONVERGENCE Difficulties in Inference from Markov Chain Simulation The Risk of Undiagnosed Slow Convergence Multiple Sequences and Overdispersed Starting Points Monitoring Convergence Using Simulation Output Output Analysis for Inference Output Analysis for Improving Efficiency MODEL DETERMINATION USING SAMPLING-BASED METHODS Introduction Classical Approaches The Bayesian Perspective and the Bayes Factor Alternative Predictive Distributions How to Use Predictive Distributions Computational Issues An Example Discussion HYPOTHESIS TESTING AND MODEL SELECTION Introduction Uses of Bayes Factors Marginal Likelihood Estimation by Importance Sampling Marginal Likelihood Estimation Using Maximum Likelihood Application: How Many Components in a Mixture? Discussion Appendix: S-PLUS Code for the Laplace-Metropolis Estimator MODEL CHECKING AND MODEL IMPROVEMENT Introduction Model Checking Using Posterior Predictive Simulation Model Improvement via Expansion Example: Hierarchical Mixture Modelling of Reaction Times STOCHASTIC SEARCH VARIABLE SELECTION Introduction A Hierarchical Bayesian Model for Variable Selection Searching the Posterior by Gibbs Sampling Extensions Constructing Stock Portfolios With SSVS Discussion BAYESIAN MODEL COMPARISON VIA JUMP DIFFUSIONS Introduction Model Choice Jump-Diffusion Sampling Mixture Deconvolution Object Recognition Variable Selection Change-Point Identification Conclusions ESTIMATION AND OPTIMIZATION OF FUNCTIONS Non-Bayesian Applications of MCMC Monte Carlo Optimization Monte Carlo Likelihood Analysis Normalizing-Constant Families Missing Data Decision Theory Which Sampling Distribution? Importance Sampling Discussion STOCHASTIC EM: METHOD AND APPLICATION Introduction The EM Algorithm The Stochastic EM Algorithm Examples GENERALIZED LINEAR MIXED MODELS Introduction Generalized Linear Models (GLMs) Bayesian Estimation of GLMs Gibbs Sampling for GLMs Generalized Linear Mixed Models (GLMMs) Specification of Random-Effect Distributions Hyperpriors and the Estimation of Hyperparameters Some Examples Discussion HIERARCHICAL LONGITUDINAL MODELLING Introduction Clinical Background Model Detail and MCMC Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC FOR NONLINEAR HIERARCHICAL MODELS Introduction Implementing MCMC Comparison of Strategies A Case Study from Pharmacokinetics-Pharmacodynamics Extensions and Discussion BAYESIAN MAPPING OF DISEASE Introduction Hypotheses and Notation Maximum Likelihood Estimation of Relative Risks Hierarchical Bayesian Model of Relative Risks Empirical Bayes Estimation of Relative Risks Fully Bayesian Estimation of Relative Risks Discussion MCMC IN IMAGE ANALYSIS Introduction The Relevance of MCMC to Image Analysis Image Models at Different Levels Methodological Innovations in MCMC Stimulated by Imaging Discussion MEASUREMENT ERROR Introduction Conditional-Independence Modelling Illustrative examples Discussion GIBBS SAMPLING METHODS IN GENETICS Introduction Standard Methods in Genetics Gibbs Sampling Approaches MCMC Maximum Likelihood Application to a Family Study of Breast Cancer Conclusions MIXTURES OF DISTRIBUTIONS: INFERENCE AND ESTIMATION Introduction The Missing Data Structure Gibbs Sampling Implementation Convergence of the Algorithm Testing for Mixtures Infinite Mixtures and Other Extensions AN ARCHAEOLOGICAL EXAMPLE: RADIOCARBON DATING Introduction Background to Radiocarbon Dating Archaeological Problems and Questions Illustrative Examples Discussion Index", "citation_count": "2", "reference_count": "11,467", "date": "1997", "authors": ["W.R. Gilks", "S. Richardson", "David Spiegelhalter"], "related_topics": ["Gibbs sampling", "Slice sampling", "Metropolis\u2013Hastings algorithm", "Markov chain Monte Carlo", "Marginal likelihood", "Bayes factor", "Importance sampling", "Bayesian inference", "Mathematical optimization", "Econometrics", "Computer science"]}
{"id": "2137659841", "references": ["2143542740", "2120838001", "138943044", "1555969862", "2137385871", "2102773363", "2138451337"], "title": "Overview of the face recognition grand challenge", "abstract": "Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.", "citation_count": "7", "reference_count": "2,767", "date": "2005", "authors": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "Jin Chang", "K. Hoffman", "J. Marques", "Jaesik Min", "W. Worek"], "related_topics": ["Face Recognition Grand Challenge", "Face detection", "Facial recognition system", "Word error rate", "Biometrics", "Machine learning", "Computer vision", "Computer science", "Artificial intelligence", "Baseline (configuration management)", "NIST"]}
{"id": "3097096317", "references": ["2217896605", "2149706766", "2155511848", "2124351082", "2159686933", "2128272608", "1975846642", "2115763357", "2101522199", "3124955340"], "title": "Robust Real-Time Face Detection", "abstract": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.", "citation_count": "21", "reference_count": "18,902", "date": "2004", "authors": ["Paul Viola", "Michael J. Jones"], "related_topics": ["Object-class detection", "Face detection", "Viola\u2013Jones object detection framework", "Face Recognition Grand Challenge", "Haar-like features", "AdaBoost", "Boosting (machine learning)", "Cascading classifiers", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2033419168", "references": ["2689627924", "1761337995", "2120954940", "2131273085", "2132234724", "3094217134", "2128716185", "2012352340", "1997011019", "2138451337"], "title": "The FERET evaluation methodology for face-recognition algorithms", "abstract": "Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1,199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.", "citation_count": "12", "reference_count": "6,296", "date": "2000", "authors": ["P.J. Phillips", "Hyeonjoon Moon", "S.A. Rizvi", "P.J. Rauss"], "related_topics": ["FERET database", "Face Recognition Grand Challenge", "Facial recognition system", "Biometrics", "Pattern recognition", "Data mining", "Artificial intelligence", "Computer science", "FERET"]}
{"id": "2098693229", "references": ["2169718527", "2130259898", "2125848778", "2055712799", "2125999363", "1507699566", "1998186877", "2138451337"], "title": "Face recognition using eigenfaces", "abstract": "An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner. &gt;", "citation_count": "8", "reference_count": "8,478", "date": "1991", "authors": ["M.A. Turk", "A.P. Pentland"], "related_topics": ["Three-dimensional face recognition", "Eigenface", "Face detection", "Facial recognition system", "Face (geometry)", "Feature vector", "Unsupervised learning", "Set (psychology)", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence", "Face space"]}
{"id": "2006793117", "references": ["2120420721", "2033419168", "2121114545", "2141503314", "2155759509", "2102760078", "2106143125", "2144855601", "2110822444"], "title": "The CMU pose, illumination, and expression database", "abstract": "In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.", "citation_count": "9", "reference_count": "2,512", "date": "2003", "authors": ["T. Sim", "S. Baker", "M. Bsat"], "related_topics": ["Facial recognition system", "Expression (mathematics)", "Computer vision", "Computer graphics (images)", "Computer science", "Database", "Artificial intelligence"]}
{"id": "2123921160", "references": ["2217896605", "2033419168", "2152826865", "2098947662", "2159686933", "2115689562", "2121647436", "2120954940", "2123977795", "2138451337"], "title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "abstract": "We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions.", "citation_count": "82", "reference_count": "5,455", "date": "2001", "authors": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "related_topics": ["Illumination problem", "Image-based modeling and rendering", "Generative model", "Face (geometry)", "Standard test image", "Facial recognition system", "Convex cone", "Iterative reconstruction", "Lambertian reflectance", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2125310925", "references": ["2024046085", "2147880316", "1484228140", "2032210760", "2143516773", "2209124607", "2033819227", "2121947440", "1566135517", "1999478155"], "title": "Recovering Surface Layout from an Image", "abstract": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene--ground orientation, relative positions of major landmarks, etc.--even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \"surface layout\" of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.\r\n\r\nIn this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.", "citation_count": "53", "reference_count": "872", "date": "2007", "authors": ["Derek Hoiem", "Alexei A. Efros", "Martial Hebert"], "related_topics": ["Orientation (computer vision)", "Image processing", "GRASP", "Pattern recognition (psychology)", "View synthesis", "Color image", "Object detection", "Perspective (graphical)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1999478155", "references": ["2752885492", "2160167256", "1971784203", "2109562068", "2132603077", "1640070940", "2121947440", "2137560895", "2167077256", "1964443764"], "title": "Efficient Graph-Based Image Segmentation", "abstract": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.", "citation_count": "17", "reference_count": "7,281", "date": "2004", "authors": ["Pedro F. Felzenszwalb", "Daniel P. Huttenlocher"], "related_topics": ["Segmentation-based object categorization", "Image segmentation", "Range segmentation", "Connected-component labeling", "Scale-space segmentation", "Graph (abstract data type)", "Minimum spanning tree-based segmentation", "Image texture", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2295107390", "references": ["2102605133", "2618530766", "2155541015", "2097117768", "1849277567", "2162915993", "2962835968", "2117539524", "2963911037", "2963542991"], "title": "Learning Deep Features for Discriminative Localization", "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.", "citation_count": "36", "reference_count": "3,911", "date": "2016", "authors": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "related_topics": ["Convolutional neural network", "Discriminative model", "Pooling", "Pattern recognition", "Machine learning", "Representation (mathematics)", "Image (mathematics)", "Computer science", "Object (computer science)", "Simplicity", "Artificial intelligence"]}
{"id": "2963173190", "references": ["2102605133", "2618530766", "2155541015", "2108598243", "2131846894", "2109255472", "2031489346", "1849277567", "2162915993", "2963542991"], "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.", "citation_count": "27", "reference_count": "3,184", "date": "2014", "authors": ["Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "related_topics": ["Convolutional neural network", "Object detection", "Source code", "Pattern recognition", "Curse of dimensionality", "Computer science", "Common ground", "Layer (object-oriented design)", "Raising (linguistics)", "Artificial intelligence", "Fisher vector"]}
{"id": "2104978738", "references": ["1563088657", "1510073064", "2151103935", "2057175746", "2153635508", "2145072179", "2148603752", "2914885528", "3145128584", "2131846894"], "title": "The pyramid match kernel: discriminative classification with sets of image features", "abstract": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches", "citation_count": "32", "reference_count": "1,993", "date": "2005", "authors": ["K. Grauman", "T. Darrell"], "related_topics": ["Variable kernel density estimation", "Radial basis function kernel", "String kernel", "Tree kernel", "Kernel method", "Kernel embedding of distributions", "Polynomial kernel", "Kernel (statistics)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "1980911747", "references": ["2124386111", "2145023731", "2124404372", "2151103935", "2154422044", "2172188317", "1625255723", "2177274842", "2033819227", "2131846894"], "title": "A Comparison of Affine Region Detectors", "abstract": "The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of `maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of `salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression.\r\n\r\nThe objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.", "citation_count": "49", "reference_count": "4,250", "date": "2005", "authors": ["K. Mikolajczyk", "T. Tuytelaars", "C. Schmid", "A. Zisserman", "J. Matas", "F. Schaffalitzky", "T. Kadir", "L. Van Gool"], "related_topics": ["Hessian affine region detector", "Harris affine region detector", "Kadir\u2013Brady saliency detector", "Affine shape adaptation", "Affine transformation", "Principal curvature-based region detector", "Maximally stable extremal regions", "Detector", "Algorithm", "Geometry", "Mathematics"]}
{"id": "2172188317", "references": ["2124386111", "2012778485", "2124087378", "2033819227", "2119747362", "2109200236", "2111308925", "2165497495", "1676552347", "2124404372"], "title": "Scale &amp; Affine Invariant Interest Point Detectors", "abstract": "In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix.\r\n\r\nOur scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point.\r\n\r\nWe present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching resultss the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points.", "citation_count": "43", "reference_count": "5,436", "date": "2004", "authors": ["Krystian Mikolajczyk", "Cordelia Schmid"], "related_topics": ["Harris affine region detector", "Affine shape adaptation", "Hessian affine region detector", "Affine transformation", "Affine combination", "Affine hull", "Affine coordinate system", "Affine group", "Algorithm", "Topology", "Mathematics"]}
{"id": "2177274842", "references": ["2163352848", "2124386111", "2145023731", "1980911747", "2151103935", "2154422044", "2057175746", "2145072179", "2033819227", "2131846894"], "title": "A performance evaluation of local descriptors", "abstract": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.", "citation_count": "47", "reference_count": "20,408", "date": "2005", "authors": ["K. Mikolajczyk", "C. Schmid"], "related_topics": ["Principal curvature-based region detector", "Hessian affine region detector", "GLOH", "Interest point detection", "Scale-invariant feature transform", "Shape context", "Contextual image classification", "Implicit Shape Model", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2147717514", "references": ["1634005169", "2752885492", "2147152072", "2427881153", "1502916507", "1956559956", "2160066518", "2295428206", "3017143921", "2152565070"], "title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "abstract": "We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors' STOC'98 and FOCS'01 papers. It unifies, generalizes and simplifies the results from those papers.", "citation_count": "66", "reference_count": "5,030", "date": "1998", "authors": ["Piotr Indyk", "Rajeev Motwani"], "related_topics": ["Nearest neighbor search", "Best bin first", "Ball tree", "Fixed-radius near neighbors", "Cover tree", "Minimum spanning tree", "Curse of dimensionality", "k-nearest neighbors algorithm", "Combinatorics", "Mathematics"]}
{"id": "2162006472", "references": ["2045533739", "2109034006", "1595303882", "1502916507", "2169351022", "2147717514", "1541459201", "2520931985", "2165533158", "2048779798"], "title": "Locality-sensitive hashing scheme based on p-stable distributions", "abstract": "We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p", "citation_count": "29", "reference_count": "3,261", "date": "2004", "authors": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S. Mirrokni"], "related_topics": ["Locality-sensitive hashing", "Dynamic perfect hashing", "Universal hashing", "Nearest neighbor search", "K-independent hashing", "2-choice hashing", "Hopscotch hashing", "Open addressing", "Combinatorics", "Mathematics"]}
{"id": "2131846894", "references": ["2124386111", "2160484851", "2124087378", "2177274842", "1660390307", "3013264884", "1541642243", "2165497495", "1676552347", "2124404372"], "title": "Video Google: a text retrieval approach to object matching in videos", "abstract": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.", "citation_count": "19", "reference_count": "7,805", "date": "2003", "authors": ["Sivic", "Zisserman"], "related_topics": ["Visual Word", "Video copy detection", "Inverted index", "Bag-of-words model in computer vision", "Vector quantization", "Visual dictionary", "Caltech 101", "Computer vision", "Pattern recognition", "Invariant (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "2111993661", "references": ["1480376833", "2119479037", "1989702938", "2130660124", "2067191022", "2154422044", "2057175746", "2177274842", "2121947440", "1579271636"], "title": "Image retrieval: Ideas, influences, and trends of the new age", "abstract": "We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research.", "citation_count": "282", "reference_count": "4,570", "date": "2008", "authors": ["Ritendra Datta", "Dhiraj Joshi", "Jia Li", "James Z. Wang"], "related_topics": ["Content-based image retrieval", "Automatic image annotation", "Image retrieval", "Emerging technologies", "Process (engineering)", "Adaptation (computer science)", "Information retrieval", "Documentation", "Computer science", "Annotation"]}
{"id": "2024046085", "references": ["2141518341", "2102201073", "1678356000", "2099968818", "1881647329", "1540007258"], "title": "Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors)", "abstract": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.", "citation_count": "6", "reference_count": "8,356", "date": "2000", "authors": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "related_topics": ["BrownBoost", "Gradient boosting", "Boosting (machine learning)", "LPBoost", "LogitBoost", "AdaBoost", "Statistical classification", "Decision rule", "Machine learning", "Econometrics", "Mathematics", "Artificial intelligence"]}
{"id": "2168002178", "references": ["3097096317", "2155511848", "2119823327", "2151103935", "2154422044", "2124087378", "2177274842", "2295106276", "2101522199", "2124386111"], "title": "Shape matching and object recognition using low distortion correspondences", "abstract": "We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.", "citation_count": "33", "reference_count": "1,117", "date": "2005", "authors": ["A.C. Berg", "T.L. Berg", "J. Malik"], "related_topics": ["Caltech 101", "k-nearest neighbors algorithm", "Thin plate spline", "Facial recognition system", "Face detection", "Cognitive neuroscience of visual object recognition", "Quadratic programming", "Spline (mathematics)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2124351162", "references": ["2740373864", "2103917701", "2049633694", "2101309634", "2134820502", "2077786999", "1785730614", "2103334940", "2104095591", "2169551590"], "title": "\"GrabCut\": interactive foreground extraction using iterated graph cuts", "abstract": "The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.", "citation_count": "16", "reference_count": "6,976", "date": "2004", "authors": ["Carsten Rother", "Vladimir Kolmogorov", "Andrew Blake"], "related_topics": ["GrabCut", "Image segmentation", "Simple interactive object extraction", "Graph cuts in computer vision", "Image editing", "Cut", "Iterative method", "Segmentation", "Pixel", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2169551590", "references": ["1991113069", "1987983010", "2132603077", "2121947440", "2113137767", "2096139825", "2104095591", "2086921140", "2098152234", "1564419782"], "title": "Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images", "abstract": "In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.", "citation_count": "21", "reference_count": "6,159", "date": "2001", "authors": ["Y.Y. Boykov", "M.-P. Jolly"], "related_topics": ["Scale-space segmentation", "Segmentation-based object categorization", "Image segmentation", "Minimum spanning tree-based segmentation", "Region growing", "Image texture", "Connected-component labeling", "GrabCut", "Graph cuts in computer vision", "Segmentation", "Simple interactive object extraction", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1484228140", "references": ["1997063559", "1634005169", "2116013899", "2170120409", "2130416410", "2123977795", "2117812871", "3017143921", "1481646516", "2138451337"], "title": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons", "abstract": "We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions.\r\n\r\nGiven a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions.", "citation_count": "44", "reference_count": "2,049", "date": "2001", "authors": ["Thomas Leung", "Jitendra Malik"], "related_topics": ["Texton", "Visual appearance", "Texture synthesis", "Pattern recognition (psychology)", "Texture (geology)", "Cluster analysis", "Basis (linear algebra)", "Normal", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2022166150", "references": ["1678959094", "1548663377", "1529522905", "2137023796", "1583295953", "2038721957", "4508078", "2075123415", "2103931177", "2122410182"], "title": "Yago: a core of semantic knowledge", "abstract": "We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.", "citation_count": "26", "reference_count": "4,078", "date": "2007", "authors": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "related_topics": ["Ontology (information science)", "WordNet", "Knowledge extraction", "Infobox", "Ontology", "Knowledge base", "RDF Schema", "Information extraction", "Correctness", "Information retrieval", "Hierarchy", "Information system", "Computer science"]}
{"id": "2160660844", "references": ["1581485226", "3146306708", "1574901103", "2199803028", "2166706824", "2038721957", "2102381086", "2115023510", "1506285740"], "title": "Mining and summarizing customer reviews", "abstract": "Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.", "citation_count": "40", "reference_count": "8,447", "date": "2004", "authors": ["Minqing Hu", "Bing Liu"], "related_topics": ["Automatic summarization", "Product (category theory)", "Sentiment analysis", "Task (project management)", "Sentence", "Data science", "Computer science", "Data mining", "Text mining", "Customer reviews"]}
{"id": "3014294089", "references": ["3001118548", "2158525457", "3009885589", "3011559677", "3008090866", "3008827533", "3014538785", "3005079553", "3011508296"], "title": "Baseline Characteristics and Outcomes of 1591 Patients Infected With SARS-CoV-2 Admitted to ICUs of the Lombardy Region, Italy.", "abstract": "Importance  In December 2019, a novel coronavirus (severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) emerged in China and has spread globally, creating a pandemic. Information about the clinical characteristics of infected patients who require intensive care is limited.  Objective  To characterize patients with coronavirus disease 2019 (COVID-19) requiring treatment in an intensive care unit (ICU) in the Lombardy region of Italy.  Design, Setting, and Participants  Retrospective case series of 1591 consecutive patients with laboratory-confirmed COVID-19 referred for ICU admission to the coordinator center (Fondazione IRCCS Ca\u2019 Granda Ospedale Maggiore Policlinico, Milan, Italy) of the COVID-19 Lombardy ICU Network and treated at one of the ICUs of the 72 hospitals in this network between February 20 and March 18, 2020. Date of final follow-up was March 25, 2020.  Exposures  SARS-CoV-2 infection confirmed by real-time reverse transcriptase\u2013polymerase chain reaction (RT-PCR) assay of nasal and pharyngeal swabs.  Main Outcomes and Measures  Demographic and clinical data were collected, including data on clinical management, respiratory failure, and patient mortality. Data were recorded by the coordinator center on an electronic worksheet during telephone calls by the staff of the COVID-19 Lombardy ICU Network.  Results  Of the 1591 patients included in the study, the median (IQR) age was 63 (56-70) years and 1304 (82%) were male. Of the 1043 patients with available data, 709 (68%) had at least 1 comorbidity and 509 (49%) had hypertension. Among 1300 patients with available respiratory support data, 1287 (99% [95% CI, 98%-99%]) needed respiratory support, including 1150 (88% [95% CI, 87%-90%]) who received mechanical ventilation and 137 (11% [95% CI, 9%-12%]) who received noninvasive ventilation. The median positive end-expiratory pressure (PEEP) was 14 (IQR, 12-16) cm H2O, and Fio2was greater than 50% in 89% of patients. The median Pao2/Fio2was 160 (IQR, 114-220). The median PEEP level was not different between younger patients (n\u2009=\u2009503 aged \u226463 years) and older patients (n\u2009=\u2009514 aged \u226564 years) (14 [IQR, 12-15] vs 14 [IQR, 12-16] cm H2O, respectively; median difference, 0 [95% CI, 0-0];P\u2009=\u2009.94). Median Fio2was lower in younger patients: 60% (IQR, 50%-80%) vs 70% (IQR, 50%-80%) (median difference, \u221210% [95% CI, \u221214% to 6%];P\u2009=\u2009.006), and median Pao2/Fio2was higher in younger patients: 163.5 (IQR, 120-230) vs 156 (IQR, 110-205) (median difference, 7 [95% CI, \u22128 to 22];P\u2009=\u2009.02). Patients with hypertension (n\u2009=\u2009509) were older than those without hypertension (n\u2009=\u2009526) (median [IQR] age, 66 years [60-72] vs 62 years [54-68];P\u2009  Conclusions and Relevance  In this case series of critically ill patients with laboratory-confirmed COVID-19 admitted to ICUs in Lombardy, Italy, the majority were older men, a large proportion required mechanical ventilation and high levels of PEEP, and ICU mortality was 26%.", "citation_count": "9", "reference_count": "3,125", "date": "2020", "authors": ["Giacomo Grasselli", "Alberto Zangrillo", "Alberto Zanella", "Massimo Antonelli", "Luca Cabrini", "Antonio Castelli", "Danilo Cereda", "Antonio Coluccello", "Giuseppe Foti", "Roberto Fumagalli", "Giorgio Iotti", "Nicola Latronico", "Luca Lorini", "Stefano Merler", "Giuseppe Natalini", "Alessandra Piatti", "Marco Vito Ranieri", "Anna Mara Scandroglio", "Enrico Storti", "Maurizio Cecconi", "Antonio Pesenti"], "related_topics": ["Intensive care", "Intensive care unit", "Respiratory failure", "Mechanical ventilation", "Retrospective cohort study", "Artificial ventilation", "Positive end-expiratory pressure", "Comorbidity", "Internal medicine", "Medicine"]}
{"id": "3010930696", "references": ["3008763357", "3011032288", "3006645647", "3009885589", "3010277308", "3005212621", "2302013022", "3024400578", "3008028633", "3009577418"], "title": "Hydroxychloroquine and azithromycin as a treatment of COVID-19: results of an open-label non-randomized clinical trial.", "abstract": "Background  Chloroquine and hydroxychloroquine have been found to be efficient on SARS-CoV-2, and reported to be efficient in Chinese COV-19 patients. We evaluate the role of hydroxychloroquine on respiratory viral loads.  Patients and methods  French Confirmed COVID-19 patients were included in a single arm protocol from early March to March 16th, to receive 600mg of hydroxychloroquine daily and their viral load in nasopharyngeal swabs was tested daily in a hospital setting. Depending on their clinical presentation, azithromycin was added to the treatment. Untreated patients from another center and cases refusing the protocol were included as negative controls. Presence and absence of virus at Day6-post inclusion was considered the end point.  Results  Six patients were asymptomatic, 22 had upper respiratory tract infection symptoms and eight had lower respiratory tract infection symptoms. Twenty cases were treated in this study and showed a significant reduction of the viral carriage at D6-post inclusion compared to controls, and much lower average carrying duration than reported of untreated patients in the literature. Azithromycin added to hydroxychloroquine was significantly more efficient for virus elimination.  Conclusion  Despite its small sample size our survey shows that hydroxychloroquine treatment is significantly associated with viral load reduction/disappearance in COVID-19 patients and its effect is reinforced by azithromycin.", "citation_count": "30", "reference_count": "4,437", "date": "2020", "authors": ["Philippe Gautret", "Jean Christophe Lagier", "Philippe Parola", "Van Thuan Hoang", "Line Meddeb", "Morgane Mailhe", "Barbara Doudier", "Johan Courjon", "Val\u00e9rie Giordanengo", "Vera Esteves Vieira", "Herv\u00e9 Tissot Dupont", "St\u00e9phane Honor\u00e9", "Philippe Colson", "Eric Chabri\u00e8re", "Bernard La Scola", "Jean Marc Rolain", "Philippe Brouqui", "Didier Raoult"], "related_topics": ["Hydroxychloroquine", "Viral load", "Lower respiratory tract infection", "Upper respiratory tract infection", "Azithromycin", "Asymptomatic", "Randomized controlled trial", "Clinical trial", "Internal medicine", "Medicine"]}
{"id": "3009885589", "references": ["3001118548", "3003668884", "3007940623", "1803784511", "3008090866", "2026274122", "2280404143", "3005079553", "3006961006", "3002108456"], "title": "Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study.", "abstract": "Summary  Background  Since December, 2019, Wuhan, China, has experienced an outbreak of coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Epidemiological and clinical characteristics of patients with COVID-19 have been reported but risk factors for mortality and a detailed clinical course of illness, including viral shedding, have not been well described.  Methods  In this retrospective, multicentre cohort study, we included all adult inpatients (\u226518 years old) with laboratory-confirmed COVID-19 from Jinyintan Hospital and Wuhan Pulmonary Hospital (Wuhan, China) who had been discharged or had died by Jan 31, 2020. Demographic, clinical, treatment, and laboratory data, including serial samples for viral RNA detection, were extracted from electronic medical records and compared between survivors and non-survivors. We used univariable and multivariable logistic regression methods to explore the risk factors associated with in-hospital death.  Findings  191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients). Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1\u00b710, 95% CI 1\u00b703\u20131\u00b717, per year increase; p=0\u00b70043), higher Sequential Organ Failure Assessment (SOFA) score (5\u00b765, 2\u00b761\u201312\u00b723; p  Interpretation  The potential risk factors of older age, high SOFA score, and d-dimer greater than 1 \u03bcg/mL could help clinicians to identify patients with poor prognosis at an early stage. Prolonged viral shedding provides the rationale for a strategy of isolation of infected patients and optimal antiviral interventions in the future.  Funding  Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences; National Science Grant for Distinguished Young Scholars; National Key Research and Development Program of China; The Beijing Science and Technology Project; and Major Projects of National Science and Technology on New Drug Creation and Development.", "citation_count": "40", "reference_count": "21,762", "date": "2020", "authors": ["Fei Zhou", "Ting Yu", "Ronghui Du", "Guohui Fan", "Ying Liu", "Zhibo Liu", "Jie Xiang", "Yeming Wang", "Bin Song", "Xiaoying Gu", "Lulu Guan", "Yuan Wei", "Hui Li", "Xudong Wu", "Jiuyang Xu", "Shengjin Tu", "Yi Zhang", "Hua Chen", "Bin Cao"], "related_topics": ["Cohort study", "Retrospective cohort study", "Odds ratio", "SOFA score", "Epidemiology", "Comorbidity", "Young adult", "Risk assessment", "Internal medicine", "Medicine"]}
{"id": "3008090866", "references": ["3001118548", "3001897055", "2286228001", "3005403371", "2470646526", "2026274122", "3003465021", "3005079553", "3004239190", "3002108456"], "title": "Clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia in Wuhan, China: a single-centered, retrospective, observational study.", "abstract": "Summary  Background  An ongoing outbreak of pneumonia associated with the severe acute respiratory coronavirus 2 (SARS-CoV-2) started in December, 2019, in Wuhan, China. Information about critically ill patients with SARS-CoV-2 infection is scarce. We aimed to describe the clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia.  Methods  In this single-centered, retrospective, observational study, we enrolled 52 critically ill adult patients with SARS-CoV-2 pneumonia who were admitted to the intensive care unit (ICU) of Wuhan Jin Yin-tan hospital (Wuhan, China) between late December, 2019, and Jan 26, 2020. Demographic data, symptoms, laboratory values, comorbidities, treatments, and clinical outcomes were all collected. Data were compared between survivors and non-survivors. The primary outcome was 28-day mortality, as of Feb 9, 2020. Secondary outcomes included incidence of SARS-CoV-2-related acute respiratory distress syndrome (ARDS) and the proportion of patients requiring mechanical ventilation.  Findings  Of 710 patients with SARS-CoV-2 pneumonia, 52 critically ill adult patients were included. The mean age of the 52 patients was 59\u00b77 (SD 13\u00b73) years, 35 (67%) were men, 21 (40%) had chronic illness, 51 (98%) had fever. 32 (61\u00b75%) patients had died at 28 days, and the median duration from admission to the intensive care unit (ICU) to death was 7 (IQR 3\u201311) days for non-survivors. Compared with survivors, non-survivors were older (64\u00b76 years [11\u00b72] vs 51\u00b79 years [12\u00b79]), more likely to develop ARDS (26 [81%] patients vs 9 [45%] patients), and more likely to receive mechanical ventilation (30 [94%] patients vs 7 [35%] patients), either invasively or non-invasively. Most patients had organ function damage, including 35 (67%) with ARDS, 15 (29%) with acute kidney injury, 12 (23%) with cardiac injury, 15 (29%) with liver dysfunction, and one (2%) with pneumothorax. 37 (71%) patients required mechanical ventilation. Hospital-acquired infection occurred in seven (13\u00b75%) patients.  Interpretation  The mortality of critically ill patients with SARS-CoV-2 pneumonia is considerable. The survival time of the non-survivors is likely to be within 1\u20132 weeks after ICU admission. Older patients (&gt;65 years) with comorbidities and ARDS are at increased risk of death. The severity of SARS-CoV-2 pneumonia poses great strain on critical care resources in hospitals, especially if they are not adequately staffed or resourced.  Funding  None.", "citation_count": "22", "reference_count": "7,506", "date": "2020", "authors": ["Xiaobo Yang", "Yuan Yu", "Jiqian Xu", "Huaqing Shu", "Jia'an Xia", "Hong Liu", "Yongran Wu", "Lu Zhang", "Zhui Yu", "Minghao Fang", "Ting Yu", "Yaxin Wang", "Shangwen Pan", "Xiaojing Zou", "Shiying Yuan", "You Shang"], "related_topics": ["Pneumonia", "ARDS", "Intensive care unit", "Mechanical ventilation", "Retrospective cohort study", "Incidence (epidemiology)", "Acute kidney injury", "Pneumothorax", "Emergency medicine", "Medicine"]}
{"id": "3008028633", "references": ["3024919756", "3031410613", "3006533361", "3004434564", "3005409672", "3082757469", "3006044875", "3034094473", "3026023364", "3033263492"], "title": "Characteristics of and Important Lessons From the Coronavirus Disease 2019 (COVID-19) Outbreak in China: Summary of a Report of 72 314 Cases From the Chinese Center for Disease Control and Prevention", "abstract": "", "citation_count": "10", "reference_count": "11,988", "date": "2020", "authors": ["Zunyou Wu", "Jennifer M. McGoogan"], "related_topics": ["Outbreak", "Pandemic", "China", "Medicine", "Viral Epidemiology", "Betacoronavirus", "Family medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Disease control"]}
{"id": "3007940623", "references": ["3001118548", "3003217347", "3002539152", "2158118659", "2256430766"], "title": "Pathological findings of COVID-19 associated with acute respiratory distress syndrome.", "abstract": "", "citation_count": "5", "reference_count": "5,565", "date": "2020", "authors": ["Zhe Xu", "Lei Shi", "Yijin Wang", "Jiyuan Zhang", "Lei Huang", "Chao Zhang", "Shuhong Liu", "Peng Zhao", "Hongxia Liu", "Li Zhu", "Yanhong Tai", "Changqing Bai", "Tingting Gao", "Jinwen Song", "Peng Xia", "Jinghui Dong", "Jingmin Zhao", "Fu Sheng Wang"], "related_topics": ["Cytokine storm", "Medicine", "Pathological", "Internal medicine", "MEDLINE", "2019-20 coronavirus outbreak", "Acute respiratory distress", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2092969802", "references": ["2006434809", "2104548316", "2107053896", "2166867592", "2119111857", "1993577573", "2132260239", "2160011624", "2025170735", "1966238900"], "title": "A decade after SARS: strategies for controlling emerging coronaviruses", "abstract": "Two novel coronaviruses have emerged in humans in the twenty-first century: severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV), both of which cause acute respiratory distress syndrome (ARDS) and are associated with high mortality rates. There are no clinically approved vaccines or antiviral drugs available for either of these infections; thus, the development of effective therapeutic and preventive strategies that can be readily applied to new emergent strains is a research priority. In this Review, we describe the emergence and identification of novel human coronaviruses over the past 10 years, discuss their key biological features, including tropism and receptor use, and summarize approaches for developing broadly effective vaccines.", "citation_count": "153", "reference_count": "595", "date": "2013", "authors": ["Rachel L. Graham", "Eric F. Donaldson", "Ralph S. Baric"], "related_topics": ["Middle East respiratory syndrome coronavirus", "ARDS", "Tissue tropism", "Viral Epidemiology", "Tropism", "Intensive care medicine", "Immunology", "Biology", "Acute respiratory distress", "High mortality", "Severe acute respiratory syndrome coronavirus"]}
{"id": "2102634410", "references": ["2107051779", "2121350896", "2626588662", "1924766221", "2157678780", "2416914730", "2041775285", "2114857071", "2017898137", "2155323443"], "title": "Fleischner Society: Glossary of Terms for Thoracic Imaging", "abstract": "Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms.", "citation_count": "134", "reference_count": "3,000", "date": "2008", "authors": ["David M Hansell", "Alexander A Bankier", "Heber MacMahon", "Theresa C McLoud", "Nestor L M\u00fcller", "Jacques Remy"], "related_topics": ["Glossary", "Radiography", "Thorax", "Medical physics", "Medicine", "Computed tomography", "Cystic lung disease", "Thoracic Radiography", "Thoracic imaging", "X ray computed"]}
{"id": "3004802901", "references": ["3006882119", "3006643024", "3008627141", "3012211282", "3010659930", "3025334942", "3034593359", "3013468450", "3007355693", "3008928918"], "title": "CT Manifestations of Two Cases of 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "", "citation_count": "0", "reference_count": "192", "date": "2020", "authors": ["Yicheng Fang", "Huangqi Zhang", "Yunyu Xu", "Jicheng Xie", "Peipei Pang", "Wenbin Ji"], "related_topics": ["Pneumonia", "Medicine", "Pathology", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Disease progression", "Lung pathology", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Tomography x ray computed", "X ray computed"]}
{"id": "2800783955", "references": ["3004906315", "3007670341", "3010278110", "3010061930", "3004668429", "3010902474", "3006643024", "3008207212", "3012817089", "3009925957"], "title": "Radiographic and CT Features of Viral Pneumonia.", "abstract": "Viruses are the most common causes of respiratory infection. The imaging findings of viral pneumonia are diverse and overlap with those of other nonviral infectious and inflammatory conditions. However, identification of the underlying viral pathogens may not always be easy. There are a number of indicators for identifying viral pathogens on the basis of imaging patterns, which are associated with the pathogenesis of viral infections. Viruses in the same viral family share a similar pathogenesis of pneumonia, and the imaging patterns have distinguishable characteristics. Although not all cases manifest with typical patterns, most typical imaging patterns of viral pneumonia can be classified according to viral families. Although a definite diagnosis cannot be achieved on the basis of imaging features alone, recognition of viral pneumonia patterns may aid in differentiating viral pathogens, thus reducing the use of antibiotics. Recently, new viruses associated with recent outbreaks including human metapneumovirus, severe acute respiratory syndrome coronavirus, and Middle East respiratory syndrome coronavirus have been discovered. The imaging findings of these emerging pathogens have been described in a few recent studies. This review focuses on the radiographic and computed tomographic patterns of viral pneumonia caused by different pathogens, including new pathogens. Clinical characteristics that could affect imaging, such as patient age and immune status, seasonal variation and community outbreaks, and pathogenesis, are also discussed. The first goal of this review is to indicate that there are imaging features that should raise the possibility of viral infections. Second, to help radiologists differentiate viral infections, viruses in the same viridae that have similar pathogenesis and can have similar imaging characteristics are shown. By considering both the clinical and radiologic characteristics, radiologists can suggest the diagnosis of viral pneumonia. \u00a9RSNA, 2018.", "citation_count": "0", "reference_count": "428", "date": "2018", "authors": ["Hyun Jung Koo", "Soyeoun Lim", "Jooae Choe", "Sang Ho Choi", "Heungsup Sung", "Kyung Hyun Do"], "related_topics": ["Viral pneumonia", "Pneumonia (non-human)", "Respiratory infection", "Human metapneumovirus", "Middle East respiratory syndrome coronavirus", "Immunology", "Pathogenesis", "Outbreak", "Antibiotics", "Medicine"]}
{"id": "2056155046", "references": ["2099918622", "1971054351", "1976741900", "2131262274", "2119467724", "2124413369", "2151996610", "2098293966", "2025170735", "2125251240"], "title": "Severe Acute Respiratory Syndrome: Temporal Lung Changes at Thin-Section CT in 30 Patients", "abstract": "PURPOSE: To evaluate lung abnormalities on serial thin-section computed tomographic (CT) scans in patients with severe acute respiratory syndrome (SARS) during acute and convalescent periods. MATERIALS AND METHODS: Serial thin-section CT scans in 30 patients (17 men, aged 42.5 years \u00b1 12.2 [SD]) with SARS were reviewed by two radiologists together for predominant patterns of lung abnormalities: ground-glass opacities, ground-glass opacities with superimposed linear opacities, consolidation, reticular pattern, and mixed pattern (consolidation, ground-glass opacities, and reticular pattern). Scans were classified according to duration in weeks after symptom onset. Longitudinal changes of specific abnormalities were documented in 17 patients with serial scans obtained during 3 weeks. Each lung was divided into three zones; each zone was evaluated for percentage of lung involvement. Summation of scores from all six lung zones provided overall CT score (maximal CT score, 24). RESULTS: Median CT scores increase...", "citation_count": "17", "reference_count": "279", "date": "2004", "authors": ["Gaik C. Ooi", "Pek L. Khong", "Nestor L. M\u00fcller", "Wai C. Yiu", "Lin J. Zhou", "James C. M. Ho", "Bing Lam", "Savvas Nicolaou", "Kenneth W. T. Tsang"], "related_topics": ["Zones of the lung", "Respiratory disease", "Lung", "Severe acute respiratory syndrome", "Pulmonary fibrosis", "Radiology", "Respiratory system", "Tomography", "Surgery", "Spontaneous remission", "Medicine"]}
{"id": "3013893137", "references": ["3001897055", "3009906937", "3009912996", "3002108456", "3003465021", "2132260239", "3010338568", "3006961006", "3004239190", "3001195213"], "title": "Virological assessment of hospitalized patients with COVID-2019.", "abstract": "Coronavirus disease 2019 (COVID-19) is an acute infection of the respiratory tract that emerged in late 20191,2. Initial outbreaks in China involved 13.8% of cases with severe courses, and 6.1% of cases with critical courses3. This severe presentation may result from the virus using a virus receptor that is expressed predominantly in the lung2,4; the same receptor tropism is thought to have determined the pathogenicity-but also aided in the control-of severe acute respiratory syndrome (SARS) in 20035. However, there are reports of cases of COVID-19 in which the patient shows mild upper respiratory tract symptoms, which suggests the potential for pre- or oligosymptomatic transmission6-8. There is an urgent need for information on virus replication, immunity and infectivity in specific sites of the body. Here we report a detailed virological analysis of nine cases of COVID-19 that provides proof of active virus replication in tissues of the upper respiratory tract. Pharyngeal virus shedding was very high during the first week of symptoms, with a peak at 7.11 \u00d7 108 RNA copies per throat swab on day 4. Infectious virus was readily isolated from samples derived from the throat or lung, but not from stool samples-in spite of high concentrations of virus RNA. Blood and urine samples never yielded virus. Active replication in the throat was confirmed by the presence of viral replicative RNA intermediates in the throat samples. We consistently detected sequence-distinct virus populations in throat and lung samples from one patient, proving independent replication. The shedding of viral RNA from sputum outlasted the end of symptoms. Seroconversion occurred after 7 days in 50% of patients (and by day 14 in all patients), but was not followed by a rapid decline in viral load. COVID-19 can present as a mild illness of the upper respiratory tract. The confirmation of active virus replication in the upper respiratory tract has implications for the containment of COVID-19.", "citation_count": "34", "reference_count": "3,947", "date": "2020", "authors": ["Roman W\u00f6lfel", "Victor M. Corman", "Wolfgang Guggemos", "Michael Seilmaier", "Sabine Zange", "Marcel A. M\u00fcller", "Daniela Niemeyer", "Terry C. Jones", "Patrick Vollmar", "Camilla Rothe", "Michael Hoelscher", "Tobias Bleicker", "Sebastian Br\u00fcnink", "Julia Schneider", "Rosina Ehmann", "Katrin Zwirglmaier", "Christian Drosten", "Clemens Wendtner"], "related_topics": ["Virus receptor", "Viral shedding", "Viral load", "Virus", "Viral replication", "Sputum", "Respiratory tract", "Seroconversion", "Virology", "Medicine"]}
{"id": "3004280078", "references": ["2103503670", "2775086803", "2195009776", "2166867592", "1993577573", "2918873120", "2132260239", "2903899730", "2298153446", "1966238900"], "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin", "abstract": "Since the outbreak of severe acute respiratory syndrome (SARS) 18\u00a0years ago, a large number of SARS-related coronaviruses (SARSr-CoVs) have been discovered in their natural reservoir host, bats1-4. Previous studies have shown that some bat SARSr-CoVs have the potential to infect humans5-7. Here we report the identification and characterization of a new coronavirus (2019-nCoV), which caused an epidemic of acute respiratory syndrome in humans in Wuhan, China. The epidemic, which started on 12 December 2019, had caused 2,794 laboratory-confirmed infections including 80 deaths by 26 January 2020. Full-length genome sequences were obtained from five patients at an early stage of the outbreak. The sequences are almost identical and share 79.6% sequence identity to SARS-CoV. Furthermore, we show that 2019-nCoV is 96% identical at the whole-genome level to a bat coronavirus. Pairwise protein sequence analysis of seven conserved non-structural proteins domains show that this virus belongs to the species of SARSr-CoV. In addition, 2019-nCoV virus isolated from the bronchoalveolar lavage fluid of a critically ill patient could be neutralized by sera from several patients. Notably, we confirmed that 2019-nCoV uses the same cell entry receptor-angiotensin converting enzyme II (ACE2)-as SARS-CoV.", "citation_count": "13", "reference_count": "11,692", "date": "2020", "authors": ["Peng Zhou", "Xing Lou Yang", "Xian Guang Wang", "Ben Hu", "Lei Zhang", "Wei Zhang", "Hao Rui Si", "Yan Zhu", "Bei Li", "Chao Lin Huang", "Hui Dong Chen", "Jing Chen", "Yun Luo", "Hua Guo", "Ren Di Jiang", "Mei Qin Liu", "Ying Chen", "Xu Rui Shen", "Xi Wang", "Xiao Shuang Zheng", "Kai Zhao", "Quan Jiao Chen", "Fei Deng", "Lin Lin Liu", "Bing Yan", "Fa Xian Zhan", "Yan Yi Wang", "Geng Fu Xiao", "Zheng Li Shi"], "related_topics": ["Coronavirus", "Betacoronavirus", "Outbreak", "Virus", "Natural reservoir", "Pneumonia", "Virology", "Rhinolophus affinis", "Bronchoalveolar lavage", "Biology"]}
{"id": "3001195213", "references": ["1703839189", "1852588318", "2793008036", "2894950287", "2132260239", "2903899730", "2884280018", "2031705962", "2167080692", "2101063972"], "title": "Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR.", "abstract": "Background The ongoing outbreak of the recently emerged novel coronavirus (2019-nCoV) poses a challenge for public health laboratories as virus isolates are unavailable while there is growing evidence that the outbreak is more widespread than initially thought, and international spread through travellers does already occur. Aim We aimed to develop and deploy robust diagnostic methodology for use in public health laboratory settings without having virus material available. Methods Here we present a validated diagnostic workflow for 2019-nCoV, its design relying on close genetic relatedness of 2019-nCoV with SARS coronavirus, making use of synthetic nucleic acid technology. Results The workflow reliably detects 2019-nCoV, and further discriminates 2019-nCoV from SARS-CoV. Through coordination between academic and public laboratories, we confirmed assay exclusivity based on 297 original clinical specimens containing a full spectrum of human respiratory viruses. Control material is made available through European Virus Archive \u2013 Global (EVAg), a European Union infrastructure project. Conclusion The present study demonstrates the enormous response capacity achieved through coordination of academic and public laboratories in national and European research networks.", "citation_count": "18", "reference_count": "4,091", "date": "2020", "authors": ["Victor M. Corman", "Olfert Landt", "Marco Kaiser", "Richard Molenkamp", "Adam Meijer", "Daniel K.W. Chu", "Tobias Bleicker", "Sebastian Br\u00fcnink", "Julia Schneider", "Marie Luisa Schmidt", "Daphne G.J.C. Mulders", "Bart L. Haagmans", "Bas Van Der Veer", "Sharon Van Den Brink", "Lisa Wijsman", "Gabriel Goderski", "Jean Louis Romette", "Joanna Ellis", "Maria Zambon", "Malik Peiris", "Herman Goossens", "Chantal Reusken", "Marion P.G. Koopmans", "Christian Drosten"], "related_topics": ["European union", "Coronavirus", "Global health", "Workflow", "Public health", "Outbreak", "Data science", "Computer science", "European research", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2112136274", "references": ["2006434809", "2049975503", "1703839189", "2149661971", "1852588318", "2107053896", "2166867592", "2109520345", "2045002682", "2119837294"], "title": "Middle East Respiratory Syndrome Coronavirus (MERS-CoV) Infection: Chest CT Findings", "abstract": "OBJECTIVE. The purpose of this study was to describe the chest CT findings in seven patients with Middle East respiratory syndrome coronavirus (MERS-CoV) infection. CONCLUSION. The most common CT finding in hospitalized patients with MERS-CoV infection is that of bilateral predominantly subpleural and basilar airspace changes, with more extensive ground-glass opacities than consolidation. The subpleural and peribronchovascular predilection of the abnormalities is suggestive of an organizing pneumonia pattern.", "citation_count": "20", "reference_count": "266", "date": "2014", "authors": ["Amr M. Ajlan", "Rayan A. Ahyad", "Lamia Ghazi Jamjoom", "Ahmed Alharthy", "Tariq A. Madani"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Coronavirus", "Internal medicine", "Pathology", "Medicine", "Chest ct", "Ct findings", "Hospitalized patients", "Infection chest", "Organizing pneumonia"]}
{"id": "3005272159", "references": ["3135910874", "3001118548", "2111742711", "3002539152", "2103118479", "2306794997", "2903899730", "3009749892", "2056155046", "3024264813"], "title": "Chest CT Findings in 2019 Novel Coronavirus (2019-nCoV) Infections from Wuhan, China: Key Points for the Radiologist.", "abstract": "", "citation_count": "15", "reference_count": "492", "date": "2020", "authors": ["Jeffrey P Kanne"], "related_topics": ["Radiology", "Medicine", "2019-20 coronavirus outbreak", "Chest ct", "Coronavirus disease 2019 (COVID-19)", "Disease progression", "Lung pathology", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Tomography x ray computed"]}
{"id": "2999318660", "references": ["2766931063", "2103503670", "2470646526", "2255243349", "2158887145", "2132260239", "2134061616", "2121494157", "1997954607"], "title": "Outbreak of pneumonia of unknown etiology in Wuhan, China: The mystery and the miracle.", "abstract": "", "citation_count": "9", "reference_count": "2,194", "date": "2020", "authors": ["Hongzhou Lu", "Charles W. Stratton", "Yi Wei Tang"], "related_topics": ["Pneumonia", "Outbreak", "Etiology", "Medicine", "Miracle", "Virology", "China", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2999364275", "references": ["3021832855", "2060809301", "2775086803", "2119111857", "2605343262", "1982533785", "2126080553", "3000376083", "2404280981"], "title": "Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission", "abstract": "", "citation_count": "9", "reference_count": "1,532", "date": "2020", "authors": ["Xintian Xu", "Ping Chen", "Jingfang Wang", "Jiannan Feng", "Hui Zhou", "Xuan Li", "Wu Zhong", "Pei Hao"], "related_topics": ["Coronavirus", "Outbreak", "Betacoronavirus", "Transmission (mechanics)", "Viral Epidemiology", "Virology", "Biology", "2019-20 coronavirus outbreak", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Spike Protein"]}
{"id": "2999409984", "references": ["3031559976", "2981752008", "3029135544", "2981657433", "3027264380", "3029903408", "3023268903", "2442480670", "3000092258", "2414957595"], "title": "The continuing 2019-nCoV epidemic threat of novel coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China.", "abstract": "", "citation_count": "10", "reference_count": "1,945", "date": "2020", "authors": ["David S. Hui", "Esam Ei Azhar", "Tariq A. Madani", "Francine Ntoumi", "Richard Kock", "Osman Dar", "Giuseppe Ippolito", "Timothy D. Mchugh", "Ziad A. Memish", "Christian Drosten", "Alimuddin Zumla", "Eskild Petersen"], "related_topics": ["Outbreak", "Global health", "Medicine", "China", "Environmental health", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2991899552", "references": ["2091139031", "2900850632", "1975461687", "2948483377", "2159340685", "2103645914", "2133979383", "2065974896", "2021046603", "2155020492"], "title": "Clinical Features Predicting Mortality Risk in Patients With Viral Pneumonia: The MuLBSTA Score.", "abstract": "Objective The aim of this study was to further clarify clinical characteristics and predict mortality risk among patients with viral pneumonia. Methods A total of 528 patients with viral pneumonia at RuiJin hospital in Shanghai from May 2015 to May 2019 were recruited. Multiplex real-time RT-PCR was used to detect respiratory viruses. Demographic information, comorbidities, routine laboratory examinations, immunological indexes, etiological detections, radiological images and treatment were collected on admission. Results 76 (14.4%) patients died within 90 days in hospital. A predictive MuLBSTA score was calculated on the basis of a multivariate logistic regression model in order to predict mortality with a weighted score that included multilobular infiltrates (OR = 5.20, 95% CI 1.41-12.52, p = 0.010; 5 points), lymphocyte \u2264 0.8\u2217109/L (OR = 4.53, 95% CI 2.55-8.05, p &lt; 0.001; 4 points), bacterial coinfection (OR = 3.71, 95% CI 2.11-6.51, p &lt; 0.001; 4 points), acute-smoker (OR = 3.19, 95% CI 1.34-6.26, p = 0.001; 3 points), quit-smoker (OR = 2.18, 95% CI 0.99-4.82, p = 0.054; 2 points), hypertension (OR = 2.39, 95% CI 1.55-4.26, p = 0.003; 2 points) and age \u226560 years (OR = 2.14, 95% CI 1.04-4.39, p = 0.038; 2 points). 12 points was used as a cut-off value for mortality risk stratification. This model showed sensitivity of 0.776, specificity of 0.778 and a better predictive ability than CURB-65 (AUROC = 0.773 vs. 0.717, p &lt; 0.001). Conclusion Here, we designed an easy-to-use clinically predictive tool for assessing 90-day mortality risk of viral pneumonia. It can accurately stratify hospitalized patients with viral pneumonia into relevant risk categories and could provide guidance to make further clinical decisions.", "citation_count": "32", "reference_count": "277", "date": "2019", "authors": ["Lingxi Guo", "Dong Wei", "Xinxin Zhang", "Yurong Wu", "Qingyun Li", "Min Zhou", "Jieming Qu"], "related_topics": ["Viral pneumonia", "Etiology", "Internal medicine", "Coinfection", "Medicine", "Hospitalized patients", "In patient", "Multivariate logistic regression model", "Routine laboratory", "Weighted score"]}
{"id": "2909194930", "references": ["2006434809", "2138324310", "2104548316", "2107053896", "2166867592", "2131262274", "2470646526", "1993577573", "2132260239", "2903899730"], "title": "From SARS to MERS, Thrusting Coronaviruses into the Spotlight", "abstract": "Coronaviruses (CoVs) have formerly been regarded as relatively harmless respiratory pathogens to humans. However, two outbreaks of severe respiratory tract infection, caused by the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV), as a result of zoonotic CoVs crossing the species barrier, caused high pathogenicity and mortality rates in human populations. This brought CoVs global attention and highlighted the importance of controlling infectious pathogens at international borders. In this review, we focus on our current understanding of the epidemiology, pathogenesis, prevention, and treatment of SARS-CoV and MERS-CoV, as well as provides details on the pivotal structure and function of the spike proteins (S proteins) on the surface of each of these viruses. For building up more suitable animal models, we compare the current animal models recapitulating pathogenesis and summarize the potential role of host receptors contributing to diverse host affinity in various species. We outline the research still needed to fully elucidate the pathogenic mechanism of these viruses, to construct reproducible animal models, and ultimately develop countermeasures to conquer not only SARS-CoV and MERS-CoV, but also these emerging coronaviral diseases.", "citation_count": "209", "reference_count": "810", "date": "2019", "authors": ["Zhiqi Song", "Yanfeng Xu", "Linlin Bao", "Ling Zhang", "Pin Yu", "Yajin Qu", "Hua Zhu", "Wenjie Zhao", "Yunlin Han", "Chuan Qin"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome coronavirus", "Outbreak", "Mechanism (biology)", "Virology", "Biology", "Pathogenicity", "Respiratory pathogens", "Severe acute respiratory syndrome coronavirus", "Structure and function"]}
{"id": "2143516773", "references": ["1997063559", "1977545325", "2121781154", "1649464328", "2121947440", "2113137767", "1554544485", "2104095591", "2913192828", "2620619910"], "title": "Fast approximate energy minimization via graph cuts", "abstract": "Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.", "citation_count": "47", "reference_count": "10,570", "date": "2001", "authors": ["Y. Boykov", "O. Veksler", "R. Zabih"], "related_topics": ["Graph cuts in computer vision", "Approximation algorithm", "Minimum cut", "Simulated annealing", "Cut", "Standard algorithms", "Energy minimization", "Computational complexity theory", "Algorithm", "Mathematical optimization", "Artificial intelligence", "Mathematics"]}
{"id": "2110158442", "references": ["2116040950", "2145023731", "1578099820", "2067191022", "1528789833", "2121947440", "2121927366", "2124351162", "2169551590", "1999478155"], "title": "Contour Detection and Hierarchical Image Segmentation", "abstract": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.", "citation_count": "76", "reference_count": "4,663", "date": "2011", "authors": ["P Arbela\u0301ez", "M Maire", "C Fowlkes", "J Malik"], "related_topics": ["Scale-space segmentation", "Segmentation-based object categorization", "Image segmentation", "Active contour model", "Object detection", "Edge detection", "Segmentation", "Image processing", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2031342017", "references": ["2217896605", "2166049352", "2108598243", "2161969291", "2110764733", "2031489346", "1576445103", "2145607950", "2120419212", "1566135517"], "title": "Unbiased look at dataset bias", "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.", "citation_count": "19", "reference_count": "1,670", "date": "2011", "authors": ["Antonio Torralba", "Alexei A. Efros"], "related_topics": ["Automatic identification and data capture", "Pascal (programming language)", "Data science", "Computer science", "Training set"]}
{"id": "2158899491", "references": ["2136922672", "2147880316", "2125838338", "2150102617", "2110798204", "2310919327", "2159080219", "2158139315", "2098162425", "2296073425"], "title": "Natural Language Processing (Almost) from Scratch", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "citation_count": "95", "reference_count": "7,454", "date": "2011", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "related_topics": ["Sequence labeling", "Named-entity recognition", "Chunking (psychology)", "Semantic role labeling", "Tag system", "Task (computing)", "Machine learning", "Natural language processing", "Computer science", "Basis (linear algebra)", "Scratch", "Artificial intelligence"]}
{"id": "2103359087", "references": ["137106866", "1567512734", "2136922672", "2161893161", "2131700150", "1983334819", "2110871230", "2083380015", "2161000554", "1498436455"], "title": "Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine", "abstract": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.", "citation_count": "23", "reference_count": "402", "date": "2010", "authors": ["George Dahl", "Marc'aurelio Ranzato", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "related_topics": ["Restricted Boltzmann machine", "TIMIT", "Word error rate", "Covariance", "Conditional independence", "Conditional probability distribution", "Speech recognition", "Pattern recognition", "Computer science", "State (computer science)", "Artificial intelligence"]}
{"id": "2913932916", "references": ["2136922672", "2147152072", "1978394996", "2150102617", "2100495367", "2116064496", "1880262756", "2162006472", "2038276547", "2157364932"], "title": "Semantic hashing", "abstract": "We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.", "citation_count": "22", "reference_count": "1,219", "date": "2009", "authors": ["Ruslan Salakhutdinov", "Geoffrey Hinton"], "related_topics": ["Feature hashing", "Latent semantic analysis", "Locality-sensitive hashing", "Hopscotch hashing", "Graphical model", "Set (abstract data type)", "Memory address", "Unsupervised learning", "Information retrieval", "Computer science"]}
{"id": "3027518954", "references": ["3047108013", "3009375872", "3001897055", "3013266235", "3036563213", "3012538234", "3023674326", "3096165595", "3095124337", "3096084589"], "title": "Pathogen genomics in public health", "abstract": "Summary Rapid advances in DNA sequencing technology (\u201cnext-generation sequencing\u201d) have inspired optimism about the potential of human genomics for \u201cprecision medicine.\u201d Meanwhile, pathogen genomic...", "citation_count": "0", "reference_count": "82", "date": "2020", "authors": ["Gregory L. Armstrong", "Duncan R. MacCannell", "Jill Taylor", "Heather A. Carleton", "Elizabeth B. Neuhaus", "Richard S. Bradbury", "James E. Posey", "Marta Gwinn"], "related_topics": ["Genomics", "DNA sequencing", "Pathogen", "Public health", "Computational biology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Human genomics", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2257005270", "references": ["2167384912", "2195009776", "2166867592", "2119111857", "311927316", "1993577573", "2116586125", "2132260239", "2170933940", "2111412754"], "title": "Coronaviruses and the human airway: a universal system for virus-host interaction studies.", "abstract": "Human coronaviruses (HCoVs) are large RNA viruses that infect the human respiratory tract. The emergence of both Severe Acute Respiratory Syndrome and Middle East Respiratory syndrome CoVs as well as the yearly circulation of four common CoVs highlights the importance of elucidating the different mechanisms employed by these viruses to evade the host immune response, determine their tropism and identify antiviral compounds. Various animal models have been established to investigate HCoV infection, including mice and non-human primates. To establish a link between the research conducted in animal models and humans, an organotypic human airway culture system, that recapitulates the human airway epithelium, has been developed. Currently, different cell culture systems are available to recapitulate the human airways, including the Air-Liquid Interface (ALI) human airway epithelium (HAE) model. Tracheobronchial HAE cultures recapitulate the primary entry point of human respiratory viruses while the alveolar model allows for elucidation of mechanisms involved in viral infection and pathogenesis in the alveoli. These organotypic human airway cultures represent a universal platform to study respiratory virus-host interaction by offering more detailed insights compared to cell lines. Additionally, the epidemic potential of this virus family highlights the need for both vaccines and antivirals. No commercial vaccine is available but various effective antivirals have been identified, some with potential for human treatment. These morphological airway cultures are also well suited for the identification of antivirals, evaluation of compound toxicity and viral inhibition.", "citation_count": "110", "reference_count": "74", "date": "2016", "authors": ["Hulda Run Jonsdottir", "Ronald Dijkman"], "related_topics": ["Tissue tropism", "Middle East respiratory syndrome", "Respiratory epithelium", "Respiratory tract", "Virus", "Tropism", "Immune system", "Immunity", "Virology", "Immunology", "Biology"]}
{"id": "2955025503", "references": ["2097213674", "2120839730", "2016253387", "2131338055", "2159773954", "2083870139", "2412526156", "2287076968", "2081835188", "2078917493"], "title": "Viral and Bacterial Etiology of Acute Febrile Respiratory Syndrome among Patients in Qinghai, China", "abstract": "Objective This study was conducted to investigate the viral and bacterial etiology and epidemiology of patients with acute febrile respiratory syndrome (AFRS) in Qinghai using a commercial routine multiplex-ligation-nucleic acid amplification test (NAT)-based assay.   Methods A total of 445 nasopharyngeal swabs specimens from patients with AFRS were analyzed using the RespiFinderSmart22kit (PathoFinder BV, Netherlands) and the LightCycler 480 real-time PCR system.   Results Among the 225 (225/445, 51%) positive specimens, 329 positive pathogens were detected, including 298 (90.58%) viruses and 31 (9%) bacteria. The most commonly detected pathogens were infiuenza virus (IFV; 37.39%; 123/329), adenovirus (AdV; 17.02%; 56/329), human coronaviruses (HCoVs; 10.94%; 36/329), rhinovirus/enterovirus (RV/EV; 10.03%; 33/329), parainfiuenza viruses (PIVs; 8.51%; 28/329), and Mycoplasma pneumoniae (M. pneu; 8.51%; 28/329), respectively. Among the co-infected cases (17.53%; 78/445), IFV/AdV and IFV/M. pneu were the most common co-infections. Most of the respiratory viruses were detected in summer and fall.   Conclusion In our study, IFV-A was the most common respiratory pathogen among 22 detected pathogens, followed by AdV, HCoV, RV/EV, PIV, and M. pneu. Bacteria appeared less frequently than viruses, and co-infection was the most common phenomenon among viral pathogens. Pathogens were distributed among different age groups and respiratory viruses were generally active in July, September, and November. Enhanced surveillance and early detection can be useful in the diagnosis, treatment, and prevention of AFRS, as well as for guiding the development of appropriate public health strategies.", "citation_count": "33", "reference_count": "9", "date": "2019", "authors": ["Gao Shan Liu", "Hong Li", "Sheng Cang Zhao", "Rou Jian Lu", "Pei Hua Niu", "Wen Jie Tan"], "related_topics": ["Enterovirus", "Rhinovirus", "Mycoplasma pneumoniae", "Virus", "Respiratory system", "Virology", "Epidemiology", "Bacteria", "Medicine", "Bacterial etiology"]}
{"id": "2792024998", "references": ["1975375203", "1783641736", "1942680103", "2782496877", "2788045019", "1967283148", "2116682907", "2081635462", "2473338860", "2793181185"], "title": "From \u201cA\u201dIV to \u201cZ\u201dIKV: Attacks from Emerging and Re-emerging Pathogens", "abstract": "100 years after the infamous \u201cSpanish flu\u201d pandemic, the 2017\u20132018 flu season has been severe, with numerous infections worldwide. In between, there have been continuous, relentless attacks from (re-)emerging viruses. To fully understand viral pathogenesis and develop effective medical countermeasures, we must strengthen current surveillance and basic research efforts.", "citation_count": "10", "reference_count": "167", "date": "2018", "authors": ["George F. Gao"], "related_topics": ["Pandemic", "Flu season", "Viral pathogenesis", "Influenza A virus", "Intensive care medicine", "Biology", "Basic research"]}
{"id": "1909499787", "references": ["2115102869", "1975375203", "1942680103", "1994871753", "2089797630", "1979807576", "2135540513", "2096145431", "2227495319", "2024845268"], "title": "MERS, SARS, and Ebola: The Role of Super-Spreaders in Infectious Disease.", "abstract": "Super-spreading occurs when a single patient infects a disproportionate number of contacts. The 2015 MERS-CoV, 2003 SARS-CoV, and to a lesser extent 2014-15 Ebola virus outbreaks were driven by super-spreaders. We summarize documented super-spreading in these outbreaks, explore contributing factors, and suggest studies to better understand super-spreading.", "citation_count": "12", "reference_count": "286", "date": "2015", "authors": ["Gary Wong", "Wenjun Liu", "Yingxia Liu", "Boping Zhou", "Yuhai Bi", "George F. Gao"], "related_topics": ["Ebola virus", "Infectious disease (medical specialty)", "Outbreak", "Virology", "Bioinformatics", "Biology", "Disease transmission", "Single patient"]}
{"id": "3002715510", "references": ["3001897055", "2126707939", "2801339009", "2002481497", "96734778", "2156273941", "2538584349", "2217313808"], "title": "Another Decade, Another Coronavirus.", "abstract": "For the third time in as many decades, a zoonotic coronavirus has crossed species to infect human populations. This virus, provisionally called 2019-nCoV, was first identified in Wuhan, China, in p...", "citation_count": "8", "reference_count": "750", "date": "2020", "authors": ["Stanley Perlman"], "related_topics": ["Coronavirus", "Pneumonia", "Virus", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus Infections", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3002533507", "references": ["3012099172", "3012284084", "3003668884", "3009834387", "3006645647", "3009912996", "3003901880", "3012731499", "3015792206", "3012454642"], "title": "A Novel Coronavirus Emerging in China - Key Questions for Impact Assessment.", "abstract": "A Novel Coronavirus Emerging in China A novel coronavirus, designated as 2019-nCoV, emerged in Wuhan, China, at the end of 2019. Although many details of the emergence of this virus remain unknown,...", "citation_count": "0", "reference_count": "1,083", "date": "2020", "authors": ["Vincent J. Munster", "Marion Koopmans", "Neeltje van Doremalen", "Debby van Riel", "Emmie de Wit"], "related_topics": ["Coronavirus", "Betacoronavirus", "China", "Impact assessment", "Economic growth", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus Infections", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3001971765", "references": ["1042757214", "2801339009", "2109088393", "2918873120", "2156614913", "2306794997", "2534644646", "2127974353", "1909499787", "2227495319"], "title": "Real-time tentative assessment of the epidemiological characteristics of novel coronavirus infections in Wuhan, China, as at 22 January 2020.", "abstract": "A novel coronavirus (2019-nCoV) causing severe acute respiratory disease emerged recently in Wuhan, China. Information on reported cases strongly indicates human-to-human spread, and the most recent information is increasingly indicative of sustained human-to-human transmission. While the overall severity profile among cases may change as more mild cases are identified, we estimate a risk of fatality among hospitalised cases at 14% (95% confidence interval: 3.9\u201332%).", "citation_count": "18", "reference_count": "408", "date": "2020", "authors": ["Peng Wu", "Xinxin Hao", "Eric H Y Lau", "Jessica Y Wong", "Kathy S M Leung", "Joseph T Wu", "Benjamin J Cowling", "Gabriel M Leung"], "related_topics": ["Communicable disease", "Coronavirus", "Epidemiology", "Risk assessment", "Confidence interval", "Transmission (medicine)", "Pediatrics", "Public health", "Medicine", "China"]}
{"id": "2149508011", "references": ["2049975503", "2113457186", "1690366459", "1852588318", "2107053896", "2166867592", "2045002682", "2160011624", "2119775949", "2145441153"], "title": "Evidence for camel-to-human transmission of MERS coronavirus", "abstract": "We describe the isolation and sequencing of Middle East respiratory syndrome coronavirus (MERS-CoV) obtained from a dromedary camel and from a patient who died of laboratory-confirmed MERS-CoV infection after close contact with camels that had rhinorrhea. Nasal swabs collected from the patient and from one of his nine camels were positive for MERS-CoV RNA. In addition, MERS-CoV was isolated from the patient and the camel. The full genome sequences of the two isolates were identical. Serologic data indicated that MERS-CoV was circulating in the camels but not in the patient before the human infection occurred. These data suggest that this fatal case of human MERS-CoV infection was transmitted through close contact with an infected camel.", "citation_count": "14", "reference_count": "774", "date": "2014", "authors": ["Esam I. Azhar", "Sherif A. El-Kafrawy", "Suha A. Farraj", "Ahmed M. Hassan", "Muneera S. Al-Saeed", "Anwar M. Hashem", "Tariq A. Madani"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Serology", "Transmission (medicine)", "rhinorrhea", "Isolation (health care)", "Virology", "Nasal Swab", "Biology", "Close contact", "Dromedary camel"]}
{"id": "3000834295", "references": ["2718090702", "3030422584", "2792208289", "3000413850", "2470646526", "3027264380", "3027659922", "2909194930", "1993435091", "3023275846"], "title": "Coronavirus Infections-More Than Just the Common Cold.", "abstract": "", "citation_count": "12", "reference_count": "1,469", "date": "2020", "authors": ["Catharine I. Paules", "Hilary D. Marston", "Anthony S. Fauci"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome coronavirus", "Common cold", "Betacoronavirus", "Medicine", "Virology", "2019-20 coronavirus outbreak", "Coronavirus Infections", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2147166346", "references": ["2104548316", "1965399019", "1606697907", "1979065938", "2132260239", "2011756067", "2318510691"], "title": "Transmission Dynamics and Control of Severe Acute Respiratory Syndrome", "abstract": "Severe acute respiratory syndrome (SARS) is a recently described illness of humans that has spread widely over the past 6 months. With the use of detailed epidemiologic data from Singapore and epidemic curves from other settings, we estimated the reproductive number for SARS in the absence of interventions and in the presence of control efforts. We estimate that a single infectious case of SARS will infect about three secondary cases in a population that has not yet instituted control measures. Public-health efforts to reduce transmission are expected to have a substantial impact on reducing the size of the epidemic.", "citation_count": "7", "reference_count": "1,565", "date": "2003", "authors": ["Marc Lipsitch", "Ted Cohen", "Ben Cooper", "James M. Robins", "Stefan Ma", "Lyn James", "Gowri Gopalakrishna", "Suok Kai Chew", "Chorh Chuan Tan", "Matthew H. Samore", "David Fisman", "Megan Murray"], "related_topics": ["Population", "Serial interval", "Transmission (mechanics)", "Epidemiology", "Contact tracing", "Intensive care medicine", "Viral disease", "Respiratory disease", "Psychological intervention"]}
{"id": "2105637133", "references": ["2141885858", "2111412754", "2103503670", "2110835349", "2116586125", "2140338292", "2134061616", "2056584399", "2025170735", "2169198329"], "title": "Discovery of Seven Novel Mammalian and Avian Coronaviruses in the Genus Deltacoronavirus Supports Bat Coronaviruses as the Gene Source of Alphacoronavirus and Betacoronavirus and Avian Coronaviruses as the Gene Source of Gammacoronavirus and Deltacoronavirus", "abstract": "Recently, we reported the discovery of three novel coronaviruses, bulbul coronavirus HKU11, thrush coronavirus HKU12, and munia coronavirus HKU13, which were identified as representatives of a novel genus, Deltacoronavirus, in the subfamily Coronavirinae. In this territory-wide molecular epidemiology study involving 3,137 mammals and 3,298 birds, we discovered seven additional novel deltacoronaviruses in pigs and birds, which we named porcine coronavirus HKU15, white-eye coronavirus HKU16, sparrow coronavirus HKU17, magpie robin coronavirus HKU18, night heron coronavirus HKU19, wigeon coronavirus HKU20, and common moorhen coronavirus HKU21. Complete genome sequencing and comparative genome analysis showed that the avian and mammalian deltacoronaviruses have similar genome characteristics and structures. They all have relatively small genomes (25.421 to 26.674 kb), the smallest among all coronaviruses. They all have a single papain-like protease domain in the nsp3 gene; an accessory gene, NS6 open reading frame (ORF), located between the M and N genes; and a variable number of accessory genes (up to four) downstream of the N gene. Moreover, they all have the same putative transcription regulatory sequence of ACACCA. Molecular clock analysis showed that the most recent common ancestor of all coronaviruses was estimated at approximately 8100 BC, and those of Alphacoronavirus, Betacoronavirus, Gammacoronavirus, and Deltacoronavirus were at approximately 2400 BC, 3300 BC, 2800 BC, and 3000 BC, respectively. From our studies, it appears that bats and birds, the warm blooded flying vertebrates, are ideal hosts for the coronavirus gene source, bats for Alphacoronavirus and Betacoronavirus and birds for Gammacoronavirus and Deltacoronavirus, to fuel coronavirus evolution and dissemination.", "citation_count": "55", "reference_count": "1,124", "date": "2012", "authors": ["Patrick C. Y. Woo", "Susanna K. P. Lau", "Carol S. F. Lam", "Candy C. Y. Lau", "Alan K. L. Tsang", "John H. N. Lau", "Ru Bai", "Jade L. L. Teng", "Chris C. C. Tsang", "Ming Wang", "Bo-Jian Zheng", "Kwok-Hung Chan", "Kwok-Yung Yuen"], "related_topics": ["Coronavirinae", "Bulbul coronavirus HKU11", "Coronavirus", "Betacoronavirus", "Alphacoronavirus", "Deltacoronavirus", "Gammacoronavirus", "Gene", "Virology", "Biology"]}
{"id": "2103503670", "references": ["2104548316", "96734778", "2127949919", "1990059132", "2116586125", "2042499956", "2134061616", "2076620790", "2025170735", "2169198329"], "title": "Bats are natural reservoirs of SARS-like coronaviruses.", "abstract": "Severe acute respiratory syndrome (SARS) emerged in 2002 to 2003 in southern China. The origin of its etiological agent, the SARS coronavirus (SARS-CoV), remains elusive. Here we report that species of bats are a natural host of coronaviruses closely related to those responsible for the SARS outbreak. These viruses, termed SARS-like coronaviruses (SL-CoVs), display greater genetic variation than SARS-CoV isolated from humans or from civets. The human and civet isolates of SARS-CoV nestle phylogenetically within the spectrum of SL-CoVs, indicating that the virus responsible for the SARS outbreak was a member of this coronavirus group.", "citation_count": "22", "reference_count": "2,301", "date": "2005", "authors": ["Wendong Li", "Zhengli Shi", "Meng Yu", "Wuze Ren", "Craig Smith", "Jonathan H. Epstein", "Hanzhong Wang", "Gary Crameri", "Zhihong Hu", "Huajun Zhang", "Jianhong Zhang", "Jennifer McEachern", "Hume Field", "Peter Daszak", "Bryan T. Eaton", "Shuyi Zhang", "Lin-Fa Wang"], "related_topics": ["Coronavirus", "Alphacoronavirus", "Disease reservoir", "Coronaviridae", "Outbreak", "Rhinolophus", "Civet", "Nidovirales", "Virology", "Biology"]}
{"id": "2140338292", "references": ["2141885858", "2104548316", "2171091522", "2116586125", "2169198329", "2132260239", "2134061616", "2111412754", "2025170735", "1966238900"], "title": "Severe acute respiratory syndrome coronavirus-like virus in Chinese horseshoe bats", "abstract": "Although the finding of severe acute respiratory syndrome coronavirus (SARS-CoV) in caged palm civets from live animal markets in China has provided evidence for interspecies transmission in the genesis of the SARS epidemic, subsequent studies suggested that the civet may have served only as an amplification host for SARS-CoV. In a surveillance study for CoV in noncaged animals from the wild areas of the Hong Kong Special Administration Region, we identified a CoV closely related to SARS-CoV (bat-SARS-CoV) from 23 (39%) of 59 anal swabs of wild Chinese horseshoe bats (Rhinolophus sinicus) by using RT-PCR. Sequencing and analysis of three bat-SARS-CoV genomes from samples collected at different dates showed that bat-SARS-CoV is closely related to SARS-CoV from humans and civets. Phylogenetic analysis showed that bat-SARS-CoV formed a distinct cluster with SARS-CoV as group 2b CoV, distantly related to known group 2 CoV. Most differences between the bat-SARS-CoV and SARS-CoV genomes were observed in the spike genes, ORF 3 and ORF 8, which are the regions where most variations also were observed between human and civet SARS-CoV genomes. In addition, the presence of a 29-bp insertion in ORF 8 of bat-SARS-CoV genome, not in most human SARS-CoV genomes, suggests that it has a common ancestor with civet SARS-CoV. Antibody against recombinant bat-SARS-CoV nucleocapsid protein was detected in 84% of Chinese horseshoe bats by using an enzyme immunoassay. Neutralizing antibody to human SARS-CoV also was detected in bats with lower viral loads. Precautions should be exercised in the handling of these animals.", "citation_count": "40", "reference_count": "1,733", "date": "2005", "authors": ["Susanna K. P. Lau", "Patrick C. Y. Woo", "Kenneth S. M. Li", "Yi Huang", "Hoi-Wah Tsoi", "Beatrice H. L. Wong", "Samson S. Y. Wong", "Suet-Yi Leung", "Kwok-Hung Chan", "Kwok-Yung Yuen"], "related_topics": ["Civet", "Alphacoronavirus", "Rhinolophus sinicus", "Phylogenetics", "Genome", "Virus", "Phylogenetic tree", "Sequence analysis", "Virology", "Genetics", "Biology"]}
{"id": "2769543984", "references": ["2006434809", "2107053896", "1947409115", "2166867592", "2002513358", "2144410942", "1757215199", "2045002682", "2115555188", "2025170735"], "title": "Human intestinal tract serves as an alternative infection route for Middle East respiratory syndrome coronavirus", "abstract": "Middle East respiratory syndrome coronavirus (MERS-CoV) has caused human respiratory infections with a high case fatality rate since 2012. However, the mode of virus transmission is not well understood. The findings of epidemiological and virological studies prompted us to hypothesize that the human gastrointestinal tract could serve as an alternative route to acquire MERS-CoV infection. We demonstrated that human primary intestinal epithelial cells, small intestine explants, and intestinal organoids were highly susceptible to MERS-CoV and can sustain robust viral replication. We also identified the evidence of enteric MERS-CoV infection in the stool specimen of a clinical patient. MERS-CoV was considerably resistant to fed-state gastrointestinal fluids but less tolerant to highly acidic fasted-state gastric fluid. In polarized Caco-2 cells cultured in Transwell inserts, apical MERS-CoV inoculation was more effective in establishing infection than basolateral inoculation. Notably, direct intragastric inoculation of MERS-CoV caused a lethal infection in human DPP4 transgenic mice. Histological examination revealed MERS-CoV enteric infection in all inoculated mice, as shown by the presence of virus-positive cells, progressive inflammation, and epithelial degeneration in small intestines, which were exaggerated in the mice pretreated with the proton pump inhibitor pantoprazole. With the progression of the enteric infection, inflammation, virus-positive cells, and live viruses emerged in the lung tissues, indicating the development of sequential respiratory infection. Taken together, these data suggest that the human intestinal tract may serve as an alternative infection route for MERS-CoV.", "citation_count": "46", "reference_count": "280", "date": "2017", "authors": ["Jie Zhou", "Cun Li", "Guangyu Zhao", "Hin Chu", "Dong Wang", "Helen Hoi-Ning Yan", "Vincent Kwok-Man Poon", "Lei Wen", "Bosco Ho-Yin Wong", "Xiaoyu Zhao", "Man Chun Chiu", "Dong Yang", "Yixin Wang", "Rex K. H. Au-Yeung", "Ivy Hau-Yee Chan", "Shihui Sun", "Jasper Fuk-Woo Chan", "Kelvin Kai-Wang To", "Ziad Ahmed Memish", "Victor M. Corman", "Christian Drosten", "Ivan Fan-Ngai Hung", "Yusen Zhou", "Suet Yi Leung", "Kwok-Yung Yuen"], "related_topics": ["Respiratory infection", "Human gastrointestinal tract", "Small intestine", "Lung", "Middle East respiratory syndrome coronavirus", "Respiratory system", "Inflammation", "Viral replication", "Immunology", "Biology"]}
{"id": "2889758689", "references": ["2311203695", "2104548316", "2775086803", "2046153984", "1993577573", "2141008678", "2141877163", "2140338292", "2298153446", "2169198329"], "title": "Genomic characterization and infectivity of a novel SARS-like coronavirus in Chinese bats", "abstract": "SARS coronavirus (SARS-CoV), the causative agent of the large SARS outbreak in 2003, originated in bats. Many SARS-like coronaviruses (SL-CoVs) have been detected in bats, particularly those that reside in China, Europe, and Africa. To further understand the evolutionary relationship between SARS-CoV and its reservoirs, 334 bats were collected from Zhoushan city, Zhejiang province, China, between 2015 and 2017. PCR amplification of the conserved coronaviral protein RdRp detected coronaviruses in 26.65% of bats belonging to this region, and this number was influenced by seasonal changes. Full genomic analyses of the two new SL-CoVs from Zhoushan (ZXC21 and ZC45) showed that their genomes were 29,732 nucleotides (nt) and 29,802 nt in length, respectively, with 13 open reading frames (ORFs). These results revealed 81% shared nucleotide identity with human/civet SARS CoVs, which was more distant than that observed previously for bat SL-CoVs in China. Importantly, using pathogenic tests, we found that the virus can reproduce and cause disease in suckling rats, and further studies showed that the virus-like particles can be observed in the brains of suckling rats by electron microscopy. Thus, this study increased our understanding of the genetic diversity of the SL-CoVs carried by bats and also provided a new perspective to study the possibility of cross-species transmission of SL-CoVs using suckling rats as an animal model.", "citation_count": "39", "reference_count": "198", "date": "2018", "authors": ["Dan Hu", "Changqiang Zhu", "Lele Ai", "Ting He", "Yi Wang", "Fuqiang Ye", "Lu Yang", "Chenxi Ding", "Xuhui Zhu", "Ruicheng Lv", "Jin Zhu", "Bachar Hassan", "Youjun Feng", "Weilong Tan", "Changjun Wang"], "related_topics": ["ORFS", "Infectivity", "Phylogenetics", "Civet", "Outbreak", "Virus", "Polymerase chain reaction", "Virulence", "Virology", "Biology"]}
{"id": "2807736175", "references": ["2548288333", "2623181050", "2101172433", "2103338644", "2000714505", "2752904261", "2752140764", "2102263282", "2164777277", "2605374894"], "title": "Saliva as a diagnostic specimen for testing respiratory virus by a point-of-care molecular assay: a diagnostic validity study.", "abstract": "Abstract  Objectives  Automated point-of-care molecular assays have greatly shortened the turnaround time of respiratory virus testing. One of the major bottlenecks now lies at the specimen collection step, especially in a busy clinical setting. Saliva is a convenient specimen type that can be provided easily by adult patients. This study assessed the diagnostic validity, specimen collection time and cost associated with the use of saliva.  Methods  This was a prospective diagnostic validity study comparing the detection rate of respiratory viruses between saliva and nasopharyngeal aspirate (NPA) among adult hospitalized patients using Xpert\u00ae Xpress Flu/RSV. The cost and time associated with the collection of saliva and nasopharyngeal specimens were also estimated.  Results  Between July and October 2017, 214 patients were recruited. The overall agreement between saliva and NPA was 93.3% (196/210, \u03ba 0.851, 95% CI 0.776\u20130.926). There was no significant difference in the detection rate of respiratory viruses between saliva and NPA (32.9% (69/210) versus 35.7% (75/210); p 0.146). The overall sensitivity and specificity were 90.8% (81.9%\u201396.2%) and 100% (97.3%\u2013100%), respectively, for saliva, and were 96.1% (88.9%\u201399.2%) and 98.5% (94.7%\u201399.8%), respectively, for NPA. The time and cost associated with the collection of saliva were 2.26-fold and 2.59-fold lower, respectively, than those of NPA.  Conclusions  Saliva specimens have high sensitivity and specificity in the detection of respiratory viruses by an automated multiplex Clinical Laboratory Improvement Amendments-waived point-of-care molecular assay when compared with those of NPA. The use of saliva also reduces the time and cost associated with specimen collection.", "citation_count": "26", "reference_count": "113", "date": "2019", "authors": ["K.K.W. To", "C.C.Y. Yip", "C.Y.W. Lai", "C.K.H. Wong", "D.T.Y. Ho", "P.K.P. Pang", "A.C.K. Ng", "K.-H. Leung", "R.W.S. Poon", "K.-H. Chan", "V.C.C. Cheng", "I.F.N. Hung", "K.-Y. Yuen"], "related_topics": ["Specimen collection", "Respiratory virus", "Saliva", "Nasopharyngeal aspirate", "Point-of-care testing", "Multiplex", "Point of care", "Respiratory system", "Gastroenterology", "Medicine", "Internal medicine"]}
{"id": "2104595316", "references": ["2112680994", "3003573988", "2159301256", "1878853999", "2070722739", "3033562259", "2133131640", "2069251911", "2096145431", "3018782651"], "title": "Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation", "abstract": "Provides systematic coverage of the mathematical theory of modelling epidemics in populations, with a clear and coherent discussion of the issues, concepts and phenomena. Mathematical modelling of epidemics is a vast and important area of study and this book helps the reader to translate, model, analyse and interpret, with numerous applications, examples and exercises to aid understanding.", "citation_count": "0", "reference_count": "3,624", "date": "2000", "authors": ["Odo Diekmann", "J. A. P Heesterbeek"], "related_topics": ["Mathematical theory", "Mathematical modelling of infectious disease", "Model building", "Interpretation (philosophy)", "Management science", "Mathematical model", "Next-generation matrix", "Area studies", "Operations research", "Medicine", "Basic Reproduction Ratio"]}
{"id": "3002533591", "references": ["3001118548", "2140763962", "3003668884", "3001465255", "3002539152", "3003573988", "2117002055", "2102187991", "3004397688", "2147166346"], "title": "Transmission Dynamics of 2019 Novel Coronavirus (2019-nCoV)", "abstract": "Background: Since December 29, 2019, pneumonia infection with 2019-nCoV has rapidly spread out from Wuhan, HubeProvince, China to most others provinces and ot", "citation_count": "22", "reference_count": "312", "date": "2020", "authors": ["Tao Liu", "Jianxiong Hu", "Min Kang", "Lifeng Lin", "Haojie Zhong", "Jianpeng Xiao", "Guanhao He", "Tie Song", "Qiong Huang", "Zuhua Rong", "Aiping Deng", "Weilin Zeng", "Xiaohua Tan", "Siqing Zeng", "Zhihua Zhu", "Jiansen Li", "Donghua Wan", "Jing Lu", "Huihong Deng", "Jianfeng He", "Wenjun Ma"], "related_topics": ["Pneumonia", "Transmission (mechanics)", "Virology", "Biology", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3004397688", "references": ["2999612210", "2107053896", "1990049863", "3002747665", "2117002055", "2102187991", "3004026249", "2147166346", "3002764620", "3026046290"], "title": "Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak.", "abstract": "Abstract   Backgrounds  An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city in China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and other countries. We present estimates of the basic reproduction number, R0, of 2019-nCoV in the early phase of the outbreak.    Methods  Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (\u03b3), we estimated R0 by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI.    Findings  The early outbreak data largely follows the exponential growth. We estimated that the mean R0 ranges from 2.24 (95%CI: 1.96\u20132.55) to 3.58 (95%CI: 2.89\u20134.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of R0.    Conclusion  The mean estimate of R0 for the 2019-nCoV ranges from 2.24 to 3.58, and is significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.", "citation_count": "17", "reference_count": "1,440", "date": "2020", "authors": ["Shi Zhao", "Qianyin Lin", "Jinjun Ran", "Salihu S Musa", "Guangpu Yang", "Weiming Wang", "Yijun Lou", "Daozhou Gao", "Lin Yang", "Daihai He", "Maggie H Wang"], "related_topics": ["Outbreak", "Serial interval", "Basic reproduction number", "Coronavirus", "Mainland China", "Estimation", "Pandemic", "Exponential growth", "Demography", "Biology"]}
{"id": "2069251911", "references": ["2463755683", "1606697907", "2131262274", "2009435671", "2104595316", "1995945562", "1965499304", "2096145431", "2147166346", "2146272590"], "title": "Superspreading and the effect of individual variation on disease emergence", "abstract": "Population-level analyses often use average quantities to describe heterogeneous systems, particularly when variation does not arise from identifiable groups. A prominent example, central to our current understanding of epidemic spread, is the basic reproductive number, R(0), which is defined as the mean number of infections caused by an infected individual in a susceptible population. Population estimates of R(0) can obscure considerable individual variation in infectiousness, as highlighted during the global emergence of severe acute respiratory syndrome (SARS) by numerous 'superspreading events' in which certain individuals infected unusually large numbers of secondary cases. For diseases transmitted by non-sexual direct contacts, such as SARS or smallpox, individual variation is difficult to measure empirically, and thus its importance for outbreak dynamics has been unclear. Here we present an integrated theoretical and statistical analysis of the influence of individual variation in infectiousness on disease emergence. Using contact tracing data from eight directly transmitted diseases, we show that the distribution of individual infectiousness around R(0) is often highly skewed. Model predictions accounting for this variation differ sharply from average-based approaches, with disease extinction more likely and outbreaks rarer but more explosive. Using these models, we explore implications for outbreak control, showing that individual-specific control measures outperform population-wide measures. Moreover, the dramatic improvements achieved through targeted control policies emphasize the need to identify predictive correlates of higher infectiousness. Our findings indicate that superspreading is a normal feature of disease spread, and to frame ongoing discussion we propose a rigorous definition for superspreading events and a method to predict their frequency.", "citation_count": "96", "reference_count": "1,992", "date": "2005", "authors": ["J. O. Lloyd-Smith", "S. J. Schreiber", "P. E. Kopp", "W. M. Getz"], "related_topics": ["Susceptible individual", "Super-spreader", "Basic reproduction number", "Outbreak", "Disease", "Contact tracing", "Variation (linguistics)", "Extinction", "Demography", "Biology"]}
{"id": "1815575713", "references": ["2006434809", "2138324310", "2107053896", "1990049863", "2166867592", "2069251911", "2096145431", "2147166346", "1968393246", "2130227690"], "title": "Transmission characteristics of MERS and SARS in the healthcare setting: a comparative study", "abstract": "The Middle East respiratory syndrome (MERS) coronavirus has caused recurrent outbreaks in the Arabian Peninsula since 2012. Although MERS has low overall human-to-human transmission potential, there is occasional amplification in the healthcare setting, a pattern reminiscent of the dynamics of the severe acute respiratory syndrome (SARS) outbreaks in 2003. Here we provide a head-to-head comparison of exposure patterns and transmission dynamics of large hospital clusters of MERS and SARS, including the most recent South Korean outbreak of MERS in 2015.", "citation_count": "45", "reference_count": "413", "date": "2015", "authors": ["Gerardo Chowell", "Fatima Abdirizak", "Sunmi Lee", "Jonggul Lee", "Eunok Jung", "Hiroshi Nishiura", "C\u00e9cile Viboud"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Middle East respiratory syndrome", "Coronavirus", "Outbreak", "Transmission (medicine)", "Virology", "Medicine", "Coronavirus Infections", "Cross infection", "Transmission potential"]}
{"id": "2096145431", "references": ["2100820722", "2104548316", "1606697907", "2131262274", "2104595316", "2124853344", "2025170735", "2169198329"], "title": "Transmission dynamics of the etiological agent of SARS in Hong Kong: impact of public health interventions.", "abstract": "We present an analysis of the first 10 weeks of the severe acute respiratory syndrome (SARS) epidemic in Hong Kong. The epidemic to date has been characterized by two large clusters-initiated by two separate \"super-spread\" events (SSEs)-and by ongoing community transmission. By fitting a stochastic model to data on 1512 cases, including these clusters, we show that the etiological agent of SARS is moderately transmissible. Excluding SSEs, we estimate that 2.7 secondary infections were generated per case on average at the start of the epidemic, with a substantial contribution from hospital transmission. Transmission rates fell during the epidemic, primarily as a result of reductions in population contact rates and improved hospital infection control, but also because of more rapid hospital attendance by symptomatic individuals. As a result, the epidemic is now in decline, although continued vigilance is necessary for this to be maintained. Restrictions on longer range population movement are shown to be a potentially useful additional control measure in some contexts. We estimate that most currently infected persons are now hospitalized, which highlights the importance of control of nosocomial transmission.", "citation_count": "8", "reference_count": "1,330", "date": "2003", "authors": ["Steven Riley", "Christophe Fraser", "Christl A. Donnelly", "Azra C. Ghani", "Laith J. Abu-Raddad", "Anthony J. Hedley", "Gabriel M. Leung", "Lai Ming Ho", "Tai Hing Lam", "Thuan Q. Thach", "Patsy Chau", "King Pan Chan", "Su Vui Lo", "Pak Yin Leung", "Thomas Tsang", "William Ho", "Koon Hung Lee", "Edith M.C. Lau", "Neil M. Ferguson", "Roy M. Anderson"], "related_topics": ["Population", "Secondary infection", "Global health", "Transmission (mechanics)", "Infection control", "Contact tracing", "Epidemiology", "Attendance", "Environmental health"]}
{"id": "1968393246", "references": ["2140763962", "2113457186", "2157725602", "2107053896", "2166867592", "2045002682", "2102187991", "2160011624", "2147166346", "2130227690"], "title": "Middle East respiratory syndrome coronavirus: quantification of the extent of the epidemic, surveillance biases, and transmissibility", "abstract": "Summary  Background  The novel Middle East respiratory syndrome coronavirus (MERS-CoV) had, as of Aug 8, 2013, caused 111 virologically confirmed or probable human cases of infection worldwide. We analysed epidemiological and genetic data to assess the extent of human infection, the performance of case detection, and the transmission potential of MERS-CoV with and without control measures.  Methods  We assembled a comprehensive database of all confirmed and probable cases from public sources and estimated the incubation period and generation time from case cluster data. Using data of numbers of visitors to the Middle East and their duration of stay, we estimated the number of symptomatic cases in the Middle East. We did independent analyses, looking at the growth in incident clusters, the growth in viral population, the reproduction number of cluster index cases, and cluster sizes to characterise the dynamical properties of the epidemic and the transmission scenario.  Findings  The estimated number of symptomatic cases up to Aug 8, 2013, is 940 (95% CI 290\u20132200), indicating that at least 62% of human symptomatic cases have not been detected. We find that the case-fatality ratio of primary cases detected via routine surveillance (74%; 95% CI 49\u201391) is biased upwards because of detection bias; the case-fatality ratio of secondary cases was 20% (7\u201342). Detection of milder cases (or clinical management) seemed to have improved in recent months. Analysis of human clusters indicated that chains of transmission were not self-sustaining when infection control was implemented, but that  R  in the absence of controls was in the range 0\u00b78\u20131\u00b73. Three independent data sources provide evidence that  R  cannot be much above 1, with an upper bound of 1\u00b72\u20131\u00b75.  Interpretation  By showing that a slowly growing epidemic is underway either in human beings or in an animal reservoir, quantification of uncertainty in transmissibility estimates, and provision of the first estimates of the scale of the epidemic and extent of case detection biases, we provide valuable information for more informed risk assessment.  Funding  Medical Research Council, Bill &amp; Melinda Gates Foundation, EU FP7, and National Institute of General Medical Sciences.", "citation_count": "18", "reference_count": "346", "date": "2014", "authors": ["Simon Cauchemez", "Christophe Fraser", "Maria D Van Kerkhove", "Christl A Donnelly", "Steven Riley", "Andrew Rambaut", "Vincent Enouf", "Sylvie van der Werf", "Neil M Ferguson"], "related_topics": ["Population", "Middle East respiratory syndrome coronavirus", "Basic reproduction number", "Epidemiology", "Risk assessment", "Respiratory tract infections", "Coronavirus", "Survival analysis", "Demography", "Medicine", "Immunology"]}
{"id": "2103441770", "references": ["2139760555", "2124985265", "2132341951", "2136145671", "2142619120", "2108234281", "2158714788", "2112113834", "2055666215", "2015292449"], "title": "Fast and accurate short read alignment with Burrows\u2013Wheeler transform", "abstract": "Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\r\n\r\nResults: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows\u2013Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ~10\u201320\u00d7 faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\r\n\r\nAvailability: http://maq.sourceforge.net \r\n\r\nContact: [email\u00a0protected]", "citation_count": "31", "reference_count": "30,028", "date": "2009", "authors": ["Heng Li", "Richard Durbin"], "related_topics": ["Hybrid genome assembly", "Sequence assembly", "2 base encoding", "Peak calling", "DNA sequencing theory", "Paired-end tag", "Burrows\u2013Wheeler transform", "FASTQ format", "Variant Call Format", "Deep sequencing", "Reference genome", "ABI Solid Sequencing", "Minion", "Nanopore sequencing", "Shotgun sequencing", "Massive parallel sequencing", "ChIP-exo", "Algorithm", "Sequence alignment", "Theoretical computer science", "Computer science", "Cancer genome sequencing", "Genomics", "Sequence analysis", "DNA sequencing", "MALBAC", "Illumina dye sequencing", "Structural variation", "Genomic Structural Variation", "Exome sequencing", "DNase-Seq", "Human genome", "Genome", "INDEL Mutation", "Variome", "ATAC-seq", "PAR-CLIP", "Indel", "Population genomics", "Exome", "Ancient DNA", "Selective sweep", "PRDM9", "Kataegis", "Candidate Gene Identification", "DNA", "POLD1", "Mutation Accumulation", "Genome resequencing", "Genotyping by sequencing", "Structural variant"]}
{"id": "2141052558", "references": ["2068187483", "1794270752", "2012220164", "2122082385", "2151736966", "2156921764", "2127847431", "2111211467", "2100030044", "2168696662"], "title": "RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies.", "abstract": "Motivation: Phylogenies are increasingly used in all fields of medical and biological research. Moreover, because of the next-generation sequencing revolution, datasets used for conducting phylogenetic analyses grow at an unprecedented pace. RAxML (Randomized Axelerated Maximum Likelihood) is a popular program for phylogenetic analyses of large datasets under maximum likelihood. Since the last RAxML paper in 2006, it has been continuously maintained and extended to accommodate the increasingly growing input datasets and to serve the needs of the user community.\r\n\r\nResults: I present some of the most notable new features and extensions of RAxML, such as a substantial extension of substitution models and supported data types, the introduction of SSE3, AVX and AVX2 vector intrinsics, techniques for reducing the memory requirements of the code and a plethora of operations for conducting post-analyses on sets of trees. In addition, an up-to-date 50-page user manual covering all new RAxML options is available.\r\n\r\nAvailability and implementation: The code is available under GNU GPL at https://github.com/stamatak/standard-RAxML.\r\n\r\nContact: gro.sti-h@sikatamats.sordnaxela\r\n\r\nSupplementary information: Supplementary data are available at Bioinformatics online.", "citation_count": "15", "reference_count": "18,694", "date": "2014", "authors": ["Alexandros Stamatakis"], "related_topics": ["Intrinsics", "Data type", "Supermatrix", "SSE3", "Phylogenomics", "Data mining", "Code (cryptography)", "Phylogenetic tree", "Software", "Biology"]}
{"id": "2804822363", "references": ["2060809301", "2152301430", "2149525061", "2158714788", "2159614853", "2154139219", "2051210555", "2015642465", "2065283382", "2142678478"], "title": "SWISS-MODEL: homology modelling of protein structures and complexes.", "abstract": "Homology modelling has matured into an important technique in structural biology, significantly contributing to narrowing the gap between known protein sequences and experimentally determined structures. Fully automated workflows and servers simplify and streamline the homology modelling process, also allowing users without a specific computational expertise to generate reliable protein models and have easy access to modelling results, their visualization and interpretation. Here, we present an update to the SWISS-MODEL server, which pioneered the field of automated modelling 25 years ago and been continuously further developed. Recently, its functionality has been extended to the modelling of homo- and heteromeric complexes. Starting from the amino acid sequences of the interacting proteins, both the stoichiometry and the overall structure of the complex are inferred by homology modelling. Other major improvements include the implementation of a new modelling engine, ProMod3 and the introduction a new local model quality estimation method, QMEANDisCo. SWISS-MODEL is freely available at https://swissmodel.expasy.org.", "citation_count": "69", "reference_count": "3,503", "date": "2018", "authors": ["Andrew Waterhouse", "Martino Bertoni", "Stefan Bienert", "Gabriel Studer", "Gerardo Tauriello", "Rafal Gumienny", "Florian T Heer", "Tjaart A P de Beer", "Christine Rempfer", "Lorenza Bordoli", "Rosalba Lepore", "Torsten Schwede"], "related_topics": ["Structural biology", "Server", "Visualization", "Protein structure", "Workflow", "Theoretical computer science", "Software", "Homology (biology)", "Biology", "Protein model"]}
{"id": "2991491848", "references": ["3010848803", "3014604938", "3013393665", "3018023298", "3005212621", "3013653866", "3015490759", "3003465021", "3006564542"], "title": "A Randomized, Controlled Trial of Ebola Virus Disease Therapeutics.", "abstract": "Abstract Background Although several experimental therapeutics for Ebola virus disease (EVD) have been developed, the safety and efficacy of the most promising therapies need to be assessed in the ...", "citation_count": "0", "reference_count": "903", "date": "2019", "authors": ["Mulangu S", "Dodd Le", "Davey Rt", "Tshiani Mbaya O", "Proschan M", "Mukadi D", "Lusakibanza Manzo M", "Nzolo D", "Tshomba Oloma A", "Ibanda A", "Ali R", "Coulibaly S", "Levine Ac", "Grais R", "Diaz J", "Lane Hc", "Muyembe-Tamfum Jj", "Sivahera B", "Camara M", "Kojan R", "Walker R", "Dighero-Kemp B", "Cao H", "Mukumbayi P", "Mbala-Kingebeni P", "Ahuka S", "Albert S", "Bonnett T", "Crozier I", "Duvenhage M", "Proffitt C", "Teitelbaum M", "Moench T", "Aboulhab J", "Barrett K", "Cahill K", "Cone K", "Eckes R", "Hensley L", "Herpin B", "Higgs E", "Ledgerwood J", "Pierson J", "Smolskis M", "Sow Y", "Tierney J", "Sivapalasingam S", "Holman W", "Gettinger N", "Vall\u00e9e D"], "related_topics": ["Ebola virus", "Randomized controlled trial", "MEDLINE", "Disease", "Young adult", "Internal medicine", "Medicine", "Extramural", "Multicenter study"]}
{"id": "3003951199", "references": ["3001897055"], "title": "Importation and Human-to-Human Transmission of a Novel Coronavirus in Vietnam.", "abstract": "Human-to-Human Coronavirus Transmission in Vietnam The authors describe transmission of 2019-nCoV from a father, who had flown with his wife from Wuhan to Hanoi, to the son, who met his father and ...", "citation_count": "1", "reference_count": "1,057", "date": "2020", "authors": ["Lan T. Phan", "Thuong V. Nguyen", "Quang C. Luong", "Thinh V. Nguyen", "Hieu T. Nguyen", "Hung Q. Le", "Thuc T. Nguyen", "Thang M. Cao", "Quang D. Pham"], "related_topics": ["Coronavirus", "Transmission (mechanics)", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Lung pathology", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Viral transmission"]}
{"id": "2605343262", "references": ["2096039130", "2033459705", "2587970647", "2532120756", "2222043208", "2259815689", "2063651055", "2104424333", "2106173155", "2557499142"], "title": "GISAID: Global initiative on sharing all influenza data - from vision to reality.", "abstract": "", "citation_count": "18", "reference_count": "983", "date": "2017", "authors": ["Yuelong Shu", "John McCauley"], "related_topics": ["Public health informatics", "Data sharing", "Global health", "Data collection", "Information Dissemination", "MEDLINE", "Pandemic", "Knowledge management", "Influenza A virus subtype H5N1", "Business"]}
{"id": "1803784511", "references": ["2161328469", "1823772832", "2168829312", "2326364273", "1979469936", "2068854215", "2328176404", "2074935456", "2070070465", "2113752525"], "title": "Acute respiratory distress syndrome: the Berlin Definition.", "abstract": "The acute respiratory distress syndrome (ARDS) was defined in 1994 by the American-European Consensus Conference (AECC); since then, issues regarding the reliability and validity of this definition have emerged. Using a consensus process, a panel of experts convened in 2011 (an initiative of the European Society of Intensive Care Medicine endorsed by the American Thoracic Society and the Society of Critical Care Medicine) developed the Berlin Definition, focusing on feasibility, reliability, validity, and objective evaluation of its performance. A draft definition proposed 3 mutually exclusive categories of ARDS based on degree of hypoxemia: mild (200 mm Hg &lt; PaO2/FIO2 \u2264 300 mm Hg), moderate (100 mm Hg &lt; PaO2/FIO2 \u2264 200 mm Hg), and severe (PaO2/FIO2 \u2264 100 mm Hg) and 4 ancillary variables for severe ARDS: radiographic severity, respiratory system compliance (\u226440 mL/cm H2O), positive end-expiratory pressure (\u226510 cm H2O), and corrected expired volume per minute (\u226510 L/min). The draft Berlin Definition was empirically evaluated using patient-level meta-analysis of 4188 patients with ARDS from 4 multicenter clinical data sets and 269 patients with ARDS from 3 single-center data sets containing physiologic information. The 4 ancillary variables did not contribute to the predictive validity of severe ARDS for mortality and were removed from the definition. Using the Berlin Definition, stages of mild, moderate, and severe ARDS were associated with increased mortality (27%; 95% CI, 24%-30%; 32%; 95% CI, 29%-34%; and 45%; 95% CI, 42%-48%, respectively; P &lt; .001) and increased median duration of mechanical ventilation in survivors (5 days; interquartile [IQR], 2-11; 7 days; IQR, 4-14; and 9 days; IQR, 5-17, respectively; P &lt; .001). Compared with the AECC definition, the final Berlin Definition had better predictive validity for mortality, with an area under the receiver operating curve of 0.577 (95% CI, 0.561-0.593) vs 0.536 (95% CI, 0.520-0.553; P &lt; .001). This updated and revised Berlin Definition for ARDS addresses a number of the limitations of the AECC definition. The approach of combining consensus discussions with empirical evaluation may serve as a model to create more accurate, evidence-based, critical illness syndrome definitions and to better inform clinical care, research, and health services planning.", "citation_count": "33", "reference_count": "8,639", "date": "2012", "authors": ["Ards Definition Task Force", "V Marco Ranieri", "Gordon D Rubenfeld", "B Taylor Thompson", "Niall D Ferguson", "Ellen Caldwell", "Eddy Fan", "Luigi Camporota", "Arthur S Slutsky"], "related_topics": ["ARDS", "Prone ventilation", "Interquartile range", "Severity of illness", "Mechanical ventilation", "Hypoxemia", "Epidemiology", "Airway pressure release ventilation", "Emergency medicine", "Pediatrics", "Medicine"]}
{"id": "3001388158", "references": ["3002539152", "3000771439"], "title": "China coronavirus: Six questions scientists are asking", "abstract": "Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.  Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.", "citation_count": "2", "reference_count": "65", "date": "2020", "authors": ["Ewen Callaway", "David Cyranoski"], "related_topics": ["Coronavirus", "China", "Geography", "Genealogy", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2279340859", "references": ["2006434809", "2112136274", "1703839189", "2111742711", "2166867592", "2103118479", "2119837294"], "title": "Middle East Respiratory Syndrome-Coronavirus Infection: A Case Report of Serial Computed Tomographic Findings in a Young Male Patient.", "abstract": "Radiologic findings of Middle East respiratory syndrome (MERS), a novel coronavirus infection, have been rarely reported. We report a 30-year-old male presented with fever, abdominal pain, and diarrhea, who was diagnosed with MERS. A chest computed tomographic scan revealed rapidly developed multifocal nodular consolidations with ground-glass opacity halo and mixed consolidation, mainly in the dependent and peripheral areas. After treatment, follow-up imaging showed that these abnormalities markedly decreased but fibrotic changes developed.", "citation_count": "7", "reference_count": "26", "date": "2016", "authors": ["Won Jin Choi", "Ki Nam Lee", "Eun Ju Kang", "Hyuck Lee"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Middle East respiratory syndrome", "Coronavirus", "Abdominal pain", "Diarrhea", "Radiology", "Radiography", "Pathology", "Medicine", "Computed tomographic", "Young male"]}
{"id": "2162899218", "references": ["2100820722", "2104548316", "2049695691", "2131262274", "2163627712", "2124413369", "2155100049", "2123101845", "2025170735", "2080286891"], "title": "Radiologic pattern of disease in patients with severe acute respiratory syndrome: the Toronto experience.", "abstract": "Severe acute respiratory syndrome (SARS) is a transmissible febrile respiratory illness caused by a recently discovered coronavirus. Various patterns of disease progression may be observed that have different implications for the prognosis in those affected by SARS. The appearance of the lungs on chest radiographs of patients with this condition may be normal or may include focal airspace opacity or multifocal or diffuse opacities. Thoracic computed tomography (CT) is more sensitive in depicting SARS than is conventional chest radiography, and CT images obtained in patients with normal chest radiographs may show extensive disease and airspace consolidation. However, because the radiologic appearance of SARS is not distinct from that of other diseases that cause lower respiratory tract infection, early identification of SARS will depend in part on the prompt recognition of clusters of cases of febrile respiratory tract illness. To aid in the differential diagnosis and management of SARS, radiologists must ...", "citation_count": "11", "reference_count": "90", "date": "2004", "authors": ["Narinder S. Paul", "Heidi Roberts", "Jagdish Butany", "Tae Bong Chung", "Wayne Gold", "Sangeeta Mehta", "Eli Konen", "Anuradha Rao", "Yves Provost", "Harry H. Hong", "Leon Zelovitsky", "Gordon L. Weisbrod"], "related_topics": ["Lower respiratory tract infection", "Differential diagnosis", "Coronavirus", "Respiratory tract", "Disease", "Respiratory system", "Radiography", "Radiology", "Pathology", "Medicine", "In patient"]}
{"id": "2080286891", "references": ["2129716802", "2119467724", "2131262274", "2151996610", "2321891473"], "title": "Severe acute respiratory syndrome: radiographic appearances and pattern of progression in 138 patients", "abstract": "PURPOSE: To retrospectively evaluate the radiographic appearances and pattern of progression of severe acute respiratory syndrome (SARS). MATERIALS AND METHODS: Chest radiographs obtained at clinical presentation and during treatment in 138 patients with confirmed SARS (66 men, 72 women; mean age, 39 years; age range, 20\u201383 years) were assessed. Radiographic appearances of pulmonary parenchymal abnormality, distribution, and extent of involvement on initial chest radiographs were documented. Recognizable patterns of radiographic progression were determined by comparing the overall mean percentage of lung involvement for each patient on serial radiographs. RESULTS: Initial chest radiographs were abnormal in 108 of 138 (78.3%) patients and showed air-space opacity. Lower lung zone (70 of 108, 64.8%) and right lung (82 of 108, 75.9%) were more commonly involved. In most patients, peripheral lung involvement was more common (81 of 108, 75.0%). Unifocal involvement (59 of 108, 54.6%) was more common than multi...", "citation_count": "5", "reference_count": "392", "date": "2003", "authors": ["K. T. Wong", "Gregory E. Antonio", "David S. C. Hui", "Nelson Lee", "Edmund H. Y. Yuen", "Alan Wu", "C. B. Leung", "Timothy Rainer", "Peter Cameron", "Sydney S. C. Chung", "Joseph J. Y. Sung", "Anil T. Ahuja"], "related_topics": ["Respiratory disease", "Lung", "Pneumonia", "Radiology", "Retrospective cohort study", "Radiography", "Surgery", "Abnormality", "Respiratory system", "Medicine", "Mean age"]}
{"id": "3009992310", "references": ["3004906315", "3001118548", "3003217347", "3003573988", "3007940623", "3004318991", "3005212621", "3003465021", "3004280078", "3001195213"], "title": "Molecular immune pathogenesis and diagnosis of COVID-19.", "abstract": "Coronavirus disease 2019 (COVID-19) is a kind of viral pneumonia which is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The emergence of SARS-CoV-2 has been marked as the third introduction of a highly pathogenic coronavirus into the human population after the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV) in the twenty-first century. In this minireview, we provide a brief introduction of the general features of SARS-CoV-2 and discuss current knowledge of molecular immune pathogenesis, diagnosis and treatment of COVID-19 on the base of the present understanding of SARS-CoV and MERS-CoV infections, which may be helpful in offering novel insights and potential therapeutic targets for combating the SARS-CoV-2 infection.", "citation_count": "73", "reference_count": "1,036", "date": "2020", "authors": ["Xiaowei Li", "Manman Geng", "Yizhao Peng", "Liesu Meng", "Shemin Lu"], "related_topics": ["Coronavirus", "Middle East respiratory syndrome coronavirus", "Viral pneumonia", "Population", "Pathogenesis", "Immunology", "Chemistry", "Coronavirus disease 2019 (COVID-19)", "Immune pathogenesis", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3008627141", "references": ["3004906315", "3005679569", "3003668884", "3002539152", "3004239190", "3003951199", "3008985036", "3004318991", "3005079553", "3004280078"], "title": "Coronavirus Disease 2019 (COVID-19): A Perspective from China", "abstract": "In December 2019, an outbreak of severe acute respiratory syndrome coronavirus 2 infection occurred in Wuhan, Hubei Province, China, and spread across China and beyond. On February 12, 2020, the World Health Organization officially named the disease caused by the novel coronavirus as coronavirus disease 2019 (COVID-19). Because most patients infected with COVID-19 had pneumonia and characteristic CT imaging patterns, radiologic examinations have become vital in early diagnosis and the assessment of disease course. To date, CT findings have been recommended as major evidence for clinical diagnosis of COVID-19 in Hubei, China. This review focuses on the etiology, epidemiology, and clinical symptoms of COVID-19 while highlighting the role of chest CT in prevention and disease control.", "citation_count": "45", "reference_count": "1,249", "date": "2020", "authors": ["Zi Yue Zu", "Meng Di Jiang", "Peng Peng Xu", "Wen Chen", "Qian Qian Ni", "Guang Ming Lu", "Long Jiang Zhang"], "related_topics": ["Coronavirus", "Outbreak", "Pneumonia", "Disease", "Epidemiology", "Pandemic", "Etiology", "Viral Epidemiology", "Pediatrics", "Medicine"]}
{"id": "3012211282", "references": ["3004906315", "3001118548", "3005679569", "3005272159", "3003901880", "3006485704", "3005079553", "3005656138", "3006354146", "3002108456"], "title": "Coronavirus Disease 2019 (COVID-19): A Systematic Review of Imaging Findings in 919 Patients.", "abstract": "OBJECTIVE. Available information on CT features of the 2019 novel coronavirus disease (COVID-19) is scattered in different publications, and a cohesive literature review has yet to be compiled. MAT...", "citation_count": "25", "reference_count": "991", "date": "2020", "authors": ["Sana Salehi", "Aidin Abedi", "Sudheer Balakrishnan", "Ali Gholamrezanezhad"], "related_topics": ["Coronavirus", "Pneumonia", "Outbreak", "Disease", "Internal medicine", "Medicine", "Computed tomography", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3008801544", "references": ["3004906315", "3001118548", "3004517278", "3004896587", "3004511262", "3003901880", "3006110666", "3006485704", "3005079553", "3006472059"], "title": "Clinical and computed tomographic imaging features of novel coronavirus pneumonia caused by SARS-CoV-2.", "abstract": "Summary   Purpose  To investigate the clinical and imaging characteristics of computed tomography (CT) in novel coronavirus pneumonia (NCP) caused by SARS-CoV-2.    Materials and methods  A retrospective analysis was performed on the imaging findings of patients confirmed with COVID-19 pneumonia who had chest CT scanning and treatment after disease onset. The clinical and imaging data were analyzed.    Results  Fifty patients were enrolled, including mild type in nine, common in 28, severe in 10 and critically severe in the rest three. Mild patients (29 years) were significantly (P  \u00b0C), 49 (98%) patients had normal or slightly reduced leukocyte count, 14 (28%) had decreased counts of lymphocytes, and 26 (52%) patients had increased C-reactive protein. Nine mild patients were negative in CT imaging. For all the other types of NCP, the lesion was in the right upper lobe in 30 cases, right middle lobe in 22, right lower lobe in 39, left upper lobe in 33 and left lower lobe in 36. The lesion was primarily located in the peripheral area under the pleura with possible extension towards the pulmonary hilum. Symmetrical lesions were seen in 26 cases and asymmetrical in 15. The density of lesion was mostly uneven with ground glass opacity as the primary presentation accompanied by partial consolidation and fibrosis.    Conclusion  CT imaging presentations of NCP are mostly patchy ground glass opacities in the peripheral areas under the pleura with partial consolidation which will be absorbed with formation of fibrotic stripes if improved. CT scanning provides important bases for early diagnosis and treatment of NCP.", "citation_count": "16", "reference_count": "414", "date": "2020", "authors": ["Yu-Huan Xu", "Jing-Hui Dong", "Wei-Min An", "Xiao-Yan Lv", "Xiao-Ping Yin", "Jian-Zeng Zhang", "Li Dong", "Xi Ma", "Hong-Jie Zhang", "Bu-Lang Gao"], "related_topics": ["Ground-glass opacity", "Lesion", "Radiology", "Fibrosis", "Peripheral", "Retrospective cohort study", "Medicine", "Computed tomography", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3014051579", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3004318991", "3003465021", "3005079553", "3001195213", "3004280078", "3002108456"], "title": "Coronavirus disease 2019 (COVID-19): current status and future perspectives.", "abstract": "Coronavirus disease 2019 (COVID-19) originated in the city of Wuhan, Hubei Province, Central China, and has spread quickly to 72 countries to date. COVID-19 is caused by a novel coronavirus, named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) [previously provisionally known as 2019 novel coronavirus (2019-nCoV)]. At present, the newly identified SARS-CoV-2 has caused a large number of deaths with tens of thousands of confirmed cases worldwide, posing a serious threat to public health. However, there are no clinically approved vaccines or specific therapeutic drugs available for COVID-19. Intensive research on the newly emerged SARS-CoV-2 is urgently needed to elucidate the pathogenic mechanisms and epidemiological characteristics and to identify potential drug targets, which will contribute to the development of effective prevention and treatment strategies. Hence, this review will focus on recent progress regarding the structure of SARS-CoV-2 and the characteristics of COVID-19, such as the aetiology, pathogenesis and epidemiological characteristics.", "citation_count": "117", "reference_count": "414", "date": "2020", "authors": ["Heng Li", "Shang-Ming Liu", "Xiao-Hua Yu", "Shi-Lin Tang", "Chao-Ke Tang"], "related_topics": ["Coronavirus", "Pandemic", "Betacoronavirus", "Epidemiology", "Public health", "Etiology", "Intensive care medicine", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3025334942", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3004318991", "3003465021", "3005079553", "2007872832", "3002108456"], "title": "Clinical Features, Diagnosis, and Treatment of COVID-19 in Hospitalized Patients: A Systematic Review of Case Reports and Case Series", "abstract": "Introduction: The 2019 novel coronavirus (COVID-19) has been declared a public health emergency worldwide. The objective of this systematic review was to characterize the clinical, diagnostic, and treatment characteristics of hospitalized patients presenting with COVID-19. Methods: We conducted a structured search using PubMed/Medline, Embase, and Web of Science to collect both case reports and case series on COVID-19 published up to April 24, 2020. There were no restrictions regarding publication language. Results: Eighty articles were included analyzing a total of 417 patients with a mean age of 48 years. The most common presenting symptom in patients who tested positive for COVID-19 was fever, reported in up to 62% of patients from 82% of the analyzed studies. Other symptoms including rhinorrhea, dizziness, and chills were less frequently reported. Additionally, in studies that reported C-reactive protein (CRP) measurements, a large majority of patients displayed an elevated CRP (60%). Progression to acute respiratory distress syndrome (ARDS) was the most common complication of patients testing positive for COVID-19 (21%). CT images displayed ground-glass opacification (GGO) patterns (80%) as well as bilateral lung involvement (69%). The most commonly used antiviral treatment modalities included, lopinavir (HIV protease inhibitor), arbidiol hydrochloride (influenza fusion inhibitor), and oseltamivir (neuraminidase inhibitor). Conclusions: Development of ARDS may play a role in estimating disease progression and mortality risk. Early detection of elevations in serum CRP, combined with a clinical COVID-19 symptom presentation may be used as a surrogate marker for the presence and severity of the disease. There is a paucity of data surrounding the efficacy of treatments. There is currently not a well-established gold standard therapy for the treatment of diagnosed COVID-19. Further prospective investigations are necessary.", "citation_count": "106", "reference_count": "29", "date": "2020", "authors": ["Azin Tahvildari", "Mahta Arbabi", "Yeganeh Farsi", "Parnian Jamshidi", "Saba Hasanzadeh", "Tess Moore Calcagno", "Mohammad Javad Nasiri", "Mehdi Mirsaeidi"], "related_topics": ["Chills", "Surrogate endpoint", "ARDS", "Oseltamivir", "rhinorrhea", "Complication", "Disease", "Lopinavir", "Internal medicine", "Medicine"]}
{"id": "3034593359", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3008827533", "3003465021", "3008028633", "3005079553", "3001195213", "3002108456"], "title": "Epidemiological and Clinical Characteristics of Cases During the Early Phase of COVID-19 Pandemic: A Systematic Review and Meta-Analysis", "abstract": "Background: On 29th December 2019, a cluster of cases displaying the symptoms of a \"pneumonia of unknown cause\" was identified in Wuhan, Hubei province of China. This systematic review and meta-analysis aims to review the epidemiological and clinical characteristics of COVID-19 cases in the early phase of the COVID-19 pandemic. Methods: The search strategy involved peer-reviewed studies published between 1st January and 11th February 2020 in Pubmed, Google scholar and China Knowledge Resource Integrated database. Publications identified were screened for their title and abstracts according to the eligibility criteria, and further shortlisted by full-text screening. Three independent reviewers extracted data from these studies, and studies were assessed for potential risk of bias. Studies comprising non-overlapping patient populations, were included for qualitative and quantitative synthesis of results. Pooled prevalence with 95% confidence intervals were calculated for patient characteristics. Results: A total of 29 publications were selected after full-text review. This comprised of 18 case reports, three case series and eight cross-sectional studies on patients admitted from mid-December of 2019 to early February of 2020. A total of 533 adult patients with pooled median age of 56 (95% CI: 49-57) and a pooled prevalence of male of 60% (95% CI: 52-68%) were admitted to hospital at a pooled median of 7 days (95% CI: 7-7) post-onset of symptoms. The most common symptoms at admission were fever, cough and fatigue, with a pooled prevalence of 90% (95% CI: 81-97%), 58% (95% CI: 47-68%), and 50% (95% CI: 29-71%), respectively. Myalgia, shortness of breath, headache, diarrhea and sore throat were less common with pooled prevalence of 27% (95% CI: 20-36%), 25% (95% CI: 15-35%), 10% (95% CI: 7-13%), 8% (95% CI: 5-13%), and 7% (95% CI: 1-15%), respectively. ICU patients had a higher proportion of shortness of breath at presentation, as well as pre-existing hypertension, cardiovascular disease and COPD, compared to non-ICU patients in 2 studies (n = 179). Conclusion: This study highlights the key epidemiological and clinical features of COVID-19 cases during the early phase of the COVID-19 pandemic.", "citation_count": "66", "reference_count": "22", "date": "2020", "authors": ["Jiayun Koh", "Shimoni Urvish Shah", "Pearleen Ee Yong Chua", "Hao Gui", "Junxiong Pang"], "related_topics": ["Epidemiology", "Meta-analysis", "Sore throat", "Confidence interval", "COPD", "Internal medicine", "myalgia", "Disease cluster", "Pneumonia", "Medicine"]}
{"id": "3013468450", "references": ["3001118548", "3001897055", "3003668884", "3002539152", "3004318991", "3008090866", "3008827533", "3003465021", "3004239190", "3002108456"], "title": "Novel Coronavirus Infection (COVID-19) in Humans: A Scoping Review and Meta-Analysis", "abstract": "A growing body of literature on the 2019 novel coronavirus (SARS-CoV-2) is becoming available, but a synthesis of available data has not been conducted. We performed a scoping review of currently available clinical, epidemiological, laboratory, and chest imaging data related to the SARS-CoV-2 infection. We searched MEDLINE, Cochrane CENTRAL, EMBASE, Scopus and LILACS from 01 January 2019 to 24 February 2020. Study selection, data extraction and risk of bias assessment were performed by two independent reviewers. Qualitative synthesis and meta-analysis were conducted using the clinical and laboratory data, and random-effects models were applied to estimate pooled results. A total of 61 studies were included (59,254 patients). The most common disease-related symptoms were fever (82%, 95% confidence interval (CI) 56%\u201399%; n = 4410), cough (61%, 95% CI 39%\u201381%; n = 3985), muscle aches and/or fatigue (36%, 95% CI 18%\u201355%; n = 3778), dyspnea (26%, 95% CI 12%\u201341%; n = 3700), headache in 12% (95% CI 4%\u201323%, n = 3598 patients), sore throat in 10% (95% CI 5%\u201317%, n = 1387) and gastrointestinal symptoms in 9% (95% CI 3%\u201317%, n = 1744). Laboratory findings were described in a lower number of patients and revealed lymphopenia (0.93 \u00d7 109/L, 95% CI 0.83\u20131.03 \u00d7 109/L, n = 464) and abnormal C-reactive protein (33.72 mg/dL, 95% CI 21.54\u201345.91 mg/dL; n = 1637). Radiological findings varied, but mostly described ground-glass opacities and consolidation. Data on treatment options were limited. All-cause mortality was 0.3% (95% CI 0.0%\u20131.0%; n = 53,631). Epidemiological studies showed that mortality was higher in males and elderly patients. The majority of reported clinical symptoms and laboratory findings related to SARS-CoV-2 infection are non-specific. Clinical suspicion, accompanied by a relevant epidemiological history, should be followed by early imaging and virological assay.", "citation_count": "84", "reference_count": "340", "date": "2020", "authors": ["Israel J\u00fanior Borges do Nascimento", "Nensi Cacic", "Hebatullah Mohamed Abdulazeem", "Thilo Caspar von Groote", "Umesh Jayarajah", "Ishanka Weerasekara", "Meisam Abdar Esfahani", "Vinicius Tassoni Civile", "Ana Marusic", "Ana Jeroncic", "Nelson Carvas Junior", "Tina Poklepovic Pericic", "Irena Zakarija-Grkovic", "Silvana Mangeon Meirelles Guimar\u00e3es", "Nicola Luigi Bragazzi", "Maria Bjorklund", "Ahmad Sofi-Mahmudi", "Mohammad Altujjar", "Maoyi Tian", "Diana Maria Cespedes Arcani", "D\u00f3nal P O'Math\u00fana", "Milena Soriano Marcolino"], "related_topics": ["Sore throat", "Epidemiology", "Confidence interval", "Meta-analysis", "Internal medicine", "Medicine", "MEDLINE", "Coronavirus", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3008928918", "references": ["3004906315", "3001118548", "3004802901", "3004668429", "3004511262", "2156614913", "3003901880", "3003465021", "3005079553", "2791599184"], "title": "First case of Coronavirus Disease 2019 (COVID-19) pneumonia in Taiwan.", "abstract": "An outbreak of respiratory illness proved to be infected by a 2019 novel coronavirus, officially named Coronavirus Disease 2019 (COVID-19), was notified first in Wuhan, China, and has spread rapidly in China and to other parts of the world. Herein, we reported the first confirmed case of novel coronavirus pneumonia (NCP) imported from China in Taiwan. This case report revealed a natural course of NCP with self-recovery, which may be a good example in comparison with medical treatments.", "citation_count": "15", "reference_count": "155", "date": "2020", "authors": ["Shao-Chung Cheng", "Yuan-Chia Chang", "Yu-Long Fan Chiang", "Yu-Chan Chien", "Mingte Cheng", "Chin-Hua Yang", "Chia-Husn Huang", "Yuan-Nian Hsu"], "related_topics": ["Pneumonia", "Betacoronavirus", "Outbreak", "Watchful waiting", "Virology", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Natural course", "Respiratory illness", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3033301213", "references": ["3007643904", "3004896487", "3003217347", "3011242477", "3004318991", "3012310845", "3035011439", "3008818676", "3004348779", "3010819577"], "title": "Does Early Childhood Vaccination Protect Against COVID-19?", "abstract": "The coronavirus disease 2019 (COVID-19) is an on-going pandemic caused by the SARS-coronavirus-2 (SARS-CoV-2) which targets the respiratory system of humans. The published data show that children, unlike adults, are less susceptible to contracting the disease. This article aims at understanding why children constitute a minor group among hospitalized COVID-19 patients. Here, we hypothesize that the measles, mumps, and rubella (MMR) vaccine could provide a broad neutralizing antibody against numbers of diseases, including COVID-19. Our hypothesis is based on the 30 amino acid sequence homology between the SARS-CoV-2 Spike (S) glycoprotein (PDB: 6VSB) of both the measles virus fusion (F1) glycoprotein (PDB: 5YXW_B) and the rubella virus envelope (E1) glycoprotein (PDB: 4ADG_A). Computational analysis of the homologous region detected the sequence as antigenic epitopes in both measles and rubella. Therefore, we believe that humoral immunity, created through the MMR vaccination, provides children with advantageous protection against COVID-19 as well, however, an experimental analysis is required.", "citation_count": "29", "reference_count": "16", "date": "2020", "authors": ["Karzan R Sidiq", "Dana Khdr Sabir", "Shakhawan M Ali", "Rimantas Kodzius"], "related_topics": ["Rubella", "Rubella virus", "Measles virus", "Vaccination", "Measles", "Immunization", "Virus", "Neutralizing antibody", "Virology", "Medicine"]}
{"id": "3037552531", "references": ["3001118548", "3007814559", "2131262274", "3009885589", "3008827533", "3008818676", "3009976289", "3005079553", "3004280078", "3009859788"], "title": "Analysis of clinical features and early warning signs in patients with severe COVID-19: A retrospective cohort study.", "abstract": "Coronavirus disease 2019 (COVID-19) was first identified in Wuhan, China, in December 2019. Although previous studies have described the clinical aspects of COVID-19, few studies have focused on the early detection of severe COVID-19. Therefore, this study aimed to identify the predictors of severe COVID-19 and to compare clinical features between patients with severe COVID-19 and those with less severe COVID-19. Patients admitted to designated hospital in the Henan Province of China who were either discharged or died prior to February 15, 2020 were enrolled retrospectively. Additionally, patients who underwent at least one of the following treatments were assigned to the severe group: continuous renal replacement therapy, high-flow oxygen absorption, noninvasive and invasive mechanical ventilation, or extracorporeal membrane oxygenation. The remaining patients were assigned to the non-severe group. Demographic information, initial symptoms, and first visit examination results were collected from the electronic medical records and compared between the groups. Multivariate logistic regression analysis was performed to determine the predictors of severe COVID-19. A receiver operating characteristic curve was used to identify a threshold for each predictor. Altogether,104 patients were enrolled in our study with 30 and 74 patients in the severe and non-severe groups, respectively. Multivariate logistic analysis indicated that patients aged \u226563 years (odds ratio = 41.0; 95% CI: 2.8, 592.4), with an absolute lymphocyte value of \u22641.02\u00d7109/L (odds ratio = 6.1; 95% CI = 1.5, 25.2) and a C-reactive protein level of \u226565.08mg/L (odds ratio = 8.9; 95% CI = 1.0, 74.2) were at a higher risk of severe illness. Thus, our results could be helpful in the early detection of patients at risk for severe illness, enabling the implementation of effective interventions and likely lowering the morbidity of COVID-19 patients.", "citation_count": "23", "reference_count": "4", "date": "2020", "authors": ["Xinkui Liu", "Xinpei Yue", "Furong Liu", "Le Wei", "Yuntian Chu", "Honghong Bao", "Yichao Dong", "Wenjie Cheng", "Linpeng Yang"], "related_topics": ["Odds ratio", "Retrospective cohort study", "Predictive value of tests", "Multivariate analysis", "Renal replacement therapy", "Logistic regression", "Extracorporeal membrane oxygenation", "Medical record", "Internal medicine", "Medicine"]}
{"id": "3031029566", "references": ["3015571324", "3008874180", "3010781325", "3002539152", "3008696669", "3035011439", "3015792206", "3008818676", "3006961006", "3012188173"], "title": "Identification of RT-PCR-Negative Asymptomatic COVID-19 Patients via Serological Testing", "abstract": "Asymptomatic individuals with coronavirus disease (COVID-19) have been identified via nucleic acid testing for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2); however, the epidemiologic characteristics and viral shedding pattern of asymptomatic patients remain largely unknown. In this study, serological testing was applied when identifying nine asymptomatic cases of COVID-19 who showed persistent negative RT-PCR test results for SARS-CoV-2 nucleic acid and no symptoms of COVID-19. Two asymptomatic cases were presumed to be index patients who had cleared the virus when their close contacts developed symptoms of COVID-19. Three of the asymptomatic cases were local individuals who spontaneously recovered before their presumed index patients developed symptoms of COVID-19. This report presents the epidemiologic and clinical characteristics of asymptomatic individuals with SARS-CoV-2 infection that were undetected on RT-PCR tests in previous epidemiologic investigations probably due to the transient viral shedding duration.", "citation_count": "19", "reference_count": "6", "date": "2020", "authors": ["Jinru Wu", "Xinyi Liu", "Dan Zhou", "Guangqian Qiu", "Miao Dai", "Qingting Yang", "Zhonghui Pan", "Ning Zhou", "Pa Wu"], "related_topics": ["Asymptomatic", "Viral shedding", "Coronavirus", "Serology", "Disease", "Virus", "Immunology", "Real-time polymerase chain reaction", "Medicine", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "3037851904", "references": ["2601317354", "3008827533", "3008818676", "2946740876", "3004280078"], "title": "Could urinary ACE2 protein level help identify individuals susceptible to SARS-CoV-2 infection and complication?", "abstract": "", "citation_count": "5", "reference_count": "1", "date": "2020", "authors": ["Xiaotian Ni", "Changqing Sun", "Yaping Tian", "Yanjie Huang", "Tongqing Gong", "Lan Song", "Xing Yang", "Kai Li", "Nairen Zheng", "Jianping Wang", "Hongxing Wu", "Ruoxian Zhang", "Yi Wang", "Guangshun Wang", "Jun Qin"], "related_topics": ["Complication", "Virology", "Urinary system", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Protein level", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3035018050", "references": ["3001118548", "3001897055", "3003668884", "3008696669", "3016535995", "3009912996", "3008827533", "3003465021", "3008818676", "3012747666"], "title": "Early Detection and Assessment of Covid-19", "abstract": "Background: Since the Covid-19 global pandemic emerged, developing countries have been facing multiple challenges over its diagnosis. We aimed to establish a relationship between the signs and symptoms of COVID-19 for early detection and assessment to reduce the transmission rate of SARS-Cov-2. Methods: We collected published data on the clinical features of Covid-19 retrospectively and categorized them into physical and blood biomarkers. Common features were assigned scores by the Borg scoring method with slight modifications and were incorporated into a newly-developed Hashmi-Asif Covid-19 assessment Chart. Correlations between signs and symptoms with the development of Covid-19 was assessed by Pearson correlation and Spearman Correlation coefficient (rho). Linear regression analysis was employed to assess the highest correlating features. The frequency of signs and symptoms in developing Covid-19 was assessed through Chi-square test two tailed with Cramer's V strength. Changes in signs and symptoms were incorporated into a chart that consisted of four tiers representing disease stages. Results: Data from 10,172 Covid-19 laboratory confirmed cases showed a correlation with Fever in 43.9% (P = 0.000) cases, cough 54.08% and dry mucus 25.68% equally significant (P = 0.000), Hyperemic pharyngeal mucus membrane 17.92% (P = 0.005), leukopenia 28.11% (P = 0.000), lymphopenia 64.35% (P = 0.000), thrombopenia 35.49% (P = 0.000), elevated Alanine aminotransferase 50.02% (P = 0.000), and Aspartate aminotransferase 34.49% (P = 0.000). The chart exhibited a maximum scoring of 39. Normal tier scoring was \u2264 12/39, mild state scoring was 13-22/39, and star values scoring was \u22657/15; this latter category on the chart means Covid-19 is progressing and quarantine should be adopted. Moderate stage scored 23-33 and severe scored 34-39 in the chart. Conclusion: The Hashmi-Asif Covid-19 Chart is significant in assessing subclinical and clinical stages of Covid-19 to reduce the transmission rate.", "citation_count": "39", "reference_count": "8", "date": "2020", "authors": ["Hafiz Abdul Sattar Hashmi", "Hafiz Muhammad Asif"], "related_topics": ["Chart", "Subclinical infection", "Spearman's rank correlation coefficient", "Linear regression", "Internal medicine", "Pearson product-moment correlation coefficient", "Stage (cooking)", "Leukopenia", "Correlation", "Medicine"]}
{"id": "3037451072", "references": [], "title": "Analysis of Risk Perceptions and Related Factors Concerning COVID-19 Epidemic in Chongqing, China.", "abstract": "To assess perceptions of risk and related factors concerning COVID-19 epidemic among residents in Chongqing city, China. With convenience sampling, a web questionnaire survey was conducted among 476 residents living in Chongqing on February 13rd to 14th in 2020, when citizens just started to get back to work. Residents\u2019 estimated perceived risks were (4.63\u2009\u00b1\u20090.57), (4.19\u2009\u00b1\u20090.76), (3.23\u2009\u00b1\u20090.91) and (2.29\u2009\u00b1\u20090.96) for the infectivity, pathogenicity, lethality and self-rated\u00a0infection possibility of COVID-19, respectively. Females (OR\u2009=\u20094.234), people with income\u2009\u2265\u20092000 yuan (2000\u20134999 yuan: OR\u2009=\u20095.052, 5000\u20139999 yuan: OR\u2009=\u20094.301,\u2009\u2265\u200910,000 yuan: OR\u2009=\u200923.459), the married status (OR\u2009=\u20091.811), the divorced status, widows or widowers (OR\u2009=\u20093.038), people living with families including children (OR\u2009=\u20095.085) or chronic patients (OR\u2009=\u20092.423) had a higher perceived risk level, as well as people who used free media websites (OR\u2009=\u20091.756), community workers (OR\u2009=\u20094.064) or community information platforms (OR\u2009=\u20092.235) as main media information sources. The perceived risk increased by 4.9% for every one-year increase of age. People who used WeChat contacts (OR\u2009=\u20090.196) as the main media information source, reported a lower perceived risk. Residents reported a high level of risk perception towards COVID-19 in Chongqing and it was impacted by the population demographic characteristics. Media information sources, including community information platforms and community workers may cause the increase of public risk perceptions.", "citation_count": "15", "reference_count": "15", "date": "2021", "authors": ["Shan He", "Siyu Chen", "Lingna Kong", "Weiwei Liu"], "related_topics": ["Population", "Risk perception", "Questionnaire", "Environmental health", "China", "Information source", "Psychology", "Perception", "Coronavirus disease 2019 (COVID-19)", "Related factors"]}
{"id": "3004668429", "references": ["1922522683", "1997020575", "2999409984", "1993577573", "2789752210", "3003901880", "2015323375", "2800783955", "2128312233", "3002108456"], "title": "Emerging 2019 Novel Coronavirus (2019-nCoV) Pneumonia.", "abstract": "BackgroundThe chest CT findings of patients with 2019 Novel Coronavirus (2019-nCoV) pneumonia have not previously been described in detail.PurposeTo investigate the clinical, laboratory, and imaging findings of emerging 2019-nCoV pneumonia in humans.Materials and MethodsFifty-one patients (25 men and 26 women; age range 16-76 years) with laboratory-confirmed 2019-nCoV infection by using real-time reverse transcription polymerase chain reaction underwent thin-section CT. The imaging findings, clinical data, and laboratory data were evaluated.ResultsFifty of 51 patients (98%) had a history of contact with individuals from the endemic center in Wuhan, China. Fever (49 of 51, 96%) and cough (24 of 51, 47%) were the most common symptoms. Most patients had a normal white blood cell count (37 of 51, 73%), neutrophil count (44 of 51, 86%), and either normal (17 of 51, 35%) or reduced (33 of 51, 65%) lymphocyte count. CT images showed pure ground-glass opacity (GGO) in 39 of 51 (77%) patients and GGO with reticular and/or interlobular septal thickening in 38 of 51 (75%) patients. GGO with consolidation was present in 30 of 51 (59%) patients, and pure consolidation was present in 28 of 51 (55%) patients. Forty-four of 51 (86%) patients had bilateral lung involvement, while 41 of 51 (80%) involved the posterior part of the lungs and 44 of 51 (86%) were peripheral. There were more consolidated lung lesions in patients 5 days or more from disease onset to CT scan versus 4 days or fewer (431 of 712 lesions vs 129 of 612 lesions; P &lt; .001). Patients older than 50 years had more consolidated lung lesions than did those aged 50 years or younger (212 of 470 vs 198 of 854; P &lt; .001). Follow-up CT in 13 patients showed improvement in seven (54%) patients and progression in four (31%) patients.ConclusionPatients with fever and/or cough and with conspicuous ground-glass opacity lesions in the peripheral and posterior lungs on CT images, combined with normal or decreased white blood cells and a history of epidemic exposure, are highly suspected of having 2019 Novel Coronavirus (2019-nCoV) pneumonia.\u00a9 RSNA, 2020.", "citation_count": "10", "reference_count": "917", "date": "2020", "authors": ["Fengxiang Song", "Nannan Shi", "Fei Shan", "Zhiyong Zhang", "Jie Shen", "Hongzhou Lu", "Yun Ling", "Yebin Jiang", "Yuxin Shi"], "related_topics": ["Pneumonia", "Absolute neutrophil count", "Lung", "Gastroenterology", "Retrospective cohort study", "Lymphocyte", "Young adult", "Real-time polymerase chain reaction", "Medicine", "Internal medicine", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3001456238", "references": ["2105637133", "311927316", "2470646526", "1993577573", "2046153984", "2794573360", "2306794997", "2148822770", "2903899730", "2017248106"], "title": "Emerging coronaviruses: Genome structure, replication, and pathogenesis.", "abstract": "The recent emergence of a novel coronavirus (2019-nCoV), which is causing an outbreak of unusual viral pneumonia in patients in Wuhan, a central city in China, is another warning of the risk of CoVs posed to public health. In this minireview, we provide a brief introduction of the general features of CoVs and describe diseases caused by different CoVs in humans and animals. This review will help understand the biology and potential risk of CoVs that exist in richness in wildlife such as bats.", "citation_count": "70", "reference_count": "1,973", "date": "2020", "authors": ["Yu Chen", "Qianyun Liu", "Deyin Guo"], "related_topics": ["Coronavirus", "Virus classification", "Outbreak", "Viral pneumonia", "Viral replication", "Genetics", "Virology", "Biology", "Genome structure", "Potential risk", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3001465255", "references": ["2006434809", "3001118548", "2162112824", "3002539152", "2156614913", "2141877163"], "title": "A novel coronavirus outbreak of global health concern.", "abstract": "", "citation_count": "6", "reference_count": "4,279", "date": "2020", "authors": ["Chen Wang", "Peter W Horby", "Frederick G Hayden", "George F Gao"], "related_topics": ["Coronavirus", "Betacoronavirus", "Outbreak", "Pneumonia", "Global health", "Medicine", "Virology", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2084576921", "references": ["1544561280", "2038706592", "2028571354", "2057637320", "2009652162", "2155485281", "1997738379", "2105275554", "1565181081", "1997343737"], "title": "Visual detection of turkey coronavirus RNA in tissues and feces by reverse-transcription loop-mediated isothermal amplification (RT-LAMP) with hydroxynaphthol blue dye", "abstract": "Abstract   A sensitive reverse-transcription loop-mediated isothermal amplification (RT-LAMP) assay was developed for the rapid visual detection of turkey coronavirus (TCoV) infection. The reaction is performed in one step in a single tube at 65\u00a0\u00b0C for 45\u00a0min, with hydroxynaphthol blue (HNB) dye added prior to amplification. The\u00a0detection limit of the RT-LAMP assay was approximately 10 2  EID 50/50\u00a0\u03bcl  TCoV genome, and no cross-reaction with other avian viruses was observed. The assay was evaluated further in tissue suspensions prepared from the ileum and ileum\u2013caecal junctions of infected turkey embryos; 100% of these samples were positive in the RT-LAMP assay. All individual feces samples collected in the field were considered positive by both conventional RT-PCR and RT-LAMP. In conclusion, RT-LAMP with HNB dye was shown to be a sensitive, simple assay for the rapid diagnosis of TCoV infection, either directly from feces or in association with virus isolation methods.", "citation_count": "14", "reference_count": "64", "date": "2010", "authors": ["Tereza C. Cardoso", "Heitor F. Ferrari", "Livia C. Bregano", "Camila Silva-Frade", "Ana Carolina G. Rosa", "Alexandre Lima de Andrade"], "related_topics": ["Hydroxynaphthol blue", "Reverse Transcription Loop-mediated Isothermal Amplification", "Loop-mediated isothermal amplification", "Turkey coronavirus", "Feces", "RNA", "Molecular biology", "Virology", "Biology", "Single tube", "Visual detection"]}
{"id": "2073600962", "references": ["2057680658", "1975134712", "2051085144", "1970186243", "2160892791", "2135086664", "2140363623", "2105275554", "1984421786", "2065860560"], "title": "Evaluation of a Direct Reverse Transcription Loop-Mediated Isothermal Amplification Method without RNA Extraction for the Detection of Human Enterovirus 71 Subgenotype C4 in Nasopharyngeal Swab Specimens", "abstract": "Human enterovirus 71 (EV71) is the major causative agent of hand, foot, and mouth disease (HFMD) worldwide and has been associated with neurological complications which resulted in fatalities during recent outbreak in Asia pacific region. A direct reverse transcription loop-mediated isothermal amplification (direct RT-LAMP) assay using heat-treated samples without RNA extraction was developed and evaluated for the detection of EV71 subgenotype C4 in nasopharyngeal swab specimens. The analytical sensitivity and specificity of the direct RT-LAMP assay were examined. The detection limit of the direct RT-LAMP assays was 1.6 of a 50% tissue culture infective dose (TCID50) per reaction and no cross-reaction was observed with control viruses including Cosackievirus A (CVA) viruses (CVA2,4,5,7,9,10,14,16, and 24), Coxsackievirus B (CVB) viruses (CVB1,2,3,4, and 5) or ECHO viruses (ECHO3,6,11, and 19). The direct RT-LAMP assay was evaluated and compared to both RT-LAMP and quantitative real-time PCR (qRT-PCR) in detecting EV71 infection with 145 nasopharyngeal swab specimens. The clinical performance demonstrated the sensitivity and specificity of direct RT-LAMP was reported to be 90.3% and 100% respectively, compared to RT-LAMP, and 86.83% and 100% respectively, compared to qRT-PCR. These data demonstrated that the direct RT-LAMP assay can potentially be developed for the point of care screening of EV71 infection in China.", "citation_count": "27", "reference_count": "31", "date": "2012", "authors": ["Kai Nie", "Shun-xiang Qi", "Yong Zhang", "Le Luo", "Yun Xie", "Meng-jie Yang", "Yi Zhang", "Jin Li", "Hongwei Shen", "Qi Li", "Xue-jun Ma"], "related_topics": ["Reverse Transcription Loop-mediated Isothermal Amplification", "Loop-mediated isothermal amplification", "Nucleic acid amplification technique", "Coxsackievirus", "Reverse transcriptase", "RNA extraction", "Polymerase chain reaction", "RNA", "Virology", "Biology"]}
{"id": "1991420168", "references": ["2120801593", "2051304065", "2141987735", "2108924397", "2052129607", "1969557290", "2129358311", "1966636515", "2067506266", "1977296748"], "title": "Utility of IgM ELISA, TaqMan real-time PCR, reverse transcription PCR, and RT-LAMP assay for the diagnosis of Chikungunya fever.", "abstract": "Chikungunya fever a re-emerging infection with expanding geographical boundaries, can mimic symptoms of other infections like dengue, malaria which makes the definitive diagnosis of the infection important. The present study compares the utility of four laboratory diagnostic methods viz. IgM capture ELISA, an in house reverse transcription PCR for the diagnosis of Chikungunya fever, TaqMan real-time PCR, and a one step reverse transcription-loop mediated isothermal amplification assay (RT-LAMP). Out of the 70 serum samples tested, 29 (41%) were positive for Chikungunya IgM antibody by ELISA and 50 (71%) samples were positive by one of the three molecular assays. CHIKV specific nucleic acid was detected in 33/70 (47%) by reverse transcription PCR, 46/70 (66%) by TaqMan real-time PCR, and 43/70 (62%) by RT-LAMP assay. A majority of the samples (62/70; 89%) were positive by at least one of the four assays used in the study. The molecular assays were more sensitive for diagnosis in the early stages of illness (2\u20135 days post onset) when antibodies were not detectable. In the later stages of illness, the IgM ELISA is a more sensitive diagnostic test. In conclusion we recommend that the IgM ELISA be used as an initial screening test followed one of the molecular assays in samples that are collected in the early phase of illness and negative for CHIKV IgM antibodies. Such as approach would enable rapid confirmation of the diagnosis and implementation of public health measures especially during outbreaks. J. Med. Virol. 84:1771\u20131778, 2012. \u00a9 2012 Wiley Periodicals, Inc.", "citation_count": "27", "reference_count": "66", "date": "2012", "authors": ["Vijayalakshmi Reddy", "Vasanthapuram Ravi", "Anita Desai", "Manmohan Parida", "Ann M. Powers", "Barbara W. Johnson"], "related_topics": ["TaqMan", "Reverse transcription polymerase chain reaction", "Loop-mediated isothermal amplification", "Chikungunya", "Dengue fever", "Real-time polymerase chain reaction", "Antibody", "Virology", "Malaria", "Biology"]}
{"id": "3011969828", "references": ["3001118548", "3000771439", "3001465255", "3003573988", "3003951199", "3003465021", "3003637715", "3004280078", "3004239190", "3001195213"], "title": "2019 Novel Coronavirus Disease (COVID-19): Paving the Road for Rapid Detection and Point-of-Care Diagnostics.", "abstract": "We believe a point-of-care (PoC) device for the rapid detection of the 2019 novel Coronavirus (SARS-CoV-2) is crucial and urgently needed. With this perspective, we give suggestions regarding a potential candidate for the rapid detection of the coronavirus disease 2019 (COVID-19), as well as factors for the preparedness and response to the outbreak of the COVID-19.", "citation_count": "32", "reference_count": "194", "date": "2020", "authors": ["Trieu Nguyen", "Dang Duong Bang", "Anders Wolff"], "related_topics": ["Preparedness", "Outbreak", "Point-of-care testing", "Disease", "Intensive care medicine", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Potential candidate", "Rapid detection", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2175815746", "references": ["2042479028", "2141627564", "1994850469", "2160892791", "2066385972", "2128110156", "2101258647", "2105275554", "2149136689", "2124449928"], "title": "Development of reverse-transcription loop-mediated isothermal amplification assay for rapid detection and differentiation of dengue virus serotypes 1-4.", "abstract": "Dengue virus (DENV), the most widely prevalent arbovirus, continues to be a threat to human health in the tropics and subtropics. Early and rapid detection of DENV infection during the acute phase of illness is crucial for proper clinical patient management and preventing the spread of infection. The aim of the current study was to develop a specific, sensitive, and robust reverse transcriptase loop-mediated isothermal amplification (RT-LAMP) assay for detection and differentiation of DENV1-4 serotypes. The method detection primers, which were designed to target the different DENV serotypes, were identified by inspection of multiple sequence alignments of the non-structural protein (NS) 2A of DENV1, NS4B of DENV2, NS4A of DENV3 and the 3\u2032 untranslated region of the NS protein of DENV4. No cross-reactions of the four serotypes were observed during the tests. The detection limits of the DENV1-4-specific RT-LAMP assays were approximately 10-copy templates per reaction. The RT-LAMP assays were ten-fold more sensitive than RT-PCR or real-time PCR. The diagnostic rate was 100\u00a0% for clinical strains of DENV, and 98.9\u00a0% of the DENV-infected patients whose samples were tested were detected by RT-LAMP. Importantly, no false-positives were detected with the new equipment and methodology that was used to avoid aerosol contamination of the samples. The RT-LAMP method used in our study is specific, sensitive, and suitable for further investigation as a useful alternative to the current methods used for clinical diagnosis of DENV1-4, especially in hospitals and laboratories that lack sophisticated diagnostic systems.", "citation_count": "43", "reference_count": "26", "date": "2015", "authors": ["Sheng-feng Hu", "Miao Li", "Lan-lan Zhong", "Shi-miao Lu", "Ze-xia Liu", "Jie-ying Pu", "Jin-sheng Wen", "Xi Huang"], "related_topics": ["Reverse Transcription Loop-mediated Isothermal Amplification", "Loop-mediated isothermal amplification", "Dengue virus", "Nucleic acid amplification technique", "Dengue fever", "Arbovirus", "Reverse transcriptase", "Parasitology", "Virology", "Microbiology", "Biology"]}
{"id": "2105275554", "references": ["2082277951", "2035792726", "2062756489", "2050717506", "1990689151", "2083121396", "2142539585", "1970137322", "2032118018", "2056636525"], "title": "Loop-mediated isothermal amplification of DNA", "abstract": "We have developed a novel method, termed loop-mediated isothermal amplification (LAMP), that amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions. This method employs a DNA polymerase and a set of four specially designed primers that recognize a total of six distinct sequences on the target DNA. An inner primer containing sequences of the sense and antisense strands of the target DNA initiates LAMP. The following strand displacement DNA synthesis primed by an outer primer releases a single-stranded DNA. This serves as template for DNA synthesis primed by the second inner and outer primers that hybridize to the other end of the target, which produces a stem\u2013loop DNA structure. In subsequent LAMP cycling one inner primer hybridizes to the loop on the product and initiates displacement DNA synthesis, yielding the original stem\u2013loop DNA and a new stem\u2013loop DNA with a stem twice as long. The cycling reaction continues with accumulation of 109 copies of target in less than an hour. The final products are stem\u2013loop DNAs with several inverted repeats of the target and cauliflower-like structures with multiple loops formed by annealing between alternately inverted repeats of the target in the same strand. Because LAMP recognizes the target by six distinct sequences initially and by four distinct sequences afterwards, it is expected to amplify the target sequence with high selectivity.", "citation_count": "15", "reference_count": "8,349", "date": "2000", "authors": ["Tsugunori Notomi", "Hiroto Okayama", "Harumi Masubuchi", "Toshihiro Yonekawa", "Keiko Watanabe", "Nobuyuki Amino", "Tetsu Hase"], "related_topics": ["DNA clamp", "Primer (molecular biology)", "Base pair", "Primase", "In vitro recombination", "DNA polymerase", "Multiple displacement amplification", "Nucleic acid thermodynamics", "Molecular biology", "Biology"]}
{"id": "2263084061", "references": ["2133724824", "2110753623", "2144345536", "2066385972", "2043762150", "1987713777", "2105275554", "2102131955", "2788073857", "2149136689"], "title": "Loop-Mediated Isothermal Amplification Assay for Identification of Five Human Plasmodium Species in Malaysia", "abstract": "The lack of rapid, affordable, and accurate diagnostic tests represents the primary hurdle affecting malaria surveillance in resource- and expertise-limited areas. Loop-mediated isothermal amplification (LAMP) is a sensitive, rapid, and cheap diagnostic method. Five species-specific LAMP assays were developed based on 18S rRNA gene. Sensitivity and specificity of LAMP results were calculated as compared with microscopic examination and nested polymerase chain reaction. LAMP reactions were highly sensitive with the detection limit of one copy for Plasmodium vivax, Plasmodium falciparum, and Plasmodium malariae and 10 copies for Plasmodium knowlesi and Plasmodium ovale. LAMP positively detected all human malaria species in all positive samples (N = 134; sensitivity = 100%) within 35 minutes. All negative samples were not amplified by LAMP (N = 67; specificity = 100%). LAMP successfully detected two samples with very low parasitemia. LAMP may offer a rapid, simple, and reliable test for the diagnosis of malaria in areas where malaria is prevalent.", "citation_count": "20", "reference_count": "38", "date": "2016", "authors": ["Yee Ling Lau", "Meng Yee Lai", "Mun Yik Fong", "Jenarun Jelip", "Rohela Mahmud"], "related_topics": ["Plasmodium malariae", "Plasmodium ovale", "Plasmodium vivax", "Plasmodium knowlesi", "Plasmodium falciparum", "Nucleic acid amplification technique", "Loop-mediated isothermal amplification", "Nested polymerase chain reaction", "Virology", "Biology"]}
{"id": "3011242477", "references": ["3007613835", "3003668884"], "title": "COVID-19 and Italy: what next?", "abstract": "Summary  The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already taken on pandemic proportions, affecting over 100 countries in a matter of weeks. A global response to prepare health systems worldwide is imperative. Although containment measures in China have reduced new cases by more than 90%, this reduction is not the case elsewhere, and Italy has been particularly affected. There is now grave concern regarding the Italian national health system's capacity to effectively respond to the needs of patients who are infected and require intensive care for SARS-CoV-2 pneumonia. The percentage of patients in intensive care reported daily in Italy between March 1 and March 11, 2020, has consistently been between 9% and 11% of patients who are actively infected. The number of patients infected since Feb 21 in Italy closely follows an exponential trend. If this trend continues for 1 more week, there will be 30\u2008000 infected patients. Intensive care units will then be at maximum capacity; up to 4000 hospital beds will be needed by mid-April, 2020. Our analysis might help political leaders and health authorities to allocate enough resources, including personnel, beds, and intensive care facilities, to manage the situation in the next few days and weeks. If the Italian outbreak follows a similar trend as in Hubei province, China, the number of newly infected patients could start to decrease within 3\u20134 days, departing from the exponential trend. However, this cannot currently be predicted because of differences between social distancing measures and the capacity to quickly build dedicated facilities in China.", "citation_count": "2", "reference_count": "2,324", "date": "2020", "authors": ["Andrea Remuzzi", "Giuseppe Remuzzi"], "related_topics": ["Intensive care", "Global health", "Pandemic", "China", "Environmental health", "Outbreak", "Social distance", "Medicine", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3018724240", "references": ["3001118548", "3011060952", "3002539152", "3003465021", "3010604545"], "title": "SARS-CoV-2 can be detected in urine, blood, anal swabs, and oropharyngeal swabs specimens.", "abstract": "Purpose  The purpose of this study was to detect severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) ribonucleic acid (RNA) in urine and blood specimens, and anal and oropharyngeal swabs from patients with confirmed SARS-CoV-2 infection, and correlated positive results with clinical findings.  Methods  Patients with confirmed SARS-CoV-2 infections were included in this study. Patients' demographic and clinical data were recorded. Quantitative real-time polymerase chain reaction was used to detect SARS-CoV-2 RNA in urine and blood specimens, and anal and oropharyngeal swabs. The study is registered at ClinicalTrials.gov (No. NCT04279782, 19 February, 2020).  Results  SARS-CoV-2 RNA was present in all four specimen types, though not all specimen types were positive simultaneously. The presence of viral RNA was not necessarily predictive of clinical symptoms, for example, the presence of viral RNA in the urine did not necessarily predict urinary tract symptoms.  Conclusions  SARS-CoV-2 can infect multiple systems, including the urinary tract. Testing different specimen types may be useful for monitoring disease changes and progression, and for establishing a prognosis.", "citation_count": "5", "reference_count": "213", "date": "2020", "authors": ["Liang Peng", "Jing Liu", "Wenxiong Xu", "Qiumin Luo", "Dabiao Chen", "Ziying Lei", "Zhanlian Huang", "Xuejun Li", "Keji Deng", "Bingliang Lin", "Zhiliang Gao"], "related_topics": ["Urinary system", "Urine", "Real-time polymerase chain reaction", "RNA", "Polymerase chain reaction", "Gastroenterology", "Virology", "Medicine", "Internal medicine", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "URINE BLOOD", "Viral rna"]}
{"id": "2158121945", "references": ["2315217144", "2905912541", "2153911335", "1536339477", "2798795107", "1856219842", "1833207062", "2093487930", "1589603082", "2465608195"], "title": "Guidelines for environmental infection control in health-care facilities. Recommendations of CDC and the Healthcare Infection Control Practices Advisory Committee (HICPAC).", "abstract": "The health-care facility environment is rarely implicated in disease transmission, except among patients who are immunocompromised. Nonetheless, inadvertent exposures to environmental pathogens (e.g., Aspergillus spp. and Legionella spp.) or airborne pathogens (e.g., Mycobacterium tuberculosis and varicella-zoster virus) can result in adverse patient outcomes and cause illness among health-care workers. Environmental infection-control strategies and engineering controls can effectively prevent these infections. The incidence of health-care--associated infections and pseudo-outbreaks can be minimized by 1) appropriate use of cleaners and disinfectants; 2) appropriate maintenance of medical equipment (e.g., automated endoscope reprocessors or hydrotherapy equipment); 3) adherence to water-quality standards for hemodialysis, and to ventilation standards for specialized care environments (e.g., airborne infection isolation rooms, protective environments, or operating rooms); and 4) prompt management of water intrusion into the facility. Routine environmental sampling is not usually advised, except for water quality determinations in hemodialysis settings and other situations where sampling is directed by epidemiologic principles, and results can be applied directly to infection-control decisions. This report reviews previous guidelines and strategies for preventing environment-associated infections in health-care facilities and offers recommendations. These include 1) evidence-based recommendations supported by studies; 2) requirements of federal agencies (e.g., Food and Drug Administration, U.S. Environmental Protection Agency, U.S. Department of Labor, Occupational Safety and Health Administration, and U.S. Department of Justice); 3) guidelines and standards from building and equipment professional organizations (e.g., American Institute of Architects, Association for the Advancement of Medical Instrumentation, and American Society of Heating, Refrigeration, and Air-Conditioning Engineers); 4) recommendations derived from scientific theory or rationale; and 5) experienced opinions based upon infection-control and engineering practices. The report also suggests a series of performance measurements as a means to evaluate infection-control efforts.", "citation_count": "500", "reference_count": "1,360", "date": "2003", "authors": ["Lynne Sehulster"], "related_topics": ["Isolation (health care)", "Infection control", "Health care", "Occupational safety and health", "Medical equipment", "Justice (ethics)", "Professional association", "Medical emergency", "Guideline", "Environmental health", "Medicine"]}
{"id": "3018334611", "references": ["3012099172", "2147350479", "2215636165", "3005510968", "3009906937", "3010449299", "3010604545", "3027866910", "3018724240", "3004280078"], "title": "Aerodynamic analysis of SARS-CoV-2 in two Wuhan hospitals.", "abstract": "The ongoing outbreak of coronavirus disease 2019\u00a0(COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1-3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.", "citation_count": "12", "reference_count": "1,118", "date": "2020", "authors": ["Yuan Liu", "Zhi Ning", "Yu Chen", "Ming Guo", "Yingle Liu", "Nirmal Kumar Gali", "Li Sun", "Yusen Duan", "Jing Cai", "Dane Westerdahl", "Xinjin Liu", "Ke Xu", "Kin fai Ho", "Haidong Kan", "Qingyan Fu", "Ke Lan"], "related_topics": ["Coronavirus", "Outbreak", "Infectivity", "Aerosol", "Virus", "Isolation (health care)", "Viral Epidemiology", "Betacoronavirus", "Virology", "Medicine"]}
{"id": "3015704123", "references": ["3010149441", "2614711400", "3011242477", "3010449299", "3010223921", "3008429297", "3010633777"], "title": "Aerosol and Surface Distribution of Severe Acute Respiratory Syndrome Coronavirus 2 in Hospital Wards, Wuhan, China, 2020.", "abstract": "To determine distribution of severe acute respiratory syndrome coronavirus 2 in hospital wards in Wuhan, China, we tested air and surface samples. Contamination was greater in intensive care units than general wards. Virus was widely distributed on floors, computer mice, trash cans, and sickbed handrails and was detected in air \u22484 m from patients.", "citation_count": "7", "reference_count": "692", "date": "2020", "authors": ["Zhen Dong Guo", "Zhong Yi Wang", "Shou Feng Zhang", "Xiao Li", "Lin Li", "Chao Li", "Yan Cui", "Rui Bin Fu", "Yun Zhu Dong", "Xiang Yang Chi", "Meng Yao Zhang", "Kun Liu", "Cheng Cao", "Bin Liu", "Ke Zhang", "Yu Wei Gao", "Bing Lu", "Wei Chen"], "related_topics": ["Intensive care", "Emergency medicine", "Medicine", "Aerosol", "2019-20 coronavirus outbreak", "Air microbiology", "Coronavirus Infections", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3010449299", "references": ["3001195213", "1815575713", "3005079553", "3010338568", "2250074178"], "title": "Air, Surface Environmental, and Personal Protective Equipment Contamination by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) From a Symptomatic Patient.", "abstract": "This study documents results of SARS-CoV-2 polymerase chain reaction (PCR) testing of environmental surfaces and personal protective equipment surrounding 3 COVID-19 patients in isolation rooms in a Singapore hospital.", "citation_count": "5", "reference_count": "1,741", "date": "2020", "authors": ["Sean Wei Xiang Ong", "Yian Kim Tan", "Po Ying Chia", "Tau Hong Lee", "Oon Tek Ng", "Michelle Su Yen Wong", "Kalisvar Marimuthu"], "related_topics": ["Isolation (health care)", "Coronavirus", "Personal protective equipment", "Equipment Contamination", "Pneumonia", "Viral shedding", "Real-time polymerase chain reaction", "Emergency medicine", "Medicine", "Contamination"]}
{"id": "3015636815", "references": ["3007643904", "3003668884", "3003217347", "3008696669", "3009906937", "2470646526", "3004318991", "2014935324", "3006961006", "3004280078"], "title": "Rapid Detection of COVID-19 Causative Virus (SARS-CoV-2) in Human Nasopharyngeal Swab Specimens Using Field-Effect Transistor-Based Biosensor.", "abstract": "Coronavirus disease 2019 (COVID-19) is a newly emerging human infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2, previously called 2019-nCoV). Based on the rapid increase in the rate of human infection, the World Health Organization (WHO) has classified the COVID-19 outbreak as a pandemic. Because no specific drugs or vaccines for COVID-19 are yet available, early diagnosis and management are crucial for containing the outbreak. Here, we report a field-effect transistor (FET)-based biosensing device for detecting SARS-CoV-2 in clinical samples. The sensor was produced by coating graphene sheets of the FET with a specific antibody against SARS-CoV-2 spike protein. The performance of the sensor was determined using antigen protein, cultured virus, and nasopharyngeal swab specimens from COVID-19 patients. Our FET device could detect the SARS-CoV-2 spike protein at concentrations of 1 fg/mL in phosphate-buffered saline and 100 fg/mL clinical transport medium. In addition, the FET sensor successfully detected SARS-CoV-2 in culture medium (limit of detection [LOD]: 1.6 \u00d7 101 pfu/mL) and clinical samples (LOD: 2.42 \u00d7 102 copies/mL). Thus, we have successfully fabricated a promising FET biosensor for SARS-CoV-2; our device is a highly sensitive immunological diagnostic method for COVID-19 that requires no sample pretreatment or labeling.", "citation_count": "20", "reference_count": "409", "date": "2020", "authors": ["Giwan Seo", "Geonhee Lee", "Mi Jeong Kim", "Seung Hwa Baek", "Minsuk Choi", "Keun Bon Ku", "Chang Seop Lee", "Sangmi Jun", "Daeui Park", "Hong Gi Kim", "Seong Jun Kim", "Jeong O. Lee", "Bum Tae Kim", "Edmond Changkyun Park", "Seung Il Kim"], "related_topics": ["Virus", "Antigen", "Detection limit", "Biosensor", "Outbreak", "Immunological Diagnostic Method", "Virology", "Field-effect transistor", "Chemistry", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "3030968929", "references": ["3012099172", "3013893137", "3002539152", "3008696669", "3010449299", "2132260239", "3010604545", "3010338568", "3006961006", "3001195213"], "title": "Detection of air and surface contamination by SARS-CoV-2 in hospital rooms of infected patients.", "abstract": "Understanding the particle size distribution in the air and patterns of environmental contamination of SARS-CoV-2 is essential for infection prevention policies. Here we screen surface and air samples from hospital rooms of COVID-19 patients for SARS-CoV-2 RNA. Environmental sampling is conducted in three airborne infection isolation rooms (AIIRs) in the ICU and 27 AIIRs in the general ward. 245 surface samples are collected. 56.7% of rooms have at least one environmental surface contaminated. High touch surface contamination is shown in ten (66.7%) out of 15 patients in the first week of illness, and three (20%) beyond the first week of illness (p = 0.01, \u03c72 test). Air sampling is performed in three of the 27 AIIRs in the general ward, and detects SARS-CoV-2 PCR-positive particles of sizes &gt;4 \u00b5m and 1-4 \u00b5m in two rooms, despite these rooms having 12 air changes per hour. This warrants further study of the airborne transmission potential of SARS-CoV-2.", "citation_count": "17", "reference_count": "282", "date": "2020", "authors": ["Po Ying Chia", "Kristen Kelli Coleman", "Yian Kim Tan", "Sean Wei Xiang Ong", "Marcus Gum", "Sok Kiang Lau", "Xiao Fang Lim", "Ai Sim Lim", "Stephanie Sutjipto", "Pei Hua Lee", "Barnaby Edward Young", "Donald K Milton", "Gregory C Gray", "Stephan Schuster", "Timothy Barkham", "Partha Pratim De", "Shawn Vasoo", "Monica Chan", "Brenda Sze Peng Ang", "Boon Huan Tan", "Yee-Sin Leo", "Oon-Tek Ng", "Michelle Su Yen Wong", "Kalisvar Marimuthu"], "related_topics": ["Airborne transmission", "Air changes per hour", "Isolation (health care)", "Contamination", "Infection control", "Veterinary medicine", "Medicine", "Coronavirus disease 2019 (COVID-19)", "General ward", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "2786098272", "references": ["2104548316", "2103503670", "2126707939", "2151461700", "1993577573", "2285897784", "2200668708", "2134061616", "2298153446", "2025170735"], "title": "Serological Evidence of Bat SARS-Related Coronavirus Infection in Humans, China", "abstract": "In our previous works, we have reported genetically diverse SARS-related coronaviruses (SARSr-CoV) in a single bat cave, Yunnan province, China, and suggested that some SARSr-CoVs may have high potential to infect humans without the necessity for an intermediate host. In this report, we developed a specific ELISA based on the nucleocapsid protein of a SARSr-CoV strain and detected its antibody in humans who are highly exposed to bat populations. From 218 human serum samples, 6 were positive against the nucleocapsid protein by ELISA and further confirmed by Western blot. For the first time, we demonstrated the SARSr-CoV had spillover to humans, although did not cause clinical diseases.", "citation_count": "15", "reference_count": "171", "date": "2018", "authors": ["Ning Wang", "Shi-Yue Li", "Xing-Lou Yang", "Hui-Min Huang", "Yu-Ji Zhang", "Hua Guo", "Chu-Ming Luo", "Maureen Miller", "Guangjian Zhu", "Aleksei A. Chmura", "Emily Hagan", "Ji-Hua Zhou", "Yun-Zhi Zhang", "Lin-Fa Wang", "Peter Daszak", "Zheng-Li Shi"], "related_topics": ["Coronavirus", "Serology", "Intermediate host", "Antibody", "Virology", "Western blot", "Medical microbiology", "Strain (biology)", "Biology", "Severe acute respiratory syndrome-related coronavirus"]}
{"id": "2021442163", "references": ["2104548316", "2158118659", "1757215199", "2116586125", "2168446943", "2169198329", "2132260239", "2134061616", "2025170735", "1966238900"], "title": "Organ distribution of severe acute respiratory syndrome (SARS) associated coronavirus (SARS-CoV) in SARS patients: implications for pathogenesis and virus transmission pathways.", "abstract": "We previously identified the major pathological changes in the respiratory and immune systems of patients who died of severe acute respiratory syndrome (SARS) but gained little information on the organ distribution of SARS-associated coronavirus (SARS-CoV). In the present study, we used a murine monoclonal antibody specific for SARS-CoV nucleoprotein, and probes specific for a SARS-CoV RNA polymerase gene fragment, for immunohistochemistry and in situ hybridization, respectively, to detect SARS-CoV systematically in tissues from patients who died of SARS. SARS-CoV was found in lung, trachea/bronchus, stomach, small intestine, distal convoluted renal tubule, sweat gland, parathyroid, pituitary, pancreas, adrenal gland, liver and cerebrum, but was not detected in oesophagus, spleen, lymph node, bone marrow, heart, aorta, cerebellum, thyroid, testis, ovary, uterus or muscle. These results suggest that, in addition to the respiratory system, the gastrointestinal tract and other organs with detectable SARS-CoV may also be targets of SARS-CoV infection. The pathological changes in these organs may be caused directly by the cytopathic effect mediated by local replication of the SARS-CoV; or indirectly as a result of systemic responses to respiratory failure or the harmful immune response induced by viral infection. In addition to viral spread through a respiratory route, SARS-CoV in the intestinal tract, kidney and sweat glands may be excreted via faeces, urine and sweat, thereby leading to virus transmission. This study provides important information for understanding the pathogenesis of SARS-CoV infection and sheds light on possible virus transmission pathways. This data will be useful for designing new strategies for prevention and treatment of SARS.", "citation_count": "29", "reference_count": "842", "date": "2004", "authors": ["Yanqing Ding", "Li He", "Qingling Zhang", "Zhongxi Huang", "Xiaoyan Che", "Jinlin Hou", "Huijun Wang", "Hong Shen", "Liwen Qiu", "Zhuguo Li", "Jian Geng", "Junjie Cai", "Huixia Han", "Xin Li", "Wei Kang", "Desheng Weng", "Ping Liang", "Shibo Jiang"], "related_topics": ["Severe acute respiratory syndrome", "Coronavirus", "Respiratory disease", "Lung", "Pathogenesis", "Sweat gland", "Coronaviridae", "Respiratory system", "Pathology", "Immunology", "Biology"]}
{"id": "3028321619", "references": ["3001118548", "3005679569", "3002539152", "3007940623", "3011610993", "3004318991", "3010233963", "3005212621", "3008028633", "3010604545"], "title": "SARS-CoV-2 Infection and the Newborn", "abstract": "Severe Acute Respiratory Syndrome Coronavirus Type 2 (SARS-CoV-2) affects people at all ages and it may be encountered in pregnant women and newborns also. The information about its clinical features, laboratory findings and prognosis in children and newborns is scarce. All the reported cases in pregnant women were in the 2nd or 3rd trimester and only 1% of them developed severe disease. Miscarriages are rare. Materno-fetal transmission of the disease is controversial. Definitive diagnosis can be made by a history of contact with a proven case, fever, pneumonia and gastrointestinal disorder and a Polymerase chain reaction (PCR) test of nasopharyngeal swabs. Lymphopenia as well as liver and renal dysfunctions may be seen. Suspected or proven cases of newborns with symptoms should be quarantined in the neonatal intensive care unit for at least 14 days with standart and droplet isolation precautions. Asymptomatic infants may be quaratined at home. Transport of the neonates should be performed in a dedicated transport incubator and ambulance with isolation precautions. There is no specific treatment for the disease, but hemodynamic stabilization of the infant, respiratory management and other daily care are essential. Drugs against cytokine storm syndrome such as corticosteroids or tocilizumab are under investigation. Routine antibiotics are not recommended. No deaths have been reported so far in the neonatal population. Families and healthcare staff should receive pyschological support. Since the infection is quite new and knowledge is constantly accumulating, following developments and continuous updates are crucial.", "citation_count": "83", "reference_count": "16", "date": "2020", "authors": ["Fahri Oval\u0131"], "related_topics": ["Neonatal intensive care unit", "Gastrointestinal disorder", "Population", "Pneumonia", "Asymptomatic", "Disease", "Pregnancy", "Transmission (medicine)", "Pediatrics", "Medicine"]}
{"id": "3025232310", "references": ["3001118548", "3007643904", "3005679569", "3003217347", "3009314935", "3001465255", "3012756997", "3013985547", "3004280078", "3002108456"], "title": "Humoral Immune Responses in COVID-19 Patients: A Window on the State of the Art.", "abstract": "The novel SARS-CoV-2 is a recently emerging virus causing a human pandemic. A great variety of symptoms associated with COVID-19 disease, ranging from mild to severe symptoms, eventually leading to death. Specific SARS-CoV-2 RT-PCR is the standard method to screen symptomatic people; however, asymptomatic subjects and subjects with undetectable viral load escape from the screening, contributing to viral spread. Currently, the lock down imposed by many governments is an important measure to contain the spread, as there is no specific antiviral therapy or a vaccine and the main treatments are supportive. Therefore, there is urgent need to characterize the virus and the viral-mediated responses, in order to develop specific diagnostic and therapeutic tools to prevent viral transmission and efficiently cure COVID-19 patients. Here, we review the current studies on two viral mediated-responses, specifically the cytokine storm occurring in a subset of patients and the antibody response triggered by the infection. Further studies are needed to explore both the dynamics and the mechanisms of the humoral immune response in COVID-19 patients, in order to guide future vaccine design and antibody-based therapies for the management of the disease.", "citation_count": "75", "reference_count": "68", "date": "2020", "authors": ["Gabriel Siracusano", "Claudia Pastori", "Lucia Lopalco"], "related_topics": ["Viral load", "Disease", "Cytokine storm", "Virus", "Immune system", "Antibody", "Pandemic", "Asymptomatic", "Immunology", "Medicine"]}
{"id": "3027541845", "references": ["3012421327", "3001118548", "3015197879", "3006645647", "3009506062", "3004348779", "2903899730", "3005079553", "3006659024", "3002108456"], "title": "Psycho-Neuroendocrine-Immune Interactions in COVID-19: Potential Impacts on Mental Health.", "abstract": "Coronavirus disease 2019 (COVID-19) is caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). The impacts of the disease may be beyond the respiratory system, also affecting mental health. Several factors may be involved in the association between COVID-19 and psychiatric outcomes, such as fear inherent in the pandemic, adverse effects of treatments, as well as financial stress, and social isolation. Herein we discuss the growing evidence suggesting that the relationship between SARS-CoV-2 and host may also trigger changes in brain and behavior. Based on the similarity of SARS-CoV-2 with other coronaviruses, it is conceivable that changes in endocrine and immune response in the periphery or in the central nervous system may be involved in the association between SARS-CoV-2 infection and impaired mental health. This is likely to be further enhanced, since millions of people worldwide are isolated in quarantine to minimize the transmission of SARS-CoV-2 and social isolation can also lead to neuroendocrine-immune changes. Accordingly, we highlight here the hypothesis that neuroendocrine-immune interactions may be involved in negative impacts of SARS-CoV-2 infection and social isolation on psychiatric issues.", "citation_count": "153", "reference_count": "25", "date": "2020", "authors": ["\u00cdcaro Raony", "Camila Saggioro de Figueiredo", "Pablo Pandolfo", "Elizabeth Giestal-de-Araujo", "Priscilla Oliveira-Silva Bomfim", "Wilson Savino"], "related_topics": ["Disease", "Social isolation", "Mental health", "Transmission (medicine)", "Pandemic", "Immune system", "Adverse effect", "Psychiatry", "Quarantine", "Medicine"]}
{"id": "3013967887", "references": ["3001118548", "3007189521", "3003668884", "3002539152", "3009885589", "3001971765", "3008827533", "3008818676", "3020184843", "3002108456"], "title": "Estimates of the severity of coronavirus disease 2019: a model-based analysis.", "abstract": "Background  In the face of rapidly changing data, a range of case fatality ratio estimates for coronavirus disease 2019 (COVID-19) have been produced that differ substantially in magnitude. We aimed to provide robust estimates, accounting for censoring and ascertainment biases.  Methods  We collected individual-case data for patients who died from COVID-19 in Hubei, mainland China (reported by national and provincial health commissions to Feb 8, 2020), and for cases outside of mainland China (from government or ministry of health websites and media reports for 37 countries, as well as Hong Kong and Macau, until Feb 25, 2020). These individual-case data were used to estimate the time between onset of symptoms and outcome (death or discharge from hospital). We next obtained age-stratified estimates of the case fatality ratio by relating the aggregate distribution of cases to the observed cumulative deaths in China, assuming a constant attack rate by age and adjusting for demography and age-based and location-based under-ascertainment. We also estimated the case fatality ratio from individual line-list data on 1334 cases identified outside of mainland China. Using data on the prevalence of PCR-confirmed cases in international residents repatriated from China, we obtained age-stratified estimates of the infection fatality ratio. Furthermore, data on age-stratified severity in a subset of 3665 cases from China were used to estimate the proportion of infected individuals who are likely to require hospitalisation.  Findings  Using data on 24 deaths that occurred in mainland China and 165 recoveries outside of China, we estimated the mean duration from onset of symptoms to death to be 17\u00b78 days (95% credible interval [CrI] 16\u00b79-19\u00b72) and to hospital discharge to be 24\u00b77 days (22\u00b79-28\u00b71). In all laboratory confirmed and clinically diagnosed cases from mainland China (n=70 117), we estimated a crude case fatality ratio (adjusted for censoring) of 3\u00b767% (95% CrI 3\u00b756-3\u00b780). However, after further adjusting for demography and under-ascertainment, we obtained a best estimate of the case fatality ratio in China of 1\u00b738% (1\u00b723-1\u00b753), with substantially higher ratios in older age groups (0\u00b732% [0\u00b727-0\u00b738] in those aged   Interpretation  These early estimates give an indication of the fatality ratio across the spectrum of COVID-19 disease and show a strong age gradient in risk of death.  Funding  UK Medical Research Council.", "citation_count": "27", "reference_count": "2,545", "date": "2020", "authors": ["Robert Verity", "Lucy C Okell", "Ilaria Dorigatti", "Peter Winskill", "Charles Whittaker", "Natsuko Imai", "Gina Cuomo-Dannenburg", "Hayley Thompson", "Patrick G T Walker", "Han Fu", "Amy Dighe", "Jamie T Griffin", "Marc Baguelin", "Sangeeta Bhatia", "Adhiratha Boonyasiri", "Anne Cori", "Zulma Cucunub\u00e1", "Rich FitzJohn", "Katy Gaythorpe", "Will Green", "Arran Hamlet", "Wes Hinsley", "Daniel Laydon", "Gemma Nedjati-Gilani", "Steven Riley", "Sabine van Elsland", "Erik Volz", "Haowei Wang", "Yuanrong Wang", "Xiaoyue Xi", "Christl A Donnelly", "Azra C Ghani", "Neil M Ferguson"], "related_topics": ["Case fatality rate", "Mainland China", "Incidence (epidemiology)", "Attack rate", "Credible interval", "Censoring (clinical trials)", "Demography", "China", "Young adult", "Medicine"]}
{"id": "3015571324", "references": ["2129542667", "3013893137", "3003668884", "3008696669", "3003573988", "3009885589", "3012756997", "3009983851", "3008294222", "3006961006"], "title": "Temporal dynamics in viral shedding and transmissibility of COVID-19.", "abstract": "We report temporal patterns of viral shedding in 94 patients with laboratory-confirmed COVID-19 and modeled COVID-19 infectiousness profiles from a separate sample of 77 infector-infectee transmission pairs. We observed the highest viral load in throat swabs at the time of symptom onset, and inferred that infectiousness peaked on or before symptom onset. We estimated that 44% (95% confidence interval, 25-69%) of secondary cases were infected during the index cases' presymptomatic stage, in settings with substantial household clustering, active case finding and quarantine outside the home. Disease control measures should be adjusted to account for probable substantial presymptomatic transmission.", "citation_count": "22", "reference_count": "3,484", "date": "2020", "authors": ["Xi He", "Eric H Y Lau", "Peng Wu", "Xilong Deng", "Jian Wang", "Xinxin Hao", "Yiu Chung Lau", "Jessica Y Wong", "Yujuan Guan", "Xinghua Tan", "Xiaoneng Mo", "Yanqing Chen", "Baolin Liao", "Weilie Chen", "Fengyu Hu", "Qing Zhang", "Mingqiu Zhong", "Yanrong Wu", "Lingzhai Zhao", "Fuchun Zhang", "Benjamin J Cowling", "Fang Li", "Gabriel M Leung"], "related_topics": ["Viral load", "Viral shedding", "Serial interval", "Viral Epidemiology", "Confidence interval", "Transmission (mechanics)", "Transmissibility (vibration)", "Throat", "Immunology", "Biology"]}
{"id": "3006642361", "references": ["3001343166", "3003668884", "3002747665", "3003573988", "3001392146", "3002533591", "3004397688", "3004026249", "3002764620", "3023259384"], "title": "The reproductive number of COVID-19 is higher compared to SARS coronavirus.", "abstract": "Teaser: Our review found the average R0 for 2019-nCoV to be 3.28, which exceeds WHO estimates of 1.4 to 2.5.", "citation_count": "14", "reference_count": "2,443", "date": "2020", "authors": ["Ying Liu", "Albert A Gayle", "Annelies Wilder-Smith", "Joacim Rockl\u00f6v"], "related_topics": ["Coronavirus", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Disease transmission", "Severe acute respiratory syndrome coronavirus", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Viral transmission"]}
{"id": "3013594674", "references": ["2122825543", "3012284084", "3001897055", "1951724000", "3003668884", "3003573988", "2097360283", "3008818676", "3008028633", "3004912618"], "title": "The effect of human mobility and control measures on the COVID-19 epidemic in China.", "abstract": "The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.", "citation_count": "38", "reference_count": "1,343", "date": "2020", "authors": ["Moritz U.G. Kraemer", "Chia Hung Yang", "Bernardo Gutierrez", "Chieh Hsi Wu", "Brennan Klein", "David M. Pigott", "Louis du Plessis", "Nuno R. Faria", "Ruoran Li", "William P. Hanage", "John S. Brownstein", "Maylis Layan", "Alessandro Vespignani", "Huaiyu Tian", "Christopher Dye", "Oliver G. Pybus", "Samuel V. Scarpino"], "related_topics": ["China", "Transmission (mechanics)", "Outbreak", "Environmental health", "Psychological intervention", "Epidemiological Monitoring", "Geography", "Coronavirus disease 2019 (COVID-19)", "Demographics", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3008443627", "references": ["2406220407"], "title": "An interactive web-based dashboard to track COVID-19 in real time.", "abstract": "", "citation_count": "1", "reference_count": "4,544", "date": "2020", "authors": ["Ensheng Dong", "Hongru Du", "Lauren Gardner"], "related_topics": ["Dashboard (business)", "Web application", "Track (disk drive)", "Human\u2013computer interaction", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Web browser"]}
{"id": "3012789146", "references": ["3001897055", "3003668884", "3009468976", "3002539152", "3003573988", "3020184843", "3004912618", "3009577418", "3004026249", "3006659024"], "title": "The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study.", "abstract": "BACKGROUND: In December, 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel coronavirus, emerged in Wuhan, China. Since then, the city of Wuhan has taken unprecedented measures in response to the outbreak, including extended school and workplace closures. We aimed to estimate the effects of physical distancing measures on the progression of the COVID-19 epidemic, hoping to provide some insights for the rest of the world. METHODS: To examine how changes in population mixing have affected outbreak progression in Wuhan, we used synthetic location-specific contact patterns in Wuhan and adapted these in the presence of school closures, extended workplace closures, and a reduction in mixing in the general community. Using these matrices and the latest estimates of the epidemiological parameters of the Wuhan outbreak, we simulated the ongoing trajectory of an outbreak in Wuhan using an age-structured susceptible-exposed-infected-removed (SEIR) model for several physical distancing measures. We fitted the latest estimates of epidemic parameters from a transmission model to data on local and internationally exported cases from Wuhan in an age-structured epidemic framework and investigated the age distribution of cases. We also simulated lifting of the control measures by allowing people to return to work in a phased-in way and looked at the effects of returning to work at different stages of the underlying outbreak (at the beginning of March or April). FINDINGS: Our projections show that physical distancing measures were most effective if the staggered return to work was at the beginning of April; this reduced the median number of infections by more than 92% (IQR 66-97) and 24% (13-90) in mid-2020 and end-2020, respectively. There are benefits to sustaining these measures until April in terms of delaying and reducing the height of the peak, median epidemic size at end-2020, and affording health-care systems more time to expand and respond. However, the modelled effects of physical distancing measures vary by the duration of infectiousness and the role school children have in the epidemic. INTERPRETATION: Restrictions on activities in Wuhan, if maintained until April, would probably help to delay the epidemic peak. Our projections suggest that premature and sudden lifting of interventions could lead to an earlier secondary peak, which could be flattened by relaxing the interventions gradually. However, there are limitations to our analysis, including large uncertainties around estimates of R0 and the duration of infectiousness. FUNDING: Bill &amp; Melinda Gates Foundation, National Institute for Health Research, Wellcome Trust, and Health Data Research UK.", "citation_count": "43", "reference_count": "1,361", "date": "2020", "authors": ["Kiesha Prem", "Yang Liu", "Timothy W Russell", "Adam J Kucharski", "Rosalind M Eggo", "Nicholas Davies", "Mark Jit", "Petra Klepac"], "related_topics": ["Psychological intervention", "Outbreak", "Demography", "Social distance", "Distancing", "Epidemiology", "Duration (project management)", "Transmission (mechanics)", "China", "Geography"]}
{"id": "3003637715", "references": ["2954438954", "2775086803", "1998201250", "2470646526", "2789368753", "2799524357", "2903899730", "1971383779", "2134061616", "3001195213"], "title": "Molecular Diagnosis of a Novel Coronavirus (2019-nCoV) Causing an Outbreak of Pneumonia.", "abstract": "BACKGROUND: A novel coronavirus of zoonotic origin (2019-nCoV) has recently been identified in patients with acute respiratory disease. This virus is genetically similar to SARS coronavirus and bat SARS-like coronaviruses. The outbreak was initially detected in Wuhan, a major city of China, but has subsequently been detected in other provinces of China. Travel-associated cases have also been reported in a few other countries. Outbreaks in health care workers indicate human-to-human transmission. Molecular tests for rapid detection of this virus are urgently needed for early identification of infected patients. METHODS: We developed two 1-step quantitative real-time reverse-transcription PCR assays to detect two different regions (ORF1b and N) of the viral genome. The primer and probe sets were designed to react with this novel coronavirus and its closely related viruses, such as SARS coronavirus. These assays were evaluated using a panel of positive and negative controls. In addition, respiratory specimens from two 2019-nCoV-infected patients were tested. RESULTS: Using RNA extracted from cells infected by SARS coronavirus as a positive control, these assays were shown to have a dynamic range of at least seven orders of magnitude (2x10-4-2000 TCID50/reaction). Using DNA plasmids as positive standards, the detection limits of these assays were found to be below 10 copies per reaction. All negative control samples were negative in the assays. Samples from two 2019-nCoV-infected patients were positive in the tests. CONCLUSIONS: The established assays can achieve a rapid detection of 2019n-CoV in human samples, thereby allowing early identification of patients.", "citation_count": "14", "reference_count": "703", "date": "2020", "authors": ["Daniel K W Chu", "Yang Pan", "Samuel M S Cheng", "Kenrie P Y Hui", "Pavithra Krishnan", "Yingzhi Liu", "Daisy Y M Ng", "Carrie K C Wan", "Peng Yang", "Quanyi Wang", "Malik Peiris", "Leo L M Poon"], "related_topics": ["Coronavirus", "Virus", "Outbreak", "Real-time polymerase chain reaction", "Transmission (medicine)", "Virology", "Plasmid", "Pneumonia", "Viral Epidemiology", "Biology"]}
{"id": "3130405932", "references": ["3015734309", "2610936380", "2101828698", "3022440138", "3030259932", "2743706615", "3025500386", "3034408674", "3032742287", "3024451700"], "title": "Surgical site infections: guidance for elective surgery during the SARS-CoV-2 pandemic - international recommendations and clinical experience.", "abstract": "Summary   Background  The COVID-19 pandemic not only had an impact on public life and healthcare facilities in general, but also affected established surgical workflows for elective procedures. The strategy to protect patients and healthcare workers from infection by SARS-CoV-2 in surgical departments has needed step-by-step development. Based on the evaluation of international recommendations and guidelines, as well as personal experiences in a clinical \u201chot spot\u201d and in a 450-bed surgical clinic, an adapted surgical site infection (SSI) prevention checklist was needed to develop concise instructions, which described roles and responsibilities of health care professionals that could be used for wider guidance in pandemic conditions.    Method  Publications of COVID-19-related recommendations and guidelines, produced by health authorities and organizations, such as WHO, US-CDC, ECDC, the American College of Surgery and the Robert Koch Institute, were retrieved, assessed and referenced up to January 31st, 2020. Additionally, clinical personal experiences in Germany were evaluated and considered.    Results  Part 1 of this guidance summarizes the experience of a tertiary care, surgical centre which utilised redundant hospital buildings for immediate spatial separation in a \u201chot spot\u201d COVID-19 area. Part 2 outlines the successful screening and isolation strategy in a surgical clinic in a region of Germany with outbreaks in surrounding medical centres. Part 3 provides the synopsis of personal experiences and international recommendations suggested for implementation during the COVID-19 pandemic.    Conclusion  Understanding of COVID-19, and SARS-CoV-2-related epidemiology, is constantly and rapidly changing, requiring continuous adaptation and re-evaluation of recommendations. Established national and local guidelines for continuation of surgical services and prevention of SSI require ongoing scrutiny and focused implementation. This manuscript presents a core facility checklist to support medical institutions to continue their clinical and surgical work during the COVID-19 pandemic.", "citation_count": "34", "reference_count": "0", "date": "2021", "authors": ["O Assadian", "M Golling", "C M Kr\u00fcger", "D Leaper", "N T Mutters", "B Roth", "A Kramer"], "related_topics": ["Health care", "Checklist", "Isolation (health care)", "Elective surgery", "Pandemic", "Triage", "Medical emergency", "Personal experience", "MEDLINE", "Medicine"]}
{"id": "3144171767", "references": ["1979977611", "1994960483", "2161461039", "64902706", "2980797340", "2047181367", "3034408674", "2067863919", "2049828661", "2026259163"], "title": "A comparative study on convective heat transfer in indoor applications", "abstract": "Abstract   Analytical solutions may not always be applicable in the calculation of the turbulent flow convective heat transfer rate, unlike the radiative and the conductive ones. Therefore, experimental correlations on convective heat transfer coefficient have been developed in enclosures. Convective heat transfer in a cavity is classified as natural, forced, and mixed convection on the basis of the driving forces (buoyant or mechanical). In recent years, there has been an increasing interest in the mixed convection, particularly in cooled ceiling \u2013 displacement ventilation indoor applications. It seems this interest is tending to increase while contamination of viruses and energy saving are growing as health and environmental concerns worldwide. Hence, the reasons behind the interest in mixed convection applications have been investigated along the paper.  This comparative study seeks to explain the progress of convective heat transfer at indoor applications by reviewing mostly experimental correlation studies in time. The mixed convection has not been widely studied experimentally in indoor applications in comparison to natural and forced convection. Therefore, this study is devoted to indicate this gap in the literature on this issue and it includes all convection types with a wide and up-to-date review, descriptions, explanations, and comparisons, as well. Moreover, almost all empirical correlations on the topic are given in tables in detail. It can be concluded that general correlations for mixed convection applications is needed. Correlations related to radiant floor cooling applications are nearly non-existent. Additionally, more experimental studies are required for various split air conditioner cases. These gaps in the literature are unveiled and comparison of applications with various convection types have been made as a first comparative study in the literature.", "citation_count": "63", "reference_count": "0", "date": "2021", "authors": ["Muhammet Camci", "Yakup Karakoyun", "Ozgen Acikgoz", "Ahmet Selim Dalkilic"], "related_topics": ["Combined forced and natural convection", "Convective heat transfer", "Forced convection", "Convection", "Heat transfer coefficient", "Displacement ventilation", "Air conditioning", "Radiative transfer", "Mechanics", "Environmental science"]}
{"id": "3006961006", "references": ["2129542667", "3024919756", "3034411794", "3018339046", "3034408674", "2133748753", "2147166346", "3004239190"], "title": "SARS-CoV-2 Viral Load in Upper Respiratory Specimens of Infected Patients.", "abstract": "SARS-CoV-2 Viral Load in Upper Respiratory Specimens The authors report results of an analysis of nasal and throat swabs from 17 patients in Zhuhai, China, who had received a diagnosis of Covid-19....", "citation_count": "8", "reference_count": "3,839", "date": "2020", "authors": ["Lirong Zou", "Feng Ruan", "Mingxing Huang", "Lijun Liang", "Huitao Huang", "Zhongsi Hong", "Jianxiang Yu", "Min Kang", "Yingchao Song", "Jinyu Xia", "Qianfang Guo", "Tie Song", "Jianfeng He", "Hui Ling Yen", "Malik Peiris", "Jie Wu"], "related_topics": ["Viral load", "Throat", "Respiratory system", "Pneumonia", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Computer assisted tomography", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3004824173", "references": ["3001118548", "2165010366", "1945961678", "3001897055", "2150120685", "3002715510", "3002539152", "1803784511", "2034462612", "3004280078"], "title": "A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version)", "abstract": "In December 2019, a new type viral pneumonia cases occurred in Wuhan, Hubei Province; and then named \u201c2019 novel coronavirus (2019-nCoV)\u201d by the World Health Organization (WHO) on 12 January 2020. For it is a never been experienced respiratory disease before and with infection ability widely and quickly, it attracted the world\u2019s attention but without treatment and control manual. For the request from frontline clinicians and public health professionals of 2019-nCoV infected pneumonia management, an evidence-based guideline urgently needs to be developed. Therefore, we drafted this guideline according to the rapid advice guidelines methodology and general rules of WHO guideline development; we also added the first-hand management data of Zhongnan Hospital of Wuhan University. This guideline includes the guideline methodology, epidemiological characteristics, disease screening and population prevention, diagnosis, treatment and control (including traditional Chinese Medicine), nosocomial infection prevention and control, and disease nursing of the 2019-nCoV. Moreover, we also provide a whole process of a successful treatment case of the severe 2019-nCoV infected pneumonia and experience and lessons of hospital rescue for 2019-nCoV infections. This rapid advice guideline is suitable for the first frontline doctors and nurses, managers of hospitals and healthcare sections, community residents, public health persons, relevant researchers, and all person who are interested in the 2019-nCoV.", "citation_count": "21", "reference_count": "1,445", "date": "2020", "authors": ["Ying-Hui Jin", "Lin Cai", "Zhen-Shun Cheng", "Hong Cheng", "Tong Deng", "Yi-Pin Fan", "Cheng Fang", "Di Huang", "Lu-Qi Huang", "Qiao Huang", "Yong Han", "Bo Hu", "Fen Hu", "Bing-Hui Li", "Yi-Rong Li", "Ke Liang", "Li-Kai Lin", "Li-Sha Luo", "Jing Ma", "Lin-Lu Ma", "Zhi-Yong Peng", "Yun-Bao Pan", "Zhen-Yu Pan", "Xue-Qun Ren", "Hui-Min Sun", "Ying Wang", "Yun-Yun Wang", "Hong Weng", "Chao-Jie Wei", "Dong-Fang Wu", "Jian Xia", "Yong Xiong", "Hai-Bo Xu", "Xiao-Mei Yao", "Yu-Feng Yuan", "Tai-Sheng Ye", "Xiao-Chun Zhang", "Ying-Wen Zhang", "Yin-Gao Zhang", "Hua-Min Zhang", "Yan Zhao", "Ming-Juan Zhao", "Hao Zi", "Xian-Tao Zeng", "Yong-Yan Wang", "Xing-Huan Wang"], "related_topics": ["Guideline", "Mass screening", "Health care", "Public health", "Population", "Nursing care", "Infection control", "Pneumonia", "Medical emergency", "Medicine"]}
{"id": "3009834387", "references": ["3001118548", "3031532178", "1974901207", "3003668884", "3005272159", "2131988685", "3003465021", "3034277126", "3004280078", "3010441732"], "title": "Evidence for gastrointestinal infection of SARS-CoV-2", "abstract": "No abstract available\r\nKeywords: ACE2; Gastrointestinal Infection; Oral-Fecal Transmission; SARS-CoV-2.", "citation_count": "13", "reference_count": "1,697", "date": "2020", "authors": ["Fei Xiao", "Meiwen Tang", "Xiaobin Zheng", "Ye Liu", "Xiaofeng Li", "Hong Shan"], "related_topics": ["Coronavirus", "Betacoronavirus", "Pneumonia", "Transmission (medicine)", "Pandemic", "Virology", "Medicine", "Coronavirus Infections", "Sars virus", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"]}
{"id": "3011863580", "references": ["3008696669", "3002533507", "3006846061", "3008352032"], "title": "Prolonged presence of SARS-CoV-2 viral RNA in faecal samples.", "abstract": "", "citation_count": "4", "reference_count": "912", "date": "2020", "authors": ["Yongjian Wu", "Cheng Guo", "Lantian Tang", "Zhongsi Hong", "Jianhui Zhou", "Xin Dong", "Huan Yin", "Qiang Xiao", "Yanping Tang", "Xiujuan Qu", "Liangjian Kuang", "Xiaomin Fang", "Nischay Mishra", "Jiahai Lu", "Hong Shan", "Guanmin Jiang", "Xi Huang"], "related_topics": ["Viral shedding", "Viral Epidemiology", "RNA", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Viral genetics", "Viral rna"]}
{"id": "3010096538", "references": ["3001118548", "3003668884", "3010930696", "3012379316", "3009885589", "3009912996", "3010233963", "2280404143", "3008028633", "3002108456"], "title": "Features, Evaluation, and Treatment of Coronavirus (COVID-19)", "abstract": "According to the World Health Organization (WHO), viral diseases continue to emerge and represent a serious issue to public health. In the last twenty years, several viral epidemics such as the severe acute respiratory syndrome coronavirus (SARS-CoV) from 2002 to 2003, and H1N1 influenza in 2009, have been recorded. Most recently, the Middle East respiratory syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia in 2012.In a timeline that reaches the present day, an epidemic of cases with unexplained low respiratory infections detected in Wuhan, the largest metropolitan area in China's Hubei province, was first reported to the WHO Country Office in China, on December 31, 2019. Published literature can trace the beginning of symptomatic individuals back to the beginning of December 2019. As they were unable to identify the causative agent, these first cases (n=29) were classified as \"pneumonia of unknown etiology.\" The Chinese Center for Disease Control and Prevention (CDC) and local CDCs organized an intensive outbreak investigation program. The etiology of this illness was attributed to a novel virus belonging to the coronavirus (CoV) family.On February 11, 2020, the WHO Director-General, Dr. Tedros Adhanom Ghebreyesus, announced that the disease caused by this new CoV was a \"COVID-19,\" which is the acronym of \"coronavirus disease 2019\". In the past twenty years, two additional CoVs epidemics have occurred. SARS-CoV provoked a large-scale epidemic beginning in China and involving two dozen countries with approximately 8000 cases and 800 deaths (fatality rate of 9,6%), and the MERS-CoV that began in Saudi Arabia and has approximately 2,500 cases and 800 deaths (fatality rate of 35%) and still causes as sporadic cases.This new virus is very contagious and has quickly spread globally. In a meeting on January 30, 2020, per the International Health Regulations (IHR, 2005), the outbreak was declared by the WHO a Public Health Emergency of International Concern (PHEIC) as it had spread to 18 countries with four countries reporting human-to-human transmission. An additional landmark occurred on February 26, 2020, as the first case of the disease, not imported from China, was recorded in the United States (US). Initially, the new virus was called 2019-nCoV. Subsequently, the task of experts of the International Committee on Taxonomy of Viruses (ICTV) termed it the SARS-CoV-2 virus as it is very similar to the one that caused the SARS outbreak (SARS-CoVs). The CoVs have become the major pathogens of emerging respiratory disease outbreaks. They are a large family of single-stranded RNA viruses (+ssRNA) that can be isolated in different animal species. For reasons yet to be explained, these viruses can cross species barriers and can cause, in humans, illness ranging from the common cold to more severe diseases such as MERS and SARS. Interestingly, these latter viruses have probably originated from bats and then moving into other mammalian hosts \u2014 the Himalayan palm civet for SARS-CoV, and the dromedary camel for MERS-CoV \u2014 before jumping to humans. The dynamics of SARS-Cov-2 are currently unknown, but there is speculation that it also has an animal origin.The potential for these viruses to grow to become a pandemic worldwide represents a serious public health risk. Concerning COVID-19, the WHO raised the threat to the CoV epidemic to the \"very high\" level, on February 28, 2020. On March 11, as the number of COVID-19 cases outside China has increased 13 times and the number of countries involved has tripled with more than 118,000 cases in 114 countries and over 4,000 deaths, WHO declared the COVID-19 a pandemic.World governments are at work to establish countermeasures to stem the devastating effects and it has been estimated that strict shutdowns may have saved 3 million lives across 11 European countries. Health organizations coordinate information flows and issues directives and guidelines to best mitigate the impact of the threat. At the same time, scientists around the world work tirelessly, and information about the transmission mechanisms, the clinical spectrum of disease, new diagnostics, and prevention and therapeutic strategies are rapidly developing. Many uncertainties remain with regard to both the virus-host interaction and the evolution of the pandemic, with specific reference to the times when it will reach its peak.At the moment, the therapeutic strategies to deal with the infection are only supportive, and prevention aimed at reducing transmission in the community is our best weapon. Aggressive isolation measures in China have led to a progressive reduction of cases. From China, the disease spread to Europe. In Italy, in geographic regions of the north, initially, and subsequently throughout the peninsula, political and health authorities have made incredible efforts to contain a shock wave that has severely tested the health system. Afterward, the COVID-19 quickly crossed the ocean and as of June 20, 2020, about 2,282,000 cases (with 121,000 deaths) have been recorded in the US, whereas Brazil with more than 1,000,000 cases and about 50,000 deaths is the most affected state in South America and the second in the world after the US. Although over time the lethality rate (total number of deaths for a given disease in relation to the total number of patients) of COVID-19 has been significantly lower than that of the SARS and MERS epidemics, the transmission of the SARS-CoV-2 virus is much larger than that of the previous viruses, with a much higher total number of deaths. It has been estimated that about one in five individuals worldwide could be at increased risk of severe COVID-19 disease if they become infected, due to underlying health conditions.In the midst of the crisis, the authors have chosen to use the \"Statpearls\" platform because, within the PubMed scenario, it represents a unique tool that may allow them to make updates in real-time. The aim, therefore, is to collect information and scientific evidence and to provide an overview of the topic that will be continuously updated.", "citation_count": "54", "reference_count": "850", "date": "2020", "authors": ["Marco Cascella", "Michael Rajnik", "Arturo Cuomo", "Scott C. Dulebohn", "Raffaela Di Napoli"], "related_topics": ["Middle East respiratory syndrome coronavirus", "Outbreak", "Case fatality rate", "Pandemic", "Public health", "International Health Regulations", "Disease", "Novel virus", "Environmental health", "Geography"]}
{"id": "3006846061", "references": ["2006434809", "1984335993", "2144410942", "2064850047", "3003465021", "3004348779", "2769543984", "3005079553", "3003464757", "3002108456"], "title": "Enteric involvement of coronaviruses: is faecal-oral transmission of SARS-CoV-2 possible?", "abstract": "", "citation_count": "14", "reference_count": "594", "date": "2020", "authors": ["Charleen Yeo", "Sanghvi Kaushal", "Danson Yeo"], "related_topics": ["Viral shedding", "Pneumonia", "Transmission (medicine)", "Virology", "Medicine", "2019-20 coronavirus outbreak", "Coronavirus disease 2019 (COVID-19)", "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)", "Viral transmission"]}
{"id": "3003464757", "references": ["2255897570", "3001118548", "3002539152", "2162496804", "2097706568", "2030966943", "2799524357", "2170933940", "2115555188", "2025170735"], "title": "Genomic characterization of the 2019 novel human-pathogenic coronavirus isolated from a patient with atypical pneumonia after visiting Wuhan.", "abstract": "A mysterious outbreak of atypical pneumonia in late 2019 was traced to a seafood wholesale market in Wuhan of China. Within a few weeks, a novel coronavirus tentatively named as 2019 novel coronavirus (2019-nCoV) was announced by the World Health Organization. We performed bioinformatics analysis on a virus genome from a patient with 2019-nCoV infection and compared it with other related coronavirus genomes. Overall, the genome of 2019-nCoV has 89% nucleotide identity with bat SARS-like-CoVZXC21 and 82% with that of human SARS-CoV. The phylogenetic trees of their orf1a/b, Spike, Envelope, Membrane and Nucleoprotein also clustered closely with those of the bat, civet and human SARS coronaviruses. However, the external subdomain of Spike's receptor binding domain of 2019-nCoV shares only 40% amino acid identity with other SARS-related coronaviruses. Remarkably, its orf3b encodes a completely novel short protein. Furthermore, its new orf8 likely encodes a secreted protein with an alpha-helix, following with a beta-sheet(s) containing six strands. Learning from the roles of civet in SARS and camel in MERS, hunting for the animal source of 2019-nCoV and its more ancestral virus would be important for understanding the origin and evolution of this novel lineage B betacoronavirus. These findings provide the basis for starting further studies on the pathogenesis, and optimizing the design of diagnostic, antiviral and vaccination strategies for this emerging infection.", "citation_count": "24", "reference_count": "1,878", "date": "2020", "authors": ["Jasper Fuk Woo Chan", "Kin Hang Kok", "Zheng Zhu", "Hin Chu", "Kelvin Kai Wang To", "Shuofeng Yuan", "Kwok Yung Yuen"], "related_topics": ["Coronavirus", "Betacoronavirus", "Civet", "Genome", "Virus", "Atypical pneumonia", "Sequence analysis", "Phylogenetics", "Virology", "Biology"]}
{"id": "2148349024", "references": ["2102605133", "2618530766", "1904365287", "2158899491", "2155541015", "2109255472", "3118608800", "1849277567", "2155893237", "2963911037"], "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks", "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).", "citation_count": "32", "reference_count": "667", "date": "2014", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "related_topics": ["Deep learning", "Unsupervised learning", "Semi-supervised learning", "Convolutional neural network", "Feature learning", "Feature (machine learning)", "Discriminative model", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1948751323", "references": ["2102605133", "2618530766", "2022508996", "2118585731", "2109255472", "2156303437", "1903029394", "2168356304", "2962835968", "1507506748"], "title": "Hypercolumns for object segmentation and fine-grained localization", "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.", "citation_count": "42", "reference_count": "1,486", "date": "2015", "authors": ["Bharath Hariharan", "Pablo Arbelaez", "Ross Girshick", "Jitendra Malik"], "related_topics": ["Image segmentation", "Pixel", "Feature (computer vision)", "Feature extraction", "Segmentation", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Artificial neural network", "Computer vision", "Point (geometry)", "Computer science", "Artificial intelligence"]}
{"id": "2167510172", "references": ["2156163116", "2149194912", "2148461049", "1624854622", "2310919327", "2143516773", "1523493493", "1969013163", "2141125852", "2132424367"], "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images", "abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.", "citation_count": "28", "reference_count": "1,463", "date": "2012", "authors": ["Dan Ciresan", "Alessandro Giusti", "Luca M. Gambardella", "J\u00fcrgen Schmidhuber"], "related_topics": ["Pixel", "Ground truth", "Artificial neural network", "Segmentation", "Gradient descent", "Image warping", "Computer vision", "Classifier (UML)", "Computer science", "Artificial intelligence"]}
{"id": "1893585201", "references": ["2102605133", "2618530766", "2136922672", "2155541015", "1959608418", "2100495367", "2099471712", "1849277567", "2155893237", "2963542991"], "title": "Learning to generate chairs with convolutional neural networks", "abstract": "We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.", "citation_count": "35", "reference_count": "688", "date": "2015", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox"], "related_topics": ["Convolutional neural network", "Representation (mathematics)", "Artificial intelligence", "Machine learning", "Similarity (geometry)", "Computer science", "Task (project management)", "Training set"]}
{"id": "1947481528", "references": ["2155893237", "2618530766", "2097117768", "2108598243", "2130942839", "1861492603", "1849277567", "2962835968", "2117539524", "2064675550"], "title": "Long-term recurrent convolutional networks for visual recognition and description", "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or \u201ctemporally deep\u201d, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.", "citation_count": "50", "reference_count": "5,117", "date": "2015", "authors": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Trevor Darrell", "Kate Saenko"], "related_topics": ["Convolutional neural network", "Deep learning", "Visual learning", "Backpropagation", "Natural language", "Pattern recognition", "Term (time)", "Benchmark (computing)", "State (computer science)", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2481240925", "references": ["2102605133", "2618530766", "2097117768", "2108598243", "2031489346", "2153579005", "2310919327", "2250539671", "2962835968", "2117539524"], "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.", "citation_count": "65", "reference_count": "4,269", "date": "2017", "authors": ["Andrej Karpathy", "Li Fei-Fei"], "related_topics": ["Language model", "Convolutional neural network", "Recurrent neural network", "Image segmentation", "Natural language", "Visualization", "Context (language use)", "Natural language processing", "Pattern recognition", "Sentence", "Computer science", "Embedding", "Artificial intelligence"]}
{"id": "2963840672", "references": ["2601564443", "2340897893", "2412782625", "2963881378", "2326925005", "1903029394", "2560023338"], "title": "Multi-Scale Context Aggregation by Dilated Convolutions", "abstract": "Abstract: State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.", "citation_count": "0", "reference_count": "4,848", "date": "2016", "authors": ["Fisher Yu", "Vladlen Koltun"], "related_topics": ["Contextual image classification", "Context (language use)", "Segmentation", "Pattern recognition", "Aggregate (data warehouse)", "Computer science", "Scale (map)", "Resolution (logic)", "Adaptation (computer science)", "Artificial intelligence", "Contextual information"]}
{"id": "2962974533", "references": ["2405756170", "2618530766", "2963981733", "2194775991", "2340897893", "1959608418", "3003301247", "2964121744", "2099471712", "1836465849"], "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization", "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.", "citation_count": "35", "reference_count": "727", "date": "2019", "authors": ["Taesung Park", "Ming-Yu Liu", "Ting-Chun Wang", "Jun-Yan Zhu"], "related_topics": ["Normalization (statistics)", "Normalization (image processing)", "Affine transformation", "Deep learning", "Pattern recognition", "Artificial intelligence", "Computer science", "Image synthesis", "Spatially adaptive"]}
{"id": "3035574324", "references": ["1533861849", "2962879692", "648143168", "2962760235", "1901129140", "2194775991", "1677182931", "3003301247", "2962835968", "2117539524"], "title": "Analyzing and Improving the Image Quality of StyleGAN", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.", "citation_count": "39", "reference_count": "604", "date": "2020", "authors": ["Tero Karras", "Samuli Laine", "Miika Aittala", "Janne Hellsten", "Jaakko Lehtinen", "Timo Aila"], "related_topics": ["Image quality", "Normalization (image processing)", "Image resolution", "Normalization (statistics)", "Unsupervised learning", "Data mining", "Data visualization", "Computer science", "Artificial intelligence"]}
{"id": "2890139949", "references": ["2618530766", "2963684088", "2963373786", "2963073614", "1901129140", "2133665775", "2099471712", "2962793481", "2962835968", "1836465849"], "title": "Generative adversarial network in medical imaging: A review.", "abstract": "Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.", "citation_count": "272", "reference_count": "410", "date": "2019", "authors": ["Xin Yi", "Ekta Walia", "Paul S. Babyn"], "related_topics": ["Generative model", "Adversarial system", "Deep learning", "Test data generation", "Consistency (database systems)", "Machine learning", "Computer science", "Medical imaging", "Segmentation", "Scheme (programming language)", "Artificial intelligence"]}
{"id": "3003301247", "references": ["3162546695", "3040894825", "3046884811", "3102761173", "3118365541", "3156252492", "3085152052", "3109317361", "3132458488"], "title": "A Style-Based Generator Architecture for Generative Adversarial Networks.", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.", "citation_count": "0", "reference_count": "2,077", "date": "2020", "authors": ["Tero Karras", "Samuli Laine", "Timo Aila"], "related_topics": ["Deep learning", "Interpolation", "Generator (mathematics)", "Artificial neural network", "Artificial intelligence", "Computer science", "Architecture", "Identity (object-oriented programming)", "Variation (game tree)", "Generative grammar"]}
{"id": "2985068832", "references": ["2962879692", "2618530766", "2963684088", "2963073614", "1959608418", "3003301247", "2331128040", "2099471712", "2739748921", "2962793481"], "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?", "abstract": "We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.", "citation_count": "37", "reference_count": "184", "date": "2019", "authors": ["Rameen Abdal", "Yipeng Qin", "Peter Wonka"], "related_topics": ["Embedding", "Image editing", "Set (abstract data type)", "Expression (mathematics)", "Face (geometry)", "Pattern recognition", "Image (mathematics)", "Image resolution", "Artificial intelligence", "Artificial neural network", "Computer science"]}
{"id": "2962883549", "references": ["2618530766", "2097117768", "2130942839", "2194775991", "2919115771", "2964121744", "2099471712", "2117539524", "1836465849", "2145339207"], "title": "Deep Learning in Mobile and Wireless Networking: A Survey", "abstract": "The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.", "citation_count": "500", "reference_count": "666", "date": "2019", "authors": ["Chaoyun Zhang", "Paul Patras", "Hamed Haddadi"], "related_topics": ["Mobile device", "Big data", "Wireless network", "Analytics", "Agile management", "User experience design", "Wireless", "Deep learning", "Multimedia", "Computer science", "Artificial intelligence"]}
{"id": "2982763192", "references": ["2519091744", "2963073614", "2963420686", "2962760235", "1901129140", "2963420272", "2331128040", "2607333215", "2963800363", "2117539524"], "title": "Free-Form Image Inpainting With Gated Convolution", "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\url{https://github.com/JiahuiYu/generative_inpainting}.", "citation_count": "51", "reference_count": "450", "date": "2019", "authors": ["Jiahui Yu", "Zhe Lin", "Jimei Yang", "Xiaohui Shen", "Xin Lu", "Thomas Huang"], "related_topics": ["Inpainting", "Convolution", "Channel (digital image)", "Pixel", "Computer vision", "Image (mathematics)", "Code (cryptography)", "Artificial intelligence", "Computer science", "Simple (abstract algebra)"]}
{"id": "2963841322", "references": ["2962974533", "2970415880", "2970315999", "2908541468", "2942074357", "3100398946", "3048510980", "2964074081", "2913399670"], "title": "Video-to-Video Synthesis", "abstract": "We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)", "citation_count": "0", "reference_count": "435", "date": "2018", "authors": ["Ting-Chun Wang", "Ming-Yu Liu", "Jun-Yan Zhu", "Guilin Liu", "Andrew Tao", "Jan Kautz", "Bryan Catanzaro"], "related_topics": ["Segmentation", "Computer vision", "Code (cryptography)", "Set (abstract data type)", "Computer science", "Image (mathematics)", "Sequence", "Function (mathematics)", "Translation (geometry)", "Artificial intelligence"]}
{"id": "2131975293", "references": ["2100830825", "2109722477", "2173213060", "2170616854", "2098935637", "1554944419", "3013264884", "2163961697", "2096125134", "2060204338"], "title": "Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing", "abstract": "We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.", "citation_count": "39", "reference_count": "5,230", "date": "2012", "authors": ["Matei Zaharia", "Mosharaf Chowdhury", "Tathagata Das", "Ankur Dave", "Justin Ma", "Murphy McCauley", "Michael J. Franklin", "Scott Shenker", "Ion Stoica"], "related_topics": ["Distributed memory", "Shared memory", "Computer cluster", "Fault tolerance", "Programming paradigm", "Spark (mathematics)", "Abstraction (linguistics)", "Distributed computing", "State (computer science)", "Computer science"]}
{"id": "2963207607", "references": ["2963143631", "2604763608", "3102564565", "2963857521", "2964082701", "2964253222", "2243397390", "2180612164"], "title": "Explaining and Harnessing Adversarial Examples", "abstract": "Abstract: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "citation_count": "0", "reference_count": "7,585", "date": "2015", "authors": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "related_topics": ["Adversarial machine learning", "Overfitting", "MNIST database", "Test set", "Artificial neural network", "Adversarial system", "Machine learning", "Computer science", "Backdoor", "Nonlinear system", "Artificial intelligence"]}
{"id": "2963382180", "references": ["2963399829", "1026270304", "3102564565", "2964159205", "2963564844", "2963685250"], "title": "Striving for Simplicity: The All Convolutional Net", "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.", "citation_count": "0", "reference_count": "2,234", "date": "2015", "authors": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin A. Riedmiller"], "related_topics": ["Convolutional neural network", "Cognitive neuroscience of visual object recognition", "Pipeline (computing)", "Pattern recognition", "Convolution", "Deconvolution", "Layer (object-oriented design)", "Computer science", "Range (mathematics)", "State (computer science)", "Artificial intelligence"]}
{"id": "1479807131", "references": ["1480376833", "2156909104", "2119821739", "2158714788", "2053186076", "2121947440", "2912934387", "2296319761", "2055043387", "3124955340"], "title": "Semi-Supervised Learning", "abstract": "In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series", "citation_count": "374", "reference_count": "5,483", "date": "2010", "authors": ["Olivier Chapelle", "Bernhard Schlkopf", "Alexander Zien"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Supervised learning", "Transduction (machine learning)", "Co-training", "Machine learning", "Computer science", "Generative grammar", "Computation", "Artificial intelligence", "Manifold regularization"]}
{"id": "2125389028", "references": ["1614298861", "1904365287", "1496559305", "2097117768", "2951446714", "154472438", "2546302380", "2099471712", "2294059674", "2123024445"], "title": "Conditional Generative Adversarial Nets", "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.", "citation_count": "15", "reference_count": "7,210", "date": "2014", "authors": ["Mehdi Mirza", "Simon Osindero"], "related_topics": ["Generative model", "Generative Design", "MNIST database", "Generative grammar", "Artificial intelligence", "Machine learning", "Generator (mathematics)", "Class (computer programming)", "Computer science", "Image translation", "Image (mathematics)"]}
{"id": "2962897886", "references": ["2145094598", "2335728318", "2108677974", "1959608418", "2097268041", "2166851633", "2044758663", "2167433878", "2951446714", "2963173382"], "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation - rules for gradient backpropagation through stochastic variables - and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.", "citation_count": "33", "reference_count": "3,211", "date": "2014", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "related_topics": ["Approximate inference", "Bayesian inference", "Backpropagation", "Inference", "Missing data", "Posterior probability", "Algorithm", "Upper and lower bounds", "Computer science"]}
{"id": "2122457239", "references": ["2154455818", "2110764733", "1479807131", "2111993661", "1560724230", "2145607950", "3118608800", "2293597654", "1566135517", "2104290444"], "title": "Semi-Supervised Learning in Gigantic Image Collections", "abstract": "With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels\" can be manually obtained on a small fraction, \"noisy labels\" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.", "citation_count": "27", "reference_count": "319", "date": "2009", "authors": ["Rob Fergus", "Yair Weiss", "Antonio Torralba"], "related_topics": ["Semi-supervised learning", "Laplacian matrix", "Pattern recognition", "Computer science", "Image (mathematics)", "Normalization (statistics)", "Fraction (mathematics)", "Artificial intelligence"]}
{"id": "1676820704", "references": ["2154642048", "2176028050", "1980501707", "3036751298", "2019363670", "2173629880", "2147800946", "1667614912", "1594031697", "2093717447"], "title": "Solving multiclass learning problems via error-correcting output codes", "abstract": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt; 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.", "citation_count": "23", "reference_count": "3,580", "date": "1994", "authors": ["Thomas G. Dietterich", "Ghulum Bakiri"], "related_topics": ["Multiclass classification", "Multi-task learning", "Semi-supervised learning", "Overfitting", "Pruning (decision trees)", "Generalization", "Backpropagation", "Concept learning", "Theoretical computer science", "Artificial intelligence", "Computer science"]}
{"id": "2158049734", "references": ["2138745909", "2144578941", "2147880316", "2008652694", "2156515921", "2139212933", "2048679005", "2097089247", "2121947440", "2107008379"], "title": "Semi-Supervised Learning for Natural Language", "abstract": "", "citation_count": "74", "reference_count": "408", "date": "2005", "authors": ["Percy Liang"], "related_topics": ["Informatics engineering", "Semi-supervised learning", "Applied science", "Natural language", "Software engineering", "Computer science", "Mechanical engineering"]}
{"id": "2407712691", "references": ["2148029428", "2136922672", "2145038566", "2001141328", "1479807131", "2407712691", "2914746235", "2310919327", "2097308346", "2139427956"], "title": "Deep Learning via Semi-Supervised Embedding", "abstract": "We show how nonlinear embedding algorithms popular for use with \"shallow\" semi-supervised learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This trick provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.", "citation_count": "13", "reference_count": "1,027", "date": "2012", "authors": ["Jason Weston", "Fr\u00e9d\u00e9ric Ratle", "Hossein Mobahi", "Ronan Collobert"], "related_topics": ["Semi-supervised learning", "Kernel method", "Embedding", "Deep learning", "Simple (abstract algebra)", "Layer (object-oriented design)", "Algorithm", "Computer science", "Artificial intelligence", "Nonlinear embedding"]}
{"id": "2136504847", "references": ["2165874743", "2001141328", "2125838338", "1479807131", "2053186076", "2097308346", "2148603752", "2121947440", "1880262756", "2114524997"], "title": "Semi-Supervised Learning Literature Survey", "abstract": "", "citation_count": "157", "reference_count": "4,491", "date": "2005", "authors": ["Xiaojin Zhu"], "related_topics": ["Literature survey", "Semi-supervised learning", "Co-training", "Technical report", "Computer science", "Natural language processing", "Artificial intelligence", "Unlabelled data"]}
{"id": "2150165932", "references": ["1746819321", "1480376833", "2119479037", "2156909104", "740415", "1564947197", "1554663460", "1618905105", "2108995755", "3023786531"], "title": "How to Explain Individual Classification Decisions", "abstract": "After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.", "citation_count": "39", "reference_count": "671", "date": "2010", "authors": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert M\u00fcller"], "related_topics": ["Decision tree", "Classifier (UML)", "Black box", "Kernel method", "Machine learning", "Mathematics", "Artificial intelligence", "Classification methods"]}
{"id": "2206858481", "references": ["2016053056", "639708223", "2109255472", "2194775991", "1677182931", "2964153729"], "title": "Visualizing and Understanding Convolutional Neural Networks", "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "citation_count": "0", "reference_count": "484", "date": "2013", "authors": ["Matthew D Zeiler", "Rob Fergus"], "related_topics": ["Convolutional neural network", "Softmax function", "Classifier (linguistics)", "Benchmark (computing)", "Feature (machine learning)", "Network model", "Visualization", "Machine learning", "Computer science", "Function (engineering)", "Artificial intelligence"]}
{"id": "2120419212", "references": ["1970255615", "2161969291", "1576520375", "2154422044", "2030536784", "2112020727", "2186094539", "2101534792", "2166770390", "1518641734"], "title": "A discriminatively trained, multiscale, deformable part model", "abstract": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.", "citation_count": "24", "reference_count": "3,079", "date": "2008", "authors": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "related_topics": ["Probabilistic latent semantic analysis", "Support vector machine", "Discriminative model", "Pascal (programming language)", "Object detection", "Machine learning", "Pattern recognition", "Histogram", "Computer science", "Grammar", "Artificial intelligence", "Training set"]}
{"id": "2120480077", "references": ["2136922672", "2130325614", "2108598243", "2110798204", "2310919327", "1782590233", "3118608800", "2100495367", "2546302380", "2168231600"], "title": "Building high-level features using large scale unsupervised learning", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "citation_count": "47", "reference_count": "2,558", "date": "2013", "authors": ["Quoc V. Le"], "related_topics": ["Autoencoder", "Unsupervised learning", "Object detection", "Facial recognition system", "Detector", "Pattern recognition", "Pixel", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2158564760", "references": ["1990873664", "2912116903", "1543242897", "42232744", "2159269332", "2153777140", "1527289589", "2029826041", "2015937190", "2145792107"], "title": "Why is image quality assessment so difficult", "abstract": "Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.", "citation_count": "10", "reference_count": "925", "date": "2002", "authors": ["Zhou Wang", "Alan C. Bovik", "Ligang Lu"], "related_topics": ["Image quality", "Subjective video quality", "Image processing", "Human visual system model", "Distortion", "Search engine indexing", "Field (computer science)", "Machine learning", "Function (engineering)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2142276208", "references": ["2148593155", "2129652681", "2058719583", "1931641413", "2166087152", "1592970628", "2147399030", "2053691921", "2117465325", "2103504761"], "title": "A new, fast, and efficient image codec based on set partitioning in hierarchical trees", "abstract": "Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.", "citation_count": "16", "reference_count": "8,658", "date": "1996", "authors": ["A. Said", "W.A. Pearlman"], "related_topics": ["Set partitioning in hierarchical trees", "Data compression", "Entropy encoding", "Image compression", "Transform coding", "Wavelet transform", "Bit plane", "Image processing", "Algorithm", "Mathematics"]}
{"id": "2912116903", "references": ["1990873664", "2118491738", "2056930330", "42232744", "2108657140", "1551978325", "2134774992", "2103232506", "1991605728", "2103504761"], "title": "Image dissimilarity", "abstract": "", "citation_count": "23", "reference_count": "143", "date": "1998", "authors": ["Jean-Bernard Martens", "Lydia Meesters"], "related_topics": ["Image (mathematics)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2115838129", "references": ["1634005169", "2145889472", "98769269", "1500256440", "1548802052", "3017143921", "2134383396", "2137234026", "2140196014", "2798909945"], "title": "Linear transform for simultaneous diagonalization of covariance and perceptual metric matrix in image coding", "abstract": "Two types ofredundancies are contained in images: statistical redundancy and psychovisual redundancy. Image representation techniques for image coding should remove both redundancies in order to obtain good results. In order to establish an appropriate representation, the standard approach to transform coding only considers the statistical redundancy, whereas the psychovisual factors are introduced after the selection ofthe representation as a simple scalar weighting in the transform domain. In this work, we take into account the psychovisual factors in the de8nition of the representation together with the statistical factors, by means of the perceptual metric and the covariance matrix, respectively. In general the ellipsoids described by these matrices are not aligned. Therefore, the optimal basis for image representation should simultaneously diagonalize both matrices. This approach to the basis selection problem has several advantages in the particular application ofimage coding. As the transform domain is Euclidean (by de8nition), the quantizer design is highly simpli8ed and at the same time, the use ofscalar quantizers is truly justi8ed. The proposed representation is compared to covariance-based representations such as the DCT and the KLT or PCA using standard JPEG-like and Max-Lloyd quantizers. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.", "citation_count": "31", "reference_count": "28", "date": "2003", "authors": ["Irene Epifanio", "Jaime Gutierrez", "Jesus Malo"], "related_topics": ["Transform coding", "Covariance matrix", "Covariance", "Image compression", "Discrete cosine transform", "Matrix (mathematics)", "Redundancy (information theory)", "Weighting", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2118217749", "references": ["2161907179", "1976709621", "2119667497", "2133665775", "2101675075", "2158787690", "2046119925", "2129768577"], "title": "JPEG2000 : image compression fundamentals, standards, and practice", "abstract": "This is nothing less than a totally essential reference for engineers and researchers in any field of work that involves the use of compressed imagery. Beginning with a thorough and up-to-date overview of the fundamentals of image compression, the authors move on to provide a complete description of the JPEG2000 standard. They then devote space to the implementation and exploitation of that standard. The final section describes other key image compression systems. This work has specific applications for those involved in the development of software and hardware solutions for multimedia, internet, and medical imaging applications.", "citation_count": "0", "reference_count": "4,977", "date": "2001", "authors": ["David S. Taubman", "Michael W. Marcellin"], "related_topics": ["Image compression", "The Internet", "Software", "JPEG 2000", "JPIP", "Multimedia", "Field (computer science)", "Computer science", "Key (cryptography)", "Work (electrical)"]}
{"id": "2124731682", "references": ["2132984323", "2151693816", "2148593155", "2156447271", "2408227189", "2180838288", "2053691921", "2107790757", "1490632837", "2103504761"], "title": "Image compression via joint statistical characterization in the wavelet domain", "abstract": "We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature.", "citation_count": "39", "reference_count": "785", "date": "1999", "authors": ["R.W. Buccigrossi", "E.P. Simoncelli"], "related_topics": ["Wavelet transform", "Wavelet", "Statistical model", "Image processing", "Image compression", "Data compression", "Markov model", "Transform coding", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2159269332", "references": ["2153777140", "2912116903", "1543242897"], "title": "A universal image quality index", "abstract": "We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality_index/demo.html.", "citation_count": "3", "reference_count": "5,977", "date": "2002", "authors": ["Zhou Wang", "A.C. Bovik"], "related_topics": ["Distortion", "Image quality", "Image processing", "Human visual system model", "Mean squared error", "Signal-to-noise ratio", "Metric (mathematics)", "Luminance", "Algorithm", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2153777140", "references": ["1487065163", "3021180913", "2170745401"], "title": "Image quality measures and their performance", "abstract": "A number of quality measures are evaluated for gray scale image compression. They are all bivariate, exploiting the differences between corresponding pixels in the original and degraded images. It is shown that although some numerical measures correlate well with the observers' response for a given compression technique, they are not reliable for an evaluation across different techniques. A graphical measure called Hosaka plots, however, can be used to appropriately specify not only the amount, but also the type of degradation in reconstructed images.", "citation_count": "3", "reference_count": "1,991", "date": "1995", "authors": ["A.M. Eskicioglu", "P.S. Fisher"], "related_topics": ["Image quality", "Image compression", "Data compression", "Image resolution", "Grayscale", "Pixel", "Transform coding", "Histogram", "Iterative reconstruction", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2053691921", "references": ["2132984323", "2166982406", "1996021349", "2129652681", "1970352604", "2186435531", "2156447271", "2098914003", "2140196014", "2103504761"], "title": "Embedded image coding using zerotrees of wavelet coefficients", "abstract": "The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decisions that distinguish an image from the \"null\" image. Using an embedded coding algorithm, an encoder can terminate the encoding at any point thereby allowing a target rate or target distortion metric to be met exactly. Also, given a bit stream, the decoder can cease decoding at any point in the bit stream and still produce exactly the same image that would have been encoded at the bit rate corresponding to the truncated bit stream. In addition to producing a fully embedded bit stream, the EZW consistently produces compression results that are competitive with virtually all known compression algorithms on standard test images. Yet this performance is achieved with a technique that requires absolutely no training, no pre-stored tables or codebooks, and requires no prior knowledge of the image source. The EZW algorithm is based on four key concepts: (1) a discrete wavelet transform or hierarchical subband decomposition, (2) prediction of the absence of significant information across scales by exploiting the self-similarity inherent in images, (3) entropy-coded successive-approximation quantization, and (4) universal lossless data compression which is achieved via adaptive arithmetic coding. &gt;", "citation_count": "33", "reference_count": "8,503", "date": "1993", "authors": ["J.M. Shapiro"], "related_topics": ["Data compression", "Set partitioning in hierarchical trees", "Lossless compression", "Arithmetic coding", "Signal compression", "Wavelet transform", "Binary image", "Discrete wavelet transform", "Algorithm", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2107790757", "references": ["2132984323", "2166982406", "2170120409", "2109863423", "2118877769", "1627054999", "2098914003", "1996021349", "1991605728", "2103504761"], "title": "Shiftable multiscale transforms", "abstract": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. &gt;", "citation_count": "47", "reference_count": "1,966", "date": "1992", "authors": ["E.P. Simoncelli", "W.T. Freeman", "E.H. Adelson", "D.J. Heeger"], "related_topics": ["Wavelet", "Orthogonal wavelet", "Wavelet transform", "Stereophotography", "Nyquist\u2013Shannon sampling theorem", "Image processing", "Orthogonal transformation", "Signal processing", "Algorithm", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2796426482", "references": ["2964015378", "2964311892", "2211722331", "2963121255", "2560609797", "2187089797", "2546066744", "2964321699", "2558748708", "1920022804"], "title": "FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation", "abstract": "Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet", "citation_count": "58", "reference_count": "384", "date": "2018", "authors": ["Yaoqing Yang", "Chen Feng", "Yiru Shen", "Dong Tian"], "related_topics": ["Point cloud", "Encoder", "Unsupervised learning", "Artificial neural network", "Grid", "Supervised learning", "Graph (abstract data type)", "Autoencoder", "Discriminative model", "Iterative reconstruction", "Algorithm", "Artificial intelligence", "Computer science", "Surface reconstruction"]}
{"id": "2962711740", "references": ["2996604169", "3007332492", "2918342466", "2916106175", "2905224888", "2907492528", "3100078588", "2963465695"], "title": "How Powerful are Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.", "citation_count": "0", "reference_count": "1,774", "date": "2018", "authors": ["Keyulu Xu", "Weihua Hu", "Jure Leskovec", "Stefanie Jegelka"], "related_topics": ["Graph (abstract data type)", "Graph isomorphism", "Feature learning", "Representation (mathematics)", "Discriminative model", "Node (networking)", "Theoretical computer science", "Computer science", "Class (biology)", "Scheme (programming language)"]}
{"id": "3100278010", "references": ["1533861849", "2964015378", "2157881433", "2253995343", "1994389483", "2963323306", "2964121744", "3098649723", "2964321699", "2054141820"], "title": "Neural Graph Collaborative Filtering", "abstract": "Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.", "citation_count": "36", "reference_count": "311", "date": "2019", "authors": ["Xiang Wang", "Xiangnan He", "Meng Wang", "Fuli Feng", "Tat-Seng Chua"], "related_topics": ["Collaborative filtering", "Recommender system", "Graph (abstract data type)", "Deep learning", "Bipartite graph", "Embedding", "Theoretical computer science", "Matrix decomposition", "Computer science", "Artificial intelligence", "Graph"]}
{"id": "2918342466", "references": ["2962767366", "2964015378", "2963121255", "2963858333", "2962711740", "2606780347", "2964321699", "2979750740", "2899771611", "1920022804"], "title": "Fast Graph Representation Learning with PyTorch Geometric", "abstract": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.", "citation_count": "44", "reference_count": "919", "date": "2019", "authors": ["Matthias Fey", "Jan Eric Lenssen"], "related_topics": ["Graph (abstract data type)", "Data structure", "CUDA", "Statistical relational learning", "Deep learning", "Point cloud", "Theoretical computer science", "Computer science", "Manifold", "Artificial intelligence", "Graph"]}
{"id": "2907492528", "references": ["639708223", "2130942839", "2963403868", "2194775991", "2310919327", "2963037989", "2160815625", "2099471712", "2157331557", "2064675550"], "title": "A Comprehensive Survey on Graph Neural Networks", "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial\u2013temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.", "citation_count": "189", "reference_count": "1,586", "date": "2021", "authors": ["Zonghan Wu", "Shirui Pan", "Fengwen Chen", "Guodong Long", "Chengqi Zhang", "Philip S. Yu"], "related_topics": ["Deep learning", "Artificial neural network", "Feature extraction", "Natural language understanding", "Machine learning", "Contextual image classification", "Video processing", "Computer science", "Kernel (linear algebra)", "Task analysis", "Interdependence", "Euclidean space", "Artificial intelligence", "Graph"]}
{"id": "2905224888", "references": ["1614298861", "2964308564", "2963446712", "2963403868", "2194775991", "2919115771", "2310919327", "2157331557", "2963341956", "2117539524"], "title": "Graph Neural Networks: A Review of Methods and Applications", "abstract": "Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.", "citation_count": "257", "reference_count": "1,098", "date": "2018", "authors": ["Jie Zhou", "Ganqu Cui", "Zhengyan Zhang", "Cheng Yang", "Zhiyuan Liu", "Lifeng Wang", "Changcheng Li", "Maosong Sun"], "related_topics": ["Artificial neural network", "Connectionism", "Scene graph", "Network architecture", "Message passing", "Theoretical computer science", "Computer science", "Fixed point", "Categorization", "Graph", "Graph neural networks"]}
{"id": "2963224980", "references": ["1614298861", "2964015378", "2127795553", "2163922914", "3104097132", "2153579005", "2053186076", "1880262756", "2962756421", "2064675550"], "title": "A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications", "abstract": "Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.", "citation_count": "146", "reference_count": "871", "date": "2018", "authors": ["Hongyun Cai", "Vincent W. Zheng", "Kevin Chen-Chuan Chang"], "related_topics": ["Graph embedding", "Graph (abstract data type)", "Graph property", "Graph theory", "Theoretical computer science", "Computer science", "Computation", "Task analysis", "Graph"]}
{"id": "2963184176", "references": ["2963684088", "2963373786", "2963073614", "2097117768", "1861492603", "2153579005", "1959608418", "2964121744", "2117539524", "1836465849"], "title": "Image Generation from Scene Graphs", "abstract": "To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.", "citation_count": "49", "reference_count": "346", "date": "2018", "authors": ["Justin Johnson", "Agrim Gupta", "Li Fei-Fei"], "related_topics": ["Image segmentation", "Visualization", "Natural language", "Segmentation", "Computer vision", "Artificial intelligence", "Computer science", "Graph", "Image generation"]}
{"id": "3100848837", "references": ["2271840356", "2964015378", "2950898568", "2624431344", "3104097132", "2153579005", "2962756421", "2964321699", "2962835968", "2296073425"], "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "abstract": "Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.", "citation_count": "29", "reference_count": "918", "date": "2018", "authors": ["Rex Ying", "Ruining He", "Kaifeng Chen", "Pong Eksombatchai", "William L. Hamilton", "Jure Leskovec"], "related_topics": ["Deep learning", "Convolutional neural network", "Recommender system", "Scalability", "Theoretical computer science", "Computer science", "Random walk", "Artificial intelligence", "Graph"]}
{"id": "2493916176", "references": ["1614298861", "2963012544", "2147152072", "2117130368", "1810943226", "2962784628", "2153579005", "1938755728", "1662133657", "2251012068"], "title": "Enriching Word Vectors with Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.\u00a0Popular models to learn such representations \u00a0ignore the morphology of words, by assigning a distinct vector to each word.\u00a0This is a limitation, especially for languages with large vocabularies and many rare words.\u00a0In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.\u00a0A vector representation is associated to each character n-gram, words being represented as the sum of these representations.\u00a0Our method is fast, allowing to train models on large corpora quickly and allows to compute word representations for words that did not appear in the training data.\u00a0We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.\u00a0By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.", "citation_count": "47", "reference_count": "5,936", "date": "2017", "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov"], "related_topics": ["Word lists by frequency", "Word embedding", "Word (computer architecture)", "Character (mathematics)", "Similarity (psychology)", "Natural language processing", "Analogy", "Speech recognition", "Representation (mathematics)", "Computer science", "Artificial intelligence", "Word representation"]}
{"id": "4919037", "references": ["2335728318", "2963574257", "1904365287", "2134557905", "2310919327", "2131241448", "3118608800", "1665214252", "2141125852", "188867022"], "title": "Regularization of Neural Networks using DropConnect", "abstract": "We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.", "citation_count": "13", "reference_count": "2,335", "date": "2013", "authors": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann Le Cun", "Rob Fergus"], "related_topics": ["Artificial neural network", "Pattern recognition", "Regularization (mathematics)", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2171490498", "references": ["2166049352", "2151693816", "2113606819", "2310919327", "2100495367", "2139427956", "2078204800", "2116064496", "2063978378", "2154332973"], "title": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition", "abstract": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.", "citation_count": "18", "reference_count": "306", "date": "2010", "authors": ["Koray Kavukcuoglu", "Marc'Aurelio Ranzato", "Yann LeCun"], "related_topics": ["Sparse approximation", "3D single-object recognition", "Neural coding", "Cognitive neuroscience of visual object recognition", "Basis function", "Representation (systemics)", "Pattern recognition", "Algorithm", "Image (mathematics)", "Inference", "Computer science", "Artificial intelligence"]}
{"id": "2097268041", "references": ["2134842679", "2096192494", "3140968660", "2108677974", "1810943226", "2310919327", "189596042", "3120740533", "2952509347", "2025768430"], "title": "Deep AutoRegressive Networks", "abstract": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.", "citation_count": "28", "reference_count": "225", "date": "2014", "authors": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "related_topics": ["Autoencoder", "Approximate inference", "Feedforward neural network", "Autoregressive model", "MNIST database", "Estimation theory", "Upper and lower bounds", "Artificial intelligence", "Computer science"]}
{"id": "2166851633", "references": ["1511986666", "2174706414", "2125838338", "1506806321", "1981457167", "2159080219", "2158266063", "1880262756", "1503398984", "2001082470"], "title": "Stochastic variational inference", "abstract": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.", "citation_count": "97", "reference_count": "1,981", "date": "2013", "authors": ["Matthew D. Hoffman", "David M. Blei", "Chong Wang", "John Paisley"], "related_topics": ["Frequentist inference", "Variational message passing", "Fiducial inference", "Predictive inference", "Inference", "Statistical inference", "Latent Dirichlet allocation", "Bayesian inference", "Theoretical computer science", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2951493172", "references": ["2127498532", "2115979064", "1496451467", "2117111086", "2187741934", "2162995719", "2142508340", "1880262756", "2108677974", "2158266063"], "title": "Variational Bayesian Inference with Stochastic Search", "abstract": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.", "citation_count": "10", "reference_count": "307", "date": "2012", "authors": ["John Paisley", "David Blei", "Michael Jordan"], "related_topics": ["Bayesian inference", "Stochastic optimization", "Marginal likelihood", "Upper and lower bounds", "Posterior probability", "Control variates", "Inference", "Bayesian probability", "Applied mathematics", "Logistic regression", "Computer science", "Joint likelihood"]}
{"id": "3104819538", "references": ["1663973292", "1992208280", "2120340025", "1545319692", "1515272691", "1516111018", "2166851633", "2165599843", "3125096521", "114517082"], "title": "Fixed-form variational posterior approximation through stochastic linear regression", "abstract": "textabstractWe propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approxi- mation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several exam- ples illustrate the speed and accuracy of our approximation method in practice.", "citation_count": "37", "reference_count": "186", "date": "2013", "authors": ["Tim Salimans", "David A. Knowles"], "related_topics": ["Exponential family", "Posterior probability", "Stochastic approximation", "Divergence (statistics)", "Approximate inference", "Distribution (mathematics)", "Bayesian probability", "Linear regression", "Applied mathematics", "Mathematics"]}
{"id": "2963173382", "references": ["2128925311", "2146502635", "2120340025", "1959608418", "1516111018", "2166851633", "2951493172", "3104819538", "1663973292"], "title": "Black Box Variational Inference", "abstract": "Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires signicant model-specic analysis. These eorts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a \\black box\" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid dicult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We nd that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.", "citation_count": "20", "reference_count": "796", "date": "2014", "authors": ["Rajesh Ranganath", "Sean Gerrish", "David M. Blei"], "related_topics": ["Inference", "Black box", "Stochastic optimization", "Monte Carlo method", "Sampling (statistics)", "Latent variable", "Algorithm", "Variance (accounting)", "Distribution (mathematics)", "Computer science"]}
{"id": "2161381512", "references": ["2102605133", "2618530766", "2108598243", "2161969291", "2151103935", "2031489346", "2310919327", "2168356304", "1849277567", "2963542991"], "title": "Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The suc- cess of CNNs is attributed to their ability to learn rich mid- level image representations as opposed to hand-designed low-level features used in other image classification meth- ods. Learning CNNs, however, amounts to estimating mil- lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi- ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep- resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.", "citation_count": "53", "reference_count": "2,979", "date": "2014", "authors": ["Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "related_topics": ["Convolutional neural network", "Contextual image classification", "Pascal (programming language)", "Machine learning", "Artificial intelligence", "Computer science", "Training set"]}
{"id": "2152826865", "references": ["2152826865", "2103876808", "2129150631", "2408227189", "2095757522", "2133001582", "2038952578", "2293264518", "2138451337", "2159173611"], "title": "Active appearance models", "abstract": "We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.", "citation_count": "28", "reference_count": "10,287", "date": "2001", "authors": ["T.F. Cootes", "G.J. Edwards", "C.J. Taylor"], "related_topics": ["Active appearance model", "Active shape model", "Point distribution model", "Statistical model", "Matching (statistics)", "Iterative method", "Image segmentation", "Blossom algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "1576520375", "references": ["2172000360", "2161969291", "2153635508", "2108646579", "2166706824", "2168356304", "1880262756", "2120419212", "1964357740"], "title": "Making large scale SVM learning practical", "abstract": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.", "citation_count": "0", "reference_count": "15,657", "date": "1999", "authors": ["Thorsten Joachims"], "related_topics": ["Ranking SVM", "Support vector machine", "Quadratic programming", "Machine learning", "Artificial intelligence", "Computer science", "Quadratic equation", "Constraint (information theory)", "Scale (chemistry)"]}
{"id": "2145072179", "references": ["2151103935", "2154422044", "2098693229", "2177274842", "2119747362", "2111308925", "1541642243", "1902027874", "2148694408", "2124386111"], "title": "PCA-SIFT: a more distinctive representation for local image descriptors", "abstract": "Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.", "citation_count": "17", "reference_count": "4,922", "date": "2004", "authors": ["Yan Ke", "R. Sukthankar"], "related_topics": ["Principal curvature-based region detector", "Feature detection (computer vision)", "GLOH", "Feature (computer vision)", "Image gradient", "Feature extraction", "Scale-invariant feature transform", "Image processing", "Image retrieval", "Image registration", "Histogram", "Principal component analysis", "Pattern recognition", "Computer vision", "Normalization (statistics)", "Computer science", "Artificial intelligence"]}
{"id": "2115763357", "references": ["2132984323", "2137346077", "2124351082", "2125848778", "1676612073", "2087347434", "2104671481", "2030989822", "2056695679", "2159173611"], "title": "A general framework for object detection", "abstract": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.", "citation_count": "16", "reference_count": "2,123", "date": "1998", "authors": ["C.P. Papageorgiou", "M. Oren", "T. Poggio"], "related_topics": ["Object-class detection", "Object detection", "Viola\u2013Jones object detection framework", "Face detection", "Representation (systemics)", "Cognitive neuroscience of visual object recognition", "Wavelet", "Segmentation", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2145889472", "references": ["1993845689", "2911607583", "2122925692", "2120838001", "2117731089", "2167034998", "2106884367", "2108384452", "2180838288", "1914401667"], "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "abstract": "The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.", "citation_count": "20", "reference_count": "6,557", "date": "1996", "authors": ["Bruno A. Olshausen", "David J. Field"], "related_topics": ["Efficient coding hypothesis", "Simple cell", "Sparse image", "Unsupervised learning", "Receptive field", "Visual cortex", "Wavelet transform", "Representation (mathematics)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2122922389", "references": ["2166049352", "2147152072", "2001141328", "2113606819", "2053186076", "2100495367", "2135046866", "1880262756", "2162915993", "2063978378"], "title": "Self-taught learning: transfer learning from unlabeled data", "abstract": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.", "citation_count": "25", "reference_count": "1,812", "date": "2007", "authors": ["Rajat Raina", "Alexis Battle", "Honglak Lee", "Benjamin Packer", "Andrew Y. Ng"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Multi-task learning", "Active learning (machine learning)", "Instance-based learning", "Online machine learning", "Stability (learning theory)", "Inductive transfer", "Transfer of learning", "Fisher kernel", "Support vector machine", "Machine learning", "Computer science", "Artificial intelligence", "Generalization error"]}
{"id": "2139427956", "references": ["2136922672", "2166049352", "2105464873", "2151103935", "2110798204", "2310919327", "1624854622", "2168002178", "2162915993", "2172174689"], "title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "abstract": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.", "citation_count": "20", "reference_count": "1,313", "date": "2007", "authors": ["M.A. Ranzato", "Fu Jie Huang", "Y.-L. Boureau", "Yann LeCun"], "related_topics": ["Unsupervised learning", "Supervised learning", "MNIST database", "Feature extraction", "Caltech 101", "Convolutional Deep Belief Networks", "Invariant (mathematics)", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Object detection", "Sigmoid function", "Computer science", "Artificial intelligence"]}
{"id": "1605688901", "references": ["2112076978", "2167277498", "2152761983", "1966280301", "2073738917", "2084812512", "1562197959", "1850527962", "2976840617", "2912934387"], "title": "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "abstract": "Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \u201cbase\u201d learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.", "citation_count": "14", "reference_count": "3,140", "date": "2000", "authors": ["Thomas G. Dietterich"], "related_topics": ["Gradient boosting", "BrownBoost", "Ensembles of classifiers", "Boosting (machine learning)", "Random forest", "LPBoost", "Ensemble learning", "Decision tree", "Machine learning", "Data mining", "Mathematics", "Artificial intelligence"]}
{"id": "2113242816", "references": ["2112076978", "2125055259", "2156909104", "2149706766", "2101522199", "1966280301", "2102734279", "1930624869", "1594031697", "2912934387"], "title": "The random subspace method for constructing decision forests", "abstract": "Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.", "citation_count": "30", "reference_count": "6,236", "date": "1998", "authors": ["Tin Kam Ho"], "related_topics": ["Random forest", "Random subspace method", "Decision tree", "Overfitting", "Ensembles of classifiers", "Binary tree", "Support vector machine", "Feature vector", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2112076978", "references": ["2137291015", "2125055259", "1966280301", "2132166479", "1670263352", "2070534370", "2093717447", "1504694836", "2912934387", "3124955340"], "title": "Experiments with a new boosting algorithm", "abstract": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.", "citation_count": "19", "reference_count": "10,889", "date": "1996", "authors": ["Yoav Freund", "Robert E. Schapire"], "related_topics": ["BrownBoost", "Boosting (machine learning)", "Gradient boosting", "AdaBoost", "LPBoost", "LogitBoost", "Ensembles of classifiers", "Stability (learning theory)", "Cascading classifiers", "Machine learning", "Pattern recognition", "Computer science", "Artificial intelligence", "Boosting methods for object categorization", "Generalization error"]}
{"id": "2067885219", "references": ["2075647286", "3104887532", "1966701961", "2032210760", "2911964244", "2168020168", "1540007258", "2053463056", "2155806188", "2167917621"], "title": "Arcing classifier (with discussion and a rejoinder by the author)", "abstract": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym \u201carcing\u201d) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.", "citation_count": "0", "reference_count": "1,939", "date": "1998", "authors": ["Leo Breiman"], "related_topics": ["Test set", "Boosting (machine learning)", "Weighted voting", "BrownBoost", "Resampling", "LPBoost", "Ensemble learning", "Artificial neural network", "Pattern recognition", "Statistics", "Mathematics", "Artificial intelligence"]}
{"id": "2152761983", "references": ["2112076978", "1680392829", "2125055259", "2140785063", "1975846642", "1995945562", "2084812512", "3017143921", "2912934387", "3124955340"], "title": "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants", "abstract": "Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only \u201chard\u201d areas but also outliers and noise.", "citation_count": "50", "reference_count": "2,325", "date": "1999", "authors": ["Eric Bauer", "Ron Kohavi"], "related_topics": ["BrownBoost", "AdaBoost", "Boosting (machine learning)", "LPBoost", "Ensembles of classifiers", "Mean squared error", "Statistical classification", "Decision tree", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2099968818", "references": ["2112076978", "2125055259", "1553313034", "2119821739", "2152761983", "1966280301", "2266946488", "2084812512", "1504694836", "3124955340"], "title": "Boosting in the limit: maximizing the margin of learned ensembles", "abstract": "The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.", "citation_count": "17", "reference_count": "365", "date": "1998", "authors": ["Adam J. Grove", "Dale Schuurmans"], "related_topics": ["BrownBoost", "LPBoost", "Boosting (machine learning)", "AdaBoost", "Linear programming", "Machine learning", "Algorithm", "Computer science", "Classifier (UML)", "Artificial intelligence", "Generalization error", "Training set"]}
{"id": "1975846642", "references": ["1605688901", "2112076978", "2156909104", "2119821739", "2032210760", "2084812512", "2087347434", "1594031697", "2912934387", "3124955340"], "title": "Boosting the margin: a new explanation for the effectiveness of voting methods", "abstract": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.", "citation_count": "36", "reference_count": "3,542", "date": "1998", "authors": ["Robert E. Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "related_topics": ["Margin classifier", "BrownBoost", "LPBoost", "Boosting (machine learning)", "Support vector machine", "Voting", "Classification rule", "LogitBoost", "Algorithm", "Statistics", "Mathematics"]}
{"id": "2120240539", "references": ["2149706766", "1594031697", "2099111195", "1676820704", "2168228682", "2154579312", "2165758113", "2076118331", "2101522199", "2912934387"], "title": "Shape quantization and recognition with randomized trees", "abstract": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred L AT E X symbols. Stateof-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on L AT E X symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context.", "citation_count": "49", "reference_count": "1,371", "date": "1997", "authors": ["Yali Amit", "Donald Geman"], "related_topics": ["Feature vector", "Tree-depth", "Decision tree", "Posterior probability", "Artificial neural network", "Classifier (UML)", "Partially ordered set", "A priori and a posteriori", "Algorithm", "Mathematics"]}
{"id": "1580948147", "references": ["2112076978", "2172195373", "2067885219", "2102201073", "2073738917", "2912934387", "2076118331", "1594031697", "1605688901", "3124955340"], "title": "Randomizing Outputs to Increase Prediction Accuracy", "abstract": "Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.", "citation_count": "14", "reference_count": "301", "date": "2000", "authors": ["Leo Breiman"], "related_topics": ["Boosting (machine learning)", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "3035524453", "references": ["2102605133", "639708223", "2108598243", "1536680647", "2194775991", "1903029394", "2099471712", "2963341956", "2962835968", "1836465849"], "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.", "citation_count": "58", "reference_count": "1,071", "date": "2020", "authors": ["Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick"], "related_topics": ["Unsupervised learning", "Feature learning", "Visualization", "Natural language processing", "Segmentation", "Artificial intelligence", "Pascal (programming language)", "Encoder", "Task analysis", "Computer science"]}
{"id": "602397586", "references": ["2102605133", "2618530766", "2016053056", "2155541015", "2145287260", "2156303437", "1849277567", "2062118960", "2113325037", "2963542991"], "title": "Flowing ConvNets for Human Pose Estimation in Videos", "abstract": "The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).", "citation_count": "37", "reference_count": "451", "date": "2015", "authors": ["Tomas Pfister", "James Charles", "Andrew Zisserman"], "related_topics": ["Pose", "Optical flow", "Graphical model", "Margin (machine learning)", "Benchmark (computing)", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2619697695", "references": ["2618530766", "343636949", "2123024445", "2183341477", "2963420272", "2964121744", "2326925005", "2962835968", "2117539524", "1836465849"], "title": "Look, Listen and Learn", "abstract": "We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself \u2013 the correspondence between the visual and the audio streams, and we introduce a novel \u201cAudio-Visual Correspondence\u201d learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.", "citation_count": "34", "reference_count": "399", "date": "2017", "authors": ["Relja Arandjelovic", "Andrew Zisserman"], "related_topics": ["Task (project management)", "Cognitive neuroscience of visual object recognition", "Set (psychology)", "Visualization", "Feature extraction", "Active listening", "Semantics", "Speech recognition", "Computer science"]}
{"id": "3101577715", "references": ["639708223", "2976600002", "2810605094", "2808631503", "2961568334"], "title": "The VIA Annotation Software for Images, Audio and Video", "abstract": "In this paper, we introduce a simple and standalone manual annotation tool for images, audio and video: the VGG Image Annotator (VIA). This is a light weight, standalone and offline software package that does not require any installation or setup and runs solely in a web browser. The VIA software allows human annotators to define and describe spatial regions in images or video frames, and temporal segments in audio or video. These manual annotations can be exported to plain text data formats such as JSON and CSV and therefore are amenable to further processing by other software tools. VIA also supports collaborative annotation of a large dataset by a group of human annotators. The BSD open source license of this software allows it to be used in any academic project or commercial application.", "citation_count": "5", "reference_count": "270", "date": "2019", "authors": ["Abhishek Dutta", "Andrew Zisserman"], "related_topics": ["Software", "Automatic image annotation", "JSON", "Plain text", "Annotation", "Information retrieval", "Computer science", "SIMPLE (military communications protocol)"]}
{"id": "219040644", "references": ["2102605133", "2618530766", "2154889144", "2163922914", "2161969291", "2151103935", "2031489346", "2100495367", "1677409904", "2155893237"], "title": "Unsupervised Learning of Visual Representations Using Videos", "abstract": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.", "citation_count": "55", "reference_count": "861", "date": "2015", "authors": ["Xiaolong Wang", "Abhinav Gupta"], "related_topics": ["Unsupervised learning", "Convolutional neural network", "Feature vector", "Minimum bounding box", "Visualization", "Eye tracking", "Cluster analysis", "Pattern recognition", "Object (computer science)", "Machine learning", "Ranking", "Computer science", "Artificial intelligence"]}
{"id": "2962865004", "references": ["2520707650", "2295107390", "1901129140", "2221409856", "2194775991", "2963420272", "219040644", "2737258237", "343636949", "2184188583"], "title": "The Sound of Pixels", "abstract": "We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.", "citation_count": "44", "reference_count": "250", "date": "2018", "authors": ["Hang Zhao", "Chuang Gan", "Andrew Rouditchenko", "Carl Vondrick", "Josh H. McDermott", "Antonio Torralba"], "related_topics": ["Pixel", "Source separation", "Computer vision", "Synchronization (computer science)", "Set (abstract data type)", "Sound (medical instrument)", "Computer science", "Volume (computing)", "Image (mathematics)", "Artificial intelligence"]}
{"id": "2808631503", "references": ["2325939864", "2963839617", "2963026686", "2145287260", "1963882359", "2096733369", "2194775991", "2150769028", "3106250896", "2963173190"], "title": "VoxCeleb2: Deep Speaker Recognition.", "abstract": "The objective of this paper is speaker recognition under noisy and unconstrained conditions. \r\nWe make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. \r\nSecond, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.", "citation_count": "30", "reference_count": "667", "date": "2018", "authors": ["Joon Son Chung", "Arsha Nagrani", "Andrew Zisserman"], "related_topics": ["Speaker recognition", "Convolutional neural network", "Margin (machine learning)", "Speech recognition", "Benchmark (computing)", "Pipeline (software)", "Computer science", "Key (cryptography)"]}
{"id": "343636949", "references": ["2102605133", "2618530766", "2155893237", "2136922672", "2108598243", "2153579005", "1903029394", "2168356304", "2962835968", "1836465849"], "title": "Unsupervised Visual Representation Learning by Context Prediction", "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.", "citation_count": "55", "reference_count": "1,279", "date": "2015", "authors": ["Carl Doersch", "Abhinav Gupta", "Alexei A. Efros"], "related_topics": ["Feature learning", "Spatial contextual awareness", "Artificial neural network", "Visualization", "Pattern recognition", "Machine learning", "Pascal (programming language)", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "1821462560", "references": ["2618530766", "1904365287", "1534477342", "2402040300", "2294370754", "2150884987", "2095705004", "2160815625", "2168231600"], "title": "Distilling the Knowledge in a Neural Network", "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.", "citation_count": "9", "reference_count": "12,360", "date": "2015", "authors": ["Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean"], "related_topics": ["Ensemble learning", "MNIST database", "Artificial neural network", "Acoustic model", "Machine learning", "Artificial intelligence", "Computer science", "Simple (abstract algebra)", "Type (model theory)", "Mixture of experts"]}
{"id": "1544827683", "references": ["2120615054", "2147527908", "2964308564", "2158899491", "1793121960", "2130942839", "2144499799", "2125436846", "2962741254", "2064675550"], "title": "Teaching machines to read and comprehend", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "citation_count": "19", "reference_count": "2,016", "date": "2015", "authors": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "related_topics": ["Reading comprehension", "Natural language", "Class (computer programming)", "Natural language processing", "Computer science", "Test (assessment)", "Scale (chemistry)", "Artificial intelligence"]}
{"id": "3161838454", "references": ["2108598243", "2963403868", "1901129140", "2340897893", "2412782625", "2095705004", "2963881378", "1903029394", "2963341956", "1836465849"], "title": "Segmenter: Transformer for Semantic Segmentation.", "abstract": "Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution based approaches, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.", "citation_count": "51", "reference_count": "0", "date": "2021", "authors": ["Robin A. M. Strudel", "Ricardo Garcia", "Ivan Laptev", "Cordelia Schmid"], "related_topics": ["Image segmentation", "Transformer (machine learning model)", "Contextual image classification", "Segmentation", "Context (language use)", "Pattern recognition", "Convolution", "Leverage (statistics)", "Image (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "3126009523", "references": ["2507009361", "2108598243", "2770804203", "2963403868", "2194775991", "2963341924", "2601450892", "2963524571", "2126579184", "2990503944"], "title": "Temporal-Relational CrossTransformers for Few-Shot Action Recognition", "abstract": "We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared. \r\nOur proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.", "citation_count": "30", "reference_count": "1", "date": "2021", "authors": ["Toby Perrett", "Alessandro Masullo", "Tilo Burghardt", "Majid Mirmehdi", "Dima Damen"], "related_topics": ["Tuple", "Set (abstract data type)", "Margin (machine learning)", "Frame (networking)", "Matching (graph theory)", "Class (set theory)", "Pattern recognition", "Construct (python library)", "Computer science", "Shot (filmmaking)", "Artificial intelligence"]}
{"id": "3143315506", "references": ["2165698076", "2097117768", "2108598243", "3034978746", "2963403868", "1861492603", "2963341924", "3118608800", "2601450892"], "title": "Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark.", "abstract": "Meta and transfer learning are two successful families of approaches to few-shot learning. Despite highly related goals, state-of-the-art advances in each family are measured largely in isolation of each other. As a result of diverging evaluation norms, a direct or thorough comparison of different approaches is challenging. To bridge this gap, we perform a cross-family study of the best transfer and meta learners on both a large-scale meta-learning benchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual Task Adaptation Benchmark, VTAB). We find that, on average, large-scale transfer methods (Big Transfer, BiT) outperform competing approaches on MD, even when trained only on ImageNet. In contrast, meta-learning approaches struggle to compete on VTAB when trained and validated on MD. However, BiT is not without limitations, and pushing for scale does not improve performance on highly out-of-distribution MD tasks. In performing this study, we reveal a number of discrepancies in evaluation norms and study some of these in light of the performance gap. We hope that this work facilitates sharing of insights from each community, and accelerates progress on few-shot learning.", "citation_count": "48", "reference_count": "0", "date": "2021", "authors": ["Vincent Dumoulin", "Neil Houlsby", "Utku Evci", "Xiaohua Zhai", "Ross Goroshin", "Sylvain Gelly", "Hugo Larochelle"], "related_topics": ["Meta learning (computer science)", "Transfer of learning", "Benchmark (computing)", "Machine learning", "Computer science", "Contrast (statistics)", "Adaptation (computer science)", "Isolation (database systems)", "Scale (social sciences)", "Bridge (nautical)", "Artificial intelligence"]}
{"id": "3159056052", "references": ["2964105864", "2194775991", "2595142274", "2963341924", "2601450892", "2753160622", "1797268635", "2963741406", "1990937109", "2035379092"], "title": "Few-Shot Classification with Feature Map Reconstruction Networks.", "abstract": "In this paper we reformulate few-shot classification as a reconstruction problem in latent space. The ability of the network to reconstruct a query feature map from support features of a given class predicts membership of the query in that class. We introduce a novel mechanism for few-shot classification by regressing directly from support features to query features in closed form, without introducing any new modules or large-scale learnable parameters. The resulting Feature Map Reconstruction Networks are both more performant and computationally efficient than previous approaches. We demonstrate consistent and substantial accuracy gains on four fine-grained benchmarks with varying neural architectures. Our model is also competitive on the non-fine-grained mini-ImageNet and tiered-ImageNet benchmarks with minimal bells and whistles.", "citation_count": "37", "reference_count": "0", "date": "2021", "authors": ["Davis Wertheimer", "Luming Tang", "Bharath Hariharan"], "related_topics": ["Feature (computer vision)", "Pattern recognition", "Class (biology)", "Computer science", "Space (commercial competition)", "Shot (filmmaking)", "Artificial intelligence", "Reconstruction problem"]}
{"id": "3136670918", "references": ["1861492603", "2194775991", "2310919327", "1821462560", "3118608800", "2963341924", "2601450892", "2970971581", "2117539524", "2964118293"], "title": "Universal Representation Learning from Multiple Domains for Few-shot Classification.", "abstract": "In this paper, we look at the problem of few-shot classification that aims to learn a classifier for previously unseen classes and domains from few labeled samples. Recent methods use adaptation networks for aligning their features to new domains or select the relevant features from multiple domain-specific feature extractors. In this work, we propose to learn a single set of universal deep representations by distilling knowledge of multiple separately trained networks after co-aligning their features with the help of adapters and centered kernel alignment. We show that the universal representations can be further refined for previously unseen domains by an efficient adaptation step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient. Our code will be available at this https URL", "citation_count": "53", "reference_count": "0", "date": "2021", "authors": ["Wei-Hong Li", "Xialei Liu", "Hakan Bilen"], "related_topics": ["Feature learning", "Feature (machine learning)", "Classifier (UML)", "Machine learning", "Set (abstract data type)", "Benchmark (computing)", "Computer science", "Code (cryptography)", "Adaptation (computer science)", "Distance education", "Artificial intelligence"]}
{"id": "3122301478", "references": ["2187089797", "3034978746", "3035524453", "2096733369", "2194775991", "1677182931", "2963341924", "2601450892", "3091905774", "2117539524"], "title": "Revisiting Contrastive Learning for Few-Shot Classification.", "abstract": "Instance discrimination based contrastive learning has emerged as a leading approach for self-supervised learning of visual representations. Yet, its generalization to novel tasks remains elusive when compared to representations learned with supervision, especially in the few-shot setting. We demonstrate how one can incorporate supervision in the instance discrimination based contrastive self-supervised learning framework to learn representations that generalize better to novel tasks. We call our approach CIDS (Contrastive Instance Discrimination with Supervision). CIDS performs favorably compared to existing algorithms on popular few-shot benchmarks like Mini-ImageNet or Tiered-ImageNet. We also propose a novel model selection algorithm that can be used in conjunction with a universal embedding trained using CIDS to outperform state-of-the-art algorithms on the challenging Meta-Dataset benchmark.", "citation_count": "48", "reference_count": "1", "date": "2021", "authors": ["Orchid Majumder", "Avinash Ravichandran", "Subhransu Maji", "Marzia Polito", "Rahul Bhotika", "Stefano Soatto"], "related_topics": ["Benchmark (computing)", "Generalization", "Model selection", "Machine learning", "Embedding", "Conjunction (grammar)", "Computer science", "Shot (filmmaking)", "Artificial intelligence"]}
{"id": "3139264293", "references": ["2618530766", "639708223", "2108598243", "2161969291", "2963403868", "2194775991", "2156303437", "2964121744", "2963341956", "1522734439"], "title": "Space-Time Crop &amp; Attend: Improving Cross-modal Video Representation Learning.", "abstract": "The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-time Crop &amp; Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.", "citation_count": "141", "reference_count": "0", "date": "2021", "authors": ["Mandela Patrick", "Yuki Markus Asano", "Bernie Huang", "Ishan Misra", "Florian Metze", "Jo\u00e3o F. Henriques", "Andrea Vedaldi"], "related_topics": ["Feature learning", "Feature vector", "Feature (computer vision)", "Pooling", "Machine learning", "Transformer (machine learning model)", "Computer science", "Modal", "Space (commercial competition)", "Scale (descriptive set theory)", "Artificial intelligence"]}
{"id": "3139613640", "references": ["639708223", "2108598243", "2108745803", "1861492603", "2194775991", "2963341924", "2798836702", "2601450892", "2962835968", "2964069537"], "title": "Few-shot Weakly-Supervised Object Detection via Directional Statistics.", "abstract": "Detecting novel objects from few examples has become an emerging topic in computer vision recently. However, these methods need fully annotated training images to learn new object categories which limits their applicability in real world scenarios such as field robotics. In this work, we propose a probabilistic multiple instance learning approach for few-shot Common Object Localization (COL) and few-shot Weakly Supervised Object Detection (WSOD). In these tasks, only image-level labels, which are much cheaper to acquire, are available. We find that operating on features extracted from the last layer of a pre-trained Faster-RCNN is more effective compared to previous episodic learning based few-shot COL methods. Our model simultaneously learns the distribution of the novel objects and localizes them via expectation-maximization steps. As a probabilistic model, we employ von Mises-Fisher (vMF) distribution which captures the semantic information better than Gaussian distribution when applied to the pre-trained embedding space. When the novel objects are localized, we utilize them to learn a linear appearance model to detect novel classes in new images. Our extensive experiments show that the proposed method, despite being simple, outperforms strong baselines in few-shot COL and WSOD, as well as large-scale WSOD tasks.", "citation_count": "41", "reference_count": "0", "date": "2021", "authors": ["Amirreza Shaban", "Amir Rahimi", "Thalaiyasingam Ajanthan", "Byron Boots", "Richard I. Hartley"], "related_topics": ["Object detection", "Active appearance model", "Object (computer science)", "Probabilistic logic", "Field (computer science)", "Statistical model", "Directional statistics", "Pattern recognition", "Embedding", "Computer science", "Artificial intelligence"]}
{"id": "3119997354", "references": ["639708223", "2108598243", "1536680647", "2963403868", "2194775991", "2919115771", "2963037989", "3106250896", "2099471712", "2963341956"], "title": "Transformers in Vision: A Survey", "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.", "citation_count": "190", "reference_count": "50", "date": "2021", "authors": ["Salman Khan", "Muzammal Naseer", "Munawar Hayat", "Syed Waqas Zamir", "Fahad Shahbaz Khan", "Mubarak Shah"], "related_topics": ["Object detection", "Transformer (machine learning model)", "Contextual image classification", "Visual reasoning", "Activity recognition", "Parallel processing (psychology)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "3135385999", "references": ["2108598243", "2194775991", "2183341477", "1821462560", "2095705004", "2963524571", "2135046866", "2062118960", "2963703618", "2806070179"], "title": "Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning.", "abstract": "In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the objective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predominantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the significance of powerful feature representations with a simple embedding network that can outperform existing sophisticated FSL algorithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been employed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Simultaneous optimization for both of these contrasting objectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transformations. These complementary sets of features help generalize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive experimentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on five popular benchmark datasets.", "citation_count": "76", "reference_count": "0", "date": "2021", "authors": ["Mamshad Nayeem Rizve", "Salman Khan", "Fahad Shahbaz Khan", "Mubarak Shah"], "related_topics": ["Feature (machine learning)", "Metric (mathematics)", "Transformation geometry", "Transformation (function)", "Benchmark (computing)", "Invariant (mathematics)", "Machine learning", "Invariant (physics)", "Embedding", "Set (abstract data type)", "Computer science", "Equivariant map", "Artificial intelligence"]}
{"id": "3159394092", "references": ["3122924117", "3034978746", "3103215803", "3098222900", "3130223764", "3043462782", "3108916577", "3092916550", "3162133897", "3128238149"], "title": "CLAR: Contrastive Learning of Auditory Representations.", "abstract": "Learning rich visual representations using contrastive self-supervised learning has been extremely successful. However, it is still a major question whether we could use a similar approach to learn superior auditory representations. In this paper, we expand on prior work (SimCLR) to learn better auditory representations. We (1) introduce various data augmentations suitable for auditory data and evaluate their impact on predictive performance, (2) show that training with time-frequency audio features substantially improves the quality of the learned representations compared to raw signals, and (3) demonstrate that training with both supervised and contrastive losses simultaneously improves the learned representations compared to self-supervised pre-training followed by supervised fine-tuning. We illustrate that by combining all these methods and with substantially less labeled data, our framework (CLAR) achieves significant improvement on prediction performance compared to supervised approach. Moreover, compared to self-supervised approach, our framework converges faster with significantly better representations.", "citation_count": "0", "reference_count": "0", "date": "2020", "authors": ["Haider Al-Tahan", "Yalda Mohsenzadeh"], "related_topics": ["Natural language processing", "Quality (business)", "Computer science", "Artificial intelligence", "Labeled data"]}
{"id": "3129170303", "references": ["2097381042", "385466589", "2842511635", "2604763608", "3034978746", "2786672974", "2138621090", "2119567691", "2962902376", "2605102758"], "title": "Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning", "abstract": "Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.", "citation_count": "54", "reference_count": "3", "date": "2021", "authors": ["Rishabh Agarwal", "Marlos C. Machado", "Pablo Samuel Castro", "Marc G Bellemare"], "related_topics": ["Reinforcement learning", "Feature learning", "Similarity (psychology)", "Generalization", "Metric (mathematics)", "Machine learning", "Structure (mathematical logic)", "Bisimulation", "Computer science", "Spurious relationship", "Artificial intelligence"]}
{"id": "3139419546", "references": ["2962879692", "2977481643", "3101442004", "2781585732", "2168359464", "3125947392", "2593768305", "2963521487", "2963906246", "2605102758"], "title": "A Metric Space Perspective on Self-Supervised Policy Adaptation", "abstract": "One of the most challenging aspects of real-world reinforcement learning (RL) is the multitude of unpredictable and ever-changing distractions that could divert an agent from what was tasked to do in its training environment. While an agent could learn from reward signals to ignore them, the complexity of the real-world can make rewards hard to acquire, or, at best, extremely sparse. A recent class of self-supervised methods have shown promise that reward-free adaptation under challenging distractions is possible. However, previous work focused on a short one-episode adaptation setting. In this letter, we consider a long-term adaptation setup that is more akin to the specifics of the real-world and propose a metric space perspective on self-supervised adaptation. We empirically describe the processes that take place in the embedding space during this adaptation process, reveal some of its undesirable effects on performance and show how they can be eliminated. Moreover, we theoretically study how actor-based and actor-free agents can further generalise to the target environment by manipulating the Lipschitz constant of the actor and critic functions.", "citation_count": "19", "reference_count": "0", "date": "2021", "authors": ["Cristian Bodnar", "Karol Hausman", "Gabriel Dulac-Arnold", "Rico Jonschkowski"], "related_topics": ["Reinforcement learning", "Adaptation (computer science)", "Process (engineering)", "Metric space", "Space (commercial competition)", "Task analysis", "Class (computer programming)", "Perspective (graphical)", "Artificial intelligence", "Computer science"]}
{"id": "3134032827", "references": ["2561238782", "2803832867", "2979454998", "2842511635", "2187089797", "3034978746", "2736601468", "2963430173", "2619947201", "2887997457"], "title": "Robust Deep Reinforcement Learning via Multi-View Information Bottleneck.", "abstract": "Deep reinforcement learning (DRL) agents are often sensitive to visual changes that were unseen in their training environments. To address this problem, we introduce a robust representation learning approach for RL. We introduce an auxiliary objective based on the multi-view information bottleneck (MIB) principle which encourages learning representations that are both predictive of the future and less sensitive to task-irrelevant distractions. This enables us to train high-performance policies that are robust to visual distractions and can generalize to unseen environments. We demonstrate that our approach can achieve SOTA performance on challenging visual control tasks, even when the background is replaced with natural videos. In addition, we show that our approach outperforms well-established baselines on generalization to unseen environments using the large-scale Procgen benchmark.", "citation_count": "34", "reference_count": "0", "date": "2021", "authors": ["Jiameng Fan", "Wenchao Li"], "related_topics": ["Reinforcement learning", "Feature learning", "Information bottleneck method", "Benchmark (computing)", "Generalization", "Machine learning", "Computer science", "Natural (music)", "Artificial intelligence"]}
{"id": "3162945626", "references": ["2105486945", "3120444854", "3162945626", "2154023516", "2158969944", "1982948368", "3096759198", "2594103415", "3007817409", "2945309379"], "title": "Offline Reinforcement Learning with Pseudometric Learning", "abstract": "Offline Reinforcement Learning methods seek to learn a policy from logged transitions of an environment, without any interaction. In the presence of function approximation, and under the assumption of limited coverage of the state-action space of the environment, it is necessary to enforce the policy to visit state-action pairs close to the support of logged transitions. In this work, we propose an iterative procedure to learn a pseudometric (closely related to bisimulation metrics) from logged transitions, and use it to define this notion of closeness. We show its convergence and extend it to the function approximation setting. We then use this pseudometric to define a new lookup based bonus in an actor-critic algorithm: PLOff. This bonus encourages the actor to stay close, in terms of the defined pseudometric, to the support of logged transitions. Finally, we evaluate the method on hand manipulation and locomotion tasks.", "citation_count": "0", "reference_count": "0", "date": "2021", "authors": ["Robert Dadashi", "Shideh Rezaeifar", "Nino Vieillard", "L\u00e9onard Hussenot", "Olivier Pietquin", "Matthieu Geist"], "related_topics": ["Pseudometric space", "Reinforcement learning", "Closeness", "Function approximation", "Bisimulation", "Theoretical computer science", "Computer science", "Convergence (routing)", "Space (commercial competition)", "Hand manipulation"]}
{"id": "1997063559", "references": ["1979622972", "1622620102", "1567885833", "2107792892", "2056760934", "2065301447", "2581275558", "2154061444", "2114220616", "2150060382"], "title": "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*", "abstract": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low-energy states (\u2018annealing\u2019), or what is the same thing, the most probable states under the Gib...", "citation_count": "48", "reference_count": "26,658", "date": "1993", "authors": ["Stuart Geman", "Donald Geman"], "related_topics": ["Gibbs sampling", "Categorical distribution", "Boltzmann distribution", "Markov random field", "Posterior probability", "Physical system", "Statistical mechanics", "Multiplicative function", "Statistical physics", "Mathematical optimization", "Mathematics"]}
{"id": "2963069010", "references": ["1632114991", "1614298861", "2964308564", "2130942839", "1810943226", "2100664567", "1895577753", "1753482797", "1423339008", "2064675550"], "title": "Grammar as a foreign language", "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.", "citation_count": "31", "reference_count": "877", "date": "2015", "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "related_topics": ["Parsing", "Syntax", "Grammar", "Domain (software engineering)", "Natural language processing", "Foreign language", "Computer science", "Subject (grammar)", "Contrast (statistics)", "Artificial intelligence"]}
{"id": "2963703618", "references": ["2805516822", "2954226438", "2900954917", "2902302021", "2997428643", "2972584841", "2764024122", "2963847595"], "title": "Dynamic Routing Between Capsules", "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.", "citation_count": "0", "reference_count": "2,887", "date": "2017", "authors": ["Sara Sabour", "Nicholas Frosst", "Geoffrey E. Hinton"], "related_topics": ["MNIST database", "Algorithm", "Scalar (mathematics)", "Computer science", "Adaptive routing"]}
{"id": "3131871335", "references": ["2121863487", "2751973545", "2964013315", "2144902422", "2964043796", "2194775991", "2963864421", "2168359464", "2883725317", "2145339207"], "title": "State Entropy Maximization with Random Encoders for Efficient Exploration", "abstract": "Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at this https URL.", "citation_count": "51", "reference_count": "0", "date": "2021", "authors": ["Younggyo Seo", "Lili Chen", "Jinwoo Shin", "Honglak Lee", "Pieter Abbeel", "Kimin Lee"], "related_topics": ["Entropy (information theory)", "Entropy (energy dispersal)", "Encoder", "Reinforcement learning", "Entropy (classical thermodynamics)", "Entropy (statistical thermodynamics)", "Maximization", "Source code", "Entropy (arrow of time)", "Estimator", "Representation (mathematics)", "Algorithm", "Computer science", "Entropy (order and disorder)", "State entropy"]}
{"id": "3131944163", "references": ["3098903812", "2982316857", "3035524453", "2095705004", "2970971581", "3136604105", "343636949", "2952509347", "1836465849", "2145339207"], "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations", "abstract": "While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, SPR, trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We've made the code associated with this work available at https://anonymous.4open.science/r/b4b93ec6-6e5d-4f43-9b53-54bdf73bea95/.", "citation_count": "47", "reference_count": "18", "date": "2021", "authors": ["Max Schwarzer", "Ankesh Anand", "Rishab Goel", "R Devon Hjelm", "Aaron Courville", "Philip Bachman"], "related_topics": ["Reinforcement learning", "Feature learning", "Maximization", "Encoder", "Machine learning", "Moving average", "Code (cryptography)", "Computer science", "Key (cryptography)", "State (computer science)", "Artificial intelligence"]}
{"id": "3094246835", "references": ["2810075754", "2165150801", "2187089797", "2964043796", "2736601468", "2949608212", "3034978746", "2963864421", "2257979135", "2145339207"], "title": "What About Taking Policy as Input of Value Function: Policy-extended Value Function Approximator", "abstract": "The value function lies in the heart of Reinforcement Learning (RL), which defines the long-term evaluation of a policy in a given state. In this paper, we propose Policy-extended Value Function Approximator (PeVFA) which extends the conventional value to be not only a function of state but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies in contrast to a conventional one with limited capacity for only one policy, inducing the new characteristic of \\emph{value generalization among policies}. From both the theoretical and empirical lens, we study value generalization along the policy improvement path (called local generalization), from which we derive a new form of Generalized Policy Iteration with PeVFA to improve the conventional learning process. Besides, we propose a framework to learn the representation of an RL policy, studying several different approaches to learn an effective policy representation from policy network parameters and state-action pairs through contrastive learning and action prediction. In our experiments, Proximal Policy Optimization (PPO) with PeVFA significantly outperforms its vanilla counterpart in MuJoCo continuous control tasks, demonstrating the effectiveness of value generalization offered by PeVFA and policy representation learning.", "citation_count": "64", "reference_count": "0", "date": "2021", "authors": ["Hongyao Tang", "Zhaopeng Meng", "Jianye Hao", "Chen Chen", "Daniel Graves", "Dong Li", "Wulong Liu", "Yaodong Yang"], "related_topics": ["Reinforcement learning", "Feature learning", "Generalization", "Value (mathematics)", "Bellman equation", "State function", "Representation (systemics)", "Mathematical optimization", "Path (graph theory)", "Computer science"]}
{"id": "3134085768", "references": ["2121863487", "2963114950", "2963985863", "2964043796", "2257979135", "2964121744", "2963120839", "2970971581", "2963374099", "2962902376"], "title": "Low-Precision Reinforcement Learning.", "abstract": "Low-precision training has become a popular approach to reduce computation time, memory footprint, and energy consumption in supervised learning. In contrast, this promising approach has not enjoyed similarly widespread adoption within the reinforcement learning (RL) community, in part because RL agents can be notoriously hard to train -- even in full precision. In this paper we consider continuous control with the state-of-the-art SAC agent and demonstrate that a naive adaptation of low-precision methods from supervised learning fails. We propose a set of six modifications, all straightforward to implement, that leaves the underlying agent unchanged but improves its numerical stability dramatically. The resulting modified SAC agent has lower memory and compute requirements while matching full-precision rewards, thus demonstrating the feasibility of low-precision RL.", "citation_count": "56", "reference_count": "0", "date": "2021", "authors": ["Johan Bjorck", "Xiangyu Chen", "Christopher De Sa", "Carla P. Gomes", "Kilian Q. Weinberger"], "related_topics": ["Reinforcement learning", "Supervised learning", "Machine learning", "Energy consumption", "Set (psychology)", "Computer science", "Matching (statistics)", "Adaptation (computer science)", "Numerical stability", "Artificial intelligence"]}
{"id": "3132674603", "references": ["2321533354", "2751973545", "2158131535", "3035524453", "3034978746", "2963864421", "2257979135", "2964121744", "343636949", "2025768430"], "title": "Reinforcement Learning with Prototypical Representations", "abstract": "Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.", "citation_count": "37", "reference_count": "0", "date": "2021", "authors": ["Denis Yarats", "Rob Fergus", "Alessandro Lazaric", "Lerrel Pinto"], "related_topics": ["Reinforcement learning", "Feature learning", "Automatic summarization", "Set (psychology)", "Representation (mathematics)", "Human\u2013computer interaction", "Task (project management)", "Computer science", "Control (linguistics)", "Sample (statistics)"]}
{"id": "3137443434", "references": ["1977655452", "2402144811", "3101442004", "2964043796", "2990747716", "2911087563", "2012587148", "2900152462", "2605102758", "2145339207"], "title": "How to Train Your HERON", "abstract": "In this letter we apply Deep Reinforcement Learning (Deep RL) and Domain Randomization to solve a navigation task in a natural environment relying solely on a 2D laser scanner. We train a model-based RL agent in simulation to follow lake and river shores and apply it on a real Unmanned Surface Vehicle in a zero-shot setup. We demonstrate that even though the agent has not been trained in the real world, it can fulfill its task successfully and adapt to changes in the robot's environment and dynamics. Finally, we show that the RL agent is more robust, faster, and more accurate than a state-aware Model-Predictive-Controller. Code, simulation environments, pre-trained models, and datasets are available at  https://github.com/AntoineRichard/Heron-RL-ICRA.git .", "citation_count": "22", "reference_count": "0", "date": "2021", "authors": ["Antoine Richard", "Stephanie Aravecchia", "Thomas Schillaci", "Matthieu Geist", "Cedric Pradalier"], "related_topics": ["Reinforcement learning", "Robot", "Task (computing)", "Vehicle dynamics", "Task analysis", "Real-time computing", "Domain (software engineering)", "Code (cryptography)", "Computer science", "Laser scanning"]}
{"id": "3102762742", "references": ["3156762592", "3133724915", "3161443521", "3135649981", "3163735823"], "title": "MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning", "abstract": "This paper introduces MDP homomorphic networks for deep reinforcement learning. MDP homomorphic networks are neural networks that are equivariant under symmetries in the joint state-action space of an MDP. Current approaches to deep reinforcement learning do not usually exploit knowledge about such structure. By building this prior knowledge into policy and value networks using an equivariance constraint, we can reduce the size of the solution space. We specifically focus on group-structured symmetries (invertible transformations). Additionally, we introduce an easy method for constructing equivariant network layers numerically, so the system designer need not solve the constraints by hand, as is typically done. We construct MDP homomorphic MLPs and CNNs that are equivariant under either a group of reflections or rotations. We show that such networks converge faster than unstructured baselines on CartPole, a grid world and Pong.", "citation_count": "0", "reference_count": "10", "date": "2020", "authors": ["Elise van der Pol", "Daniel E. Worrall", "Herke van Hoof", "Frans A. Oliehoek", "Max Welling"], "related_topics": ["Reinforcement learning", "Artificial neural network", "Equivariant map", "Homomorphic encryption", "Theoretical computer science", "Grid", "Homogeneous space", "Group (mathematics)", "Structure (category theory)", "Computer science"]}
{"id": "3107857059", "references": ["3034978746", "2962935454", "2534269850", "2790376986", "2900152462", "2953494151", "1585160083", "2963680188", "2605102758", "2143891888"], "title": "Intervention Design for Effective Sim2Real Transfer.", "abstract": "The goal of this work is to address the recent success of domain randomization and data augmentation for the sim2real setting. We explain this success through the lens of causal inference, positioning domain randomization and data augmentation as interventions on the environment which encourage invariance to irrelevant features. Such interventions include visual perturbations that have no effect on reward and dynamics. This encourages the learning algorithm to be robust to these types of variations and learn to attend to the true causal mechanisms for solving the task. This connection leads to two key findings: (1) perturbations to the environment do not have to be realistic, but merely show variation along dimensions that also vary in the real world, and (2) use of an explicit invariance-inducing objective improves generalization in sim2sim and sim2real transfer settings over just data augmentation or domain randomization alone. We demonstrate the capability of our method by performing zero-shot transfer of a robot arm reach task on a 7DoF Jaco arm learning from pixel observations.", "citation_count": "32", "reference_count": "1", "date": "2020", "authors": ["Melissa Mozifian", "Amy Zhang", "Joelle Pineau", "David Meger"], "related_topics": ["Causal inference", "Task (project management)", "Generalization", "Domain (software engineering)", "Machine learning", "Variation (game tree)", "Computer science", "Key (cryptography)", "Randomization", "Artificial intelligence"]}
{"id": "3133595589", "references": ["2121863487", "2964299589", "2963403868", "2194775991", "2310919327", "2963864421", "2149933564", "2279098554", "2952509347", "2145339207"], "title": "Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings", "abstract": "Recent advances in off-policy deep reinforcement learning (RL) have led to impressive success in complex tasks from visual observations. Experience replay improves sample-efficiency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efficient Reinforcement Learning (SEER), a simple modification of existing off-policy RL methods, to address these computational and memory requirements. To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experiments, we show that SEER does not degrade the performance of RL agents while significantly saving computation and memory across a diverse set of DeepMind Control environments and Atari games. Finally, we show that SEER is useful for computation-efficient transfer learning in RL because lower layers of CNNs extract generalizable features, which can be used for different tasks and domains.", "citation_count": "54", "reference_count": "0", "date": "2021", "authors": ["Lili Chen", "Kimin Lee", "Aravind Srinivas", "Pieter Abbeel"], "related_topics": ["Reinforcement learning", "Convolutional neural network", "Transfer of learning", "Machine learning", "Process (computing)", "Encoder", "Computer science", "Set (abstract data type)", "Artificial intelligence"]}
{"id": "3132869322", "references": ["2964015378", "2963207607", "3034978746", "2027731328", "2963312446", "2964253222", "2970971581", "2243397390", "2964153729", "2887997457"], "title": "Towards Robust Graph Contrastive Learning.", "abstract": "We study the problem of adversarially robust self-supervised learning on graphs. In the contrastive learning framework, we introduce a new method that increases the adversarial robustness of the learned representations through i) adversarial transformations and ii) transformations that not only remove but also insert edges. We evaluate the learned representations in a preliminary set of experiments, obtaining promising results. We believe this work takes an important step towards incorporating robustness as a viable auxiliary task in graph contrastive learning.", "citation_count": "40", "reference_count": "1", "date": "2021", "authors": ["Nikola Jovanovic", "Zhao Meng", "Lukas Faber", "Roger Wattenhofer"], "related_topics": ["Robustness (computer science)", "Theoretical computer science", "Set (psychology)", "Computer science", "Task (project management)", "Graph"]}
{"id": "3154313998", "references": ["2964015378", "1479807131", "2963858333", "3034978746", "3035524453", "2964121744", "2970971581", "2883725317", "2887997457", "2903158431"], "title": "When Contrastive Learning Meets Active Learning: A Novel Graph Active Learning Paradigm with Self-Supervision.", "abstract": "This paper studies active learning (AL) on graphs, whose purpose is to discover the most informative nodes to maximize the performance of graph neural networks (GNNs). Previously, most graph AL methods focus on learning node representations from a carefully selected labeled dataset with large amount of unlabeled data neglected. Motivated by the success of contrastive learning (CL), we propose a novel paradigm that seamlessly integrates graph AL with CL. While being able to leverage the power of abundant unlabeled data in a self-supervised manner, nodes selected by AL further provide semantic information that can better guide representation learning. Besides, previous work measures the informativeness of nodes without considering the neighborhood propagation scheme of GNNs, so that noisy nodes may be selected. We argue that due to the smoothing nature of GNNs, the central nodes from homophilous subgraphs should benefit the model training most. To this end, we present a minimax selection scheme that explicitly harnesses neighborhood information and discover homophilous subgraphs to facilitate active selection. Comprehensive, confounding-free experiments on five public datasets demonstrate the superiority of our method over state-of-the-arts.", "citation_count": "34", "reference_count": "0", "date": "2021", "authors": ["Yanqiao Zhu", "Weizhi Xu", "Qiang Liu", "Shu Wu"], "related_topics": ["Active learning (machine learning)", "Feature learning", "Node (computer science)", "Machine learning", "Minimax", "Smoothing", "Computer science", "Leverage (statistics)", "Focus (linguistics)", "Selection (linguistics)", "Artificial intelligence"]}
{"id": "3135138557", "references": ["2225156818", "2038276547", "2548228487", "2811124557", "2963858333", "2547875792", "2963695795", "1959608418", "2979750740", "2964153729"], "title": "Deep Graph Structure Learning for Robust Representations: A Survey.", "abstract": "Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Specifically, we first formulate a general paradigm of GSL, and then review state-of-the-art methods classified by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.", "citation_count": "59", "reference_count": "0", "date": "2021", "authors": ["Yanqiao Zhu", "Weizhi Xu", "Jinghao Zhang", "Qiang Liu", "Shu Wu", "Liang Wang"], "related_topics": ["Perfect graph", "Robustness (computer science)", "Structure (mathematical logic)", "Theoretical computer science", "Computer science", "Noise (video)", "Point (typography)", "Current (mathematics)", "Quality (business)", "Graph", "Structure learning"]}
{"id": "3134210100", "references": ["1614298861", "2964015378", "2963858333", "3104097132", "2130354913", "2624431344", "3034978746", "2153579005", "3043547428", "2962756421"], "title": "Graph Self-Supervised Learning: A Survey.", "abstract": "Deep learning on graphs has attracted significant interest recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from other domains like computer vision/natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We divide these into four categories according to the design of their pretext tasks. We further discuss the remaining challenges and potential future directions in this research field.", "citation_count": "47", "reference_count": "1", "date": "2021", "authors": ["Yixin Liu", "Shirui Pan", "Ming Jin", "Chuan Zhou", "Feng Xia", "Philip S. Yu"], "related_topics": ["Supervised learning", "Deep learning", "Field (computer science)", "Machine learning", "Robustness (computer science)", "Computer science", "Generalization (learning)", "Artificial intelligence", "Graph", "Self supervised learning"]}
{"id": "1518768680", "references": ["2154642048", "2066444522", "2148426685", "2165612380", "1999114220", "2047620598", "2103318667", "1977182536", "2090543924"], "title": "Towards building contextual representations of word senses using statistical models", "abstract": "A b s t r a c t Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to focus either on very local context or on topical context. Both components axe needed for word sense resolution. A contextual representation of a word sense consists of topical context and local context. Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora. We review an experiment evaluating three statistical classifiers that automatically extract topical context. An experiment designed to examine human subject performance with similar input is described. Finally, we investigate a method for automatically extracting local context from a corpus. Preliminary results show improved perfor-", "citation_count": "9", "reference_count": "100", "date": "1996", "authors": ["Claudia Leacock", "Geoffrey Towell", "Ellen M. Voorhees"], "related_topics": ["Context (language use)", "Natural language processing", "Word (computer architecture)", "Statistical model", "Construct (python library)", "Representation (systemics)", "Resolution (logic)", "Focus (linguistics)", "Computer science", "Subject (grammar)", "Artificial intelligence"]}
{"id": "2103318667", "references": ["1536719366", "2109334311", "2020159140", "2017580301", "13823885", "2114826854", "1483126227", "2064332540", "1971220772", "1634667895"], "title": "Contextual correlates of semantic similarity", "abstract": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.", "citation_count": "31", "reference_count": "2,030", "date": "1991", "authors": ["George A. Miller", "Walter G. Charles"], "related_topics": ["Semantic similarity", "Similarity (psychology)", "Contextual Associations", "Semantics", "Sentence", "Noun", "Syntax", "Pragmatics", "Natural language processing", "Linguistics", "Psychology", "Artificial intelligence"]}
{"id": "2017668967", "references": ["2069736034", "1576632330", "2103318667", "2052262800", "2059799772", "2123987305", "1503404806", "2025589690", "2013596317", "2040300040"], "title": "Semantic networks of English.", "abstract": "Principles of lexical semantics developed in the course of building an on-line lexical database are discussed. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages.", "citation_count": "41", "reference_count": "499", "date": "1991", "authors": ["George A. Miller", "Christiane Fellbaum"], "related_topics": ["Semantic computing", "Semantic network", "Semantic similarity", "Semantic property", "Componential analysis", "Lexical database", "Lexical semantics", "Semantic compression", "Linguistics", "Psychology"]}
{"id": "2065157922", "references": ["2081687495", "2012908435", "2017668967", "2007780422", "2102381086", "1483126227"], "title": "A semantic concordance", "abstract": "A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances am proposed.", "citation_count": "6", "reference_count": "795", "date": "1993", "authors": ["George A. Miller", "Claudia Leacock", "Randee Tengi", "Ross T. Bunker"], "related_topics": ["Explicit semantic analysis", "Semantic similarity", "Semantic compression", "WordNet", "Semantic property", "Lexicon", "Brown Corpus", "Semantic HTML", "Natural language processing", "Information retrieval", "Computer science", "Artificial intelligence"]}
{"id": "2102381086", "references": ["1933657216", "2103318667", "2017668967", "2052262800", "2059799772", "2123987305", "2090626368", "1483126227", "2013596317", "2040300040"], "title": "Introduction to WordNet: An On-line Lexical Database", "abstract": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.", "citation_count": "81", "reference_count": "6,807", "date": "1990", "authors": ["George A. Miller", "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "Katherine J. Miller"], "related_topics": ["Lexical database", "WordNet", "eXtended WordNet", "EuroWordNet", "Train of thought", "Natural language processing", "Simple (philosophy)", "Computer science", "Interrupt", "Word (computer architecture)", "Artificial intelligence"]}
{"id": "13823885", "references": ["2316811974", "2017580301", "1995875735", "2045585593"], "title": "The categorization of sentential contexts", "abstract": "A new experimental method, involving the sorting of linguistic contexts, is shown to be effective in discriminating the contexts of polysemous words as well as the contexts of synonyms of those words. These results are interpreted as support for the claim that the method of sorting linguistic contexts is a valid technique for studying the contextual information available to support inferences about word meanings.", "citation_count": "4", "reference_count": "9", "date": "1988", "authors": ["Walter G. Charles"], "related_topics": ["Context (archaeology)", "Categorization", "Psycholinguistics", "Sorting", "Lexico", "Linguistics", "Natural language processing", "Word (computer architecture)", "Computer science", "Artificial intelligence", "Contextual information", "Semantic relation"]}
{"id": "1483126227", "references": ["1632114991", "2141766660", "1659833910", "2117805756", "2117400858", "2136930489", "2081580037", "2102381086", "1997161938"], "title": "FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR", "abstract": "", "citation_count": "0", "reference_count": "2,918", "date": "1983", "authors": ["W. Nelson Francis", "Henry Ku\u010dera", "Andrew W. Mackie"], "related_topics": ["Lexicon", "Grammar", "Brown Corpus", "Regular and irregular verbs", "Linguistics", "Natural language processing", "Computer science", "Frequency analysis", "Artificial intelligence"]}
{"id": "1964724001", "references": ["2155199877", "2800289289", "2954064014", "1983993791", "2029469881", "1573763320", "1968104963", "2052740976", "2082612735", "2161831609"], "title": "Exploratory Projection Pursuit", "abstract": "Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ...", "citation_count": "16", "reference_count": "1,158", "date": "1987", "authors": ["Jerome H. Friedman"], "related_topics": ["Projection pursuit", "Exploratory data analysis", "Cluster analysis", "Covariance", "Density estimation", "Pattern recognition (psychology)", "Data mapping", "Statistical graphics", "Data mining", "Mathematics"]}
{"id": "2315016682", "references": ["1785063204", "1811843574", "2096230369", "2100544508"], "title": "Feature extraction using an unsupervised neural network", "abstract": "A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing distinguishing features in the data is presented. A statistical framework for the parameter estimation problem associated with this neural network is given and its connection to exploratory projection pursuit methods is established. The network is shown to minimize a loss function (projection index) over a set of parameters, yielding an optimal decision rule under some norm. A specific projection index that favors directions possessing multimodality is presented. This leads to a similar form to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982).\r\n\r\nThe importance of a dimensionality reduction principal based, solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using principal components and back-propagation network.", "citation_count": "0", "reference_count": "120", "date": "1999", "authors": ["Nathan Intrator"], "related_topics": ["Dimensionality reduction", "Time delay neural network", "Projection pursuit", "Feature extraction", "Deep learning", "Artificial neural network", "Projection (set theory)", "Principal component analysis", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2725061391", "references": ["1507849272", "2293063825", "2154642048"], "title": "A mean field theory learning algorithm for neural networks", "abstract": "", "citation_count": "3", "reference_count": "687", "date": "1987", "authors": ["Carsten Peterson", "James R. Anderson"], "related_topics": ["Types of artificial neural networks", "Wake-sleep algorithm", "Deep learning", "Artificial neural network", "Competitive learning", "Unsupervised learning", "Rprop", "Feedforward neural network", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2049633694", "references": ["2000084758", "1982585616", "2086699924", "2100358124", "2121493622", "2074673068", "2144578442", "2327022120", "1575431606", "2403035479"], "title": "Maximum likelihood from incomplete data via the EM algorithm", "abstract": "", "citation_count": "74", "reference_count": "66,114", "date": "1977", "authors": ["Arthur P. Dempster", "Nan M. Laird", "Donald B. Rubin"], "related_topics": ["Maximum likelihood sequence estimation", "Expectation\u2013maximization algorithm", "MM algorithm", "Mixture model", "Baum\u2013Welch algorithm", "Forward\u2013backward algorithm", "Cluster-weighted modeling", "Observed information", "Mathematics", "Statistics", "Pattern recognition"]}
{"id": "2121407732", "references": ["2098405376", "2044579390", "2171814052", "2156142001", "2098710805", "2096335861", "1966385142", "2165225968", "2289748525", "3123226880"], "title": "Finite Mixture Distributions", "abstract": "1 General introduction.- 1.1 Introduction.- 1.2 Some applications of finite mixture distributions.- 1.3 Definition.- 1.4 Estimation methods.- 1.4.1 Maximum likelihood.- 1.4.2 Bayesian estimation.- 1.4.3 Inversion and error minimization.- 1.4.4 Other methods.- 1.4.5 Estimating the number of components.- 1.5 Summary.- 2 Mixtures of normal distributions.- 2.1 Introduction.- 2.2 Some descriptive properties of mixtures of normal distributions.- 2.3 Estimating the parameters in normal mixture distributions.- 2.3.1 Method of moments estimation.- 2.3.2 Maximum likelihood estimation.- 2.3.3 Maximum likelihood estimates for grouped data.- 2.3.4 Obtaining initial parameter values for the maximum likelihood estimation algorithms.- 2.3.5 Graphical estimation techniques.- 2.3.6 Other estimation methods.- 2.4 Summary.- 3 Mixtures of exponential and other continuous distributions.- 3.1 Exponential mixtures.- 3.2 Estimating exponential mixture parameters.- 3.2.1 The method of moments and generalizations.- 3.2.2 Maximum likelihood.- 3.3 Properties of exponential mixtures.- 3.4 Other continuous distributions.- 3.4.1 Non-central chi-squared distribution.- 3.4.2 Non-central F distribution.- 3.4.3 Beta distributions.- 3.4.4 Doubly non-central t distribution.- 3.4.5 Planck's distribution.- 3.4.6 Logistic.- 3.4.7 Laplace.- 3.4.8 Weibull.- 3.4.9 Gamma.- 3.5 Mixtures of different component types.- 3.6 Summary.- 4 Mixtures of discrete distributions.- 4.1 Introduction.- 4.2 Mixtures of binomial distributions.- 4.2.1 Moment estimators for binomial mixtures.- 4.2.2 Maximum likelihood estimators for mixtures of binomial distributions.- 4.2.3 Other estimation methods for mixtures of binomial distributions.- 4.3 Mixtures of Poisson distributions.- 4.3.1 Moment estimators for mixtures of Poisson distributions.- 4.3.2 Maximum likelihood estimators for a Poisson mixture.- 4.4 Mixtures of Poisson and binomial distributions.- 4.5 Mixtures of other discrete distributions.- 4.6 Summary.- 5 Miscellaneous topics.- 5.1 Introduction.- 5.2 Determining the number of components in a mixture.- 5.2.1 Informal diagnostic tools for the detection of mixtures.- 5.2.2 Testing hypotheses on the number of components in a mixture.- 5.3 Probability density function estimation.- 5.4 Miscellaneous problems.- 5.5 Summary.- References.", "citation_count": "0", "reference_count": "2,043", "date": "1981", "authors": ["Brian Everitt", "D. J. Hand"], "related_topics": ["Estimating equations", "Normal distribution", "Poisson distribution", "Beta distribution", "Estimator", "F-distribution", "Bayes estimator", "Weibull distribution", "Applied mathematics", "Statistics", "Mathematics"]}
{"id": "1513873506", "references": ["1567512734", "2057565703", "2149801992", "1565709818", "2130416410", "2581275558", "2615953416", "2138309709", "2033057584", "2013164703"], "title": "Annealed importance sampling", "abstract": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.", "citation_count": "19", "reference_count": "1,306", "date": "2001", "authors": ["Radford M. Neal"], "related_topics": ["Slice sampling", "Importance sampling", "Markov chain Monte Carlo", "Markov chain", "Umbrella sampling", "Autocorrelation", "Sequence", "Thermodynamic integration", "Statistical physics", "Mathematical optimization", "Mathematics"]}
{"id": "2135094946", "references": ["1530042113", "1513861746", "2004308928", "1515272691", "2914659449", "1516111018", "2169415915", "2798766386", "2137813581", "2019599312"], "title": "A new class of upper bounds on the log partition function", "abstract": "We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.", "citation_count": "40", "reference_count": "584", "date": "2005", "authors": ["M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky"], "related_topics": ["Partition function (quantum field theory)", "Upper and lower bounds", "Fixed point", "Local optimum", "Variational method", "Belief propagation", "Factor graph", "Markov random field", "Combinatorics", "Mathematics"]}
{"id": "2064630666", "references": ["2136922672", "2124914669", "2110798204", "2137983211", "2100495367", "205159212", "2116064496", "2172174689", "2613634265"], "title": "Representational power of restricted boltzmann machines and deep belief networks", "abstract": "Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.", "citation_count": "21", "reference_count": "788", "date": "2008", "authors": ["Nicolas Le Roux", "Yoshua Bengio"], "related_topics": ["Restricted Boltzmann machine", "Deep belief network", "Boltzmann machine", "Deep learning", "Artificial neural network", "Statistical model", "Inference", "Artificial intelligence", "Block (data storage)", "Mathematics"]}
{"id": "2169415915", "references": ["2099111195", "2125838338", "1530042113", "2159080219", "2987657883", "2121606987", "1516111018", "2798766386", "2137813581", "1746680969"], "title": "Constructing free-energy approximations and generalized belief propagation algorithms", "abstract": "Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the \"Bethe method\", the \"junction graph method\", the \"cluster variation method\", and the \"region graph method\". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.", "citation_count": "56", "reference_count": "1,756", "date": "2005", "authors": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "related_topics": ["Belief propagation", "Approximation algorithm", "Factor graph", "Graph (abstract data type)", "Graph theory", "Stationary point", "Fixed point", "Coding theory", "Algorithm", "Mathematics"]}
{"id": "66838807", "references": ["1813659000", "2130313186", "2482531687", "1802356529", "2157444450", "2130416410", "2095844239", "2116064496", "1651266332", "1568229137"], "title": "On Contrastive Divergence Learning.", "abstract": "", "citation_count": "15", "reference_count": "803", "date": "2005", "authors": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Geoffrey E. Hinton"], "related_topics": ["Computer science", "Linguistics", "Pattern recognition", "Artificial intelligence", "Contrastive divergence"]}
{"id": "2158164339", "references": ["2136922672", "2123236823", "2293741035", "2124914669", "2147010501", "2310919327", "2116064496", "2248685949", "2114153178", "1991942383"], "title": "Modeling Human Motion Using Binary Latent Variables", "abstract": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.", "citation_count": "14", "reference_count": "887", "date": "2006", "authors": ["Graham W. Taylor", "Geoffrey E. Hinton", "Sam T. Roweis"], "related_topics": ["Probabilistic latent semantic analysis", "Latent variable", "Latent class model", "Generative model", "Motion capture", "Motion (physics)", "Inference", "Set (abstract data type)", "Algorithm", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2164019165", "references": ["2118020653", "2117130368", "71795751", "2131462252", "1970381522", "2158139315", "1423339008", "2081580037", "1532325895", "2132339004"], "title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.", "citation_count": "36", "reference_count": "1,370", "date": "2012", "authors": ["Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng"], "related_topics": ["Word lists by frequency", "Word (computer architecture)", "Language model", "Polysemy", "Context (language use)", "Semantics", "Natural language processing", "Homonym", "Computer science", "Representation (mathematics)", "Artificial intelligence"]}
{"id": "2118020653", "references": ["2114535528", "2005422315", "2147152072", "1574901103", "2140785063", "2149684865", "1978394996", "2097089247", "2053463056", "2435251607"], "title": "Machine learning in automated text categorization", "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.", "citation_count": "165", "reference_count": "10,897", "date": "2002", "authors": ["Fabrizio Sebastiani"], "related_topics": ["Multi-task learning", "Categorization", "Document classification", "Classifier (UML)", "Knowledge engineering", "Email filtering", "Rocchio algorithm", "Software portability", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2964267515", "references": ["2963339397", "2962809918", "2964223283", "2551396370", "2962985038", "2740747242", "2963748441"], "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "abstract": "Abstract: We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "citation_count": "0", "reference_count": "445", "date": "2016", "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "related_topics": ["Language model", "Reading (process)", "Encoding (memory)", "Goldilocks principle", "Explicit memory", "Generality", "Natural language processing", "Function (engineering)", "Standard language", "Computer science", "Artificial intelligence"]}
{"id": "2962809918", "references": ["2964267515", "2964091467", "1793121960", "2584341106", "1902237438", "2962790689", "1544827683", "2250539671", "2125436846", "2250861254"], "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1", "citation_count": "19", "reference_count": "450", "date": "2016", "authors": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "related_topics": ["Task (project management)", "Reading comprehension", "Natural language processing", "Computer science", "Key (cryptography)", "Artificial neural network", "Factor (programming language)", "Artificial intelligence"]}
{"id": "2251349042", "references": ["1508977358", "2189089430", "2251673953", "2252136820", "1923162067", "2123661878", "2163561827", "1559723967", "1496189301", "2118781169"], "title": "Learning to Automatically Solve Algebra Word Problems", "abstract": "We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.", "citation_count": "30", "reference_count": "230", "date": "2014", "authors": ["Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay"], "related_topics": ["Word problem (mathematics education)", "System of linear equations", "Task (project management)", "Construct (python library)", "Theoretical computer science", "Natural language processing", "Algebra", "Computer science", "Algebra over a field", "Artificial intelligence", "Sentence boundary disambiguation"]}
{"id": "2171278097", "references": ["2122537498", "2107658650", "2158823144", "2087064593", "2080278171", "2047221353", "2150884987", "2988119488", "2081580037", "2096797897"], "title": "Building Watson: An Overview of the DeepQA Project", "abstract": "IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.", "citation_count": "22", "reference_count": "1,770", "date": "2010", "authors": ["David A. Ferrucci", "Eric W. Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John M. Prager", "Nico Schlaefer", "Christopher A. Welty"], "related_topics": ["Watson", "IBM", "Champion", "Architecture", "Data science", "Computer science", "Field (computer science)", "Operations research", "Extensible architecture"]}
{"id": "2962790689", "references": ["2964091467", "2561715562", "2962809918", "2963448850", "1933349210", "2964223283", "2768661419"], "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "abstract": "Abstract: One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.", "citation_count": "0", "reference_count": "661", "date": "2016", "authors": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M. Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "related_topics": ["Question answering", "Set (psychology)", "AI-complete", "Natural language", "Chaining", "Reading comprehension", "Human\u2013computer interaction", "Computer science", "Simple (philosophy)", "Measure (data warehouse)"]}
{"id": "2125436846", "references": ["2142898321", "2167090521", "2989499211", "1979532929", "2126631960", "2525127255", "2096979215", "2097550833", "3126123353", "3122078363"], "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "abstract": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone\u2019s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today\u2019s computers and algorithms.", "citation_count": "25", "reference_count": "537", "date": "2013", "authors": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "related_topics": ["Reading comprehension", "Comprehension", "Relationship extraction", "Textual entailment", "Semantic role labeling", "Information extraction", "Multiple choice", "Causal reasoning", "Parsing", "Grammar", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2251818205", "references": ["2070246124", "2251921768", "2989499211", "2120735855", "2131744502", "2153579005", "1514986335", "1591825359", "2118091490", "2125313055"], "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "abstract": "We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.", "citation_count": "10", "reference_count": "564", "date": "2015", "authors": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "related_topics": ["Sentence", "Question answering", "Selection (linguistics)", "Set (abstract data type)", "Information retrieval", "Matching (statistics)", "Task (project management)", "Computer science", "Component (UML)", "Process (engineering)"]}
{"id": "1596324102", "references": ["2100677568", "2133462743", "2137130182", "2963305465", "2019363670", "2008906462", "2147169507", "2040884411", "1501500081", "2126385963"], "title": "Machine Learning: An Artificial Intelligence Approach", "abstract": "This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis.", "citation_count": "0", "reference_count": "2,546", "date": "2013", "authors": ["R. S. Michalski", "J. G. Carbonell", "T. M. Mitchell"], "related_topics": ["Robot learning", "Explanation-based learning", "Algorithmic learning theory", "Learning sciences", "Hyper-heuristic", "Knowledge acquisition", "Expert system", "Knowledge base", "Data science", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1569296262", "references": ["2100677568", "2121863487", "1557517019", "2964352247", "1944672", "2155027007", "2119717200", "2604636228", "2107726111"], "title": "Temporal credit assignment in reinforcement learning", "abstract": "", "citation_count": "0", "reference_count": "966", "date": "1984", "authors": ["Richard Stuart Sutton"], "related_topics": ["Reinforcement learning", "Computer science", "Machine learning", "Artificial intelligence", "Credit assignment", "Error-driven learning"]}
{"id": "2075379212", "references": ["2100677568", "2117740169", "1480546554", "2086645750", "2014478203", "2142645441", "1586184796", "2161984370", "2136796925", "2144979178"], "title": "Finite Markov chains", "abstract": "", "citation_count": "0", "reference_count": "4,972", "date": "1976", "authors": ["John G. Kemeny", "J. Laurie Snell"], "related_topics": ["Examples of Markov chains", "Markov chain", "Markov chain mixing time", "Markov kernel", "Markov property", "Markov renewal process", "Lumpability", "Markov process", "Computer science", "Statistical physics"]}
{"id": "2019207321", "references": ["2154642048", "2171277043", "2079325629", "1965324089", "2155399784", "2138484437", "2330022088", "2895674046", "1492221128", "2062706881"], "title": "ANFIS: adaptive-network-based fuzzy inference system", "abstract": "The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-line in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. &gt;", "citation_count": "59", "reference_count": "18,394", "date": "1993", "authors": ["J.-S.R. Jang"], "related_topics": ["Adaptive neuro fuzzy inference system", "Neuro-fuzzy", "Fuzzy control system", "Fuzzy set operations", "Fuzzy logic", "Artificial neural network", "Knowledge-based systems", "Inference", "Automatic control", "Control system", "Artificial intelligence", "Nonlinear system", "Computer science"]}
{"id": "2148885430", "references": ["2019207321", "2079325629", "2068052921", "1516289514", "2090167557", "2032173185", "2912565176", "1992176519", "2914825457", "2062706881"], "title": "A Survey on Analysis and Design of Model-Based Fuzzy Control Systems", "abstract": "Fuzzy logic control was originally introduced and developed as a model free control design approach. However, it unfortunately suffers from criticism of lacking of systematic stability analysis and controller design though it has a great success in industry applications. In the past ten years or so, prevailing research efforts on fuzzy logic control have been devoted to model-based fuzzy control systems that guarantee not only stability but also performance of closed-loop fuzzy control systems. This paper presents a survey on recent developments (or state of the art) of analysis and design of model based fuzzy control systems. Attention will be focused on stability analysis and controller design based on the so-called Takagi-Sugeno fuzzy models or fuzzy dynamic models. Perspectives of model based fuzzy control in future are also discussed", "citation_count": "344", "reference_count": "1,764", "date": "2006", "authors": ["Gang Feng"], "related_topics": ["Fuzzy electronics", "Fuzzy set operations", "Fuzzy logic", "Fuzzy control system", "Neuro-fuzzy", "Adaptive neuro fuzzy inference system", "Adaptive control", "Lyapunov function", "Industrial engineering", "Artificial intelligence", "Computer science"]}
{"id": "2044535354", "references": ["2913465992", "2802739963", "1569320505", "2109246257", "3133603318", "1490746760", "1973240854", "2157202423", "2114001875", "1568229137"], "title": "Detection of abrupt changes: theory and application", "abstract": "This book is downloadable from http://www.irisa.fr/sisthem/kniga/. Many monitoring problems can be stated as the problem of detecting a change in the parameters of a static or dynamic stochastic system. The main goal of this book is to describe a unified framework for the design and the performance analysis of the algorithms for solving these change detection problems. Also the book contains the key mathematical background necessary for this purpose. Finally links with the analytical redundancy approach to fault detection in linear systems are established. We call abrupt change any change in the parameters of the system that occurs either instantaneously or at least very fast with respect to the sampling period of the measurements. Abrupt changes by no means refer to changes with large magnitude; on the contrary, in most applications the main problem is to detect small changes. Moreover, in some applications, the early warning of small - and not necessarily fast - changes is of crucial interest in order to avoid the economic or even catastrophic consequences that can result from an accumulation of such small changes. For example, small faults arising in the sensors of a navigation system can result, through the underlying integration, in serious errors in the estimated position of the plane. Another example is the early warning of small deviations from the normal operating conditions of an industrial process. The early detection of slight changes in the state of the process allows to plan in a more adequate manner the periods during which the process should be inspected and possibly repaired, and thus to reduce the exploitation costs.", "citation_count": "219", "reference_count": "6,051", "date": "1993", "authors": ["Mich\u00e8le Basseville", "Igor V. Nikiforov"], "related_topics": ["Change detection", "Warning system", "Fault detection and isolation", "Redundancy (engineering)", "Linear system", "Navigation system", "Dynamical systems theory", "Multidimensional signal processing", "Control theory", "Computer science", "Statistics"]}
{"id": "2131215403", "references": ["1969090956", "2097560155", "2179427518", "2099111195", "1509562192", "1569320505", "1980440818", "1644749979", "2112927743", "2581275558"], "title": "Noise in the nervous system.", "abstract": "Noise \u2014 random disturbances of signals \u2014 poses a fundamental problem for information processing and affects all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-to-trial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.", "citation_count": "193", "reference_count": "2,294", "date": "2008", "authors": ["A. Aldo Faisal", "Luc P. J. Selen", "Daniel M. Wolpert"], "related_topics": ["Noise", "Neuronal noise", "Information processing", "Nervous system", "Information theory", "Function (engineering)", "Electronic engineering", "Neuroscience", "Psychology", "Neural variability"]}
{"id": "2119717200", "references": ["2100677568", "2154642048", "1652505363", "2152475379", "1569320505", "1547224907", "2286699414", "1538558539", "3011120880", "1583833196"], "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "abstract": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.", "citation_count": "27", "reference_count": "9,097", "date": "1992", "authors": ["Ronald J. Williams"], "related_topics": ["Learning classifier system", "Reinforcement learning", "Backpropagation", "Gradient descent", "Connectionism", "Artificial intelligence", "Machine learning", "Associative property", "Algorithm", "Computer science", "Gradient theorem", "Class (computer programming)"]}
{"id": "2095227410", "references": ["2491188924", "1481834144", "2170120409", "1569320505", "2002500823", "1554878117", "2031391169", "2003449296", "1986922155", "3022377941"], "title": "Manufacturing Automation: Metal Cutting Mechanics, Machine Tool Vibrations, and CNC Design", "abstract": "Metal cutting is a widely used method of producing manufactured products. The technology of metal cutting has advanced considerably along with new materials, computers, and sensors. This new edition treats the scientific principles of metal cutting and their practical application to manufacturing problems. It begins with metal cutting mechanics, principles of vibration, and experimental modal analysis applied to solving shop floor problems. Notable is the in-depth coverage of chatter vibrations, a problem experienced daily by manufacturing engineers. The essential topics of programming, design, and automation of CNC (computer numerical control) machine tools, NC (numerical control) programming, and CAD/CAM technology are discussed. The text also covers the selection of drive actuators, feedback sensors, modeling and control of feed drives, the design of real time trajectory generation and interpolation algorithms, and CNC-oriented error analysis in detail. Each chapter includes examples drawn from industry, design projects, and homework problems. This book is ideal for advanced undergraduate and graduate students, as well as practicing engineers.", "citation_count": "66", "reference_count": "3,674", "date": "2000", "authors": ["Yusuf Altintas"], "related_topics": ["Computer-aided manufacturing", "Numerical control", "Machine tool", "Machining", "Automation", "Manufacturing engineering", "Engineering drawing", "Modal analysis", "Engineering", "CAD", "Actuator", "Mechanics"]}
{"id": "2749680651", "references": ["2134673975", "3029645440", "2164278908", "1561941139", "1978956894", "3141595720", "2098432798", "1993170675", "2798766386", "2296319761"], "title": "Predictive Control for Linear and Hybrid Systems", "abstract": "Model Predictive Control (MPC), the dominant advanced control approach in industry over the past twenty-five years, is presented comprehensively in this unique book. With a simple, unified approach, and with attention to real-time implementation, it covers predictive control theory including the stability, feasibility, and robustness of MPC controllers. The theory of explicit MPC, where the nonlinear optimal feedback controller can be calculated efficiently, is presented in the context of linear systems with linear constraints, switched linear systems, and, more generally, linear hybrid systems. Drawing upon years of practical experience and using numerous examples and illustrative applications, the authors discuss the techniques required to design predictive control laws, including algorithms for polyhedral manipulations, mathematical and multiparametric programming and how to validate the theoretical properties and to implement predictive control policies. The most important algorithms feature in an accompanying free online MATLAB toolbox, which allows easy access to sample solutions. Predictive Control for Linear and Hybrid Systems is an ideal reference for graduate, postgraduate and advanced control practitioners interested in theory and/or implementation aspects of predictive control.", "citation_count": "247", "reference_count": "857", "date": "2017", "authors": ["Francesco Borrelli", "Alberto Bemporad", "Manfred Morari"], "related_topics": ["Model predictive control", "Linear system", "Hybrid system", "Robustness (computer science)", "Nonlinear system", "Control engineering", "Computer science", "Feedback controller", "Matlab toolbox"]}
{"id": "1481420047", "references": ["1997063559", "2160225702", "2170120409", "1569320505", "1963565426", "2053197265", "2085261163", "1635989058", "2571050459", "2104095591"], "title": "Contour Tracking by Stochastic Propagation of Conditional Density", "abstract": "The problem of tracking curves in dense visual clutter is a challenging one. Trackers based on Kalman filters are of limited use; because they are based on Gaussian densities which are unimodal, they cannot represent simultaneous alternative hypotheses. Extensions to the Kalman filter to handle multiple data associations work satisfactorily in the simple case of point targets, but do not extend naturally to continuous curves. A new, stochastic algorithm is proposed here, the Condensation algorithm \u2014 Conditional Density Propagation over time. It uses \u2018factored sampling\u2019, a method previously applied to interpretation of static images, in which the distribution of possible interpretations is represented by a randomly generated set of representatives. The Condensation algorithm combines factored sampling with learned dynamical models to propagate an entire probability distribution for object position and shape, over time. The result is highly robust tracking of agile motion in clutter, markedly superior to what has previously been attainable from Kalman filtering. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.", "citation_count": "29", "reference_count": "2,966", "date": "1996", "authors": ["Michael Isard", "Andrew Blake"], "related_topics": ["Condensation algorithm", "Conditional probability distribution", "Kalman filter", "Continuous-time stochastic process", "Stochastic approximation", "Probability distribution", "Stochastic optimization", "Stochastic control", "Stable process", "Clutter", "Gaussian", "Sampling (statistics)", "Algorithm", "Mathematical optimization", "Affine space", "Computer science"]}
{"id": "2161406034", "references": ["1997063559", "2170120409", "1569320505", "2077611006", "2085261163", "2571050459", "2104095591", "2098613108", "1560013842", "1481420047"], "title": "C ONDENSATION \u2014Conditional Density Propagation forVisual Tracking", "abstract": "The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses \u201cfactored sampling\u201d, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.", "citation_count": "50", "reference_count": "8,650", "date": "1998", "authors": ["Michael Isard", "Andrew Blake"], "related_topics": ["Condensation algorithm", "Conditional probability distribution", "Probability distribution", "Kalman filter", "Motion estimation", "Gaussian", "Sampling (statistics)", "Tracking (particle physics)", "Algorithm", "Mathematics"]}
{"id": "2073257493", "references": ["2045597501", "2098683904", "2068868410", "2053127376", "2040187703", "1509703770", "2007780422", "2006769754", "2147311265", "2154634575"], "title": "An interactive activation model of context effects in letter perception: I. An account of basic findings.", "abstract": "", "citation_count": "45", "reference_count": "6,750", "date": "1981", "authors": ["James L. McClelland", "David E. Rumelhart"], "related_topics": ["Context effect", "Visual perception", "Perception", "Missing letter effect", "Word superiority effect", "Transposed letter effect", "Contextual Associations", "Cognitive psychology", "Cognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1505136099", "references": ["2100677568", "2126404188", "2154642048", "2144323544", "2150884987", "2016534914", "2119717200", "1686514609", "2112841646", "2080759927"], "title": "Learning by statistical cooperation of self-interested neuron-like computing elements.", "abstract": "Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here we describe an adaptive element that is robust enough to learn to cooperate with other elements like itself in order to further its self-interests. It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems. We then place the approach in its proper historical and theoretical perspective through comparison with a number of related algorithms. A secondary aim of this article is to suggest that beyond what is explicitly illustrated here, there is a wealth of ideas from game theory and allied disciplines such as mathematical economics that can be of use in thinking about cooperative computation in both nervous systems and man-made systems.", "citation_count": "0", "reference_count": "272", "date": "1985", "authors": ["Barto Ag"], "related_topics": ["Game theory", "Terminology", "Adaptation (computer science)", "Element (category theory)", "Computer science", "Theoretical computer science", "Computation", "Order (exchange)", "Perspective (graphical)", "Developmental psychology", "Game theoretic"]}
{"id": "2115647291", "references": ["2005639687", "2037893691", "2175030280", "121934918", "2021878536", "1539777654", "1979887415", "2954951251", "2099305423", "2064302241"], "title": "Direct manipulation interfaces", "abstract": "Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation.", "citation_count": "20", "reference_count": "2,231", "date": "1985", "authors": ["Edwin L. Hutchins", "James D. Hollan", "Donald A. Norman"], "related_topics": ["Direct manipulation interface", "Interface (Java)", "Information processing", "Property (programming)", "Human\u2013computer interaction", "Vocabulary", "Relation (database)", "Computer science", "Data processing", "Feeling"]}
{"id": "2021878536", "references": ["2154642048", "2082897112", "74870555", "2132920211", "1535418185", "2080957047", "3121257585", "2160271672"], "title": "User Centered System Design: New Perspectives on Human-Computer Interaction", "abstract": "Contents: S.W. Draper, D.A. Norman, C. Lewis, Introduction. Part I:User Centered System Design. K. Hooper, Architectural Design: An Analogy. L.J. Bannon, Issues in Design: Some Notes. D.A. Norman, Cognitive Engineering. Part II:The Interface Experience. B.K. Laurel, Interface as Mimesis. E.L. Hutchins, J.D. Hollan, D.A. NormanDirect Manipulation Interfaces. A.A. diSessa, Notes on the Future of Programming: Breaking the Utility Barrier. Part III:Users' Understandings. M.S. Riley, User Understanding. C. Lewis, Understanding What's Happening in System Interactions. D. Owen, Naive Theories of Computation. A.A. diSessa, Models of Computation. W. Mark, Knowledge-Based Interface Design. Part IV:User Activities. A. Cypher, The Structure of Users' Activities. Y. Miyata, D.A. Norman, Psychological Issues in Support of Multiple Activities. R. Reichman, Communication Paradigms for a Window System. Part V:Toward a Pragmatics of Human-Machine Communication. W. Buxton, There's More to Interaction Than Meets the Eye: Some Issues in Manual Input. S.W. Draper, Display Managers as the Basis for User-Machine Communication. Part VI:Information Flow. D. Owen, Answers First, Then Questions. C.E. O'Malley, Helping Users Help Themselves. L.J. Bannon, Helping Users Help Each Other. C. Lewis, D.A. Norman, Designing for Error. L.J. Bannon, Computer-Mediated Communication. Part VII:The Context of Computing. J.S. Brown, From Cognitive to Social Ergonomics and Beyond.", "citation_count": "0", "reference_count": "4,667", "date": "1986", "authors": ["Donald A. Norman", "Stephen W. Draper"], "related_topics": ["User experience design", "User interface", "Cognitive ergonomics", "Context (language use)", "Analogy", "Pragmatics", "Interface (Java)", "Human\u2013computer interaction", "Information flow", "Computer science"]}
{"id": "1490454746", "references": ["2113653296", "2073257493", "1514711945", "2103170504", "2010315761"], "title": "Feature discovery by competitive learning", "abstract": "This paper reporis the results of our studies with an unsupervised learning paradigm which we have called \u201cCompetitive Learning.\u201d We have examined competitive learning using both computer simulation and formal analysis and hove found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide o way to discover the salient, general features which can be used to classify o set of patterns. We show how o very simply competitive mechanism con discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We 0150 show how these feature detectors con form the basis of o multilayer system that con serve to learn categorizations of stimulus sets which ore not linearly separable. We show how the use of correlated stimuli con serve IX o kind of \u201cteaching\u201d input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism o very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is cm essentially nonassociative stotisticol learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in o more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features importont in the description of the stimulus environment in which the system finds itself.", "citation_count": "5", "reference_count": "1,782", "date": "1988", "authors": ["David E. Rumelhart", "David Zipser"], "related_topics": ["Competitive learning", "Feature learning", "Instance-based learning", "Unsupervised learning", "Semi-supervised learning", "Self-organizing map", "Artificial neural network", "Linear separability", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1998442441", "references": ["2042264548", "2154642048", "2110485445", "1554576613", "2137983211", "2138484437", "2581275558", "2064675550", "1498436455"], "title": "Neural networks for the prediction and forecasting of water resources variables: a review of modelling issues and applications", "abstract": "Abstract   Artificial Neural Networks (ANNs) are being used increasingly to predict and forecast water resources variables. In this paper, the steps that should be followed in the development of such models are outlined. These include the choice of performance criteria, the division and pre-processing of the available data, the determination of appropriate model inputs and network architecture, optimisation of the connection weights (training) and model validation. The options available to modellers at each of these steps are discussed and the issues that should be considered are highlighted. A review of 43 papers dealing with the use of neural network models for the prediction and forecasting of water resources variables is undertaken in terms of the modelling process adopted. In all but two of the papers reviewed, feedforward networks are used. The vast majority of these networks are trained using the backpropagation algorithm. Issues in relation to the optimal division of the available data, data pre-processing and the choice of appropriate model inputs are seldom considered. In addition, the process of choosing appropriate stopping criteria and optimising network geometry and internal network parameters is generally described poorly or carried out inadequately. All of the above factors can result in non-optimal model performance and an inability to draw meaningful comparisons between different models. Future research efforts should be directed towards the development of guidelines which assist with the development of ANN models and the choice of when ANNs should be used in preference to alternative approaches, the assessment of methods for extracting the knowledge that is contained in the connection weights of trained ANNs and the incorporation of uncertainty into ANN models.", "citation_count": "184", "reference_count": "2,497", "date": "2000", "authors": ["Holger R. Maier", "Graeme C. Dandy"], "related_topics": ["Artificial neural network", "Backpropagation", "Process (engineering)", "Network architecture", "Relation (database)", "Machine learning", "Computer science", "Feed forward", "Preference", "Water resources", "Artificial intelligence"]}
{"id": "2143956139", "references": ["2154642048", "2171277043", "2103496339", "94523489", "2019363670", "2165758113", "65738273", "2581275558", "3017143921", "1971735090"], "title": "Networks for approximation and learning", "abstract": "The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. &gt;", "citation_count": "65", "reference_count": "4,598", "date": "1990", "authors": ["T. Poggio", "F. Girosi"], "related_topics": ["Regularization perspectives on support vector machines", "Backus\u2013Gilbert method", "Tikhonov regularization", "Proximal gradient methods for learning", "Regularization (mathematics)", "Approximation theory", "Artificial neural network", "Cluster analysis", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2809684781", "references": ["2136922672", "2295598076", "2133665775", "2153635508", "2135046866", "2116064496", "2122410182", "2064675550", "2912934387", "2145339207"], "title": "A comprehensive survey on machine learning for networking: evolution, applications and research opportunities", "abstract": "Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.", "citation_count": "412", "reference_count": "417", "date": "2018", "authors": ["Raouf Boutaba", "Mohammad A. Salahuddin", "Noura Limam", "Sara Ayoubi", "Nashid Shahriar", "Felipe Estrada-Solano", "Oscar M. Caicedo"], "related_topics": ["Network security", "Fault management", "Network congestion", "Quality of service", "Resource (project management)", "Computer Applications", "Automation", "Computer science", "Key (cryptography)", "Machine learning", "Artificial intelligence"]}
{"id": "1993717606", "references": ["2172000360", "2119821739", "1498436455", "2137983211", "1596717185", "2148603752", "2111072639", "1964357740", "2912934387", "3124955340"], "title": "Extreme learning machines: a survey", "abstract": "Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.", "citation_count": "124", "reference_count": "1,842", "date": "2011", "authors": ["Guang-Bin Huang", "Dian Hui Wang", "Yuan Lan"], "related_topics": ["Extreme learning machine", "Computational intelligence", "Artificial neural network", "Support vector machine", "Machine learning", "Artificial intelligence", "Scalability", "Pattern recognition (psychology)", "Computer science", "Generalization", "Feed forward"]}
{"id": "2093229042", "references": ["1480376833", "2156909104", "740415", "2038669746", "2044771513", "2148603752", "2018044188", "1964357740", "2126105956", "3023786531"], "title": "Recent advances in surrogate-based optimization", "abstract": "Abstract   The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.", "citation_count": "80", "reference_count": "1,799", "date": "2009", "authors": ["Alexander I.J. Forrester", "Andy J. Keane"], "related_topics": ["Engineering optimization", "Probabilistic-based design optimization", "Global optimization", "Design methods", "Strengths and weaknesses", "Aerospace", "Systems engineering", "Management science", "State (computer science)", "Engineering", "Surrogate based optimization"]}
{"id": "2149723649", "references": ["1964168965", "1548502347", "2112032710", "2171277043", "1965324089", "94523489", "2146200922", "2138484437", "2087197472", "2118020555"], "title": "A general regression neural network", "abstract": "A memory-based network that provides estimates of continuous variables and converges to the underlying (linear or nonlinear) regression surface is described. The general regression neural network (GRNN) is a one-pass learning algorithm with a highly parallel structure. It is shown that, even with sparse data in a multidimensional measurement space, the algorithm provides smooth transitions from one observed value to another. The algorithmic form can be used for any regression problem in which an assumption of linearity is not justified. &gt;", "citation_count": "19", "reference_count": "5,443", "date": "1991", "authors": ["D.F. Specht"], "related_topics": ["Polynomial regression", "Proper linear model", "Regression analysis", "Bayesian multivariate linear regression", "Artificial neural network", "Multidimensional systems", "Parallel algorithm", "Regression", "Sparse matrix", "Nonlinear system", "Algorithm", "Theoretical computer science", "Computer science"]}
{"id": "1553004968", "references": ["2042264548", "2154642048", "1555915743", "2103496339", "2110485445", "2125838338", "1991848143", "2049633694", "2147800946", "2798909945"], "title": "Connectionist Speech Recognition: A Hybrid Approach", "abstract": "From the Publisher:\r\nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing.", "citation_count": "201", "reference_count": "1,650", "date": "1993", "authors": ["Herve A. Bourlard", "Nelson Morgan"], "related_topics": ["Time delay neural network", "Speech processing", "Feature (machine learning)", "Artificial neural network", "Pattern recognition (psychology)", "Hidden Markov model", "Connectionism", "Hybrid system", "Machine learning", "Artificial intelligence", "Speech recognition", "Computer science"]}
{"id": "2155399784", "references": ["2039402388", "2171277043", "2103496339", "2102380305", "94523489", "2078841894", "2296618885", "2581275558", "1524100745", "1971735090"], "title": "Orthogonal least squares learning algorithm for radial basis function networks", "abstract": "The radial basis function network offers a viable alternative to the two-layer neural network in many applications of signal processing. A common learning algorithm for radial basis function networks is based on first choosing randomly some data points as radial basis function centers and then using singular-value decomposition to solve for the weights of the network. Such a procedure has several drawbacks, and, in particular, an arbitrary selection of centers is clearly unsatisfactory. The authors propose an alternative learning procedure based on the orthogonal least-squares method. The procedure chooses radial basis function centers one by one in a rational way until an adequate network has been constructed. In the algorithm, each selected center maximizes the increment to the explained variance or energy of the desired output and does not suffer numerical ill-conditioning problems. The orthogonal least-squares learning strategy provides a simple and efficient means for fitting radial basis function networks. This is illustrated using examples taken from two different signal processing applications. &gt;", "citation_count": "22", "reference_count": "4,687", "date": "1991", "authors": ["S. Chen", "C.F.N. Cowan", "P.M. Grant"], "related_topics": ["Radial basis function network", "Basis function", "Radial basis function", "Feedforward neural network", "Artificial neural network", "Multidimensional signal processing", "Singular value decomposition", "Hierarchical RBF", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2133321814", "references": ["1570834090", "2079325629", "1965324089", "1989555277", "2581275558", "1497256448", "2912565176", "1992176519", "2016589492", "2062706881"], "title": "Neuro-fuzzy modeling and control", "abstract": "Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called adaptive-network-based fuzzy inference system (ANFIS), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed. &gt;", "citation_count": "66", "reference_count": "3,143", "date": "1995", "authors": ["J.-S.R. Jang", "Chuen-Tsai Sun"], "related_topics": ["Adaptive neuro fuzzy inference system", "Neuro-fuzzy", "Fuzzy control system", "Fuzzy logic", "Fuzzy set", "Adaptive control", "Artificial neural network", "Adaptive system", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2164278908", "references": ["2145096794", "3029645440", "2156909104", "2173213060", "2296616510", "2135046866", "1554944419", "2100556411", "2296319761", "2129638195"], "title": "Distributed Optimization and Statistical Learning Via the Alternating Direction Method of Multipliers", "abstract": "Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas\u2013Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.", "citation_count": "175", "reference_count": "15,215", "date": "2011", "authors": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "related_topics": ["Online machine learning", "Statistical learning theory", "Convex optimization", "Lasso (statistics)", "Support vector machine", "Basis pursuit", "Covariance", "Theoretical computer science", "Large numbers", "Mathematical optimization", "Computer science"]}
{"id": "2096544401", "references": ["2100830825", "2109722477", "2567948266", "1854214752", "2173213060", "2170616854", "1512387364", "2063978378", "2189465200", "2114507260"], "title": "Distributed GraphLab: a framework for machine learning and data mining in the cloud", "abstract": "While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees.We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.", "citation_count": "37", "reference_count": "1,384", "date": "2012", "authors": ["Yucheng Low", "Danny Bickson", "Joseph Gonzalez", "Carlos Guestrin", "Aapo Kyrola", "Joseph M. Hellerstein"], "related_topics": ["Snapshot algorithm", "Asynchronous communication", "Cloud computing", "Fault tolerance", "Data processing system", "Data consistency", "Distributed computing", "Network congestion", "Software versioning", "Computer science", "Data mining", "Machine learning", "Artificial intelligence"]}
{"id": "2951781666", "references": ["2113651538", "2121082877", "1992208280", "2173213060", "2150102617", "2202343345", "2035720976", "2798766386", "2124608575", "2113137767"], "title": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.", "citation_count": "27", "reference_count": "1,924", "date": "2011", "authors": ["Feng Niu", "Benjamin Recht", "Christopher Re", "Stephen J. Wright"], "related_topics": ["Stochastic gradient descent", "Optimization problem", "Synchronization (computer science)", "Non-blocking algorithm", "Rate of convergence", "Parallel computing", "Computer science", "Scheme (programming language)", "Work (physics)"]}
{"id": "2044212084", "references": ["2158893758", "1583497301", "2037710455", "1603765807", "2107396783", "2101517602", "2015410655", "2114791779", "2165744313", "2159715570"], "title": "Distributed Subgradient Methods for Multi-Agent Optimization", "abstract": "We study a distributed computation model for optimizing a sum of convex objective functions corresponding to multiple agents. For solving this (not necessarily smooth) optimization problem, we consider a subgradient method that is distributed among the agents. The method involves every agent minimizing his/her own objective function while exchanging information locally with other agents in the network over a time-varying topology. We provide convergence results and convergence rate estimates for the subgradient method. Our convergence rate results explicitly characterize the tradeoff between a desired accuracy of the generated approximate optimal solutions and the number of iterations needed to achieve the accuracy.", "citation_count": "31", "reference_count": "2,664", "date": "2009", "authors": ["A. Nedic", "A. Ozdaglar"], "related_topics": ["Subgradient method", "Convex optimization", "Rate of convergence", "Optimization problem", "Multi-agent system", "Mathematical optimization", "Computation", "Stochastic process", "Regular polygon", "Computer science"]}
{"id": "2963433607", "references": ["2952204734", "2963616027", "3034942609", "3114728946", "2963563140", "2963959597", "2969215180", "3129410129"], "title": "Optimization Methods for Large-Scale Machine Learning", "abstract": "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques th...", "citation_count": "0", "reference_count": "1,605", "date": "2018", "authors": ["L\u00e9on Bottou", "Frank E. Curtis", "Jorge Nocedal"], "related_topics": ["Optimization problem", "Nonlinear programming", "Context (language use)", "Machine learning", "Computer science", "Scale (chemistry)", "Artificial intelligence", "Deep neural networks", "Improved performance", "Optimization algorithm", "Optimization methods"]}
{"id": "2010630450", "references": ["2109159967", "2149170915", "2136530738", "2037710455", "2058254491", "1603765807", "2065092668", "2034420299", "2143690081", "2296319761"], "title": "User Association for Load Balancing in Heterogeneous Cellular Networks", "abstract": "For small cell technology to significantly increase the capacity of tower-based cellular networks, mobile users will need to be actively pushed onto the more lightly loaded tiers (corresponding to, e.g., pico and femtocells), even if they offer a lower instantaneous SINR than the macrocell base station (BS). Optimizing a function of the long-term rate for each user requires (in general) a massive utility maximization problem over all the SINRs and BS loads. On the other hand, an actual implementation will likely resort to a simple biasing approach where a BS in tier j is treated as having its SINR multiplied by a factor Aj \u2265 1, which makes it appear more attractive than the heavily-loaded macrocell. This paper bridges the gap between these approaches through several physical relaxations of the network-wide association problem, whose solution is NP hard. We provide a low-complexity distributed algorithm that converges to a near-optimal solution with a theoretical performance guarantee, and we observe that simple per-tier biasing loses surprisingly little, if the bias values Aj are chosen carefully. Numerical results show a large (3.5x) throughput gain for cell-edge users and a 2x rate gain for median users relative to a maximizing received power association.", "citation_count": "33", "reference_count": "1,181", "date": "2013", "authors": ["Qiaoyang Ye", "Beiyu Rong", "Yudong Chen", "M. Al-Shalash", "C. Caramanis", "J. G. Andrews"], "related_topics": ["Macrocell", "Small cell", "Utility maximization problem", "Femtocell", "Cellular network", "Load balancing (computing)", "Throughput", "Base station", "Distributed algorithm", "Computer network", "Computer science"]}
{"id": "2114791779", "references": ["2044212084", "2107396783", "1603765807", "2101517602", "2145574455", "2106221286", "1543439990", "2015410655", "2165744313", "1583497301"], "title": "Constrained Consensus and Optimization in Multi-Agent Networks", "abstract": "We present distributed algorithms that can be used by multiple agents to align their estimates with a particular value over a network with time-varying connectivity. Our framework is general in that this value can represent a consensus value among multiple agents or an optimal solution of an optimization problem, where the global objective function is a combination of local agent objective functions. Our main focus is on constrained problems where the estimates of each agent are restricted to lie in different convex sets. To highlight the effects of constraints, we first consider a constrained consensus problem and present a distributed \"projected consensus algorithm\" in which agents combine their local averaging operation with projection on their individual constraint sets. This algorithm can be viewed as a version of an alternating projection method with weights that are varying over time and across agents. We establish convergence and convergence rate results for the projected consensus algorithm. We next study a constrained optimization problem for optimizing the sum of local objective functions of the agents subject to the intersection of their local constraint sets. We present a distributed \"projected subgradient algorithm\" which involves each agent performing a local averaging operation, taking a subgradient step to minimize its own objective function, and projecting on its constraint set. We show that, with an appropriately selected stepsize rule, the agent estimates generated by this algorithm converge to the same optimal solution for the cases when the weights are constant and equal, and when the weights are time-varying but all agents have the same constraint set.", "citation_count": "28", "reference_count": "1,596", "date": "2010", "authors": ["A. Nedic", "A. Ozdaglar", "P.A. Parrilo"], "related_topics": ["Subgradient method", "Constrained optimization", "Distributed algorithm", "Optimization problem", "Consensus", "Multi-agent system", "Constraint (information theory)", "Autonomous agent", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "2165299997", "references": ["1580876177", "1576818901", "2166843422", "1639032689", "2109364787", "2135646341"], "title": "A modified particle swarm optimizer", "abstract": "Evolutionary computation techniques, genetic algorithms, evolutionary strategies and genetic programming are motivated by the evolution of nature. A population of individuals, which encode the problem solutions are manipulated according to the rule of survival of the fittest through \"genetic\" operations, such as mutation, crossover and reproduction. A best solution is evolved through the generations. In contrast to evolutionary computation techniques, Eberhart and Kennedy developed a different algorithm through simulating social behavior (R.C. Eberhart et al., 1996; R.C. Eberhart and J. Kennedy, 1996; J. Kennedy and R.C. Eberhart, 1995; J. Kennedy, 1997). As in other algorithms, a population of individuals exists. This algorithm is called particle swarm optimization (PSO) since it resembles a school of flying birds. In a particle swarm optimizer, instead of using genetic operators, these individuals are \"evolved\" by cooperation and competition among the individuals themselves through generations. Each particle adjusts its flying according to its own flying experience and its companions' flying experience. We introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the significant and effective impact of this new parameter on the particle swarm optimizer.", "citation_count": "6", "reference_count": "14,398", "date": "1998", "authors": ["Y. Shi", "R. Eberhart"], "related_topics": ["Particle swarm optimization", "Multi-swarm optimization", "Evolutionary computation", "Imperialist competitive algorithm", "Genetic programming", "Population", "Crossover", "Mutation (genetic algorithm)", "Artificial intelligence", "Mathematical optimization", "Computer science"]}
{"id": "1999284878", "references": ["2143560894", "2144317842", "2145479420", "1573676079", "1639032689", "2152195021", "2613176274", "2287814884", "1519405745"], "title": "Teaching-learning-based optimization: A novel method for constrained mechanical design optimization problems", "abstract": "A new efficient optimization method, called 'Teaching-Learning-Based Optimization (TLBO)', is proposed in this paper for the optimization of mechanical design problems. This method works on the effect of influence of a teacher on learners. Like other nature-inspired algorithms, TLBO is also a population-based method and uses a population of solutions to proceed to the global solution. The population is considered as a group of learners or a class of learners. The process of TLBO is divided into two parts: the first part consists of the 'Teacher Phase' and the second part consists of the 'Learner Phase'. 'Teacher Phase' means learning from the teacher and 'Learner Phase' means learning by the interaction between learners. The basic philosophy of the TLBO method is explained in detail. To check the effectiveness of the method it is tested on five different constrained benchmark test functions with different characteristics, four different benchmark mechanical design problems and six mechanical design optimization problems which have real world applications. The effectiveness of the TLBO method is compared with the other population-based optimization algorithms based on the best solution, average solution, convergence rate and computational effort. Results show that TLBO is more effective and efficient than the other optimization methods for the mechanical design optimization problems considered. This novel optimization method can be easily extended to other engineering design optimization problems.", "citation_count": "30", "reference_count": "2,684", "date": "2011", "authors": ["R. V. Rao", "V. J. Savsani", "D. P. Vakharia"], "related_topics": ["Engineering optimization", "Continuous optimization", "Probabilistic-based design optimization", "Multi-objective optimization", "Multi-swarm optimization", "Test functions for optimization", "Optimization problem", "Population", "Mathematical optimization", "Computer science"]}
{"id": "2167101736", "references": ["2119479037", "2154053567", "2143426320", "2124776405", "2153635508", "1995945562", "2152195021", "1639032689", "2904250082", "2099741732"], "title": "A survey on feature selection methods", "abstract": "Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.", "citation_count": "80", "reference_count": "2,694", "date": "2014", "authors": ["Girish Chandrashekar", "Ferat Sahin"], "related_topics": ["Feature selection", "Feature (machine learning)", "Feature extraction", "Pattern recognition (psychology)", "Variable elimination", "Machine learning", "Data mining", "Filter (signal processing)", "Dimension (data warehouse)", "Computer science", "Computation", "Artificial intelligence"]}
{"id": "2097571405", "references": ["2028569720", "1606791384", "2062663664", "2156728410", "1639032689", "1587100796", "2581275558", "2326587081", "123765585", "2133671888"], "title": "An Introduction to Genetic Algorithms", "abstract": "From the Publisher:\r\n\r\n\"This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book.\r\n-- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. \r\nGenetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. \r\nThe descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting \"general purpose\" nature of genetic algorithms as search methods that can be employed across disciplines. \r\nAn Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. \r\nThe first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.", "citation_count": "72", "reference_count": "17,058", "date": "1996", "authors": ["Melanie Mitchell"], "related_topics": ["Genetic programming", "Evolutionary computation", "Scientific modelling", "Artificial life", "Terminology", "Game theory", "Field (computer science)", "Data science", "Computational model", "Computer science", "Artificial intelligence"]}
{"id": "2168081761", "references": ["2790374560", "2117640392", "2013205100", "2027945080", "315572163", "1498178627", "1639032689", "1997600725", "2142183404", "2102248717"], "title": "Biogeography-Based Optimization", "abstract": "Biogeography is the study of the geographical distribution of biological organisms. Mathematical equations that govern the distribution of organisms were first discovered and developed during the 1960s. The mindset of the engineer is that we can learn from nature. This motivates the application of biogeography to optimization problems. Just as the mathematics of biological genetics inspired the development of genetic algorithms (GAs), and the mathematics of biological neurons inspired the development of artificial neural networks, this paper considers the mathematics of biogeography as the basis for the development of a new field: biogeography-based optimization (BBO). We discuss natural biogeography and its mathematics, and then discuss how it can be used to solve optimization problems. We see that BBO has features in common with other biology-based optimization methods, such as GAs and particle swarm optimization (PSO). This makes BBO applicable to many of the same types of problems that GAs and PSO are used for, namely, high-dimension problems with multiple local optima. However, BBO also has some features that are unique among biology-based optimization methods. We demonstrate the performance of BBO on a set of 14 standard benchmarks and compare it with seven other biology-based optimization algorithms. We also demonstrate BBO on a real-world sensor selection problem for aircraft engine health estimation.", "citation_count": "33", "reference_count": "3,203", "date": "2008", "authors": ["D. Simon"], "related_topics": ["Multi-swarm optimization", "Optimization problem", "Evolutionary algorithm", "Genetic algorithm", "Local search (optimization)", "Swarm intelligence", "Local optimum", "Particle swarm optimization", "Artificial intelligence", "Mathematical optimization"]}
{"id": "2017337590", "references": ["1680392829", "2125055259", "2149706766", "1639032689", "2084812512", "3124955340", "2340020088", "1594031697", "2912934387", "2122410182"], "title": "Wrappers for feature subset selection", "abstract": "Abstract   In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.", "citation_count": "122", "reference_count": "10,177", "date": "1997", "authors": ["Ron Kohavi", "George H. John"], "related_topics": ["Feature selection", "Minimum redundancy feature selection", "Selection (relational algebra)", "Feature (computer vision)", "Decision tree", "Markov blanket", "Relation (database)", "Filter (signal processing)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2165171393", "references": ["2116661285", "2167159964", "2152551290", "1639032689", "1585939719", "1553373771", "2125899728", "2106334424", "2126105956", "1504943474"], "title": "Handling multiple objectives with particle swarm optimization", "abstract": "This paper presents an approach in which Pareto dominance is incorporated into particle swarm optimization (PSO) in order to allow this heuristic to handle problems with several objective functions. Unlike other current proposals to extend PSO to solve multiobjective optimization problems, our algorithm uses a secondary (i.e., external) repository of particles that is later used by other particles to guide their own flight. We also incorporate a special mutation operator that enriches the exploratory capabilities of our algorithm. The proposed approach is validated using several test functions and metrics taken from the standard literature on evolutionary multiobjective optimization. Results indicate that the approach is highly competitive and that can be considered a viable alternative to solve multiobjective optimization problems.", "citation_count": "33", "reference_count": "3,825", "date": "2004", "authors": ["C.A.C. Coello", "G.T. Pulido", "M.S. Lechuga"], "related_topics": ["Multi-swarm optimization", "Metaheuristic", "Multi-objective optimization", "Imperialist competitive algorithm", "Particle swarm optimization", "Test functions for optimization", "Evolutionary algorithm", "Swarm intelligence", "Mathematical optimization", "Mathematics"]}
{"id": "1501500081", "references": ["2099111195", "2140190241", "2165874743", "2156718197", "2049633694", "1639032689", "1673310716", "1679913846", "2148694408", "1992419399"], "title": "A Survey of Clustering Data Mining Techniques", "abstract": "Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clustering is therefore related to many disciplines and plays an important role in a broad range of applications. The applications of clustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining. This survey concentrates on clustering algorithms from a data mining perspective.", "citation_count": "210", "reference_count": "3,528", "date": "2006", "authors": ["Pavel Berkhin"], "related_topics": ["Cluster analysis", "Consensus clustering", "Hierarchical clustering", "Clustering high-dimensional data", "Brown clustering", "Conceptual clustering", "Biclustering", "Data modeling", "Data mining", "Computer science"]}
{"id": "2106334424", "references": ["2116661285", "2121365620", "1558919105", "2151554678", "1639032689", "2261054240", "1497256448", "2125899728", "1905847227", "1504943474"], "title": "Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach", "abstract": "Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem.", "citation_count": "45", "reference_count": "8,566", "date": "1999", "authors": ["E. Zitzler", "L. Thiele"], "related_topics": ["Evolutionary algorithm", "Multi-objective optimization", "Evolutionary computation", "Knapsack problem", "Optimization problem", "Pareto principle", "Population", "Mating pool", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "2042264548", "references": ["2015634532", "3001909141", "35336130", "2016812413", "2156562940", "2132832266", "1973070179", "2536517113", "2113229797", "2017224880"], "title": "An introduction to computing with neural nets", "abstract": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets.", "citation_count": "0", "reference_count": "12,289", "date": "1988", "authors": ["Richard P. Lippmann"], "related_topics": ["Artificial neural network", "Cluster analysis", "Field (computer science)", "Massively parallel", "Very-large-scale integration", "Artificial intelligence", "Network topology", "Noise (video)", "Computer science", "Gaussian"]}
{"id": "2098257210", "references": ["2099111195", "2798333393", "1549664537", "2133475491", "2118040894", "2132818688", "2130509920", "1667950888", "2613173048", "2145417574"], "title": "Fading channels: information-theoretic and communications aspects", "abstract": "In this paper we review the most peculiar and interesting information-theoretic and communications features of fading channels. We first describe the statistical models of fading channels which are frequently used in the analysis and design of communication systems. Next, we focus on the information theory of fading channels, by emphasizing capacity as the most important performance measure. Both single-user and multiuser transmission are examined. Further, we describe how the structure of fading channels impacts code design, and finally overview equalization of fading multipath channels.", "citation_count": "440", "reference_count": "2,445", "date": "1998", "authors": ["E. Biglieri", "J. Proakis", "S. Shamai"], "related_topics": ["Fading", "Channel state information", "Diversity scheme", "Communications system", "Channel capacity", "Wireless", "Transmission (telecommunications)", "Equalization (audio)", "Communication channel", "Electronic engineering", "Telecommunications", "Computer science", "Channel code"]}
{"id": "2604319603", "references": ["2102605133", "2618530766", "2155893237", "2097117768", "2194775991", "2919115771", "1903029394", "2962835968", "2117539524", "1836465849"], "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey", "abstract": "Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.", "citation_count": "136", "reference_count": "1,662", "date": "2017", "authors": ["Vivienne Sze", "Yu-Hsin Chen", "Tien-Ju Yang", "Joel S. Emer"], "related_topics": ["Machine learning", "Computer science", "Efficient energy use", "Field (computer science)", "Benchmarking", "Throughput (business)", "Key (cryptography)", "Robotics", "Computational complexity theory", "Software deployment", "Artificial intelligence"]}
{"id": "2025605741", "references": ["2171960770", "2149684865", "2110325612", "2097726984", "2042281163", "1971040550", "1660390307", "2101409192", "2100235918", "2054141820"], "title": "Recommender systems survey", "abstract": "Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance.", "citation_count": "246", "reference_count": "2,786", "date": "2013", "authors": ["J. Bobadilla", "F. Ortega", "A. Hernando", "A. Guti\u00e9Rrez"], "related_topics": ["Recommender system", "Collaborative filtering", "Cold start", "Personally identifiable information", "World Wide Web", "Computer science", "Internet of Things"]}
{"id": "1570963478", "references": ["1601740268", "1510073064", "2099111195", "2119821739", "2148603752", "2087347434", "3124955340", "2168405694", "2137813581", "3023786531"], "title": "Prediction, learning, and games", "abstract": "This important text and reference for researchers and students in machine learning, game theory, statistics and information theory offers a comprehensive treatment of the problem of predicting individual sequences. Unlike standard statistical approaches to forecasting, prediction of individual sequences does not impose any probabilistic assumption on the data-generating mechanism. Yet, prediction algorithms can be constructed that work well for all possible sequences, in the sense that their performance is always nearly as good as the best forecasting strategy in a given reference class. The central theme is the model of prediction using expert advice, a general framework within which many related problems can be cast and discussed. Repeated game playing, adaptive data compression, sequential investment in the stock market, sequential pattern analysis, and several other problems are viewed as instances of the experts' framework and analyzed from a common nonstochastic standpoint that often reveals new and intriguing connections.", "citation_count": "266", "reference_count": "3,645", "date": "2006", "authors": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "related_topics": ["Repeated game", "Game theory", "Probabilistic logic", "Information theory", "Computational statistics", "Machine learning", "Data compression", "Artificial intelligence", "Computer science", "Stock market", "Theme (narrative)"]}
{"id": "114517082", "references": ["2113651538", "2154642048", "2147880316", "1652505363", "2119821739", "2150102617", "1535810436", "2135046866", "2068484625", "2035720976"], "title": "Large-Scale Machine Learning with Stochastic Gradient Descent", "abstract": "During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.", "citation_count": "24", "reference_count": "4,731", "date": "2010", "authors": ["L\u00e9on Bottou"], "related_topics": ["Stochastic gradient descent", "Gradient method", "Gradient descent", "Backpropagation", "Restricted Boltzmann machine", "Wake-sleep algorithm", "Computational complexity theory", "Context (language use)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1746819321", "references": ["2078206416", "2156909104", "3140968660", "2170120409", "1554663460", "2148603752", "2117812871", "2296319761", "2798909945", "3023786531"], "title": "Gaussian Processes for Machine Learning", "abstract": "A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines. Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.", "citation_count": "180", "reference_count": "23,786", "date": "2005", "authors": ["Carl Edward Rasmussen", "Christopher K I Williams"], "related_topics": ["Active learning (machine learning)", "Semi-supervised learning", "Instance-based learning", "Online machine learning", "Relevance vector machine", "Computational learning theory", "Kernel method", "Unsupervised learning", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1570448133", "references": ["2140190241", "2156909104", "2119821739", "2138621811", "2139212933", "1995945562", "1639032689", "1554663460", "3013264884", "2912934387"], "title": "Data Mining: Practical Machine Learning Tools and Techniques", "abstract": "Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization", "citation_count": "202", "reference_count": "37,112", "date": "1999", "authors": ["Ian H. Witten", "Eibe Frank", "Mark A. Hall"], "related_topics": ["Active learning (machine learning)", "Instance-based learning", "Algorithmic learning theory", "Data stream mining", "Ensemble learning", "Hyper-heuristic", "Cluster analysis", "Association rule learning", "Data science", "Computer science", "Machine learning", "Data mining", "Artificial intelligence"]}
{"id": "2163352848", "references": ["2124353687", "2132047332", "2098347925", "2159988601", "2039051707", "2021751319", "2136343973", "3017143921", "2106798282", "1993655741"], "title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "abstract": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns.", "citation_count": "46", "reference_count": "16,231", "date": "2002", "authors": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "related_topics": ["Local binary patterns", "Binary pattern", "Image texture", "Invariant (physics)", "Texture Descriptor", "Multiresolution analysis", "Contextual image classification", "Operator (computer programming)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2117812871", "references": ["2112076978", "2046079134", "2156909104", "2119821739", "1536929369", "2147800946", "1554663460", "1679913846", "2912934387", "3124955340"], "title": "Pattern recognition and neural networks", "abstract": "From the Publisher:\r\nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.", "citation_count": "102", "reference_count": "9,263", "date": "1996", "authors": ["Brian D. Ripley", "N. L. Hjort"], "related_topics": ["Feature (machine learning)", "Intelligent character recognition", "Pattern recognition (psychology)", "Neocognitron", "Cellular neural network", "Artificial neural network", "Relation (database)", "Computer science", "Pattern recognition", "Subject (documents)", "Artificial intelligence"]}
{"id": "1528676759", "references": ["1854214752", "3122305203", "2138621811", "2146819619", "1605188341", "2109100253", "1987497363", "2107726111", "2610670723", "2122410182"], "title": "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations", "abstract": "This exciting and pioneering new overview of multiagent systems, which are online systems composed of multiple interacting intelligent agents, i.e., online trading, offers a newly seen computer science perspective on multiagent systems, while integrating ideas from operations research, game theory, economics, logic, and even philosophy and linguistics. The authors emphasize foundations to create a broad and rigorous treatment of their subject, with thorough presentations of distributed problem solving, game theory, multiagent communication and learning, social choice, mechanism design, auctions, cooperative game theory, and modal logics of knowledge and belief. For each topic, basic concepts are introduced, examples are given, proofs of key results are offered, and algorithmic considerations are examined. An appendix covers background material in probability theory, classical logic, Markov decision processes and mathematical programming. Written by two of the leading researchers of this engaging field, this book will surely serve as THE reference for researchers in the fastest-growing area of computer science, and be used as a text for advanced undergraduate or graduate courses.", "citation_count": "269", "reference_count": "2,902", "date": "2008", "authors": ["Yoav Shoham", "Kevin Leyton-Brown"], "related_topics": ["Algorithmic game theory", "Game theory", "Multi-agent system", "Cooperative game theory", "Mechanism design", "Markov decision process", "Intelligent agent", "Mathematical proof", "Management science", "Computer science"]}
{"id": "2964043796", "references": ["2121863487", "2963477884", "2964161785", "2949608212", "1515851193", "2155968351", "1757796397", "2952509347", "2145339207"], "title": "Asynchronous methods for deep reinforcement learning", "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.", "citation_count": "30", "reference_count": "4,975", "date": "2016", "authors": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Timothy P. Lillicrap", "David Silver", "Koray Kavukcuoglu"], "related_topics": ["Asynchronous communication", "Artificial neural network", "Reinforcement learning", "Deep learning", "Gradient descent", "Artificial intelligence", "Domain (software engineering)", "Computer science", "Task (computing)", "Motor control"]}
{"id": "2886851211", "references": ["639708223", "2097117768", "1536680647", "2194775991", "2963864421", "3118608800", "2612445135", "2963163009", "2962835968", "1836465849"], "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices", "abstract": "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy.", "citation_count": "46", "reference_count": "629", "date": "2018", "authors": ["Yihui He", "Ji Lin", "Zhijian Liu", "Hanrui Wang", "Li-Jia Li", "Song Han"], "related_topics": ["Speedup", "Compression (functional analysis)", "Pipeline (computing)", "Artificial neural network", "Reduction (complexity)", "Computer engineering", "Pixel", "Computer science"]}
{"id": "2963923407", "references": ["2963704132", "3101283005", "3035717769", "3023640063", "2962804251", "3098053103", "3102848167", "3162439934", "3115727409", "2889347284"], "title": "Addressing Function Approximation Error in Actor-Critic Methods", "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.", "citation_count": "0", "reference_count": "840", "date": "2018", "authors": ["Scott Fujimoto", "Herke van Hoof", "David Meger"], "related_topics": ["Reinforcement learning", "Function approximation", "Limit (mathematics)", "Value (mathematics)", "Mathematical optimization", "Computer science", "State (computer science)"]}
{"id": "2155968351", "references": ["2100677568", "166862392", "1515851193", "2310919327", "1658008008", "2168405694", "3011120880", "2107726111", "2952509347", "2145339207"], "title": "Deep reinforcement learning with double Q-Learning", "abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.", "citation_count": "26", "reference_count": "3,313", "date": "2016", "authors": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "related_topics": ["Q-learning", "Reinforcement learning", "Artificial neural network", "Function approximation", "Artificial intelligence", "Computer science", "Domain (software engineering)", "Action (philosophy)"]}
{"id": "2107726111", "references": ["2100677568", "1652505363", "2019363670", "1639032689", "2000836282", "2098432798", "2119717200", "2119567691", "1497256448", "1963547452"], "title": "Reinforcement learning: a survey", "abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.", "citation_count": "110", "reference_count": "8,679", "date": "1996", "authors": ["Leslie Pack Kaelbling", "Michael L. Littman", "Andrew W. Moore"], "related_topics": ["Learning classifier system", "Reinforcement learning", "Robot learning", "Proactive learning", "Q-learning", "Algorithmic learning theory", "Instance-based learning", "Computational learning theory", "Artificial intelligence", "Computer science"]}
{"id": "1547925194", "references": ["2100677568", "2121863487", "1515851193", "2109910161", "2131600418", "2117726420", "3011120880", "1576452626", "2130005627", "2178806388"], "title": "An emphatic approach to the problem of off-policy temporal-difference learning", "abstract": "In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(\u03b3)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(\u03b3), and GQ(\u03bb). Compared to these methods, our emphatic TD(\u03bb) is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.", "citation_count": "59", "reference_count": "140", "date": "2016", "authors": ["Richard S. Sutton", "A. Rupam Mahmood", "Martha White"], "related_topics": ["Stability (learning theory)", "Temporal difference learning", "Function approximation", "Parametric statistics", "Bootstrapping", "Algorithm", "Convergence (routing)", "Computation", "Mathematical optimization", "Discounting", "Mathematics"]}
{"id": "1542941925", "references": ["1545148916", "1641379095", "1557517019", "134786152", "1549353711", "32403112", "2028145673", "1610678877", "137532508", "2178806388"], "title": "Markov games as a framework for multi-agent reinforcement learning", "abstract": "In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.", "citation_count": "12", "reference_count": "2,701", "date": "1994", "authors": ["Michael L. Littman"], "related_topics": ["Partially observable Markov decision process", "Markov decision process", "Q-learning", "Reinforcement learning", "Markov chain", "Probabilistic logic", "Artificial intelligence", "Computer science", "Simple (abstract algebra)"]}
{"id": "2158091072", "references": ["2100677568", "2341171179", "1993740947", "1596324102", "1610678877", "1569296262", "3011120880", "2895674046", "1583833196", "2178806388"], "title": "The Convergence of TD(\u03bb) for General \u03bb", "abstract": "The method of temporal differences (TD) is one way of making consistent predictions about the future. This paper uses some analysis of Watkins (1989) to extend a convergence theorem due to Sutton (1988) from the case which only uses information from adjacent time steps to that involving information from arbitrary ones.\r\n\r\nIt also considers how this version of TD behaves in the face of linearly dependent representations for states\u2014demonstrating that it still converges, but to a different answer from the least mean squares algorithm. Finally it adapts Watkins' theorem that \\cal Q-learning, his closely related prediction and action learning method, converges with probability one, to demonstrate this strong form of convergence for a slightly modified version of TD.", "citation_count": "26", "reference_count": "377", "date": "1992", "authors": ["Peter Dayan"], "related_topics": ["Reinforcement learning", "Convergence (routing)", "Linear independence", "Face (geometry)", "Algorithm", "Mathematics", "Action learning", "Least mean square algorithm"]}
{"id": "1592847719", "references": ["2121863487", "1564534945", "32403112", "1515851193", "2168359464", "2099529102", "2109910161", "3011120880", "1576452626", "2107726111"], "title": "Recent Advances in Hierarchical Reinforcement Learning", "abstract": "Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.", "citation_count": "88", "reference_count": "1,302", "date": "2003", "authors": ["Andrew G. Barto", "Sridhar Mahadevan"], "related_topics": ["Reinforcement learning", "Hierarchical organization", "Markov decision process", "Curse of dimensionality", "Abstraction (linguistics)", "Encoding (memory)", "Artificial intelligence", "Computer science", "Hierarchy", "Observability"]}
{"id": "2995201943", "references": ["2148061495", "2406349003", "2038420319", "2765854388", "2159269332", "2158449659", "2139412680", "2767547957", "2161160262", "2178806388"], "title": "A survey on machine learning for data fusion", "abstract": "Abstract   Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.", "citation_count": "76", "reference_count": "69", "date": "2020", "authors": ["Tong Meng", "Xuyang Jing", "Zheng Yan", "Witold Pedrycz"], "related_topics": ["Sensor fusion", "Field (computer science)", "Raw data", "Probabilistic logic", "Machine learning", "Computer science", "State (computer science)", "Range (mathematics)", "Imperfect", "Artificial intelligence"]}
{"id": "2117355432", "references": ["1563088657", "3122732203", "166862392", "1484867920", "1529558080", "1542886316", "2119567691", "1576452626", "2130005627", "3023786531"], "title": "Finite-Time Bounds for Fitted Value Iteration", "abstract": "In this paper we develop a theoretical analysis of the performance of sampling-based fitted value iteration (FVI) to solve infinite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of finite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufficiently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted Lp-norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reflects how well the function space is \"aligned\" with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical findings.", "citation_count": "60", "reference_count": "339", "date": "2008", "authors": ["R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "related_topics": ["Kernel (statistics)", "Markov decision process", "Rate of convergence", "Function space", "Measure (mathematics)", "Markov process", "Scale (descriptive set theory)", "Artificial neural network", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2019003829", "references": ["2132051999", "2014889099", "1671906456", "2153319198", "2097147952", "2027377866", "2148831787", "2000042664", "1976969221", "1572272766"], "title": "Shortest-path queries in static networks", "abstract": "We consider the point-to-point (approximate) shortest-path query problem, which is the following generalization of the classical single-source (SSSP) and all-pairs shortest-path (APSP) problems: we are first presented with a network (graph). A so-called preprocessing algorithm may compute certain information (a data structure or index) to prepare for the next phase. After this preprocessing step, applications may ask shortest-path or distance queries, which should be answered as fast as possible.Due to its many applications in areas such as transportation, networking, and social science, this problem has been considered by researchers from various communities (sometimes under different names): algorithm engineers construct fast route planning methods; database and information systems researchers investigate materialization tradeoffs, query processing on spatial networks, and reachability queries; and theoretical computer scientists analyze distance oracles and sparse spanners. Related problems are considered for compact routing and distance labeling schemes in networking and distributed computing and for metric embeddings in geometry as well.In this survey, we review selected approaches, algorithms, and results on shortest-path queries from these fields, with the main focus lying on the tradeoff between the index size and the query time. We survey methods for general graphs as well as specialized methods for restricted graph classes, in particular for those classes with arguable practical significance such as planar graphs and complex networks.", "citation_count": "249", "reference_count": "211", "date": "2014", "authors": ["Christian Sommer"], "related_topics": ["Spatial query", "Complex network", "Shortest path problem", "Reachability", "Planar graph", "Data structure", "Theoretical computer science", "Information system", "Computer science", "Preprocessor"]}
{"id": "2104753538", "references": ["2121863487", "1484867920", "1515851193", "1564947197", "1501503542", "1529558080", "1542886316", "1995713768", "2312609093", "1576452626"], "title": "Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path", "abstract": "In this paper we consider the problem of finding a near-optimal policy in a continuous space, discounted Markovian Decision Problem (MDP) by employing value-function-based methods when only a single trajectory of a fixed policy is available as the input. We study a policy-iteration algorithm where the iterates are obtained via empirical risk minimization with a risk function that penalizes high magnitudes of the Bellman-residual. Our main result is a finite-sample, high-probability bound on the performance of the computed policy that depends on the mixing rate of the trajectory, the capacity of the function set as measured by a novel capacity concept (the VC-crossing dimension), the approximation power of the function set and the controllability properties of the MDP. Moreover, we prove that when a linear parameterization is used the new algorithm is equivalent to Least-Squares Policy Iteration. To the best of our knowledge this is the first theoretical result for off-policy control learning over continuous state-spaces using a single trajectory.", "citation_count": "44", "reference_count": "337", "date": "2008", "authors": ["Andr\u00e1s Antos", "Csaba Szepesv\u00e1ri", "R\u00e9mi Munos"], "related_topics": ["Empirical risk minimization", "Bellman equation", "Iterated function", "Reinforcement learning", "Decision problem", "Controllability", "Residual", "Markov process", "Mathematical optimization", "Mathematics"]}
{"id": "2108384452", "references": ["2038085771", "1667165204", "2019502123", "2006544565", "1996355918", "2096789154", "2124776405", "2056211671", "2180838288", "2099741732"], "title": "An information-maximization approach to blind separation and blind deconvolution", "abstract": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing.", "citation_count": "50", "reference_count": "11,140", "date": "1995", "authors": ["Anthony J. Bell", "Terrence J. Sejnowski"], "related_topics": ["Blind signal separation", "Blind deconvolution", "Independent component analysis", "Source separation", "Maximization", "Infomax", "Deconvolution", "Information transfer", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2042986967", "references": ["2106378689", "2009803313", "2058937865", "2142022975", "2148673189", "1992928037", "2037455079", "2056898464", "1969186119", "2161455936"], "title": "An Effective Heuristic Algorithm for the Traveling-Salesman Problem", "abstract": "This paper discusses a highly effective heuristic procedure for generating optimum and near-optimum solutions for the symmetric traveling-salesman problem. The procedure is based on a general approach to heuristics that is believed to have wide applicability in combinatorial optimization problems. The procedure produces optimum solutions for all problems tested, \"classical\" problems appearing in the literature, as well as randomly generated test problems, up to 110 cities. Run times grow approximately as n2; in absolute terms, a typical 100-city problem requires less than 25 seconds for one case GE635, and about three minutes to obtain the optimum with above 95 per cent confidence.", "citation_count": "11", "reference_count": "4,727", "date": "1973", "authors": ["S. Lin", "B. W. Kernighan"], "related_topics": ["Lin\u2013Kernighan heuristic", "Greedy algorithm", "2-opt", "Bottleneck traveling salesman problem", "Optimization problem", "Traveling purchaser problem", "Christofides algorithm", "Travelling salesman problem", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "1492640216", "references": ["2118573797", "1530490667", "2132238781", "2004617458", "2168747298", "2107941094", "2118051273", "2154929945", "1801849579", "2001422417"], "title": "Optimization, Learning and Natural Algorithms", "abstract": "", "citation_count": "0", "reference_count": "5,221", "date": "1992", "authors": ["M. Dorigo"], "related_topics": ["Metaheuristic", "Quality control and genetic algorithms", "Learning classifier system", "Test functions for optimization", "Instance-based learning", "Evolutionary computation", "Computational learning theory", "Active learning (machine learning)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2104670598", "references": ["2084792706", "2168000780", "3139666781", "2047422346", "1513769081", "2581275558", "2164855953", "2143037347", "307896644", "2016688797"], "title": "Tabu Search\u2014Part II", "abstract": "This is the second half of a two part series devoted to the tabu search metastrategy for optimization problems. Part I introduced the fundamental ideas of tabu search as an approach for guiding other heuristics to overcome the limitations of local optimality, both in a deterministic and a probabilistic framework. Part I also reported successful applications from a wide range of settings, in which tabu search frequently made it possible to obtain higher quality solutions than previously obtained with competing strategies, generally with less computational effort. Part II, in this issue, examines refinements and more advanced aspects of tabu search. Following a brief review of notation, Part II introduces new dynamic strategies for managing tabu lists, allowing fuller exploitation of underlying evaluation functions. In turn, the elements of staged search and structured move sets are characterized, which bear on the issue of finiteness. Three ways of applying tabu search to the solution of integer programmin...", "citation_count": "22", "reference_count": "9,401", "date": "1989", "authors": ["Fred W. Glover"], "related_topics": ["Tabu search", "Guided Local Search", "Hill climbing", "Heuristics", "Heuristic (computer science)", "Optimization problem", "Mathematical optimization", "Notation", "Range (mathematics)", "Mathematics"]}
{"id": "2152150600", "references": ["1820051232", "2144317842", "2076063813", "2125213524", "1504943474", "1595159159", "2107941094", "2125899728", "1577668191", "1992419399"], "title": "Genetic Algorithms in Search", "abstract": "", "citation_count": "0", "reference_count": "17,726", "date": "1989", "authors": ["D. E. Goldberg"], "related_topics": ["Quality control and genetic algorithms", "Genetic representation", "Cultural algorithm", "Truncation selection", "Population-based incremental learning", "Meta-optimization", "Genetic algorithm", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1497256448", "references": ["2076063813", "2152195021", "2107941094", "2134967712", "1595498733", "2109364787", "3102641634", "1992419399"], "title": "Adaptation in natural and artificial systems", "abstract": "", "citation_count": "0", "reference_count": "59,176", "date": "1992", "authors": ["John H. Holland"], "related_topics": ["Artificial development", "Artificial creation", "Adaptation (computer science)", "Evolutionary acquisition of neural topologies", "Genetic fuzzy systems", "Computer science", "Effective fitness", "Evolutionary programming", "Natural (archaeology)", "Artificial intelligence"]}
{"id": "3011460294", "references": ["2097034581", "1595159159", "2119040341", "2110196981", "2107941094", "2153936654", "2123950200", "2143185749", "2016268734", "2149056569"], "title": "Genetic Algorithms in Search, Optimization &amp; Machine Learning", "abstract": "", "citation_count": "0", "reference_count": "2,556", "date": "1989", "authors": ["D. E. Goldberg"], "related_topics": ["Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2297395784", "references": ["2042986967", "1993251125", "2089890700", "2037294630", "2104670598", "1528149415", "2020982711", "2131253923"], "title": "Distributed Optimization by Ant Colonies", "abstract": "Ants colonies exhibit very interesting behaviours: even if a single ant only has simple capabilities, the behaviour of a whole ant colony is highly structured. This is the result of coordinated interactions. But, as communication possibilities among ants are very limited, interactions must be based on very simple flows of information. In this paper we explore the implications that the study of ants behaviour can have on problem solving and optimization. We introduce a distributed problem solving environment and propose its use to search for a solution to the travelling salesman problem.", "citation_count": "8", "reference_count": "4,886", "date": "1992", "authors": ["Alberto Colorni", "Marco Dorigo", "Vittorio Maniezzo", "Francisco Varela", "P. Bourgine"], "related_topics": ["Ant colony", "Travelling salesman problem", "Problem solving environment", "Simple (abstract algebra)", "Mathematical optimization", "Computer science"]}
{"id": "1547224907", "references": ["2321533354", "2076063813", "1562911371", "2310919327", "1516111018", "2116064496", "2137813581", "2133671888", "2072128103"], "title": "Learning and relearning in Boltzmann machines", "abstract": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References", "citation_count": "0", "reference_count": "2,094", "date": "1986", "authors": ["G. E. Hinton", "T. J. Sejnowski"], "related_topics": ["Boltzmann machine", "Computation", "Relaxation (approximation)", "Artificial intelligence", "Computer science", "Boltzmann constant"]}
{"id": "2101706260", "references": ["1813659000", "28412257", "2104867159", "1547224907", "2150884987", "2159080219", "2100559472", "2116064496", "1667072054", "2912934387"], "title": "Recognizing handwritten digits using hierarchical products of experts", "abstract": "The product of experts learning procedure can discover a set of stochastic binary features that constitute a nonlinear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, a hierarchy of separate models can be learned, for each digit class. Each model in the hierarchy learns a layer of binary feature detectors that model the probability distribution of vectors of activity of feature detectors in the layer below. The models in the hierarchy are trained sequentially and each model uses a layer of binary feature detectors to learn a generative model of the patterns of feature activities in the preceding layer. After training, each layer of feature detectors produces a separate, unnormalized log probability score. With three layers of feature detectors for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data.", "citation_count": "15", "reference_count": "83", "date": "2002", "authors": ["G. Mayraz", "G.E. Hinton"], "related_topics": ["Generative model", "Feature (computer vision)", "Discriminative model", "Feature extraction", "Product of experts", "Standard test image", "Artificial neural network", "Handwriting recognition", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2083380015", "references": ["1593793857", "1652505363", "2083875149", "1992880122", "1547224907", "2159080219", "2049633694", "1507849272", "2166698530", "1498436455"], "title": "Connectionist learning of belief networks", "abstract": "Abstract   Connectionist learning procedures are presented for \u201csigmoid\u201d and \u201cnoisy-OR\u201d varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the \u201cGibbs sampling\u201d simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for \u201cBoltzmann machines\u201d, and like it, allows the use of \u201chidden\u201d variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the \u201cnegative phase\u201d of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.", "citation_count": "21", "reference_count": "688", "date": "1992", "authors": ["Radford M. Neal"], "related_topics": ["Restricted Boltzmann machine", "Boltzmann machine", "Computational learning theory", "Feature learning", "Unsupervised learning", "Instance-based learning", "Semi-supervised learning", "Deep belief network", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2114153178", "references": ["1813659000", "2113341759", "2125027820", "1547224907", "2121647436", "2159080219", "2116064496", "2128716185", "1902027874", "2138451337"], "title": "Rate-coded Restricted Boltzmann Machines for Face Recognition", "abstract": "We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.", "citation_count": "12", "reference_count": "183", "date": "2000", "authors": ["Yee Whye Teh", "Geoffrey E. Hinton"], "related_topics": ["Feature (computer vision)", "Generative model", "Posterior probability", "Standard test image", "Boltzmann machine", "Facial recognition system", "Face (geometry)", "Pattern recognition", "Data mining", "Computer science", "Artificial intelligence"]}
{"id": "1746680969", "references": ["2166049352", "1755360231", "2166851633", "2097089247", "2116064496", "1880262756", "1873332500", "2072128103"], "title": "Learning in graphical models", "abstract": "Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams.", "citation_count": "0", "reference_count": "2,680", "date": "1999", "authors": ["Michael I. Jordan"], "related_topics": ["Graphical model", "Bayesian network", "Markov chain Monte Carlo", "Inference", "Artificial neural network", "Cluster analysis", "Latent variable", "Gaussian process", "Artificial intelligence", "Computer science"]}
{"id": "2047870719", "references": ["2046079134", "1991848143", "2005314985", "2098929365", "65738273", "2913399920", "3017143921", "2166322089", "2002182716", "22297218"], "title": "Topology representing networks", "abstract": "Abstract   A Hebbian adaptation rule with winner-take-all like competition is introduced. It is shown that this competitive Hebbian rule forms so-called Delaunay triangulations, which play an important role in computational geometry for efficiently solving proximity problems. Given a set of neural units i, i = 1,\u2026, N, the synaptic weights of which can be interpreted as pointers wi, i = 1,\u2026, N in RD, the competitive Hebbian rule leads to a connectivity structure between the units i that corresponds to the Delaunay triangulation of the set of pointers wi. Such competitive Hebbian rule develops connections (Cij &gt; 0) between neural units i, j with neighboring receptive fields (Voronoi polygons) Vi, Vj, whereas between all other units i, j no connections evolve (Cij = 0). Combined with a procedure that distributes the pointers wi over a given feature manifold M, for example, a submanifold M \u2282 RD, the competitive Hebbian rule provides a novel approach to the problem of constructing topology preserving feature maps and representing intricately structured manifolds. The competitive Hebbian rule connects only neural units, the receptive fields (Voronoi polygons) Vi, Vj of which are adjacent on the given manifold M. This leads to a connectivity structure that defines a perfectly topology preserving map and forms a discrete, path preserving representation of M, also in cases where M has an intricate topology. This makes this novel approach particularly useful in all applications where neighborhood relations have to be exploited or the shape and topology of submanifolds have to be take into account.", "citation_count": "48", "reference_count": "1,138", "date": "1994", "authors": ["Thomas Martinetz", "Klaus Schulten"], "related_topics": ["Hebbian theory", "Delaunay triangulation", "Proximity problems", "Manifold", "Computational geometry", "Path (graph theory)", "Submanifold", "Topology (chemistry)", "Topology", "Mathematics"]}
{"id": "2107636931", "references": ["2051719061", "2137969290", "2146610201", "2125027820", "2049633694", "1554663460", "65738273", "2044758663", "2166698530", "1679913846"], "title": "GTM: the generative topographic mapping", "abstract": "Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping, for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline.", "citation_count": "38", "reference_count": "1,762", "date": "1998", "authors": ["Christopher M. Bishop", "Markus Svens\u00e9n", "Christopher K. I. Williams"], "related_topics": ["Generative topographic map", "Latent variable model", "Probabilistic latent semantic analysis", "Latent variable", "Expectation\u2013maximization algorithm", "Self-organizing map", "Toy problem", "Probability density function", "Pattern recognition", "Artificial intelligence", "Mathematics"]}
{"id": "2070320140", "references": ["2135463994", "2142912032", "94523489", "1963565426", "2053197265", "2095757522", "2123977795", "1981025032", "2138835141", "2138451337"], "title": "Image representations for visual learning.", "abstract": "Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models. Many of these techniques depend on a representation of images that induce a linear vector space structure and in principle requires dense feature correspondence. This image representation allows the use of learning techniques for the analysis of images (for computer vision) as well as for the synthesis of images (for computer graphics).", "citation_count": "24", "reference_count": "403", "date": "1996", "authors": ["David Beymer", "Tomaso Poggio"], "related_topics": ["Image-based modeling and rendering", "Image processing", "Scale-space axioms", "Feature (computer vision)", "Computer graphics", "Visual learning", "Image analysis", "Cognitive neuroscience of visual object recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2587818897", "references": ["2923209360", "2097171181", "2803352163", "2141102773", "2793609185", "1959198933", "2990335928", "1542727819", "2606802079", "2129094658"], "title": "Decadal trends in the North Atlantic oscillation: regional temperatures and precipitation", "abstract": "Greenland ice-core data have revealed large decadal climate variations over the North Atlantic that can be related to a major source of low-frequency variability, the North Atlantic Oscillation. Over the past decade, the Oscillation has remained in one extreme phase during the winters, contributing significantly to the recent wintertime warmth across Europe and to cold conditions in the northwest Atlantic. An evaluation of the atmospheric moisture budget reveals coherent large-scale changes since 1980 that are linked to recent dry conditions over southern Europe and the Mediterranean, whereas northern Europe and parts of Scandinavia have generally experienced wetter than normal conditions.", "citation_count": "0", "reference_count": "8,907", "date": "1996", "authors": ["J. W. Hurrell"], "related_topics": ["North Atlantic oscillation", "Atlantic Equatorial mode", "Atlantic multidecadal oscillation", "Mediterranean climate", "Precipitation", "Climatology", "Oceanography", "Environmental science", "Atmospheric moisture"]}
{"id": "2123977795", "references": ["1996773532", "2143956139", "2086479969", "2170120409", "2135346934", "2098693229", "2130259898", "2105815873", "2053197265", "2026311529"], "title": "Visual learning and recognition of 3-D objects from appearance", "abstract": "The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary from scene to scene. A compact representation of object appearance is proposed that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a manifold. Given an unknown input image, the recognition system projects the image to eigenspace. The object is recognized based on the manifold it lies on. The exact position of the projection on the manifold determines the object's pose in the image. A variety of experiments are conducted using objects with complex appearance characteristics. The performance of the recognition and pose estimation algorithms is studied using over a thousand input images of sample objects. Sensitivity of recognition to the number of eigenspace dimensions and the number of learning samples is analyzed. For the objects used, appearance representation in eigenspaces with less than 20 dimensions produces accurate recognition results with an average pose estimation error of about 1.0 degree. A near real-time recognition system with 20 complex objects in the database has been developed. The paper is concluded with a discussion on various issues related to the proposed learning and recognition methodology.", "citation_count": "24", "reference_count": "3,045", "date": "1995", "authors": ["Hiroshi Murase", "Shree K. Nayar"], "related_topics": ["3D pose estimation", "Pose", "3D single-object recognition", "Pattern recognition (psychology)", "Object (computer science)", "Learning object", "Projection (set theory)", "Deep-sky object", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2122538988", "references": ["2154642048", "145476170", "2103496339", "1965324089", "2131329059", "2158863190", "3121126077", "3017143921", "1507849272", "2078626246"], "title": "Nonlinear principal component analysis using autoassociative neural networks", "abstract": "Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal \u201cbottleneck\u201d layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.", "citation_count": "21", "reference_count": "2,616", "date": "1991", "authors": ["Mark A. Kramer"], "related_topics": ["Dimensionality reduction", "Principal component analysis", "Feedforward neural network", "Artificial neural network", "Feature vector", "Curse of dimensionality", "Nonlinear system", "Exploratory data analysis", "Pattern recognition", "Statistics", "Mathematics", "Artificial intelligence"]}
{"id": "2099741732", "references": ["2140352766", "2171074980", "1560089794", "1996355918", "2114018052", "2018388266", "2225937484", "2098301339", "1995963238", "2796930440"], "title": "Independent component analysis, a new concept?", "abstract": "Abstract   The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.", "citation_count": "49", "reference_count": "11,030", "date": "1994", "authors": ["Pierre Comon"], "related_topics": ["Independent component analysis", "FastICA", "Principal component analysis", "Blind signal separation", "Mutual information", "Independence (probability theory)", "Infomax", "Multivariate random variable", "Algorithm", "Calculus", "Mathematics"]}
{"id": "3029645440", "references": ["2142058898", "2145096794", "2164278908", "2123871098", "2577537660", "196761320", "2156598602", "2134967712", "2109449402", "2159211495"], "title": "Numerical Optimization", "abstract": "Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems.  For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.", "citation_count": "0", "reference_count": "14,252", "date": "2008", "authors": ["Jorge Nocedal", "Stephen J. Wright"], "related_topics": ["Continuous optimization", "Nonlinear programming", "Field (computer science)", "Management science", "CUTEr", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Trust region", "Quadratic programming", "Focus (computing)"]}
{"id": "2150134853", "references": ["1997063559", "2022735534", "2145023731", "1973976434", "2133155955", "1968245656", "2002312729", "2109863423", "2114487471", "2913192828"], "title": "Scale-space and edge detection using anisotropic diffusion", "abstract": "A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image. &gt;", "citation_count": "18", "reference_count": "16,025", "date": "1990", "authors": ["P. Perona", "J. Malik"], "related_topics": ["Anisotropic diffusion", "Edge-preserving smoothing", "Smoothing", "Scale space", "Edge detection", "Scale-space axioms", "Structure tensor", "Diffusion filter", "Algorithm", "Computer vision", "Artificial intelligence", "Mathematics"]}
{"id": "2121947440", "references": ["1997063559", "1578099820", "100944330", "1971784203", "2132603077", "2121947440", "2114030927", "2114487471", "2913192828", "2798909945"], "title": "Normalized cuts and image segmentation", "abstract": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.", "citation_count": "25", "reference_count": "19,516", "date": "2000", "authors": ["Jianbo Shi", "J. Malik"], "related_topics": ["Image segmentation", "Spectral clustering", "Minimum spanning tree-based segmentation", "Graph partition", "Graph (abstract data type)", "Spectral graph theory", "Graph theory", "Cluster analysis", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1979711143", "references": ["1979675141", "2156909104", "2069317438", "2119821739", "1530699444", "2011395874", "2087347434", "2148603752", "1496612019", "1667072054"], "title": "Large margin classification using the perceptron algorithm", "abstract": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt\u2018s perceptron algorithm with Helmbold and Warmuth\u2018s leave-one-out method. Like Vapnik\u2018s maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik\u2018s algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort.", "citation_count": "22", "reference_count": "1,612", "date": "1998", "authors": ["Yoav Freund", "Robert E. Schapire"], "related_topics": ["Perceptron", "Linear classifier", "Linear separability", "Computation", "Classifier (UML)", "Pattern recognition", "Algorithm", "Mathematics", "Artificial intelligence", "High dimensional"]}
{"id": "200434350", "references": ["2065060195", "1578099820", "2147152072", "2160167256", "2171009857", "2138621811", "2323009482", "2130891992", "1640070940", "2121947440"], "title": "A Random Walks View of Spectral Segmentation.", "abstract": "We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge ows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.", "citation_count": "10", "reference_count": "841", "date": "2001", "authors": ["Marina Meila", "Jianbo Shi"], "related_topics": ["Cluster analysis", "Random walk", "Heterogeneous random walk in one dimension", "Loop-erased random walk", "Segmentation", "Quantum walk", "Eigenvalues and eigenvectors", "Stochastic matrix", "Pattern recognition", "Discrete mathematics", "Computer science", "Artificial intelligence"]}
{"id": "2165874743", "references": ["1578099820", "2160167256", "2171009857", "658559791", "2130891992", "2067976091", "2123320529", "1981193610", "2140095548", "2141376824"], "title": "On Spectral Clustering: Analysis and an algorithm", "abstract": "Despite many empirical successes of spectral clustering methods\u2014 algorithms that cluster points using eigenvectors of matrices derived from the data\u2014there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.", "citation_count": "13", "reference_count": "9,863", "date": "2001", "authors": ["Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss"], "related_topics": ["Cluster analysis", "Correlation clustering", "Fuzzy clustering", "Canopy clustering algorithm", "Constrained clustering", "CURE data clustering algorithm", "Clustering high-dimensional data", "Biclustering", "Algorithm", "Mathematics"]}
{"id": "1511160855", "references": ["1979711143", "1578099820", "2149684865", "2139212933", "2097308346", "1576213419", "2122837498", "2124637492", "2009570821", "3023786531"], "title": "Diffusion Kernels on Graphs and Other Discrete Input Spaces", "abstract": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.", "citation_count": "19", "reference_count": "1,077", "date": "2002", "authors": ["Risi Imre Kondor", "John D. Lafferty"], "related_topics": ["Kernel (category theory)", "Discretization", "Heat kernel", "Euclidean distance matrix", "Gaussian function", "Euclidean space", "Diffusion equation", "Matrix exponential", "Algebra", "Discrete mathematics", "Mathematics"]}
{"id": "2113592823", "references": ["2165874743", "2160167256", "1574877594", "2166473218", "2158001550", "2139578439", "2048679005", "2122837498", "2140095548", "2107008379"], "title": "Cluster Kernels for Semi-Supervised Learning", "abstract": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach.", "citation_count": "13", "reference_count": "643", "date": "2002", "authors": ["Olivier Chapelle", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "related_topics": ["Semi-supervised learning", "Kernel (linear algebra)", "Classifier (UML)", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1585385982", "references": ["2154455818", "2139823104", "1479807131", "2108646579", "2136504847", "2118978333", "1990334093", "2104290444", "2114524997"], "title": "Learning from Labeled and Unlabeled Data using Graph Mincuts", "abstract": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.", "citation_count": "0", "reference_count": "1,363", "date": "2001", "authors": ["Avrim Blum", "Shuchi Chawla"], "related_topics": ["Pairwise comparison", "Machine learning", "Computer science", "Artificial intelligence", "Graph", "Training set"]}
{"id": "2122837498", "references": ["2001141328", "2127086485", "2017753243", "1585385982", "2120720283", "2161813919"], "title": "Partially labeled classification with Markov random walks", "abstract": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.", "citation_count": "6", "reference_count": "731", "date": "2001", "authors": ["Martin Szummer", "Tommi Jaakkola"], "related_topics": ["Random walk", "Markov chain", "Margin (machine learning)", "Probabilistic logic", "Representation (mathematics)", "Regularization (mathematics)", "Scale (ratio)", "Set (abstract data type)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2154579312", "references": ["169539560", "56903235", "2154642048", "1965770722", "2058841211", "2091987367", "2147800946", "2114766824", "2153988646", "2157475639"], "title": "Handwritten Digit Recognition with a Back-Propagation Network", "abstract": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.", "citation_count": "11", "reference_count": "4,098", "date": "1989", "authors": ["Yann LeCun", "Bernhard E. Boser", "John S. Denker", "Donnie Henderson", "R. E. Howard", "Wayne E. Hubbard", "Lawrence D. Jackel"], "related_topics": ["Word error rate", "Task (project management)", "Backpropagation", "Speech recognition", "Pattern recognition", "Computer science", "Normalization (statistics)", "Artificial intelligence", "Digit recognition"]}
{"id": "108654854", "references": ["1578099820", "100944330", "638069330", "2139320601", "2237678354", "1522020530", "2051616415", "181601562", "2131183115", "1567771969"], "title": "Higher eigenvalues and isoperimetric inequalities on Riemannian manifolds and graphs", "abstract": "5 Analysis on weighted graphs 23 5.1 Measures on graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2 Discrete Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.3 Green\u2019s formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.4 Integration versus Summation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5 Eigenvalues of Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.6 Heat kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.7 Co-area formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Research supported in part by NSF Grant No. DMS 98-01446 Supported by EPSRC Fellowship B/94/AF/1782 Supported in part by NSF Grant No. DMS 95-04834", "citation_count": "31", "reference_count": "109", "date": "2000", "authors": ["Fan Chung", "Alexander Grigor\u2019yan", "Shing-Tung Yau"], "related_topics": ["Isoperimetric inequality", "Isoperimetric dimension", "Laplacian matrix", "Heat kernel", "Laplace operator", "Ricci-flat manifold", "Riemannian geometry", "Exponential map (Riemannian geometry)", "Combinatorics", "Mathematics", "Mathematical analysis"]}
{"id": "2106346128", "references": ["2154642048", "2147152072", "2110485445", "145476170", "1982370770", "183625566", "1971844566", "2121553911", "1983578042", "2051812123"], "title": "Learning distributed representations of concepts using linear relational embedding", "abstract": "We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.", "citation_count": "16", "reference_count": "117", "date": "2001", "authors": ["A. Paccanaro", "G.E. Hinton"], "related_topics": ["Relational algebra", "Feature learning", "Concept learning", "Binary relation", "Representation (mathematics)", "Generalization", "Matrix multiplication", "Relation (database)", "Embedding", "Discriminative model", "Theoretical computer science", "Artificial intelligence", "Computer science"]}
{"id": "23758216", "references": ["2186428165", "2121601095", "2171277043", "2076063813", "2137570937", "2002016471", "2095757522", "2161160262", "2144499799", "1992419399"], "title": "Self-organization and associative memory: 3rd edition", "abstract": "", "citation_count": "0", "reference_count": "2,003", "date": "1989", "authors": ["T. Kohonen"], "related_topics": ["Content-addressable memory", "Computer science", "Cognitive science", "Self-organization"]}
{"id": "2159174312", "references": ["2047870719", "2143956139", "2107636931", "2151391352", "23758216", "1991848143", "2114309103", "1580684925", "2124776405", "2123421115"], "title": "Mapping a Manifold of Perceptual Observations", "abstract": "Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space.", "citation_count": "11", "reference_count": "373", "date": "1997", "authors": ["Joshua B. Tenenbaum"], "related_topics": ["Isomap", "Nonlinear dimensionality reduction", "Diffusion map", "Dimensionality reduction", "Intrinsic metric", "Geodesic", "Manifold", "Feature vector", "Algorithm", "Topology", "Mathematics"]}
{"id": "2100659887", "references": ["1579840964", "2156113848", "2092642599", "2164371886", "2093439427"], "title": "A database for handwritten text recognition research", "abstract": "An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. &gt;", "citation_count": "5", "reference_count": "1,784", "date": "1994", "authors": ["J.J. Hull"], "related_topics": ["Alphanumeric", "Pattern recognition (psychology)", "Digital image", "Grayscale", "Pixel", "Database", "Computer science", "State (computer science)", "Text recognition"]}
{"id": "2148694408", "references": ["2121601095", "1479807131", "2138621811", "2139047213", "2145962650", "2145072179", "78159342", "2140095548", "2106053110"], "title": "Principal Component Analysis", "abstract": "Introduction * Properties of Population Principal Components * Properties of Sample Principal Components * Interpreting Principal Components: Examples * Graphical Representation of Data Using Principal Components * Choosing a Subset of Principal Components or Variables * Principal Component Analysis and Factor Analysis * Principal Components in Regression Analysis * Principal Components Used with Other Multivariate Techniques * Outlier Detection, Influential Observations and Robust Estimation * Rotation and Interpretation of Principal Components * Principal Component Analysis for Time Series and Other Non-Independent Data * Principal Component Analysis for Special Types of Data * Generalizations and Adaptations of Principal Component Analysis", "citation_count": "0", "reference_count": "53,967", "date": "1986", "authors": ["Ian Jolliffe"], "related_topics": ["Principal component analysis", "Multilinear principal component analysis", "Kernel principal component analysis", "Relationship square", "Correspondence analysis", "Multiple correspondence analysis", "Dimensionality reduction", "Population", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2019020850", "references": ["1997063559", "2159537329", "2045682702", "2049633694", "2077990749", "2148394752", "2095757522", "2581275558", "3017143921", "2117812871"], "title": "Data Visualization by Multimensional Scaling: A Deterministic Annealing Approach", "abstract": "Abstract   Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space. The quality of a data embedding is measured by a stress function which compares proximity values with Euclidean distances of the respective points. The corresponding minimization problem is non-convex and sensitive to local minima. We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping, derived in the framework of maximum entropy estimation. Experimental results demonstrate the superiority of our optimization technique compared to conventional gradient descent methods.", "citation_count": "36", "reference_count": "91", "date": "1996", "authors": ["Hansjoerg Klock", "Joachim M. Buhmann"], "related_topics": ["Nonlinear dimensionality reduction", "Sammon mapping", "Stress majorization", "Gradient descent", "Euclidean space", "Maxima and minima", "Principle of maximum entropy", "Scaling", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "1513400187", "references": ["1652775531", "2014889099", "2149237601", "2141870784", "2111366547", "2053186076", "2063532964", "2078962046", "1978259121", "2109278577"], "title": "Data Structures and Network Algorithms", "abstract": "Foundations Disjoint Sets Heaps Search Trees Linking and Cutting Trees Minimum Spanning Trees Shortest Paths Network Flows Matchings.", "citation_count": "0", "reference_count": "3,107", "date": "1983", "authors": ["Robert Endre Tarjan"], "related_topics": ["Weight-balanced tree", "Spanning tree", "Disjoint sets", "Skew heap", "Fibonacci heap", "Expected linear time MST algorithm", "Data structure", "Flow network", "Theoretical computer science", "Mathematics"]}
{"id": "255556494", "references": ["1570448133", "1480376833", "2140190241", "1565377632", "2911964244", "1995945562", "2112090702", "3013264884", "2008620264", "1663973292"], "title": "Data Mining and Analysis: Fundamental Concepts and Algorithms", "abstract": "The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more", "citation_count": "136", "reference_count": "1,177", "date": "2014", "authors": ["Mohammed J. Zaki", "Wagner Meira"], "related_topics": ["Concept mining", "Analytics", "Business intelligence", "Exploratory data analysis", "Cluster analysis", "Data science", "Implementation", "Kernel method", "Computer science", "Data mining", "As is", "Algorithm"]}
{"id": "1591018827", "references": ["3099803807", "2609271878", "1805381866", "2173213060", "2044602433", "2014356541", "1585274809", "1575104486", "2101121254", "1490180844"], "title": "A Data-Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition", "abstract": "The Koopman operator is a linear but infinite-dimensional operator that governs the evolution of scalar observables defined on the state space of an autonomous dynamical system and is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. In this manuscript, we present a data-driven method for approximating the leading eigenvalues, eigenfunctions, and modes of the Koopman operator. The method requires a data set of snapshot pairs and a dictionary of scalar observables, but does not require explicit governing equations or interaction with a \u201cblack box\u201d integrator. We will show that this approach is, in effect, an extension of dynamic mode decomposition (DMD), which has been used to approximate the Koopman eigenvalues and modes. Furthermore, if the data provided to the method are generated by a Markov process instead of a deterministic dynamical system, the algorithm approximates the eigenfunctions of the Kolmogorov backward equation, which could be considered as the \u201cstochastic Koopman operator\u201d (Mezic in Nonlinear Dynamics 41(1\u20133): 309\u2013325,\u00a02005). Finally, four illustrative examples are presented: two that highlight the quantitative performance of the method when presented with either deterministic or stochastic data and two that show potential applications of the Koopman eigenfunctions.", "citation_count": "57", "reference_count": "727", "date": "2015", "authors": ["Matthew O. Williams", "Ioannis G. Kevrekidis", "Clarence W. Rowley"], "related_topics": ["Composition operator", "Dynamic mode decomposition", "Eigenvalues and eigenvectors", "Eigenfunction", "Nonlinear system", "Observable", "Scalar (mathematics)", "Markov process", "Applied mathematics", "Mathematical analysis", "Mathematics"]}
{"id": "2109574129", "references": ["2136922672", "2140190241", "2163922914", "2173213060", "1981420413", "2100495367", "1554944419", "1631356911", "1494137514", "2072128103"], "title": "Data-intensive applications, challenges, techniques and technologies: A survey on Big Data", "abstract": "It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. \u00a9 2014 Elsevier Inc. All rights reserved.", "citation_count": "187", "reference_count": "3,034", "date": "2014", "authors": ["C. L. Philip Chen", "Chun-Yang Zhang"], "related_topics": ["Big data", "e-Science", "Data visualization", "Data-intensive computing", "Cloud computing", "Data science", "Information science", "National security", "Computer science", "Business activities"]}
{"id": "2162584119", "references": ["2107625277", "2118295263", "2142097792", "2144042033", "2158155342", "2134289299", "1979723077", "2052648234", "2100235918", "1742512077"], "title": "TRY - a global database of plant traits", "abstract": "Plant traits \u2013 the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs \u2013 determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy-in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log-normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation \u2013 but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait-based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.", "citation_count": "243", "reference_count": "2,085", "date": "2011", "authors": ["J. Kattge", "S. D\u00edaz", "S. Lavorel", "I. C. Prentice", "P. Leadley", "G. B\u00f6nisch", "E. Garnier", "M. Westoby", "Peter B Reich", "I. J. Wright", "J. H C Cornelissen", "C. Violle", "S. P. Harrison", "P. M. Van Bodegom", "M. Reichstein", "B. J. Enquist", "N. A. Soudzilovskaia", "D. D. Ackerly", "M. Anand", "O. Atkin", "M. Bahn", "T. R. Baker", "D. Baldocchi", "R. Bekker", "C. C. Blanco", "B. Blonder", "W. J. Bond", "R. Bradstock", "D. E. Bunker", "F. Casanoves", "Jeannine M Cavender-Bares", "J. Q. Chambers", "F. S. Chapin", "J. Chave", "D. Coomes", "W. K. Cornwell", "J. M. Craine", "B. H. Dobrin", "L. Duarte", "W. Durka", "J. Elser", "G. Esser", "M. Estiarte", "W. F. Fagan", "J. Fang", "F. Fern\u00e1ndez-M\u00e9ndez", "A. Fidelis", "B. Finegan", "O. Flores", "H. Ford"], "related_topics": ["Functional ecology", "Trait", "Plant functional type", "Vegetation", "Species richness", "Context (language use)", "Range (biology)", "Biological dispersal", "Ecology", "Biology", "Database"]}
{"id": "2963460103", "references": ["3102641634", "2001141328", "2187089797", "3104097132", "2148847267", "2157331557", "2962756421", "1673310716", "1888005072", "2064675550"], "title": "Representation Learning on Graphs: Methods and Applications", "abstract": "Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.", "citation_count": "37", "reference_count": "1,017", "date": "2017", "authors": ["William L. Hamilton", "Rex Ying", "Jure Leskovec"], "related_topics": ["Feature learning", "Deep learning", "Degree (graph theory)", "Heuristics", "Nonlinear dimensionality reduction", "Structure (mathematical logic)", "Theoretical computer science", "Encoding (memory)", "Domain (software engineering)", "Computer science", "Artificial intelligence"]}
{"id": "2142228262", "references": ["2004003571", "2068071220", "2001968606", "2146519989", "1978382377", "2152316618", "1589055062", "2156908459", "2137263269", "1972736931"], "title": "Asymptotically optimal block quantization", "abstract": "In 1948 W. R. Bennett used a companding model for nonuniform quantization and proposed the formula D \\: = \\: \\frac{1}{12N^{2}} \\: \\int \\: p(x) [ E(x) ]^{-2} \\dx for the mean-square quantizing error where N is the number of levels, p (x) is the probability density of the input, and E \\prime (x) is the slope of the compressor curve. The formula, an approximation based on the assumption that the number of levels is large and overload distortion is negligible, is a useful tool for analytical studies of quantization. This paper gives a heuristic argument generalizing Bennett's formula to block quantization where a vector of random variables is quantized. The approach is again based on the asymptotic situation where N , the number of quantized output vectors, is very large. Using the resulting heuristic formula, an optimization is performed leading to an expression for the minimum quantizing noise attainable for any block quantizer of a given block size k . The results are consistent with Zador's results and specialize to known results for the one- and two-dimensional cases and for the case of infinite block length (k \\rightarrow \\infty) . The same heuristic approach also gives an alternate derivation of a bound of Elias for multidimensional quantization. Our approach leads to a rigorous method for obtaining upper bounds on the minimum distortion for block quantizers. In particular, for k = 3 we give a tight upper bound that may in fact be exact. The idea of representing a block quantizer by a block \"compressor\" mapping followed with an optimal quantizer for uniformly distributed random vectors is also explored. It is not always possible to represent an optimal quantizer with this block companding model.", "citation_count": "12", "reference_count": "1,347", "date": "1979", "authors": ["A. Gersho"], "related_topics": ["Quantization (signal processing)", "Upper and lower bounds", "Heuristic argument", "Block size", "Random variable", "Asymptotically optimal algorithm", "Companding", "Multidimensional systems", "Discrete mathematics", "Combinatorics", "Mathematics"]}
{"id": "2089419199", "references": ["2142228262", "1976356564", "2001968606", "1973387369"], "title": "Asymptotic quantization error of continuous signals and the quantization dimension", "abstract": "Extensions of the limiting qnanfizafion error formula of Bennet are proved. These are of the form D_{s,k}(N,F)=N^{-\\beta}B , where N is the number of output levels, D_{s,k}(N,F) is the s th moment of the metric distance between quantizer input and output, \\beta,B&gt;0,k=s/\\beta is the signal space dimension, and F is the signal distribution. If a suitably well-behaved k -dimensional signal density f(x) exists, B=b_{s,k}[\\int f^{\\rho}(x)dx]^{1/ \\rho},\\rho=k/(s+k) , and b_{s,k} does not depend on f . For k=1,s=2 this reduces to Bennett's formula. If F is the Cantor distribution on [0,1],0 and this k equals the fractal dimension of the Cantor set [12,13] . Random quantization, optimal quantization in the presence of an output information constraint, and quantization noise in high dimensional spaces are also investigated.", "citation_count": "4", "reference_count": "630", "date": "1982", "authors": ["P. Zador"], "related_topics": ["Cantor distribution", "Quantization (signal processing)", "Cantor set", "Fractal", "Combinatorics", "Discrete mathematics", "Fractal dimension", "Mathematics", "High dimensional", "Limiting", "Space dimension"]}
{"id": "2166116275", "references": ["2112027492", "2095734615", "2103496339", "2108959409", "2137983211", "2151029520", "2095546965", "2084544490", "2044828368", "1559907478"], "title": "Universal approximation bounds for superpositions of a sigmoidal function", "abstract": "Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. &gt;", "citation_count": "25", "reference_count": "2,981", "date": "1993", "authors": ["A.R. Barron"], "related_topics": ["Approximation error", "Linear approximation", "Function approximation", "Universal approximation theorem", "Approximation theory", "Sigmoid function", "Series expansion", "Mean squared error", "Applied mathematics", "Discrete mathematics", "Mathematics"]}
{"id": "2063971957", "references": ["1944592753", "2797583072", "1498436455"], "title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "abstract": "The standard form of back-propagation learning is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other. The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discovery depth in random dot stereograms of curved surfaces.", "citation_count": "3", "reference_count": "468", "date": "1992", "authors": ["Suzanna Becker", "Geoffrey E. Hinton"], "related_topics": ["Perceptual learning", "Artificial neural network", "Modality (human\u2013computer interaction)", "Random dot stereogram", "Pattern recognition (psychology)", "Object (computer science)", "Backpropagation", "Computer vision", "Perception", "Computer science", "Artificial intelligence"]}
{"id": "2137983211", "references": ["2103496339", "2097415784", "2416739038", "2090270852", "1581292930", "2137983211", "2056099894", "135768573", "1654142532"], "title": "Multilayer feedforward networks are universal approximators", "abstract": "Abstract   This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.", "citation_count": "9", "reference_count": "21,034", "date": "1989", "authors": ["K. Hornik", "M. Stinchcombe", "H. White"], "related_topics": ["Universal approximation theorem", "Borel measure", "Feed forward", "Function (mathematics)", "Stone\u2013Weierstrass theorem", "Algorithm", "Degree (graph theory)", "Class (philosophy)", "Mathematics", "Finite dimensional space"]}
{"id": "2079782346", "references": ["2154642048", "1968908999", "1520168181", "2019363670", "2165758113", "2142635246", "3137895569", "2322002063", "2098545770", "2020246210"], "title": "Statistical theory of learning curves under entropic loss criterion", "abstract": "The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.", "citation_count": "47", "reference_count": "215", "date": "1993", "authors": ["Shun-Ichi Amari", "Noboru Murata"], "related_topics": ["Early stopping", "Artificial neural network", "Generalization", "Entropy (information theory)", "Conditional probability distribution", "Statistical theory", "Stochastic modelling", "Learning curve", "Applied mathematics", "Mathematics", "Statistics"]}
{"id": "5731987", "references": ["2017257315", "2023963201", "1981479913", "2115907784", "2122925692", "1564660545", "2133884101", "2131329059", "2078626246", "2432567885"], "title": "Original Contribution: Principal components, minor components, and linear neural networks", "abstract": "Many neural network realizations have been recently proposed for the statistical technique of Principal Component Analysis (PCA). Explicit connections between numerical constrained adaptive algorithms and neural networks with constrained Hebbian learning rules are reviewed. The Stochastic Gradient Ascent (SGA) neural network is proposed and shown to be closely related to the Generalized Hebbian Algorithm (GHA). The SGA behaves better for extracting the less dominant eigenvectors. The SGA algorithm is further extended to the case of learning minor components. The symmetrical Subspace Network is known to give a rotated basis of the dominant eigenvector subspace, but usually not the true eigenvectors themselves. Two extensions are proposed: in the first one, each neuron has a scalar parameter which breaks the symmetry. True eigenvectors are obtained in a local and fully parallel learning rule. In the second one, the case of an arbitrary number of parallel neurons is considered, not necessarily less than the input vector dimension.", "citation_count": "28", "reference_count": "1,160", "date": "1992", "authors": ["Erkki Oja"], "related_topics": ["Generalized Hebbian Algorithm", "Oja's rule", "Artificial neural network", "Hebbian theory", "Eigenvalues and eigenvectors", "Principal component analysis", "Gradient descent", "Subspace topology", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2162604518", "references": ["1634005169", "2142228262", "2089419199", "1565930783", "2186435531", "2063678710", "2801840425", "2134383396", "2119352491", "2029495080"], "title": "Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources", "abstract": "Lattice vector quantizer design procedures for nonuniform sources are presented. The procedures yield lattice vector quantizers with excellent performance and retaining the structure required for fast quantization. Analytical methods for truncating and scaling lattices to be used in vector quantizations are given, and their utility is demonstrated for independent and identically distributed (i.i.d.) Gaussian and Laplacian sources. An analytical technique for piecewise linear multidimensional compandor designs is evaluated for i.i.d. Gaussian and Laplacian sources by comparing its performance to that of the other vector quantizers. &gt;", "citation_count": "39", "reference_count": "132", "date": "1993", "authors": ["D.G. Jeong", "J.D. Gibson"], "related_topics": ["Vector Laplacian", "Vector quantization", "Quantization (signal processing)", "Gaussian", "Piecewise linear function", "Piecewise", "Multidimensional systems", "Laplace operator", "Applied mathematics", "Discrete mathematics", "Mathematics"]}
{"id": "1995169133", "references": ["1997063559", "1652505363", "2112325651", "2056760934", "2157629899", "2581275558", "1981025738", "1507849272", "2293063825", "2171850596"], "title": "Boltzmann machines for speech recognition", "abstract": "Boltzmann machines offer a new and exciting approach to automatic speech recognition, and provide a rigorous mathematical formalism for parallel computing arrays. In this paper we briefly summarize Boltzmann machine theory, and present results showing their ability to recognize both static and time-varying speech patterns. A machine with 2000 units was able to distinguish between the 11 steady-state vowels in English with an accuracy of 85%. The stability of the learning algorithm and methods of preprocessing and coding speech data before feeding it to the machine are also discussed. A new type of unit called a carry input unit, which involves a type of state-feedback, was developed for the processing of time-varying patterns and this was tested on a few short sentences. Use is made of the implications of recent work into associative memory, and the modelling of neural arrays to suggest a good configuration of Boltzmann machines for this sort of pattern recognition.", "citation_count": "15", "reference_count": "101", "date": "1986", "authors": ["R. W. Prager", "T. D. Harrison", "F. Fallside"], "related_topics": ["Boltzmann machine", "Content-addressable memory", "sort", "Speech recognition", "Preprocessor", "Coding (social sciences)", "Boltzmann constant", "Computer science", "Speech patterns"]}
{"id": "2591802459", "references": ["2098429061", "2056415038", "1971074050", "1978443256", "1489504112", "2164152532", "1994618660", "1993197592", "2148694408", "2123806929"], "title": "G\u2010maximization: an unsupervised learning procedure for discovering regularities", "abstract": "Hill climbing is used to maximize an information theoretic measure of the difference betwen the actual behavior of a unit and the behavior that would be predicted by a statistician who knew the first order statistics of the inputs but believed them to be independent. This causes the unit to detect higher order correlations among its inputs. Initial simulations are presented, and seem encouraging. We describe an extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input.", "citation_count": "0", "reference_count": "68", "date": "2008", "authors": ["Barak A. Pearlmutter", "Geoffrey E. Hinton"], "related_topics": ["Competitive learning", "Unsupervised learning", "Population", "Hill climbing", "Maximization", "Machine learning", "Artificial intelligence", "Measure (mathematics)", "Statistician", "Structure (mathematical logic)", "Mathematics"]}
{"id": "2010581677", "references": ["2161278885", "2065973527", "2079724156", "2160133692", "2041273609"], "title": "A Theory of Adaptive Pattern Classifiers", "abstract": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions.", "citation_count": "5", "reference_count": "710", "date": "1967", "authors": ["Shunichi Amari"], "related_topics": ["Probability distribution", "Decision boundary", "Distribution (mathematics)", "Discriminant", "Weight", "Convergence (routing)", "Function (mathematics)", "Adaptive system", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2155487652", "references": ["2007057443", "2133155955", "2121203842", "2003370853", "2038584908", "1995756857", "2006500012", "2109863423", "2073974819", "2740373864"], "title": "On Edge Detection", "abstract": "Edge detection is the process that attempts to characterize the intensity changes in the image in terms of the physical processes that have originated them. A critical, intermediate goal of edge detection is the detection and characterization of significant intensity changes. This paper discusses this part of the edge detection problem. To characterize the types of intensity changes derivatives of different types, and possibly different scales, are needed. Thus, we consider this part of edge detection as a problem in numerical differentiation. We show that numerical differentiation of images is an ill-posed problem in the sense of Hadamard. Differentiation needs to be regularized by a regularizing filtering operation before differentiation. This shows that this part of edge detection consists of two steps, a filtering step and a differentiation step. Following this perspective, the paper discusses in detail the following theoretical aspects of edge detection. 1) The properties of different types of filters-with minimal uncertainty, with a bandpass spectrum, and with limited support-are derived. Minimal uncertainty filters optimize a tradeoff between computational efficiency and regularizing properties. 2) Relationships among several 2-D differential operators are established. In particular, we characterize the relation between the Laplacian and the second directional derivative along the gradient. Zero crossings of the Laplacian are not the only features computed in early vision. 3) Geometrical and topological properties of the zero crossings of differential operators are studied in terms of transversality and Morse theory.", "citation_count": "27", "reference_count": "1,321", "date": "1986", "authors": ["Vincent Torre", "Tomaso A. Poggio"], "related_topics": ["Edge detection", "Numerical differentiation", "Laplace operator", "Directional derivative", "Differential operator", "Hadamard transform", "Morse theory", "Regularization (mathematics)", "Algorithm", "Mathematical analysis", "Artificial intelligence", "Mathematics"]}
{"id": "3004157836", "references": ["2006304022", "2135029798", "2124715982", "2140190241", "2038840577", "3102014803", "2085937320", "2170776626", "2151165908", "2100245965"], "title": "Numerical Recipes, The Art of Scientific Computing", "abstract": "", "citation_count": "0", "reference_count": "21,776", "date": "1987", "authors": ["William H. Press", "Brian P. Flannery", "Saul A. Teukolsky", "William T. Vetterling", "Harvey Gould"], "related_topics": ["Data science", "Physics"]}
{"id": "1634005169", "references": ["2186428165", "2111918405", "1976709621", "2140190241", "2160547390", "2147717514", "2141362318", "1501500081", "2161160262", "2116467012"], "title": "Vector Quantization and Signal Compression", "abstract": "1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems.", "citation_count": "0", "reference_count": "9,417", "date": "1991", "authors": ["Allen Gersho", "Robert M. Gray"], "related_topics": ["Vector quantization", "Quantization (signal processing)", "Linde\u2013Buzo\u2013Gray algorithm", "Huffman coding", "Arithmetic coding", "Entropy encoding", "Coding theory", "Transform coding", "Algorithm", "Theoretical computer science", "Mathematics"]}
{"id": "2096710051", "references": ["1500470240", "2106596127", "2168175751", "2165887549", "2142635246", "2054658115", "1974513581", "2113638573", "2058815839", "2019833178"], "title": "Detection of signals by information theoretic criteria", "abstract": "A new approach is presented to the problem of detecting the number of signals in a multichannel time-series, based on the application of the information theoretic criteria for model selection introduced by Akaike (AIC) and by Schwartz and Rissanen (MDL). Unlike the conventional hypothesis testing based approach, the new approach does not requite any subjective threshold settings; the number of signals is obtained merely by minimizing the AIC or the MDL criteria. Simulation results that illustrate the performance of the new method for the detection of the number of signals received by a sensor array are presented.", "citation_count": "24", "reference_count": "4,275", "date": "1985", "authors": ["M. Wax", "T. Kailath"], "related_topics": ["Akaike information criterion", "Statistical hypothesis testing", "Model selection", "Information theory", "Detection theory", "Sensor array", "Covariance matrix", "Signal processing", "Algorithm", "Electronic engineering", "Mathematics"]}
{"id": "2017977879", "references": ["2801830100", "2030748132", "2112081648", "2166163519", "2091886411", "2611591252", "3000332379", "2024081693", "1506069954"], "title": "Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting", "abstract": "Abstract Locally weighted regression, or loess, is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. With local fitting we can estimate a much wider class of regression surfaces than with the usual classes of parametric functions, such as polynomials. The goal of this article is to show, through applications, how loess can be used for three purposes: data exploration, diagnostic checking of parametric models, and providing a nonparametric regression surface. Along the way, the following methodology is introduced: (a) a multivariate smoothing procedure that is an extension of univariate locally weighted regression; (b) statistical procedures that are analogous to those used in the least-squares fitting of parametric functions; (c) several graphical methods that are useful tools for understanding loess estimates and checking the a...", "citation_count": "40", "reference_count": "5,977", "date": "1988", "authors": ["William S. Cleveland", "Susan J. Devlin"], "related_topics": ["Local regression", "Nonparametric regression", "Regression diagnostic", "Segmented regression", "Polynomial regression", "Regression analysis", "Smoothing", "Univariate", "Statistics", "Mathematics"]}
{"id": "2913399920", "references": ["2583466288", "2142228262", "2164240509", "2127218421", "2021760654", "2040336387", "1995875735", "2150418026", "2134383396", "2119352491"], "title": "Vector quantization", "abstract": "A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.", "citation_count": "44", "reference_count": "4,386", "date": "1984", "authors": ["R. Gray"], "related_topics": ["Vector quantization", "Quantization (signal processing)", "Speech coding", "Linde\u2013Buzo\u2013Gray algorithm", "Data compression", "Information theory", "Speech processing", "Scalar (mathematics)", "Algorithm", "Computer science"]}
{"id": "2140196014", "references": ["1992371956", "2019972422", "1572731687", "2026723094", "1561761812", "2073501560", "2150013946", "2162168771", "1501108238"], "title": "The JPEG still picture compression standard", "abstract": "A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method. &gt;", "citation_count": "9", "reference_count": "7,421", "date": "1992", "authors": ["G.K. Wallace"], "related_topics": ["Lossless JPEG", "JPEG 2000", "JPEG", "JPEG File Interchange Format", "Quantization (image processing)", "Lossy compression", "Data compression", "Compression artifact", "Lossless compression", "Transform coding", "Discrete cosine transform", "Signal compression", "Grayscale", "Digital image", "Quantization (signal processing)", "Image processing", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1971735090", "references": ["2042264548", "2154642048", "3108739439", "2189011649", "2152088994", "1613359937", "3036751298", "3040500874", "2105393299", "1554576613"], "title": "On the approximate realization of continuous mappings by neural networks", "abstract": "Abstract   In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.", "citation_count": "15", "reference_count": "6,046", "date": "1989", "authors": ["K. Funahashi"], "related_topics": ["Universal approximation theorem", "Sigmoid function", "Artificial neural network", "Realization (systems)", "Backpropagation", "Point (geometry)", "Topology", "Discrete mathematics", "Unit (ring theory)", "Mathematics", "Hidden layer"]}
{"id": "2076870593", "references": ["2074308372", "1991985107", "1910429665", "2167531287", "1910258660", "2055408552", "2167089286", "2042776884", "2076915589", "2020100877"], "title": "The neuronal basis of behavior in Tritonia. I. Functional organization of the central nervous system.", "abstract": "An account is presented of the brain (paired cerebral, pleural, and pedal ganglia) of the nudibranch mollusc Tritonia diomedia. The major efferent nerve fibers are related to their nerve cell bodies and their functional roles identified as far as possible. The channels of sesory input relating to some of these neurons are given so as to provide an overall view of the organization of the brain.\r\n\r\n\r\n\r\nA standardized system of abbreviation and notation for the central ganglia, nerve trunks, and gaint somata is proposed. The system of references is intended to provide a guide to the location in the ganglia of many of the smaller neurons of which the functional attributes are known, but which cannot be consistently recognized on visual criteria alone. A system of rectangular coordinates is proposed which is applied to the natural outline of the ganglia. In addition, a system of cell notation is described which is independent of the co-ordinates used to define the position of the cell on the grid. Cell which by reason of their size, pigmentation, characteristic location and physiological attributes are consistently recognizable from animal to animal are numbered. Two principles were followed in numbering cells; (i) the series begins at unity in each ganglion; (ii) cell homologues in opposite ganglia are given the same number, but distinguished by prefixing the abbreviation for the ganglion in which they occur. It is considered that the system will facilitate the exchange of information between workers on the same species, and also benefit the comparison of neural organization of behavior in closely related forms.\r\n\r\n\r\n\r\nThe brain is organized in an almost exactly bilaterally symmetrical manner. There are a few bilateral neural pathways, but the major functional routes are ipsilateral. A few motorneurons, which are uniquely identifiable anatomically, cause unique, discrete movements. Others are in small groups sharing overlapping or similar functions.", "citation_count": "15", "reference_count": "202", "date": "1973", "authors": ["A. O. D. Willows", "D. A. Dorsett", "G. Hoyle"], "related_topics": ["Efferent nerve", "Ganglion", "Central nervous system", "Neuroscience", "Tritonia (gastropod)", "Anatomy", "Basis (universal algebra)", "Biology", "Cell bodies", "Functional organization", "Rectangular coordinates"]}
{"id": "2970228278", "references": ["2133659563", "2020272049", "3101436801", "2414319931", "2113200940", "2011571485", "2144205806", "2293063825", "1522901071"], "title": "The Neurophysiological Basis of Mind", "abstract": "", "citation_count": "0", "reference_count": "588", "date": "1953", "authors": ["Sir John C. Eccles"], "related_topics": ["Neurophysiology", "Basis (linear algebra)", "Cognitive science", "Psychology"]}
{"id": "2086789740", "references": ["2186428165", "607505555", "2140190241", "2963305465", "2962790689", "2017337590", "2293063825", "1736726159"], "title": "Perceptrons: An Introduction to Computational Geometry", "abstract": "Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons", "citation_count": "0", "reference_count": "3,259", "date": "1969", "authors": ["Marvin Lee Minsky", "Seymour Papert"], "related_topics": ["Computational geometry", "Perceptron", "Artificial intelligence", "Computer science", "Cognition"]}
{"id": "2155511848", "references": ["2217896605", "2156909104", "2138560582", "2140785063", "2159686933", "2166713160", "2151777012", "2117812871", "1658679052", "3124955340"], "title": "A statistical method for 3D object detection applied to faces and cars", "abstract": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.", "citation_count": "10", "reference_count": "1,891", "date": "2000", "authors": ["H. Schneiderman", "T. Kanade"], "related_topics": ["Viola\u2013Jones object detection framework", "Object detection", "Face detection", "Cognitive neuroscience of visual object recognition", "Histogram", "Object (computer science)", "Wavelet", "Pattern recognition", "Computer vision", "Rotation (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "1516111018", "references": ["1993845689", "2397866408", "2567948266", "2099111195", "2159080219", "2049633694", "2084812512", "2171265988", "1573186872", "1746680969"], "title": "An introduction to variational methods for graphical models", "abstract": "This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.", "citation_count": "59", "reference_count": "3,111", "date": "1999", "authors": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "related_topics": ["Graphical model", "Variational Bayesian methods", "Variational message passing", "Approximate inference", "Bayesian network", "Inference", "Boltzmann machine", "Markov chain", "Theoretical computer science", "Mathematical optimization", "Mathematics"]}
{"id": "2160225842", "references": ["1949116567", "2217896605", "2155511848", "2124351082", "2124087378", "2156406284", "2164598857", "2152473410", "2124722975", "1564419782"], "title": "Learning a Sparse Representation for Object Detection", "abstract": "We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches.", "citation_count": "21", "reference_count": "767", "date": "2002", "authors": ["Shivani Agarwal", "Dan Roth"], "related_topics": ["Object-class detection", "Object detection", "Viola\u2013Jones object detection framework", "Object model", "Spatial relation", "Implicit Shape Model", "Sparse approximation", "Test set", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2124351082", "references": ["2156909104", "2119821739", "2137346077", "2159686933", "2125713050", "26816478", "2087347434", "2084844503", "2056695679", "2159173611"], "title": "Training support vector machines: an application to face detection", "abstract": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&amp;T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.", "citation_count": "13", "reference_count": "3,894", "date": "1997", "authors": ["E. Osuna", "R. Freund", "F. Girosit"], "related_topics": ["Sequential minimal optimization", "Least squares support vector machine", "Support vector machine", "Quadratic programming", "Optimization problem", "Artificial neural network", "Face detection", "Statistical classification", "Mathematical optimization", "Mathematics"]}
{"id": "2141376824", "references": ["1997063559", "1634005169", "1578099820", "2145023731", "2160167256", "2121927366", "2121947440", "3017143921", "2114487471", "1490632837"], "title": "Contour and Texture Analysis for Image Segmentation", "abstract": "This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.", "citation_count": "38", "reference_count": "1,600", "date": "2001", "authors": ["Jitendra Malik", "Serge Belongie", "Thomas Leung", "Jianbo Shi"], "related_topics": ["Image texture", "Texture filtering", "Texture compression", "Image segmentation", "Grayscale", "Pixel", "Texton", "Texture (geology)", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2295106276", "references": ["2096840836", "2108444897", "2310919327", "2106404777", "2062104878", "2095757522", "2123977795", "2100318434", "2089181482", "2101522199"], "title": "Matching shapes", "abstract": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.", "citation_count": "22", "reference_count": "584", "date": "2001", "authors": ["S. Belongie", "J. Malik", "J. Puzicha"], "related_topics": ["Shape analysis (digital geometry)", "Shape context", "Similarity (geometry)", "Correspondence problem", "Discriminative model", "Cognitive neuroscience of visual object recognition", "Assignment problem", "Pattern recognition", "Mathematics", "Artificial intelligence", "Image matching"]}
{"id": "1992825118", "references": ["2217896605", "2145073242", "2155511848", "2143023146", "2032210760", "2164598857", "2115763357", "2162919312", "2089181482", "3124955340"], "title": "Detecting Pedestrians Using Patterns of Motion and Appearance", "abstract": "This paper describes a pedestrian detection system that integratesimage intensity information with motion information.We use a detection style algorithm that scans a detectorover two consecutive frames of a video sequence. Thedetector is trained (using AdaBoost) to take advantage ofboth motion and appearance information to detect a walkingperson. Past approaches have built detectors based onmotion information or detectors based on appearance information,but ours is the first to combine both sources ofinformation in a single detector. The implementation describedruns at about 4 frames/second, detects pedestriansat very small scales (as small as 20x15 pixels), and has avery low false positive rate.Our approach builds on the detection work of Viola andJones. Novel contributions of this paper include: i) developmentof a representation of image motion which is extremelyefficient, and ii) implementation of a state of theart pedestrian detection system which operates on low resolutionimages under difficult conditions (such as rain andsnow).", "citation_count": "14", "reference_count": "3,208", "date": "2003", "authors": ["Paul Viola", "Michael J. Jones", "Daniel Snow"], "related_topics": ["Pedestrian detection", "AdaBoost", "Pixel", "Boosting (machine learning)", "Computer vision", "Implicit Shape Model", "Detector", "Computer science", "Artificial intelligence"]}
{"id": "1608462934", "references": ["2132984323", "2217896605", "2156909104", "2124351082", "2159686933", "2139212933", "2128272608", "2140235142", "2148603752", "26816478"], "title": "A Trainable System for Object Detection", "abstract": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.", "citation_count": "33", "reference_count": "1,779", "date": "2000", "authors": ["Constantine Papageorgiou", "Tomaso Poggio"], "related_topics": ["Object-class detection", "Object detection", "Viola\u2013Jones object detection framework", "Face detection", "Representation (systemics)", "Haar wavelet", "Pattern recognition (psychology)", "Face (geometry)", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2156539399", "references": ["2155511848", "2154422044", "1555563476", "2177274842", "2097041931", "2164598857", "1608462934", "2152473410", "2502277634", "2124386111"], "title": "Human Detection Based on a Probabilistic Assembly of Robust Part Detectors", "abstract": "We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the partrsquos appearance. Feature selection and the part detectors are learnt from training images using AdaBoost. The detection algorithm is very efficient as (i) all part detectors use the same initial features, (ii) a coarse-to-fine cascade approach is used for part detection, (iii) a part assembly strategy reduces the number of spurious detections and the search space. The results outperform existing human detectors.", "citation_count": "23", "reference_count": "962", "date": "2004", "authors": ["Krystian Mikolajczyk", "Cordelia Schmid", "Andrew Zisserman"], "related_topics": ["Face detection", "AdaBoost", "Feature selection", "Clutter", "Probabilistic logic", "Pattern recognition", "Computer vision", "Detector", "Key (cryptography)", "Computer science", "Artificial intelligence"]}
{"id": "2152473410", "references": ["2112076978", "2217896605", "2132984323", "2156909104", "2152761983", "2149684865", "2139212933", "2159686933", "2115763357", "2912934387"], "title": "Example-based object detection in images by components", "abstract": "We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background.", "citation_count": "24", "reference_count": "1,428", "date": "2001", "authors": ["A. Mohan", "C. Papageorgiou", "T. Poggio"], "related_topics": ["One-class classification", "Object detection", "Classifier (UML)", "Data classification", "Computer vision", "Pattern recognition", "Detector", "Computer science", "Artificial intelligence"]}
{"id": "2113606819", "references": ["2140499889", "2145889472", "2105464873", "2146672645", "2074376560", "2078204800", "2101933716", "2063978378", "2004915807", "16591383"], "title": "Efficient sparse coding algorithms", "abstract": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.", "citation_count": "14", "reference_count": "3,225", "date": "2006", "authors": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng"], "related_topics": ["Sparse approximation", "K-SVD", "Neural coding", "Computational problem", "Optimization problem", "Convex optimization", "Speedup", "Least squares", "Algorithm", "Theoretical computer science", "Mathematics"]}
{"id": "2161516371", "references": ["2097074225", "2105464873", "2149760002", "2118963448", "2160547390", "2296616510", "2053186076", "2153663612", "2165939075", "2050834445"], "title": "Image super-resolution as sparse representation of raw image patches", "abstract": "This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse and the recovered high-resolution image is competitive or even superior in quality to images produced by other SR methods.", "citation_count": "26", "reference_count": "1,803", "date": "2008", "authors": ["Jianchao Yang", "J. Wright", "T. Huang", "Yi Ma"], "related_topics": ["K-SVD", "Sparse approximation", "Image processing", "Image quality", "Image resolution", "Compressed sensing", "Feature extraction", "Iterative reconstruction", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1624854622", "references": ["1949116567", "3097096317", "2134557905", "2155511848", "2149194912", "2154422044", "2057175746", "2171188998", "2152473410", "2124386111"], "title": "Object recognition with features inspired by visual cortex", "abstract": "We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.", "citation_count": "24", "reference_count": "1,203", "date": "2005", "authors": ["T. Serre", "L. Wolf", "T. Poggio"], "related_topics": ["Cognitive neuroscience of visual object recognition", "3D single-object recognition", "Feature (machine learning)", "Pattern recognition (psychology)", "Object detection", "Feature extraction", "Caltech 101", "Object (computer science)", "Face detection", "Visual cortex", "Edge detection", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2134731454", "references": ["2127314673", "2147152072", "2072773380", "1956559956", "2049633694", "2064580901", "1524704912", "1902027874", "1983578042", "2798909945"], "title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis", "abstract": "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.", "citation_count": "23", "reference_count": "3,185", "date": "2001", "authors": ["Thomas Hofmann"], "related_topics": ["Probabilistic latent semantic analysis", "Latent semantic analysis", "Latent Dirichlet allocation", "Document-term matrix", "Semantic computing", "Latent class model", "Mixture model", "Probabilistic logic", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2914885528", "references": ["2489504689", "3021212382", "2913703059", "2136654525", "2115738369", "2069266228", "2119204143", "2172373809", "2125756925", "2415527960"], "title": "Color indexing", "abstract": "Computer vision is moving into a new era in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, unconstrained environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the identity of an object with a known location, and determining the location of a known object. Color can be successfully used for both tasks.\r\nThis dissertation demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection which allows real-time indexing into a large database of stored models. It demonstrates techniques for dealing with crowded scenes and with models with similar color signatures. For solving the location problem it introduces an algorithm called Histogram Backprojection which performs this task efficiently in crowded scenes.", "citation_count": "23", "reference_count": "8,631", "date": "1991", "authors": ["Michael James Swain", "Dana H. Ballard"], "related_topics": ["Color normalization", "Content-based image retrieval", "Histogram", "Intersection", "Object (computer science)", "Search engine indexing", "Computer vision", "Identity (object-oriented programming)", "Robot", "Computer science", "Artificial intelligence"]}
{"id": "2165828254", "references": ["2166049352", "2104978738", "2151103935", "2057175746", "2310919327", "1624854622", "2147800946", "2168002178", "2162915993", "1484228140"], "title": "SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition", "abstract": "We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(\u00b10.56%) at 15 training images per class, and 66.23%(\u00b10.48%) at 30 training images.", "citation_count": "40", "reference_count": "1,532", "date": "2006", "authors": ["Hao Zhang", "A.C. Berg", "M. Maire", "J. Malik"], "related_topics": ["k-nearest neighbors algorithm", "Support vector machine", "Caltech 101", "MNIST database", "Computational complexity theory", "Pattern recognition (psychology)", "Discriminative model", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2122090912", "references": ["2145295623", "1966096622", "2151052953", "1999613943", "2134731454", "2049455633", "2118079529", "2135001774", "2165395308", "1902027874"], "title": "Maximum-Margin Matrix Factorization", "abstract": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them.", "citation_count": "14", "reference_count": "1,252", "date": "2004", "authors": ["Nathan Srebro", "Jason Rennie", "Tommi S. Jaakkola"], "related_topics": ["Matrix decomposition", "Margin (machine learning)", "Algebra", "Discrete mathematics", "Mathematics", "Generalization error"]}
{"id": "1612003148", "references": ["2127314673", "2147152072", "2056029990", "2140842551", "1956559956", "2049633694", "2134731454", "2143144851", "1983578042", "2107743791"], "title": "Probabilistic latent semantic analysis", "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.", "citation_count": "13", "reference_count": "3,044", "date": "1999", "authors": ["Thomas Hofmann"], "related_topics": ["Probabilistic latent semantic analysis", "Latent semantic analysis", "Document-term matrix", "Latent class model", "Latent Dirichlet allocation", "Explicit semantic analysis", "Latent variable", "Non-negative matrix factorization", "Machine learning", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "205159212", "references": ["2136922672", "2124914669", "2117154949", "2157444450", "2100495367", "2159737176", "2116064496", "2130556178", "2157364932", "2144935315"], "title": "Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure", "abstract": "", "citation_count": "18", "reference_count": "507", "date": "2007", "authors": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "related_topics": ["k-nearest neighbors algorithm", "Neighbourhood (mathematics)", "Theoretical computer science", "Machine learning", "Mathematics", "Artificial intelligence", "Nonlinear embedding"]}
{"id": "2165395308", "references": ["1516172206", "1496451467", "1673941785", "2252194958", "1568698519", "2170653751", "2117354486", "2135001774", "2021680564", "2139451327"], "title": "Weighted low-rank approximations", "abstract": "We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task.", "citation_count": "13", "reference_count": "919", "date": "2003", "authors": ["Nathan Srebro", "Tommi Jaakkola"], "related_topics": ["Rank (linear algebra)", "Matrix (mathematics)", "Context (language use)", "Representation (mathematics)", "Collaborative filtering", "Applied mathematics", "Mathematical optimization", "Simple (abstract algebra)", "Noise", "Mathematics", "Task (computing)"]}
{"id": "1989702938", "references": ["2217896605", "2033419168", "2121601095", "2156909104", "2152826865", "2121647436", "2164598857", "2038952578", "2108384452", "2138451337"], "title": "Face recognition: A literature survey", "abstract": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.", "citation_count": "162", "reference_count": "9,023", "date": "2003", "authors": ["W. Zhao", "R. Chellappa", "P. J. Phillips", "A. Rosenfeld"], "related_topics": ["Three-dimensional face recognition", "Face Recognition Grand Challenge", "Literature survey", "Eigenface", "Facial recognition system", "Face hallucination", "FERET database", "Perception", "Data science", "Computer science", "Computer vision", "Artificial intelligence"]}
{"id": "2098947662", "references": ["2135463994", "1993867646", "2121863133", "2113341759", "2138313032", "2030234875", "2112684592", "2130506643", "2138451337", "2157418942"], "title": "View-based and modular eigenspaces for face recognition", "abstract": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated. &gt;", "citation_count": "13", "reference_count": "2,984", "date": "1994", "authors": ["Pentland", "Moghaddam", "Starner"], "related_topics": ["Three-dimensional face recognition", "3D single-object recognition", "Feature (machine learning)", "Eigenface", "Facial recognition system", "Feature extraction", "Face (geometry)", "Orientation (computer vision)", "Pattern recognition", "Computer vision", "Salient", "Computer science", "Artificial intelligence"]}
{"id": "2155759509", "references": ["2120420721", "2033419168", "2121114545", "2141503314", "2125127226", "2102760078", "2106143125", "2144855601", "2110822444"], "title": "The CMU Pose, Illumination, and Expression (PIE) database", "abstract": "Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.", "citation_count": "9", "reference_count": "3,895", "date": "2002", "authors": ["T. Sim", "S. Baker", "M. Bsat"], "related_topics": ["Facial recognition system", "Computer vision", "Computer graphics (images)", "Expression (mathematics)", "Database", "Computer science", "Artificial intelligence", "Image storage"]}
{"id": "1981627423", "references": ["145832685", "1541540802", "1492160417", "2402164836", "3139316869", "2138162238", "1512426208", "2020149918"], "title": "Planning in a hierarchy of abstraction spaces", "abstract": "Abstract   A problem domain can be represented as a hierarchy of abstraction spaces in which successively finer levels of detail are introduced. The problem solver ABSTRIPS, a modification of STRIPS, can define an abstraction space hierarchy from the STRIPS representation of a problem domain, and it can utilize the hierarchy in solving problems. Examples of the system's performance are presented that demonstrate the significant increases in problem-solving power that this approach provides. Then some further implications of the hierarchical planning approach are explored.", "citation_count": "8", "reference_count": "1,980", "date": "1974", "authors": ["Earl D. Sacerdoti"], "related_topics": ["Analytical hierarchy", "Hierarchy", "Problem domain", "Abstraction (linguistics)", "Representation (mathematics)", "Theoretical computer science", "Space (mathematics)", "STRIPS", "Discrete mathematics", "Mathematics", "Problem solver"]}
{"id": "2119409989", "references": ["2112161977", "1970001627", "2479583503"], "title": "Programs with common sense", "abstract": "Abstract : This paper discusses programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sentences. When an imperative sentence is deduced the program takes a corresponding action. These actions may include printing sentences, moving sentences on lists, and reinitiating the basic deduction process on these lists.", "citation_count": "3", "reference_count": "1,284", "date": "1960", "authors": ["John McCarthy"], "related_topics": ["Imperative logic", "Sentence", "Formal language", "First-order logic", "Action (philosophy)", "Common sense", "Computer programming", "Natural language processing", "Process (engineering)", "Computer science", "Artificial intelligence"]}
{"id": "3025398977", "references": ["2065505622", "2099587183", "3082495007", "2024523255", "1965024406", "2040886120", "2964309938", "2083464739", "2144373421", "3094500671"], "title": "Time Enough for Love", "abstract": "", "citation_count": "0", "reference_count": "68", "date": "1973", "authors": ["Robert A. Heinlein"], "related_topics": []}
{"id": "1572038152", "references": ["2293009092", "2051567680", "125130877", "2009328995", "1981627423", "1613651675", "2021337406", "1567972407", "2084335986", "2017626051"], "title": "Search reduction in hierarchical problem solving", "abstract": "It has long been recognized that hierarchical problem solving can be used to reduce search. Yet, there has been little analysis of the problem-solving method and few experimental results. This paper provides the first comprehensive analytical and empirical demonstrations of the effectiveness of hierarchical problem solving. First, the paper shows analytically that hierarchical problem solving can reduce the size of the search space from exponential to linear in the solution length and identifies a sufficient set of assumptions for such reductions in search. Second, it presents empirical results both in a domain that meets all of these assumptions as well as in domains in which these assumptions do not strictly hold. Third, the paper explores the conditions under which hierarchical problem solving will be effective in practice.", "citation_count": "13", "reference_count": "136", "date": "1991", "authors": ["Craig A. Knoblock"], "related_topics": ["General Group Problem Solving (GGPS) Model", "Reduction (complexity)", "Set (abstract data type)", "Mathematical optimization", "Exponential function", "Computer science", "Space (mathematics)"]}
{"id": "2077902449", "references": ["2742819845", "1979675141", "2009551863", "2611627047", "2099111195", "3125873018", "2317700292", "3124873412", "2106887613", "3124955340"], "title": "The Nonstochastic Multiarmed Bandit Problem", "abstract": "In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.\r\nIn this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2). We show by a matching lower bound that this is the best possible.\r\nWe also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N1/2 T-1/2). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2).", "citation_count": "16", "reference_count": "2,133", "date": "2003", "authors": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "related_topics": ["Multi-armed bandit", "Stochastic game", "Minimax", "Statistical assumption", "Matching (graph theory)", "Sequence", "Upper and lower bounds", "Stochastic process", "Mathematical optimization", "Mathematical economics", "Mathematics"]}
{"id": "2009551863", "references": ["1998498767", "2035879260", "2104912596"], "title": "Asymptotically efficient adaptive allocation rules", "abstract": "", "citation_count": "3", "reference_count": "2,685", "date": "1985", "authors": ["T.L Lai", "Herbert Robbins"], "related_topics": ["Mathematical optimization", "Multi-armed bandit", "Thompson sampling", "Mathematics"]}
{"id": "1512919909", "references": ["1655990431", "1575388622", "1650504995", "2914656440", "2161521419", "2009533501", "2021061679", "2122410182"], "title": "A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes", "abstract": "A critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor \u03b3 and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration\u2014rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter.\r\n\r\nOur algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs (Kearns, Mansour, &amp; Ng. Neural information processing systems 13, to appear).", "citation_count": "18", "reference_count": "825", "date": "2002", "authors": ["Michael Kearns", "Yishay Mansour", "Andrew Y. Ng"], "related_topics": ["Partially observable Markov decision process", "Markov decision process", "Q-learning", "Markov model", "State space", "Reinforcement learning", "Tree (data structure)", "Generative model", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "2101861158", "references": ["2146628995", "1561981064", "1532584713", "2145224651", "1863869622", "1978304080", "2014932765", "2131600418", "2083347533", "1630331242"], "title": "The challenge of poker", "abstract": "Poker is an interesting test-bed for artificial intelligence research. It is a game of imperfect information, where multiple competing agents must deal with probabilistic knowledge, risk assessment, and possible deception, not unlike decisions made in the real world. Opponent modeling is another difficult problem in decision-making applications, and it is essential to achieving high performance in poker. This paper describes the design considerations and architecture of the poker program Poki. In addition to methods for hand evaluation and betting strategy, Poki uses learning techniques to construct statistical models of each opponent, and dynamically adapts to exploit observed patterns and tendencies. The result is a program capable of playing reasonably strong poker, but there remains considerable research to be done to play at world-class level. Copyright 2001 Elsevier Science B.V.", "citation_count": "35", "reference_count": "427", "date": "2002", "authors": ["Darse Billings", "Aaron Davidson", "Jonathan Schaeffer", "Duane Szafron"], "related_topics": ["Texas hold 'em", "Planning poker", "Perfect information", "Deception", "Probabilistic logic", "Exploit", "Construct (philosophy)", "Computer science", "Human\u2013computer interaction", "Machine learning", "Statistical model", "Artificial intelligence"]}
{"id": "1551466210", "references": ["1969481231", "2101861158", "2134440396", "2787384486", "2014969765", "1863869622", "2033016725", "2014932765", "2581275558", "2107549951"], "title": "MONTE-CARLO GO DEVELOPMENTS", "abstract": "We describe two Go programs, Olga and Oleg, developed by a Monte-Carlo approach that is simpler than Bruegmann\u2019s (1993) approach. Our method is based on Abramson (1990). We performed experiments, to assess ideas on (1) progressive pruning, (2) all moves as first heuristic, (3) temperature, (4) simulated annealing, and (5) depth-two tree search within the Monte-Carlo framework. Progressive pruning and the all moves as first heuristic are good speed-up enhancements that do not deteriorate the level of the program too much. Then, using a constant temperature is an adequate and simple heuristic that is about as good as simulated annealing. The depth-two heuristic gives deceptive results at the moment. The results of our Monte-Carlo programs against knowledge-based programs on 9x9 boards are promising. Finally, the ever-increasing power of computers lead us to think that Monte-Carlo approaches are worth considering for computer Go in the future.", "citation_count": "12", "reference_count": "216", "date": "2004", "authors": ["Bruno Bouzy", "Bernard Helmstetter"], "related_topics": ["Null-move heuristic", "Computer Go", "Heuristic", "Simulated annealing", "Pruning (decision trees)", "Heuristics", "Tree (data structure)", "Monte Carlo method", "Algorithm", "Computer science"]}
{"id": "1562282139", "references": ["2173567424", "1532584713", "1551466210", "2785370368", "2153367522", "2120032275", "2914656440", "2131600418", "2995879998"], "title": "Monte Carlo Planning in RTS Games.", "abstract": "", "citation_count": "10", "reference_count": "156", "date": "2005", "authors": ["Michael Chung", "Michael Buro", "Jonathan Schaeffer"], "related_topics": ["Monte Carlo method", "Monte Carlo tree search", "Computer science", "Algorithm", "Simulation"]}
{"id": "1863869622", "references": ["2100677568", "1655990431", "2101861158", "1532584713", "2468925818", "2131600418", "2107549951", "2135997697", "1976776761"], "title": "World-championship-caliber Scrabble", "abstract": "Computer Scrabble programs have achieved a level of performance that exceeds that of the strongest human players. MAVEN was the first program to demonstrate this against human opposition. Scrabble is a game of imperfect information with a large branching factor. The techniques successfully applied in two-player games such as chess do not work here. MAVEN combines a selective move generator, simulations of likely game scenarios, and the B* algorithm to produce a world-championship-caliber Scrabble-playing program.", "citation_count": "16", "reference_count": "186", "date": "2002", "authors": ["Brian Sheppard"], "related_topics": ["World championship", "Perfect information", "Computer science", "Generator (computer programming)", "Artificial intelligence", "Caliber"]}
{"id": "2168405694", "references": ["1977823770", "2121863487", "1497256448", "2009551863", "1515851193", "1983962754", "2317700292", "2010029425", "2000080679", "1970097186"], "title": "Finite-time Analysis of the Multiarmed Bandit Problem", "abstract": "Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.", "citation_count": "13", "reference_count": "5,631", "date": "2002", "authors": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "related_topics": ["Regret", "Multi-armed bandit", "Thompson sampling", "Reinforcement learning", "Dilemma", "Monte Carlo tree search", "Bounded function", "Mathematical economics", "General game playing", "Mathematical optimization", "Computer science"]}
{"id": "2135997697", "references": ["2100677568", "2202514763", "2103626435", "2148173263", "2151368309", "2117341272", "2098432798", "2131600418"], "title": "On-line Policy Improvement using Monte-Carlo Search", "abstract": "We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers.\r\n\r\nWe have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.", "citation_count": "8", "reference_count": "278", "date": "1996", "authors": ["Gerald Tesauro", "Gregory R. Galperin"], "related_topics": ["Adaptive control", "Monte Carlo method", "Reduction (complexity)", "Control theory", "Word error rate", "Artificial neural network", "Mathematical optimization", "Computer science", "Line (geometry)"]}
{"id": "2016647253", "references": ["1976625337", "2009551863", "1512919909", "2098432798", "2168405694", "2000080679", "2140481308", "2113789941", "3145128584"], "title": "An Adaptive Sampling Algorithm for Solving Markov Decision Processes", "abstract": "Based on recent results for multiarmed bandit problems, we propose an adaptive sampling algorithm that approximates the optimal value of a finite-horizon Markov decision process (MDP) with finite state and action spaces. The algorithm adaptively chooses which action to sample as the sampling process proceeds and generates an asymptotically unbiased estimator, whose bias is bounded by a quantity that converges to zero at rate (lnN)/ N, whereN is the total number of samples that are used per state sampled in each stage. The worst-case running-time complexity of the algorithm isO(( |A|N) H ), independent of the size of the state space, where | A| is the size of the action space andH is the horizon length. The algorithm can be used to create an approximate receding horizon control to solve infinite-horizon MDPs. To illustrate the algorithm, computational results are reported on simple examples from inventory control.", "citation_count": "18", "reference_count": "143", "date": "2005", "authors": ["Hyeong Soo Chang", "Michael C. Fu", "Jiaqiao Hu", "Steven I. Marcus"], "related_topics": ["Markov process", "Adaptive algorithm", "Markov decision process", "Adaptive sampling", "State space", "Bias of an estimator", "Sampling (statistics)", "Dynamic programming", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "13294968", "references": ["2121863487", "2134042548", "1515851193", "2075268401", "1646707810", "1547105496", "1592648094", "2104753538", "594357522", "2130005627"], "title": "Toward Off-Policy Learning Control with Function Approximation", "abstract": "We present the first temporal-difference learning algorithm for off-policy control with unrestricted linear function approximation whose per-time-step complexity is linear in the number of features. Our algorithm, Greedy-GQ, is an extension of recent work on gradient temporal-difference learning, which has hitherto been restricted to a prediction (policy evaluation) setting, to a control setting in which the target policy is greedy with respect to a linear approximation to the optimal action-value function. A limitation of our control setting is that we require the behavior policy to be stationary. We call this setting latent learning because the optimal policy, though learned, is not manifest in behavior. Popular off-policy algorithms such as Q-learning are known to be unstable in this setting when used with linear function approximation.", "citation_count": "19", "reference_count": "255", "date": "2010", "authors": ["Hamid R. Maei", "Csaba Szepesv ri", "Shalabh Bhatnagar", "Richard S. Sutton"], "related_topics": ["Q-learning", "Function approximation", "Linear approximation", "Linear function (calculus)", "Function (mathematics)", "Latent learning", "Mathematical optimization", "Extension (predicate logic)", "Control (management)", "Mathematics"]}
{"id": "2149390907", "references": ["2009207944", "1575388622", "2334782222", "3022778360", "1977970897", "2170400507", "2121075864", "2225341423", "2798766386", "2011039300"], "title": "Learning symbolic models of stochastic domains", "abstract": "In this article, we work towards the goal of developing agents that can learn to act in complex worlds. We develop a probabilistic, relational planning rule representation that compactly models noisy, nondeterministic action effects, and show how such rules can be effectively learned. Through experiments in simple planning domains and a 3D simulated blocks world with realistic physics, we demonstrate that this learning algorithm allows agents to effectively model world dynamics.", "citation_count": "35", "reference_count": "249", "date": "2007", "authors": ["Hanna M. Pasula", "Luke S. Zettlemoyer", "Leslie Pack Kaelbling"], "related_topics": ["Blocks world", "Nondeterministic algorithm", "Probabilistic logic", "Representation (mathematics)", "Artificial intelligence", "Computer science", "Action (philosophy)", "Simple (abstract algebra)", "Dynamics (music)"]}
{"id": "2134042548", "references": ["2100677568", "2121863487", "2103626435", "2006903949", "1515851193", "2075268401", "1646707810", "2531891978", "2117341272", "2139418546"], "title": "Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation", "abstract": "We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(\u03bb), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms' effectiveness.", "citation_count": "17", "reference_count": "242", "date": "2009", "authors": ["Shalabh Bhatnagar", "Doina Precup", "David Silver", "Richard S Sutton", "Hamid R. Maei", "Csaba Szepesv\u00e1ri"], "related_topics": ["Function approximation", "Approximation algorithm", "Bellman equation", "Function (mathematics)", "Linear function (calculus)", "Temporal difference learning", "Artificial neural network", "Computational complexity theory", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2075268401", "references": ["2100677568", "1778554682", "2065134213", "2072931156", "1646707810", "2132351269", "2109910161", "2071983464", "2139418546", "2104753538"], "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "abstract": "Sutton, Szepesvari and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.", "citation_count": "19", "reference_count": "519", "date": "2009", "authors": ["Richard S. Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesv\u00e1ri", "Eric Wiewiora"], "related_topics": ["Gradient descent", "Temporal difference learning", "Linear function", "Function (mathematics)", "Convergence (routing)", "Algorithm", "Term (time)", "Computer Go", "Zero (complex analysis)", "Artificial intelligence", "Mathematics"]}
{"id": "2062937587", "references": ["1992764401", "2137171066", "2228018563", "2140243223", "2059039791", "3022135529", "1993750641", "266716723", "2052417512", "1601336606"], "title": "Neo: learning conceptual knowledge by sensorimotor interaction with an environment", "abstract": "Recent developments in philosophy, linguistics, developmental psychology and arti cial intelligence make it possible to envision a developmental path for an arti cial agent, grounded in activity-based sensorimotor representations. This paper describes how Neo, an arti cial agent, learns concepts by interacting with its simulated environment. Relatively little prior structure is required to learn fairly accurate representations of objects, activities, locations and other aspects of Neo's experience. We show how classes (categories) can be abstracted from these representations, and discuss how our representation might be extended to express physical schemas, general, domain-independent activities that could be the building blocks of concept formation.", "citation_count": "20", "reference_count": "97", "date": "1997", "authors": ["Paul R. Cohen", "Marc S. Atkin", "Tim Oates", "Carole R. Beal"], "related_topics": ["Concept learning", "Cognitive science", "Representation (arts)", "Structure (mathematical logic)", "Artificial intelligence", "Computer science"]}
{"id": "2109910161", "references": ["2911432472", "2121863487", "1598634407", "1595483645", "2333196491", "73143588", "2017103958", "1979071892", "2131600418", "2009533501"], "title": "Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning", "abstract": "Learning, planning, and representing knowledge at multiple levels of temporal ab- straction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforce- ment learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options\u2014closed-loop policies for taking ac- tion over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as mus- cle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning frame- work in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic pro- gramming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: 1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, 2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and 3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.", "citation_count": "75", "reference_count": "2,884", "date": "1999", "authors": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "related_topics": ["Reinforcement learning", "Markov decision process", "Abstraction (linguistics)", "Set (psychology)", "Hierarchy (mathematics)", "Function approximation", "Object (computer science)", "Artificial intelligence", "Computer science", "Action (philosophy)"]}
{"id": "1491843047", "references": ["2100677568", "2121863487", "1969166509", "1506514354", "1610678877", "2225341423", "1551329771", "1569296262", "2021061679", "1583833196"], "title": "Integrated architecture for learning, planning, and reacting based on approximating dynamic programming", "abstract": "This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.", "citation_count": "17", "reference_count": "1,768", "date": "1990", "authors": ["Richard S. Sutton"], "related_topics": ["Reinforcement learning", "Temporal difference learning", "Intelligent decision support system", "Data structure", "Trial and error", "Dynamic programming", "Reactive system", "Class (computer programming)", "Artificial intelligence", "Computer science", "Reinforcement"]}
{"id": "2165150801", "references": ["2121863487", "2024390895", "1515851193", "2075268401", "1825869920", "2155027007", "2119717200", "2114537044", "2130801532", "2130005627"], "title": "Deterministic Policy Gradient Algorithms", "abstract": "In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.", "citation_count": "20", "reference_count": "2,080", "date": "2014", "authors": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "related_topics": ["Deterministic algorithm", "Reinforcement learning", "Mathematical optimization", "Function (mathematics)", "Algorithm", "Simple (abstract algebra)", "Action (physics)", "Mathematics", "Exploratory behaviour"]}
{"id": "2100235918", "references": ["2121863487", "2171960770", "2147152072", "2140190241", "1994389483", "2110325612", "2159080219", "2042281163", "1971040550", "1880262756"], "title": "A survey of collaborative filtering techniques", "abstract": "As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.", "citation_count": "125", "reference_count": "4,000", "date": "2009", "authors": ["Xiaoyuan Su", "Taghi M. Khoshgoftaar"], "related_topics": ["Collaborative filtering", "Recommender system", "Scalability", "Machine learning", "Computer science", "Artificial intelligence", "Privacy protection"]}
{"id": "2020135152", "references": ["2100677568", "2124175081", "1778554682", "2121863487", "1625390266", "1714211023", "1515851193", "1491843047", "2168405694", "1888434271"], "title": "Combining online and offline knowledge in UCT", "abstract": "The UCT algorithm learns a value function online using sample-based search. The TD(\u03bb) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.", "citation_count": "16", "reference_count": "701", "date": "2007", "authors": ["Sylvain Gelly", "David Silver"], "related_topics": ["Monte Carlo tree search", "Online and offline", "Computer Go", "Search tree", "General game playing", "Machine learning", "Artificial intelligence", "Bellman equation", "Computer science", "Sample (statistics)", "Value (mathematics)"]}
{"id": "1500868819", "references": ["2135129960", "2107650339", "202670472", "1714211023", "2075848246", "2160637580", "2123932647", "2153975459", "1492505991", "1888434271"], "title": "COMPUTING \u201cELO RATINGS\u201d OF MOVE PATTERNS IN THE GAME OF GO", "abstract": "Move patterns are an essential method to incorporate do- main knowledge into Go-playing programs. This paper presents a new Bayesian technique for supervised learning of such patterns from game records, based on a generalization of Elo ratings. Each sample move in the training data is considered as a victory of a team of pattern features. Elo ratings of individual pattern features are computed from these victo- ries, and can be used in previously unseen positions to compute a prob- ability distribution over legal moves. In this approach, several pattern features may be combined, without an exponential cost in the number of features. Despite a very small number of training games (652), this algorithm outperforms most previous pattern-learning algorithms, both in terms of mean log-evidence ( 2.69), and prediction rate (34.9%). A 19\u25ca 19 Monte-Carlo program improved with these patterns reached the level of the strongest classical programs. and little domain expertise. This paper presents a new supervised pattern-learning algorithm, based on the Bradley-Terry model. The Bradley-Terry model is the theoretical basis of the Elo rating system. The principle of Elo ratings, as applied to chess, is that each player gets a numerical strength estimation, computed from the observation of past game results. From the ratings of players, it is possible to estimate a probability distribution over the outcome of future games. The same principle", "citation_count": "17", "reference_count": "440", "date": "2007", "authors": ["R\u00e9mi Coulom"], "related_topics": ["Computer Go", "Supervised learning", "Outcome (game theory)", "Probability distribution", "Generalization", "Bayesian probability", "Sample (statistics)", "Machine learning", "Artificial intelligence", "Subject-matter expert", "Computer science"]}
{"id": "1714211023", "references": ["2100677568", "2121863487", "1532172966", "1512919909", "1551466210", "1536615069", "1515851193", "2033016725", "2312609093", "2016647253"], "title": "Efficient selectivity and backup operators in Monte-Carlo tree search", "abstract": "A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to minmax as the number of simulations grows. This approach provides a finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9 \u00d7 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.", "citation_count": "29", "reference_count": "1,266", "date": "2006", "authors": ["R\u00e9mi Coulom"], "related_topics": ["Interval tree", "Search tree", "Segment tree", "Tree traversal", "Optimal binary search tree", "Fractal tree index", "Incremental decision tree", "Order statistic tree", "Mathematical optimization", "Mathematics"]}
{"id": "2171084228", "references": ["2020135152", "2110962519", "1625390266", "1714211023", "2134802714", "2168359464", "2144913588", "2168405694", "2099430963", "2122659384"], "title": "Monte-Carlo Planning in Large POMDPs", "abstract": "This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 x 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.", "citation_count": "16", "reference_count": "935", "date": "2010", "authors": ["David Silver", "Joel Veness"], "related_topics": ["Partially observable Markov decision process", "Black box", "Domain knowledge", "Curse of dimensionality", "Benchmark (computing)", "Tree (data structure)", "Probability distribution", "Monte Carlo method", "Mathematical optimization", "Artificial intelligence", "Computer science", "Sampling (statistics)", "Observable"]}
{"id": "131069610", "references": ["2141664020", "2126316555", "2148361676", "2107338474", "1984505032", "2343568200", "1996183429", "2192586580", "2409009991"], "title": "Rapidly-exploring random trees : a new tool for path planning", "abstract": "", "citation_count": "0", "reference_count": "3,251", "date": "1998", "authors": ["S. Lavalle"], "related_topics": ["Any-angle path planning", "Random tree", "Probabilistic roadmap", "Motion planning", "Kinodynamic planning", "Mathematical optimization", "Mathematics", "Rapidly exploring random tree"]}
{"id": "1510812122", "references": ["2009551863", "2103012681", "3122363772", "1570963478", "2168405694", "1998498767", "2098339418"], "title": "Minimax policies for adversarial and stochastic bandits", "abstract": "We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit prob- lem. Concretely, we remove an extraneous loga- rithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate log- arithmic in the number of plays.", "citation_count": "7", "reference_count": "362", "date": "2009", "authors": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "related_topics": ["Normalization (statistics)", "Minimax", "Upper and lower bounds", "Randomized algorithm", "Mathematical optimization", "Mathematics", "Adversarial system"]}
{"id": "1888434271", "references": ["202670472", "2121863487", "1625390266", "1714211023", "183472599", "55358780", "2033016725", "2123932647", "3122363772", "2168405694"], "title": "Modi\ufb01cation of UCT with Patterns in Monte-Carlo Go", "abstract": "Algorithm UCB1 for multi-armed bandit problem has already been extended to Algorithm UCT (Upper bound Confidence for Tree) which works for minimax tree search. We have developed a Monte-Carlo Go program, MoGo, which is the first computer Go program using UCT. We explain our modification of UCT for Go application and also the intelligent random simulation with patterns which has improved significantly the performance of MoGo. UCT combined with pruning techniques for large Go board is discussed, as well as parallelization of UCT. MoGo is now a top level Go program on $9\\times9$ and $13\\times13$ Go boards.", "citation_count": "14", "reference_count": "508", "date": "2006", "authors": ["Sylvain Gelly", "Yizao Wang", "R\u00e9mi Munos", "Olivier Teytaud"], "related_topics": ["Computer Go", "Minimax", "Tree (data structure)", "Pruning (decision trees)", "Monte Carlo method", "Upper and lower bounds", "Algorithm", "Computer science", "Random simulation"]}
{"id": "2122410182", "references": ["1570448133", "2022166150", "2335728318", "2436001372", "2076063813", "2126316555", "2017337590", "2072955302", "2139356617", "2135790056"], "title": "Artificial Intelligence: A Modern Approach", "abstract": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.", "citation_count": "0", "reference_count": "42,568", "date": "2020", "authors": ["Stuart J. Russell", "Peter Norvig"], "related_topics": ["Symbolic artificial intelligence", "Artificial Intelligence System", "Artificial intelligence, situated approach", "Marketing and artificial intelligence", "Artificial psychology", "AI-complete", "Procedural reasoning system", "Computational intelligence", "Artificial intelligence", "Computer science"]}
{"id": "2074429597", "references": ["1971086298", "2427881153", "1996510517", "1502916507", "2165949425", "2147717514", "1575476631", "2056773559"], "title": "The design and analysis of spatial data structures", "abstract": "", "citation_count": "0", "reference_count": "4,171", "date": "1989", "authors": ["Hanan Samet"], "related_topics": ["Spatial analysis", "Computer science", "Remote sensing"]}
{"id": "1956559956", "references": ["2147152072", "2140190241", "2031489346", "2110325612", "2031454541", "2100235918", "2037227137", "1880262756", "1902027874", "2158266063"], "title": "Introduction to Modern Information Retrieval", "abstract": "", "citation_count": "0", "reference_count": "15,135", "date": "1983", "authors": ["Gerard Salton", "Michael J. McGill"], "related_topics": ["Human\u2013computer information retrieval", "Term Discrimination", "Vector space model", "Computer science", "Extended Boolean model", "Information retrieval", "University level"]}
{"id": "2125148312", "references": ["2008297189", "2102796633", "2103384342", "2021751319", "2138313032", "2095757522", "2168977926", "2171181782", "2093191240", "1996021349"], "title": "Texture features for browsing and retrieval of image data", "abstract": "Image content based retrieval is emerging as an important research area with application to digital libraries and multimedia databases. The focus of this paper is on the image processing aspects and in particular using texture information for browsing and retrieval of large image data. We propose the use of Gabor wavelet features for texture analysis and provide a comprehensive experimental evaluation. Comparisons with other multiresolution texture features using the Brodatz texture database indicate that the Gabor features provide the best pattern retrieval accuracy. An application to browsing large air photos is illustrated.", "citation_count": "20", "reference_count": "5,360", "date": "1996", "authors": ["B.S. Manjunath", "W.Y. Ma"], "related_topics": ["Image texture", "Content-based image retrieval", "Visual Word", "Image retrieval", "Automatic image annotation", "Texture Descriptor", "Gabor wavelet", "Image processing", "Feature extraction", "Wavelet", "Wavelet transform", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2160066518", "references": ["2113180829", "2154956324", "2130660124", "2111993661", "2427881153", "2147069236", "2145607950", "2147717514", "2109868644", "2135705692"], "title": "Query by image and video content: the QBIC system", "abstract": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. &gt;", "citation_count": "0", "reference_count": "8,575", "date": "1997", "authors": ["Myron Flickner", "Harpreet Sawhney", "Wayne Niblack", "Jonathan Ashley", "Qian Huang", "Byron Dom", "Monika Gorkani", "Jim Hafner", "Denis Lee", "Dragutin Petkovic", "David Steele", "Peter Yanker"], "related_topics": ["Information retrieval", "Computer science", "Key (cryptography)", "Image (mathematics)", "Motion (physics)", "IBM", "Content (measure theory)", "Graphical query", "Image content", "Object motion"]}
{"id": "1541459201", "references": ["1992810975", "2145725688", "2118269922", "2074429597", "2238624099", "2104128006", "2160066518", "2157092487", "2151135734", "1969294188"], "title": "A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces", "abstract": "For similarity search in high-dimensional vector spaces (or \u2018HDVSs\u2019), researchers have proposed a number of new methods (or adaptations of existing methods) based, in the main, on data-space partitioning. However, the performance of these methods generally degrades as dimensionality increases. Although this phenomenon-known as the \u2018dimensional curse\u2019-is well known, little or no quantitative a.nalysis of the phenomenon is available. In this paper, we provide a detailed analysis of partitioning and clustering techniques for similarity search in HDVSs. We show formally that these methods exhibit linear complexity at high dimensionality, and that existing methods are outperformed on average by a simple sequential scan if the number of dimensions exceeds around 10. Consequently, we come up with an alternative organization based on approximations to make the unavoidable sequential scan as fast as possible. We describe a simple vector approximation scheme, called VA-file, and report on an experimental evaluation of this and of two tree-based index methods (an R*-tree and an X-tree).", "citation_count": "35", "reference_count": "2,342", "date": "1998", "authors": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "related_topics": ["Nearest neighbor search", "iDistance", "Curse of dimensionality", "Cluster analysis", "Tree (data structure)", "Vector space", "Algorithm", "Simple (abstract algebra)", "Computer science"]}
{"id": "2295428206", "references": ["2070991879", "1972418517", "1593563200", "1568495775", "2068871408", "2052207834", "2069816168", "1963547452", "1979740015", "2148043549"], "title": "Randomized Algorithms", "abstract": "For many applications, a randomized algorithm is either the simplest or the fastest algorithm available, and sometimes both. This book introduces the basic concepts in the design and analysis of randomized algorithms. The first part of the text presents basic tools such as probability theory and probabilistic analysis that are frequently used in algorithmic applications. Algorithmic examples are also given to illustrate the use of each tool in a concrete setting. In the second part of the book, each chapter focuses on an important area to which randomized algorithms can be applied, providing a comprehensive and representative selection of the algorithms that might be used in each of these areas. Although written primarily as a text for advanced undergraduates and graduate students, this book should also prove invaluable as a reference for professionals and researchers.", "citation_count": "50", "reference_count": "7,211", "date": "1995", "authors": ["Rajeev Motwani", "Prabhakar Raghavan"], "related_topics": ["Probabilistic analysis of algorithms", "Randomized algorithm", "Seven Basic Tools of Quality", "Computer science", "Theoretical computer science", "Selection (linguistics)", "Closest string", "Probability theory", "Graduate students", "Randomized algorithms as zero-sum games"]}
{"id": "1506806321", "references": ["2157881433", "2096873754", "1977655452", "2087692915", "2076063813", "2964074409", "2083842231", "2166851633", "1903029394", "2054279472"], "title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "abstract": "", "citation_count": "0", "reference_count": "14,082", "date": "2006", "authors": ["Christopher M. Bishop"], "related_topics": ["Feature (machine learning)", "Unsupervised learning", "Pattern recognition (psychology)", "Information science", "Computer science", "Machine learning", "Pattern recognition", "Artificial intelligence"]}
{"id": "2168359464", "references": ["2025460523", "2105934661", "1552169927", "2125838338", "1576818901", "1978304080", "2098432798", "2119567691", "4789806", "1963547452"], "title": "Planning and Acting in Partially Observable Stochastic Domains", "abstract": "In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.", "citation_count": "70", "reference_count": "4,373", "date": "1998", "authors": ["Leslie Pack Kaelbling", "Michael L. Littman", "Anthony R. Cassandra"], "related_topics": ["Partially observable Markov decision process", "Predictive state representation", "Markov decision process", "Markov model", "Observable", "Mathematical optimization", "Control theory", "Mathematics", "Off line"]}
{"id": "2119567691", "references": ["2341171179", "2035446426", "2067733412", "2130184319", "154446778", "2319878520", "1571552128", "3038830718", "2096630263", "2036791211"], "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "abstract": "From the Publisher:\r\nThe past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a \"theorem-proof\" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic", "citation_count": "12", "reference_count": "15,048", "date": "1994", "authors": ["Martin L. Puterman"], "related_topics": ["Partially observable Markov decision process", "Markov decision process", "Markov model", "Variable-order Bayesian network", "Markov process", "Markov algorithm", "Gittins index", "Stochastic programming", "Mathematical economics", "Mathematical optimization", "Mathematics"]}
{"id": "1576452626", "references": ["1664011265", "1504946885", "2099618002", "2076063813", "2022406843", "2099201756", "2155027007", "2963423916", "2963985863"], "title": "Neuro-dynamic programming", "abstract": "From the Publisher:\r\nThis is the first textbook that fully explains the neuro-dynamic programming/reinforcement learning methodology, which is a recent breakthrough in the practical application of neural networks and dynamic programming to complex problems of planning, optimal decision making, and intelligent control.", "citation_count": "0", "reference_count": "7,023", "date": "1996", "authors": ["Dimitri P. Bertsekas", "John N. Tsitsiklis"], "related_topics": ["Inductive programming", "Functional logic programming", "Functional reactive programming", "Programming domain", "Reactive programming", "Programming paradigm", "Symbolic programming", "Markov decision process", "Artificial intelligence", "Computer science"]}
{"id": "2034806191", "references": ["1763311249", "2121863487", "1557517019", "2039546655", "2122585011", "1554663460", "1638203394", "1497256448", "2107726111", "2064675550"], "title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990\u20132010)", "abstract": "The simple, but general formal theory of fun and intrinsic motivation and creativity (1990-2010) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old, but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, and humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown, but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical, but nonoptimal implementations (1991, 1995, and 1997-2002) are reviewed, as well as several recent variants by others (2005-2010). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation.", "citation_count": "113", "reference_count": "698", "date": "2010", "authors": ["Ju\u0308rgen Schmidhuber"], "related_topics": ["Theory", "Active learning", "Creativity", "Surprise", "Cognitive science", "Field (computer science)", "Event (computing)", "Artificial intelligence", "Computer science", "Implementation", "Novelty"]}
{"id": "2099397840", "references": ["2136922672", "2099587183", "1625390266", "2111935653", "2119814172", "2952509347", "102487131", "2120480077", "196214544"], "title": "A Neuroevolution Approach to General Atari Game Playing", "abstract": "This paper addresses the challenge of learning to play many different video games with little domain-specific knowledge. Specifically, it introduces a neuroevolution approach to general Atari 2600 game playing. Four neuroevolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuroevolution agents represent different points along the spectrum of algorithmic sophistication\u2014including weight evolution on topologically fixed neural networks (conventional neuroevolution), covariance matrix adaptation evolution strategy (CMA\u2013ES),  neuroevolution of augmenting topologies (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e., HyperNEAT) allow scaling to higher dimensional representations (i.e., the raw game screen). Previous approaches based on temporal-difference (TD) learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuroevolution is a promising approach to general video game playing (GVGP).", "citation_count": "49", "reference_count": "194", "date": "2014", "authors": ["Matthew Hausknecht", "Joel Lehman", "Risto Miikkulainen", "Peter Stone"], "related_topics": ["Neuroevolution", "Neuroevolution of augmenting topologies", "HyperNEAT", "General video game playing", "Evolutionary acquisition of neural topologies", "Evolutionary computation", "Artificial neural network", "CMA-ES", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2792315573", "references": ["2156909104", "1731081199", "1959608418", "2257979135", "3124955340", "2122410182", "2766447205", "1924770834", "2064675550", "2145339207"], "title": "Machine learning &amp; artificial intelligence in the quantum domain: a review of recent progress.", "abstract": "Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research-quantum information versus machine learning (ML) and artificial intelligence (AI)-have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our 'big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement-exploring what ML/AI can do for quantum physics and vice versa-researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.", "citation_count": "353", "reference_count": "364", "date": "2018", "authors": ["Vedran Dunjko", "Hans J. Briegel"], "related_topics": ["Quantum machine learning", "Quantum technology", "Quantum information", "Quantum computer", "Interactive Learning", "Big data", "Field (computer science)", "Quantum", "Artificial intelligence", "Machine learning", "Physics"]}
{"id": "2073384958", "references": ["2121863487", "1625390266", "1970789124", "2098432798", "1601081659", "2168405694", "2119567691", "1576452626", "2895674046", "2107726111"], "title": "Algorithms for Reinforcement Learning", "abstract": "Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.", "citation_count": "189", "reference_count": "1,237", "date": "2010", "authors": ["Csaba Szepesvari"], "related_topics": ["Learning classifier system", "Reinforcement learning", "Unsupervised learning", "Robot learning", "Active learning", "Computational learning theory", "Instance-based learning", "Algorithmic learning theory", "Artificial intelligence", "Machine learning", "Computer science", "Algorithm"]}
{"id": "1564034482", "references": ["2086032301", "2157376352", "2010242617", "2158935941", "1996874802", "1481865416", "1747101780", "2013391942", "1507849272", "2114900706"], "title": "Integrated information theory of consciousness: an updated account", "abstract": "This article presents an updated account of integrated information theory of consciousness (liT) and some of its implications. /IT stems from thought experiments that lead to phenomenological axioms (existence, compositionality, information, integration, exclusion) and corresponding ontological postulates. The information axiom asserts that every experience is spec~fic - it is what it is by differing in its particular way from a large repertoire of alternatives. The integration axiom asserts that each experience is unified- it cannot be reduced to independent components. The exclusion axiom asserts that every experience is definite - it is limited to particular things and not others and flows at a particular speed and resolution. /IT formalizes these intuitions with postulates. The information postulate states that only \"differences that make a difference\" from the intrinsic perpective of a system matter: a mechanism generates cause-effect information if its present state has selective past causes and selective future effects within a system. The integration postulate states that only information that is irreducible matters: mechanisms generate integrated information only to the extent that the information they generate cannot be partitioned into that generated within independent components. The exclusion postulate states that only maxima of integrated information matter: a mechanism specifies only one maximally irreducible set of past causes and future effects - a concept. A complex is a set of elements specifying a maximally irreducible constellation of concepts, where the maximum is evaluated over elements and at the optimal spatiatemporal scale. Its concepts specify a maximally integrated conceptual information structure or quale, which is identical with an experience. Finally, changes in information integration upon exposure to the environment reflect a system's ability to match the causal structure of the world. After introducing an updated definition of information integration and related quantities, the article presents some theoretical considerations about the relationship between information and causation and about the relational structure of concepts within a qua/e. It also explores the relationship between the temporal grain size of information integration and the dynamic of metastable states in the corticothalamic complex. Finally, it summarizes how liT accounts for empirical findings about the neural substrate of consciousness, and how various aspects of phenomenology may in principle be addressed in terms of the geometry of information integration.", "citation_count": "47", "reference_count": "319", "date": "2012", "authors": ["Giulio Tononi"], "related_topics": ["Information integration", "Integrated information theory", "Information structure", "Axiom", "Principle of compositionality", "Information theory", "Set (psychology)", "Causal structure", "Cognitive science", "Communication", "Psychology"]}
{"id": "2101493843", "references": ["2611071497", "2121863487", "2151554678", "1515851193", "2102988123", "2021551171", "1638203394", "2013391942", "1576452626", "2171301064"], "title": "Universal Intelligence: A Definition of Machine Intelligence", "abstract": "A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: we take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.", "citation_count": "95", "reference_count": "552", "date": "2007", "authors": ["Shane Legg", "Marcus Hutter"], "related_topics": ["Intelligence cycle (target-centric approach)", "Human intelligence", "Symbolic artificial intelligence", "Computational intelligence", "Marketing and artificial intelligence", "Artificial intelligence, situated approach", "AI-complete", "Artificial psychology", "Artificial intelligence", "Computer science"]}
{"id": "2140190241", "references": ["1570448133", "2156909104", "2911964244", "1995945562", "1639032689", "1554663460", "2148603752", "1554944419", "2912565176", "2008620264"], "title": "Data Mining: Concepts and Techniques", "abstract": "The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data", "citation_count": "485", "reference_count": "50,570", "date": "2000", "authors": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "related_topics": ["Web mining", "Data stream mining", "Concept mining", "Data warehouse", "Predictive Model Markup Language", "K-optimal pattern discovery", "Field (computer science)", "Association rule learning", "Data science", "Computer science", "Data mining"]}
{"id": "2139212933", "references": ["2432517183", "2610857016", "2156909104", "740415", "2119821739", "1554663460", "2148603752", "2981264952", "2140095548", "2087347434"], "title": "A Tutorial on Support Vector Machines for Pattern Recognition", "abstract": "The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.", "citation_count": "54", "reference_count": "30,394", "date": "1998", "authors": ["Christopher J. C. Burges"], "related_topics": ["Margin classifier", "Least squares support vector machine", "Relevance vector machine", "VC dimension", "Sequential minimal optimization", "Support vector machine", "Structural risk minimization", "Structured support vector machine", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2148603752", "references": ["2156909104"], "title": "Statistical learning theory", "abstract": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.", "citation_count": "1", "reference_count": "67,324", "date": "1998", "authors": ["Vladimir Naumovich Vapnik"], "related_topics": ["Algorithmic learning theory", "Statistical learning theory", "Stability (learning theory)", "Computational learning theory", "Empirical risk minimization", "Statistical theory", "Generalization", "VC dimension", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2129812935", "references": ["1989702938", "2164452299", "2156909104", "2121647436", "2135046866", "2078204800", "2163808566", "2296319761", "2129638195", "2138451337"], "title": "Robust Face Recognition via Sparse Representation", "abstract": "We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.", "citation_count": "65", "reference_count": "10,849", "date": "2009", "authors": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma"], "related_topics": ["Sparse approximation", "K-SVD", "Feature vector", "Eigenface", "Feature extraction", "Facial recognition system", "Statistical classification", "Robustness (computer science)", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2168228682", "references": ["2151328054", "2137291015", "2093465006", "2166501286", "2087347434", "2162363099", "2093717447", "2056763477"], "title": "Comparison of classifier methods: a case study in handwritten digit recognition", "abstract": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.", "citation_count": "8", "reference_count": "936", "date": "1994", "authors": ["L. Bottou", "C. Cortes", "J.S. Denker", "H. Drucker", "I. Guyon", "L.D. Jackel", "Y. LeCun", "U.A. Muller", "E. Sackinger", "P. Simard", "V. Vapnik"], "related_topics": ["Handwriting recognition", "Classifier (UML)", "Pattern recognition", "Speech recognition", "NIST", "Computer science", "Artificial intelligence", "Digit recognition", "Training set"]}
{"id": "1530699444", "references": ["2137775453", "2119479037", "607505555", "2119821739", "2139212933", "1975846642", "2087347434", "2132549764", "1964357740", "3124955340"], "title": "Estimation of Dependences Based on Empirical Data", "abstract": "Realism and Instrumentalism: Classical Statistics and VC Theory (1960-1980).- Falsifiability and Parsimony: VC Dimension and the Number of Entities (1980-2000).- Noninductive Methods of Inference: Direct Inference Instead of Generalization (2000-...).- The Big Picture.", "citation_count": "0", "reference_count": "5,091", "date": "2010", "authors": ["Vladimir Naumovich Vapnik"], "related_topics": ["Inference", "VC dimension", "Generalization", "Falsifiability", "Mathematical economics", "Instrumentalism", "Algorithm", "Mathematics", "Realism", "Estimation", "Empirical data"]}
{"id": "1568787085", "references": ["1973224984", "2119821739", "2277406607", "2010332406", "2415856871", "1548139318", "2159737176", "1595098149", "2125145210", "2138882494"], "title": "Neural-Network and k-Nearest-neighbor Classifiers", "abstract": "The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance.", "citation_count": "0", "reference_count": "24", "date": "1991", "authors": ["J. Bromley", "E. Sackinger"], "related_topics": ["Time delay neural network", "Probabilistic neural network", "Artificial neural network", "Classifier (UML)", "k-nearest neighbors algorithm", "Pattern recognition", "Computer science", "Artificial intelligence", "Neural network classifier"]}
{"id": "5594912", "references": ["2113651538", "2119479037", "2119821739", "1479807131", "1574877594", "3104887532", "2110652811", "2122124659", "2154952480"], "title": "Estimation of Dependences Based on Empirical Data: Springer Series in Statistics (Springer Series in Statistics)", "abstract": "", "citation_count": "0", "reference_count": "451", "date": "1982", "authors": ["Vladimir Vapnik"], "related_topics": ["Series (mathematics)", "Computer science", "Estimation", "Statistics", "Empirical data"]}
{"id": "2504871398", "references": ["2109722477", "1787224781", "2218318129", "2105728138", "2119821739", "2124537004", "2803163155", "2914746235", "1613249581", "2160958420"], "title": "Learning representations by back-propagation errors, nature", "abstract": "", "citation_count": "0", "reference_count": "1,461", "date": "1986", "authors": ["DE Rumelhart", "GE Hinton", "RJ William"], "related_topics": ["Cognitive science", "Backpropagation", "Psychology"]}
{"id": "2145096794", "references": ["3029645440", "2103559027", "2099641086", "2136235822", "2012365979", "2158537680", "2078204800", "2154332973", "2798909945", "2116148865"], "title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information", "abstract": "This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f/spl isin/C/sup N/ and a randomly chosen set of frequencies /spl Omega/. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set /spl Omega/? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=/spl sigma//sub /spl tau//spl isin/T/f(/spl tau/)/spl delta/(t-/spl tau/) obeying |T|/spl les/C/sub M//spl middot/(log N)/sup -1/ /spl middot/ |/spl Omega/| for some constant C/sub M/&gt;0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N/sup -M/), f can be reconstructed exactly as the solution to the /spl lscr//sub 1/ minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C/sub M/ which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|/spl middot/logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N/sup -M/) would in general require a number of frequency samples at least proportional to |T|/spl middot/logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.", "citation_count": "31", "reference_count": "17,504", "date": "2006", "authors": ["E.J. Candes", "J. Romberg", "T. Tao"], "related_topics": ["Convex optimization", "Free probability", "Trigonometric polynomial", "Fourier series", "Binary logarithm", "Omega", "Sigma", "Piecewise", "Combinatorics", "Discrete mathematics", "Mathematics"]}
{"id": "2136235822", "references": ["2099641086", "2115090644", "2167839759", "2086869478", "2133866430", "2154332973"], "title": "Sparse representations in unions of bases", "abstract": "The purpose of this correspondence is to generalize a result by Donoho and Huo and Elad and Bruckstein on sparse representations of signals in a union of two orthonormal bases for R/sup N/. We consider general (redundant) dictionaries for R/sup N/, and derive sufficient conditions for having unique sparse representations of signals in such dictionaries. The special case where the dictionary is given by the union of L/spl ges/2 orthonormal bases for R/sup N/ is studied in more detail. In particular, it is proved that the result of Donoho and Huo, concerning the replacement of the /spl lscr//sup 0/ optimization problem with a linear programming problem when searching for sparse representations, has an analog for dictionaries that may be highly redundant.", "citation_count": "6", "reference_count": "1,028", "date": "2003", "authors": ["R. Gribonval", "M. Nielsen"], "related_topics": ["Sparse approximation", "Orthonormal basis", "Optimization problem", "Sparse matrix", "Linear programming", "Discrete mathematics", "Special case", "Mathematics", "Nonlinear approximation"]}
{"id": "2099641086", "references": ["2125455772", "2151693816", "1604810369", "2115755118", "1997149618", "2033367330", "2066462711", "2078204800", "1916685473", "2062024414"], "title": "Uncertainty principles and ideal atomic decomposition", "abstract": "Suppose a discrete-time signal S(t), 0/spl les/t&lt;N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1/sub {t=/spl tau/}/ and sinusoids exp{2/spl pi/iwt/N}//spl radic/N. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l/sup 1/ norm of the coefficients among all decompositions. Here \"highly sparse\" means that N/sub t/+N/sub w/&lt;/spl radic/N/2 where N/sub t/ is the number of time atoms, N/sub w/ is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l/sup 1/ uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinusoids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l/sup 1/ norm atomic decomposition. The condition \"sufficiently sparse\" becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2/sup j/2/.", "citation_count": "30", "reference_count": "2,365", "date": "2001", "authors": ["D.L. Donoho", "X. Huo"], "related_topics": ["Sparse approximation", "Superposition principle", "Norm (mathematics)", "Wavelet", "Basis pursuit", "Uncertainty principle", "Convex optimization", "Time\u2013frequency analysis", "Combinatorics", "Discrete mathematics", "Mathematics"]}
{"id": "2050834445", "references": ["2145096794", "2151693816", "2097323375", "2136235822", "2099641086", "2156447271", "1573820523", "2078204800", "2154332973", "2116148865"], "title": "For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution", "abstract": "We consider linear equations y = \u03a6x where y is a given vector in \u211dn and \u03a6 is a given n \u00d7 m matrix with n   0 so that for large n and for all \u03a6's except a negligible fraction, the following property holds: For every y having a representation y = \u03a6x0by a coefficient vector x0 \u2208 \u211dmwith fewer than \u03c1 \u00b7 n nonzeros, the solution x1of the 1-minimization problem\r\n\r\n\r\n\r\nis unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems\u2014greedy algorithms and thresholding\u2014perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. \u00a9 2006 Wiley Periodicals, Inc.", "citation_count": "26", "reference_count": "3,709", "date": "2006", "authors": ["David L. Donoho"], "related_topics": ["Underdetermined system", "Eigenvalues and eigenvectors", "Norm (mathematics)", "System of linear equations", "M-matrix", "Banach space", "Wishart distribution", "Linear equation", "Combinatorics", "Mathematics"]}
{"id": "2078204800", "references": ["2151693816", "2103559027", "2099641086", "2128659236", "2156447271", "2611147814", "2146842127", "2152328854", "2798909945", "2062024414"], "title": "Atomic Decomposition by Basis Pursuit", "abstract": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries---stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).\r\nBasis pursuit (BP) is a principle for decomposing a signal into an \"optimal\"' superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising.\r\nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.", "citation_count": "43", "reference_count": "24,846", "date": "2001", "authors": ["Scott Shaobing Chen", "David L. Donoho", "Michael A. Saunders"], "related_topics": ["Basis pursuit", "Basis pursuit denoising", "Wavelet packet decomposition", "Total variation denoising", "Wavelet", "Matching pursuit", "Orthogonal basis", "Quadratic programming", "Algorithm", "Mathematics"]}
{"id": "2154332973", "references": ["2125455772", "2610857016", "2151693816", "1604810369", "2136235822", "2099641086", "2115755118", "2167839759", "2078204800", "1995963238"], "title": "Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization", "abstract": "Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = \u2211k \u03b3(k)dk, with scalar coefficients \u03b3(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l1 norm of the coefficients \u03b3. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.", "citation_count": "19", "reference_count": "3,409", "date": "2003", "authors": ["David L. Donoho", "Michael Elad"], "related_topics": ["Sparse approximation", "Convex optimization", "Linear combination", "Basis pursuit", "Combinatorial optimization", "Minification", "Discrete mathematics", "Scalar (mathematics)", "Special case", "Computer science"]}
{"id": "2129638195", "references": ["2145096794", "2103559027", "2115755118", "2129131372", "2099641086", "2136235822", "2296616510", "2078204800", "2154332973", "2050834445"], "title": "Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?", "abstract": "Suppose we are given a vector f in a class FsubeRopfN , e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n)lesRmiddotn-1p/, where R&gt;0 and p&gt;0. Suppose that we take measurements yk=langf# ,Xkrang,k=1,...,K, where the Xk are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0&lt;p&lt;1 and with overwhelming probability, our reconstruction ft, defined as the solution to the constraints yk=langf# ,Xkrang with minimal lscr1 norm, obeys parf-f#parlscr2lesCp middotRmiddot(K/logN)-r, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed", "citation_count": "45", "reference_count": "7,858", "date": "2006", "authors": ["E.J. Candes", "T. Tao"], "related_topics": ["Norm (mathematics)", "Restricted isometry property", "Random projection", "Random matrix", "Concentration of measure", "Gaussian", "Fourier series", "Basis pursuit", "Discrete mathematics", "Combinatorics", "Mathematics"]}
{"id": "2116148865", "references": ["2610857016", "2151693816", "2136235822", "2099641086", "1605417594", "2156447271", "391578156", "2167839759", "2078204800", "2154332973"], "title": "Greed is good: algorithmic results for sparse approximation", "abstract": "This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho's basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms.", "citation_count": "30", "reference_count": "4,030", "date": "2004", "authors": ["J.A. Tropp"], "related_topics": ["Sparse approximation", "Matching pursuit", "Basis pursuit", "Approximation algorithm", "Sparse matrix", "Restricted isometry property", "Greedy algorithm", "Approximation theory", "Theoretical computer science", "Mathematics"]}
{"id": "2097323375", "references": ["2132680427", "2610857016", "2151693816", "2069912449", "2136235822", "2099641086", "2078204800", "2135046866", "2154332973", "2116148865"], "title": "Stable recovery of sparse overcomplete representations in the presence of noise", "abstract": "Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.", "citation_count": "46", "reference_count": "2,618", "date": "2004", "authors": ["D.L. Donoho", "M. Elad", "V.N. Temlyakov"], "related_topics": ["Sparse approximation", "K-SVD", "Matching pursuit", "Basis pursuit", "Approximation algorithm", "Noise (signal processing)", "Signal processing", "Stability (learning theory)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2050880896", "references": ["59771946", "2101491865", "67772112", "2110505738", "2018332268", "2296616510", "2020919250", "2109504624", "2146842127", "2069912449"], "title": "Unconditional Bases Are Optimal Bases for Data Compression and for Statistical Estimation", "abstract": "Abstract   An orthogonal basis of L2 which is also an unconditional basis of a functional space    F    is an optimal basis for compressing, estimating, and recovering functions in    F   . Simple thresholding operations, applied in the unconditional basis, work essentially better for compressing, estimating, and recovering than they do in any other orthogonal basis. In fact, simple thresholding in an unconditional basis works essentially better for recovery and estimation than other methods, period. (Performance is measured in an asymptotic minimax sense.) As an application, we formalize and prove Mallat\u2032s Heuristic, which says that wavelet bases are optimal for representing functions containing singularities, when there may be an arbitrary number of singularities, arbitrarily distributed.", "citation_count": "0", "reference_count": "576", "date": "1993", "authors": ["David L. Donoho"], "related_topics": ["Orthogonal basis", "Basis (linear algebra)", "Thresholding", "Minimax", "Wavelet", "Heuristic", "Simple (abstract algebra)", "Data compression", "Applied mathematics", "Discrete mathematics", "Mathematics"]}
{"id": "2147656689", "references": ["2122825543", "2145096794", "2610857016", "2099111195", "2115755118", "2135046866", "2078204800", "2296319761", "2129638195", "2116148865"], "title": "Just relax: convex programming methods for identifying sparse signals in noise", "abstract": "This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis", "citation_count": "64", "reference_count": "1,273", "date": "2006", "authors": ["J.A. Tropp"], "related_topics": ["Convex optimization", "Proper convex function", "Relaxation (iterative method)", "Linear combination", "Sparse approximation", "Iterative method", "Time complexity", "Noise (signal processing)", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2096613063", "references": ["1634005169", "2142276208", "2132984323", "2099111195", "2037612300", "1584610719", "2053691921", "2098914003", "1996021349", "2062024414"], "title": "Data compression and harmonic analysis", "abstract": "In this paper we review some recent interactions between harmonic analysis and data compression. The story goes back of course to Shannon's R(D) theory in the case of Gaussian stationary processes, which says that transforming into a Fourier basis followed by block coding gives an optimal lossy compression technique; practical developments like transform-based image compression have been inspired by this result. In this paper we also discuss connections perhaps less familiar to the information theory community, growing out of the field of harmonic analysis. Recent harmonic analysis constructions, such as wavelet transforms and Gabor transforms, are essentially optimal transforms for transform coding in certain settings. Some of these transforms are under consideration for future compression standards. We discuss some of the lessons of harmonic analysis in this century. Typically, the problems and achievements of this field have involved goals that were not obviously related to practical data compression, and have used a language not immediately accessible to outsiders. Nevertheless, through an extensive generalization of what Shannon called the \"sampling theorem\", harmonic analysis has succeeded in developing new forms of functional representation which turn out to have significant data compression interpretations. We explain why harmonic analysis has interacted with data compression, and we describe some interesting recent ideas in the field that may affect data compression in the future.", "citation_count": "84", "reference_count": "588", "date": "1998", "authors": ["D.L. Donoho", "M. Vetterli", "R.A. DeVore", "I. Daubechies"], "related_topics": ["Data compression", "Image compression", "Lossy compression", "Information theory", "Transform coding", "Harmonic analysis", "Wavelet transform", "Entropy (information theory)", "Algorithm", "Discrete mathematics", "Mathematics"]}
{"id": "2012365979", "references": ["2125455772", "2047424291", "2151693816", "2042194938", "2101610102", "1899006432", "2095546965", "2080745194", "1979750072", "1970950689"], "title": "Near-optimal sparse fourier representations via sampling", "abstract": "(MATH) We give an algorithm for finding a Fourier representation R of B terms for a given discrete signal signal A of length N, such that $\\|\\signal-\\repn\\|_2^2$ is within the factor (1 +e) of best possible $\\|\\signal-\\repn_\\opt\\|_2^2$. Our algorithm can access A by reading its values on a sample set T \u2286[0,N), chosen randomly from a (non-product) distribution of our choice, independent of A. That is, we sample non-adaptively. The total time cost of the algorithm is polynomial in B log(N)log(M)e (where M is the ratio of largest to smallest numerical quantity encountered), which implies a similar bound for the number of samples.", "citation_count": "17", "reference_count": "355", "date": "2002", "authors": ["A. C. Gilbert", "S. Guha", "P. Indyk", "S. Muthukrishnan", "M. Strauss"], "related_topics": ["Polynomial", "Discrete-time signal", "Fourier transform", "Distribution (mathematics)", "Combinatorics", "Sampling (statistics)", "Set (abstract data type)", "Mathematics", "Fourier representation", "Time cost"]}
{"id": "2053463056", "references": ["2114535528", "2112076978", "2096152098", "2067885219", "1966280301", "1956559956", "1975846642", "2032210760", "1670263352", "3124955340"], "title": "BoosTexter: A Boosting-based Systemfor Text Categorization", "abstract": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.", "citation_count": "37", "reference_count": "2,960", "date": "2000", "authors": ["Robert E. Schapire", "Yoram Singer"], "related_topics": ["Boosting (machine learning)", "Categorization", "Multiclass classification", "Multi-label classification", "Machine learning", "Classifier chains", "Computer science", "Artificial intelligence", "Boosting methods for object categorization", "Multi label learning", "Text categorization"]}
{"id": "3146306708", "references": ["2022204871", "71795751", "2084046180", "2114524997", "2108646579", "2166706824", "2160660844", "2097726431"], "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \u201csubtle nuances\u201d) and a negative semantic orientation when it has bad associations (e.g., \u201cvery cavalier\u201d). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \u201cexcellent\u201d minus the mutual information between the given phrase and the word \u201cpoor\u201d. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.", "citation_count": "0", "reference_count": "7,093", "date": "2002", "authors": ["Peter", "Turney"], "related_topics": ["Phrase", "Sentiment analysis", "Mutual information", "Natural language processing", "Orientation (computer vision)", "Word (computer architecture)", "Computer science", "Artificial intelligence", "Movie reviews", "Opinion extraction", "Unsupervised algorithm"]}
{"id": "1964357740", "references": ["1563088657", "2156909104", "2119821739", "2139212933", "2124776405", "2153635508", "1995945562", "1554663460", "2148603752", "2078204800"], "title": "A tutorial on support vector regression", "abstract": "In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.", "citation_count": "155", "reference_count": "11,502", "date": "2004", "authors": ["Alex J. Smola", "Bernhard Sch\u00f6lkopf"], "related_topics": ["Least squares support vector machine", "Relevance vector machine", "Structured support vector machine", "Support vector machine", "Machine learning", "Quadratic equation", "Regularization (mathematics)", "Computer science", "Regular polygon", "Artificial intelligence", "Support vector regression model"]}
{"id": "2115023510", "references": ["2127314673", "2066590388", "1565863475", "2167435923", "2199803028", "1998442272", "2166776180", "2166706824", "2098162425", "2075718943"], "title": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews", "abstract": "The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.", "citation_count": "24", "reference_count": "2,997", "date": "2003", "authors": ["Kushal Dave", "Steve Lawrence", "David M. Pennock"], "related_topics": ["Sentiment analysis", "Heuristics", "Classifier (UML)", "Document classification", "Feature extraction", "Information retrieval", "Computer science", "Opinion extraction", "Product reviews"]}
{"id": "2114524997", "references": ["2752885492", "3146306708", "2199803028", "1977545325", "2080558111", "2143516773", "2088622183", "2166706824", "2115023510"], "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts", "abstract": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.", "citation_count": "22", "reference_count": "4,097", "date": "2004", "authors": ["Bo Pang", "Lillian Lee"], "related_topics": ["Sentiment analysis", "Automatic summarization", "Polarity (physics)", "Natural language processing", "Computer science", "Subjectivity", "Artificial intelligence"]}
{"id": "2096152098", "references": ["1550206324", "2118020653", "2019759670", "1479807131", "2149684865", "2097089247", "2053463056", "2964321699"], "title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization", "abstract": "Abstract : A probabilistic analysis of the Rocchio relevance feedback algorithm, one of the most popular learning methods from information retrieval, is presented in a text categorization framework. The analysis results in a probabilistic version of the Rocchio classifier and offers an explanation for the TFIDF word weighting heuristic. The Rocchio classifier, its probabilistic variant and a standard naive Bayes classifier are compared on three text categorization tasks. The results suggest that the probabilistic algorithms are preferable to the heuristic Rocchio classifier.", "citation_count": "0", "reference_count": "2,151", "date": "1997", "authors": ["Thorsten Joachims"], "related_topics": ["Rocchio algorithm", "Relevance feedback", "Naive Bayes classifier", "tf\u2013idf", "Probabilistic analysis of algorithms", "Probabilistic logic", "Classifier (UML)", "Text processing", "Weighting", "Rule-based system", "Machine learning", "Bayes' theorem", "Computer science", "Random variable", "Artificial intelligence"]}
{"id": "2120779048", "references": ["2118020653", "2147152072", "2100935296", "2136930489", "1972644898", "1956559956", "1647729745", "2038721957", "1660390307", "2053921957"], "title": "Computing semantic relatedness using Wikipedia-based explicit semantic analysis", "abstract": "Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.", "citation_count": "30", "reference_count": "2,820", "date": "2007", "authors": ["Evgeniy Gabrilovich", "Shaul Markovitch"], "related_topics": ["Explicit semantic analysis", "Semantic computing", "Semantic similarity", "Semantic compression", "Natural language", "Natural language processing", "Meaning (linguistics)", "Space (commercial competition)", "Computer science", "Natural (music)", "Artificial intelligence"]}
{"id": "1548663377", "references": ["1502876877", "1576520375", "2047221353", "1521686387", "2148603752", "1660390307", "2287077086", "1523949738"], "title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "abstract": "We present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model significantly outperforms a less informed baseline.", "citation_count": "8", "reference_count": "1,127", "date": "2006", "authors": ["Razvan C. Bunescu", "Marius Pasca"], "related_topics": ["Entity linking", "Online encyclopedia", "Support vector machine", "Natural language processing", "Kernel (statistics)", "Structure (mathematical logic)", "Exploit", "Computer science", "Artificial intelligence", "High coverage", "Open domain"]}
{"id": "2165897980", "references": ["2128859735", "2066277072", "2107658650", "2099111195", "2139212933", "2153635508", "1638203394", "2144221002", "2166064672", "1983578042"], "title": "The Google Similarity Distance", "abstract": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of \"society\" is \"database,\" and the equivalent of \"use\" is \"a way to search the database\". We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories", "citation_count": "30", "reference_count": "2,250", "date": "2007", "authors": ["R.L. Cilibrasi", "P.M.B. Vitanyi"], "related_topics": ["Normalized Google distance", "WordNet", "Information distance", "Normalized compression distance", "Semantics", "Information extraction", "Similarity (network science)", "Similitude", "Information retrieval", "Computer science", "Search engine"]}
{"id": "2131357087", "references": ["2145766604", "1548663377", "1574901103", "1978394996", "2064418625", "1972644898", "1525595230", "2081580037", "158057341", "23685451"], "title": "Wikify!: linking documents to encyclopedic knowledge", "abstract": "This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.", "citation_count": "27", "reference_count": "1,274", "date": "2007", "authors": ["Rada Mihalcea", "Andras Csomai"], "related_topics": ["Keyword extraction", "Online encyclopedia", "Entity linking", "Information retrieval", "Resource (project management)", "Natural language processing", "Computer science", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "1960027552", "references": ["2120779048", "2158997610", "2100335205", "1548663377", "2131357087", "2165897980", "2103318667", "158057341", "86887328", "2053921957"], "title": "An effective, low-cost measure of semantic relatedness obtained from Wikipedia links", "abstract": "This paper describes a new technique for obtaining measures of semantic relatedness. Like other recent approaches, it uses Wikipedia to provide structured world knowledge about the terms of interest. Our approach is unique in that it does so using the hyperlink structure of Wikipedia rather than its category hierarchy or textual content. Evaluation with manually defined measures of semantic relatedness reveals this to be an effective compromise between the ease of computation of the former approach and the accuracy of the latter.", "citation_count": "15", "reference_count": "1,001", "date": "2008", "authors": ["David Milne", "Ian H. Witten"], "related_topics": ["Semantic similarity", "Hyperlink", "Information retrieval", "Natural language processing", "Hierarchy", "Structure (mathematical logic)", "Computer science", "Computation", "Measure (data warehouse)", "Compromise", "Artificial intelligence"]}
{"id": "158057341", "references": ["2156909104", "2151170651", "1548663377", "2136930489", "2109943925", "2017337590", "2096175520", "2038721957", "1567365482", "1983578042"], "title": "WikiRelate! computing semantic relatedness using wikipedia", "abstract": "Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by integrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the performance of an NLP application processing naturally occurring texts.", "citation_count": "30", "reference_count": "1,151", "date": "2006", "authors": ["Michael Strube", "Simone Paolo Ponzetto"], "related_topics": ["Normalized Google distance", "WordNet", "Semantic similarity", "Information retrieval", "Natural language processing", "Knowledge base", "Computer science", "Search engine", "Artificial intelligence"]}
{"id": "86887328", "references": ["2144578941", "2120779048", "1548663377", "2068882115", "2153911474", "2113227740", "158057341", "1570542661", "2029433174", "2068737686"], "title": "Large-Scale Named Entity Disambiguation Based on Wikipedia Data", "abstract": "This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.", "citation_count": "20", "reference_count": "1,436", "date": "2007", "authors": ["Silviu Cucerzan"], "related_topics": ["Entity linking", "Information extraction", "Context (language use)", "Natural language processing", "Information retrieval", "Computer science", "Process (engineering)", "Scale (chemistry)", "Artificial intelligence"]}
{"id": "2100341149", "references": ["2022166150", "2088314245", "2125055259", "2100335205", "2131357087", "1960027552", "89857650", "1504694836", "2611471614", "102708294"], "title": "Learning to link with wikipedia", "abstract": "This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75%. This performance is constant whether the system is evaluated on Wikipedia articles or \"real world\" documents.This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics.", "citation_count": "17", "reference_count": "1,513", "date": "2008", "authors": ["David Milne", "Ian H. Witten"], "related_topics": ["Automatic summarization", "Knowledge base", "Entity linking", "Precision and recall", "Search engine indexing", "Semantics", "Cluster analysis", "Information retrieval", "Natural language processing", "Computer science", "Task (project management)", "Artificial intelligence"]}
{"id": "2161204834", "references": ["1632114991", "2153439141", "1496659931", "2439178139", "2787704407", "1798971027", "2102924265", "1994851566"], "title": "Tree-bank Grammars", "abstract": "By a \"tree-bank grammar\" we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we &amp; though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.", "citation_count": "9", "reference_count": "482", "date": "1996", "authors": ["Eugene Charniak"], "related_topics": ["Tree-adjoining grammar", "Context-free grammar", "Regular tree grammar", "Generative grammar", "L-attributed grammar", "Link grammar", "Phrase structure grammar", "Ambiguous grammar", "Parsing expression grammar", "Emergent grammar", "Parsing", "Indexed grammar", "Adaptive grammar", "Affix grammar", "Extended Affix Grammar", "Mildly context-sensitive grammar formalism", "Synchronous context-free grammar", "Attribute grammar", "Operator-precedence grammar", "Statistical parsing", "Grammar", "Data-oriented parsing", "Rule-based machine translation", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2153439141", "references": ["2099345940", "1924403233", "1575431606", "2087165009", "2121227244", "1594031697", "1542847127"], "title": "Statistical Decision-Tree Models for Parsing", "abstract": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.", "citation_count": "7", "reference_count": "808", "date": "1995", "authors": ["David M. Magerman"], "related_topics": ["Top-down parsing", "Statistical parsing", "Parser combinator", "Parsing", "Data-oriented parsing", "Treebank", "Sentence", "Rule-based machine translation", "Syntax", "Natural language", "Natural language processing", "Grammar", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "1859173823", "references": ["2056367827", "80619934", "1981724541", "1548013757", "2132796688", "2099247782", "1967428646", "1977182536", "1794966349", "2162288523"], "title": "Structural ambiguity and lexical relations", "abstract": "We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning.", "citation_count": "12", "reference_count": "907", "date": "1993", "authors": ["Donald Hindle", "Mats Rooth"], "related_topics": ["Parsing", "Relative strength", "Association (object-oriented programming)", "Natural language processing", "Basis (linear algebra)", "Speech recognition", "Computer science", "Approximate solution", "Artificial intelligence", "Prepositional phrase", "Structural ambiguity"]}
{"id": "2092654472", "references": ["1632114991", "2147880316", "1574901103", "3021452258", "1535015163", "2002089154", "1986543644", "2107890099", "2110882317", "1773803948"], "title": "Head-Driven Statistical Models for Natural Language Parsing", "abstract": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.", "citation_count": "61", "reference_count": "2,528", "date": "2003", "authors": ["Michael Collins"], "related_topics": ["Data-oriented parsing", "Statistical parsing", "S-attributed grammar", "Bottom-up parsing", "Top-down parsing", "Parsing", "Parser combinator", "Treebank", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1567570606", "references": ["1632114991", "2153439141", "2161204834", "2127314673", "2787704407", "170869742", "2110882317"], "title": "Statistical parsing with a context-free grammar and word statistics", "abstract": "We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance.", "citation_count": "7", "reference_count": "762", "date": "1997", "authors": ["Eugene Charniak"], "related_topics": ["Statistical parsing", "Top-down parsing", "Bottom-up parsing", "Parsing", "Parser combinator", "S-attributed grammar", "Top-down parsing language", "Data-oriented parsing", "Language model", "Context-free grammar", "Sentence", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "1551104980", "references": ["1632114991", "2161204834", "1507020546", "1859173823", "1567570606", "1972573551", "1994851566", "2085575316", "2076118331", "2110882317"], "title": "PCFG models of linguistic tree representations", "abstract": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.", "citation_count": "16", "reference_count": "477", "date": "1998", "authors": ["Mark Johnson"], "related_topics": ["Treebank", "Data-oriented parsing", "Tree (data structure)", "Parsing", "Statistical parsing", "Precision and recall", "Independence (mathematical logic)", "Natural language processing", "Variation (game tree)", "Computer science", "Linguistics", "Artificial intelligence"]}
{"id": "1535015163", "references": ["1632114991", "2153439141", "2161204834", "2092654472", "1567570606", "1551104980", "2166394306", "2096175520", "1986543644", "1953828586"], "title": "A maximum-entropy-inspired parser", "abstract": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.", "citation_count": "16", "reference_count": "2,405", "date": "2000", "authors": ["Eugene Charniak"], "related_topics": ["Parsing", "Statistical parsing", "Treebank", "Data-oriented parsing", "Principle of maximum entropy", "Word error rate", "Head (linguistics)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2170716495", "references": ["2110485445", "2165545766", "2805516822", "2097606805", "1608805534", "2139621418", "2058105398", "2549835527", "2124126925"], "title": "Aspects of the Theory of Syntax", "abstract": "Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon", "citation_count": "0", "reference_count": "43,034", "date": "1965", "authors": ["Noam Chomsky"], "related_topics": ["Generative grammar", "c-command", "Phrase structure grammar", "Context-free grammar", "Generative second-language acquisition", "Linguistic competence", "Transformational grammar", "Principles and parameters", "Linguistics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2110882317", "references": ["1632114991", "2153439141", "2439178139", "1859173823", "1623072288", "2441154163", "2069912724", "2099247782", "2087165009", "1773803948"], "title": "A New Statistical Parser Based on Bigram Lexical Dependencies", "abstract": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy.", "citation_count": "14", "reference_count": "902", "date": "1996", "authors": ["Michael John Collins"], "related_topics": ["Simple LR parser", "Top-down parsing", "Bigram", "Parser combinator", "Statistical parsing", "Parsing", "Parse tree", "Data-oriented parsing", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2155693943", "references": ["1632114991", "2153439141", "2161204834", "2108321481", "2092654472", "1986543644", "3021452258", "1535015163", "2949237929", "2123893795"], "title": "Immediate-Head Parsing for Language Models", "abstract": "We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.", "citation_count": "18", "reference_count": "450", "date": "2001", "authors": ["Eugene Charniak"], "related_topics": ["Perplexity", "LR parser", "Simple LR parser", "Parser combinator", "GLR parser", "Top-down parsing", "Top-down parsing language", "Parsing", "LL grammar", "Language model", "Trigram", "Grammar", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2022204871", "references": ["3146306708", "2199803028", "2080558111", "2014902591", "2112422413", "2053463056", "2115023510", "2160660844", "2114524997"], "title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "abstract": "This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.", "citation_count": "22", "reference_count": "3,963", "date": "2005", "authors": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann"], "related_topics": ["Polarity (physics)", "Sentiment analysis", "Phrase", "Pattern recognition", "Computer science", "Artificial intelligence", "Stance detection"]}
{"id": "1587094587", "references": ["2019655958", "2186428165", "2155988679", "2161163382", "2749069611", "1951724000", "2125001590", "2111684210", "2141845152", "2115709314"], "title": "Mixed-Effects Models in S and S-PLUS", "abstract": "Linear Mixed-Effects * Theory and Computational Methods for LME Models * Structure of Grouped Data * Fitting LME Models * Extending the Basic LME Model * Nonlinear Mixed-Effects * Theory and Computational Methods for NLME Models * Fitting NLME Models", "citation_count": "0", "reference_count": "15,902", "date": "2000", "authors": ["Josae C. Pinheiro", "Douglas M. Bates"], "related_topics": ["Grouped data", "Nonlinear system", "Applied mathematics", "Structure (category theory)", "Econometrics", "Mathematics", "Mixed effects", "Nonlinear mixed effects model"]}
{"id": "2140833774", "references": ["2025768430", "2136922672", "2117130368", "2110798204", "2310919327", "2100495367", "2116064496", "2099741732", "2108384452", "2072128103"], "title": "Exploring Strategies for Training Deep Neural Networks", "abstract": "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.", "citation_count": "60", "reference_count": "1,068", "date": "2009", "authors": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Pascal Lamblin"], "related_topics": ["Deep learning", "Deep belief network", "Restricted Boltzmann machine", "Artificial neural network", "Boltzmann machine", "Unsupervised learning", "Optimization problem", "Generative model", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1566018662", "references": ["2150824314", "2158997610", "2101105183", "2117805756", "1978394996", "2100935296", "1647729745", "2038721957", "2525127255", "1567365482"], "title": "Corpus-based and knowledge-based measures of text semantic similarity", "abstract": "This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.", "citation_count": "31", "reference_count": "1,545", "date": "2006", "authors": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava"], "related_topics": ["Semantic similarity", "Explicit semantic analysis", "Normalized compression distance", "Semantic computing", "Similarity (psychology)", "Paraphrase", "Natural language processing", "Metric (mathematics)", "Information retrieval", "Word error rate", "Computer science", "Synonym", "Artificial intelligence"]}
{"id": "2095739681", "references": ["2136922672", "2134557905", "2158899491", "2156909104", "2110764733", "2110798204", "2310919327", "1423339008", "2162915993", "2143891888"], "title": "From machine learning to machine reasoning", "abstract": "A plausible definition of \"reasoning\" could be \"algebraically manipulating previously acquired knowledge in order to answer a new question\". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.\r\n\r\nThis observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \"all-purpose\" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.", "citation_count": "53", "reference_count": "264", "date": "2014", "authors": ["L\u00e9on Bottou"], "related_topics": ["Inference", "Transduction (machine learning)", "Concatenation", "Optical character recognition", "Logical disjunction", "Language model", "Algebraic operation", "Artificial intelligence", "Computer science", "Machine learning", "Fine-tuning"]}
{"id": "2122646361", "references": ["2147880316", "1506281249", "2162800060", "2156909104", "2132870739", "1565377632", "2158454296", "1673310716", "1679913846", "2148694408"], "title": "Anomaly detection: A survey", "abstract": "Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.", "citation_count": "372", "reference_count": "9,735", "date": "2009", "authors": ["Varun Chandola", "Arindam Banerjee", "Vipin Kumar"], "related_topics": ["Anomaly detection", "Local outlier factor", "Domain (software engineering)", "Machine learning", "Data mining", "Computer science", "Computational complexity theory", "Key (cryptography)", "Outlier", "Anomalous behavior", "Artificial intelligence", "Research areas"]}
{"id": "2118585731", "references": ["2142623206", "2150102617", "2097360283", "2109943925", "2118286367", "2153635508", "2165966284", "1602492977", "2087347434", "2035720976"], "title": "LIBLINEAR: A Library for Large Linear Classification", "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.", "citation_count": "17", "reference_count": "8,793", "date": "2008", "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "related_topics": ["Linear classifier", "Support vector machine", "Native-language identification", "Machine learning", "Data mining", "Logistic regression", "Computer science", "Artificial intelligence", "Linear svm", "Open source", "Sparse data sets"]}
{"id": "2118434577", "references": ["2964308564", "2101105183", "2130942839", "1810943226", "1815076433", "2100664567", "1753482797", "2157331557", "2124807415", "2153653739"], "title": "Addressing the Rare Word Problem in Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT\u201914 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT\u201914 contest task.", "citation_count": "18", "reference_count": "711", "date": "2015", "authors": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "related_topics": ["Machine translation", "Word problem (mathematics education)", "Word (computer architecture)", "Sentence", "Speech recognition", "Natural language processing", "Symbol (chemistry)", "Task (project management)", "Computer science", "Translation (geometry)", "Artificial intelligence"]}
{"id": "1854214752", "references": ["2148606196", "2624431344", "2101196063", "2335755599", "2115022330", "2108646579", "3013264884", "1888005072"], "title": "The PageRank Citation Ranking : Bringing Order to the Web", "abstract": "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.", "citation_count": "0", "reference_count": "23,699", "date": "1999", "authors": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "related_topics": ["PageRank", "Link farm", "HITS algorithm", "Web crawler", "Web page", "Web search engine", "Focused crawler", "Google matrix", "SimRank", "Spamdexing", "TrustRank", "Swoogle", "Webgraph", "Content farm", "Ranking", "Link analysis", "Information retrieval", "World Wide Web", "Computer science"]}
{"id": "2101491865", "references": ["2154455818", "2139823104", "2159929956", "2136396015", "2158787690", "2163398148", "2020919250", "2018047324", "2132914434", "2124386111"], "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "abstract": "In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.", "citation_count": "59", "reference_count": "2,612", "date": "2013", "authors": ["David I Shuman", "Sunil K. Narang", "Pascal Frossard", "Antonio Ortega", "Pierre Vandergheynst"], "related_topics": ["Modular decomposition", "Topological graph theory", "Graph theory", "Graph drawing", "Data domain", "Frequency domain", "Signal processing", "Clustering high-dimensional data", "Theoretical computer science", "Mathematics"]}
{"id": "2116341502", "references": ["2147880316", "1652505363", "1479807131", "2138621811", "2124776405", "2285257517", "1554663460", "2148603752", "3013264884", "2293063825"], "title": "The Graph Neural Network Model", "abstract": "Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.", "citation_count": "100", "reference_count": "2,358", "date": "2009", "authors": ["F. Scarselli", "M. Gori", "Ah Chung Tsoi", "M. Hagenbuchner", "G. Monfardini"], "related_topics": ["Graph (abstract data type)", "Directed graph", "Moral graph", "Graph theory", "Random geometric graph", "Directed acyclic graph", "Null graph", "Voltage graph", "Artificial neural network", "Cycle graph", "Supervised learning", "Null model", "Theoretical computer science", "Euclidean space", "Computer science", "Graph"]}
{"id": "2158787690", "references": ["2158940042", "2142276208", "2124386111", "1578099820", "2118217749", "2611093227", "2115755118", "2113945798", "2053691921", "2062024414"], "title": "Wavelets on graphs via spectral graph theory", "abstract": "We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we define the scaled wavelet operator Ttg = g(tL). The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on g, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing L. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.", "citation_count": "61", "reference_count": "1,455", "date": "2011", "authors": ["David K. Hammond", "Pierre Vandergheynst", "R\u00e9mi Gribonval"], "related_topics": ["Line graph", "Spectral graph theory", "Wavelet", "Diffusion wavelets", "Voltage graph", "Fast wavelet transform", "Graph power", "Cubic graph", "Discrete mathematics", "Mathematics"]}
{"id": "603908379", "references": ["2102605133", "2068730032", "2335728318", "1904365287", "2104657103", "2310919327", "1514535095", "2962741254", "2117539524", "1836465849"], "title": "Spatial transformer networks", "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.", "citation_count": "29", "reference_count": "4,273", "date": "2015", "authors": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "related_topics": ["Transformer (machine learning model)", "Convolutional neural network", "Artificial neural network", "Invariant (physics)", "Image warping", "Theoretical computer science", "Differentiable function", "Mathematics"]}
{"id": "2070232376", "references": ["1981885118", "2143131337", "2340149117", "1509342228", "2082817855", "2095117703", "2118953734", "2059586807", "2161455936", "2114030927"], "title": "A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs", "abstract": "Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [Bui and Jones, Proc. of the 6th SIAM Conference on Parallel Processing for Scientific Computing, 1993, 445--452; Hendrickson and Leland, A Multilevel Algorithm for Partitioning Graphs, Tech. report SAND 93-1301, Sandia National Laboratories, Albuquerque, NM, 1993]. From the early work it was clear that multilevel techniques held great promise; however, it was not known if they can be made to consistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the effectiveness of many different choices for all three phases: coarsening, partition of the coarsest graph, and refinement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the final partition obtained after multilevel refinement. We also present a much faster variation of the Kernighan--Lin (KL) algorithm for refining during uncoarsening. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute fill-reducing orderings for sparse matrices, it produces orderings that have substantially smaller fill than the widely used multiple minimum degree algorithm.", "citation_count": "43", "reference_count": "6,367", "date": "1998", "authors": ["George Karypis", "Vipin Kumar"], "related_topics": ["Graph partition", "Modular decomposition", "Strength of a graph", "Pathwidth", "Partition refinement", "Independent set", "Comparability graph", "Graph product", "Algorithm", "Discrete mathematics", "Computer science"]}
{"id": "1832693441", "references": ["2120615054", "2618530766", "1904365287", "2158899491", "2251939518", "2146502635", "2143612262", "2310919327", "2153579005", "2062118960"], "title": "Convolutional Neural Networks for Sentence Classification", "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "citation_count": "30", "reference_count": "9,234", "date": "2014", "authors": ["Yoon Kim"], "related_topics": ["Convolutional neural network", "Hyperparameter", "Sentiment analysis", "Word (computer architecture)", "Pattern recognition", "Machine learning", "Sentence", "Computer science", "State (computer science)", "Simple (abstract algebra)", "Series (mathematics)", "Artificial intelligence"]}
{"id": "2008652694", "references": ["1632114991", "1979711143", "2144087279", "2147880316", "1934019294", "2117400858", "2127713198", "1623072288", "2131297983", "1773803948"], "title": "Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms", "abstract": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.", "citation_count": "12", "reference_count": "2,444", "date": "2002", "authors": ["Michael Collins"], "related_topics": ["Maximum-entropy Markov model", "Hidden Markov model", "Viterbi algorithm", "Structured prediction", "Iterative Viterbi decoding", "Perceptron", "Conditional random field", "Discriminative model", "Chunking (psychology)", "Viterbi decoder", "Noun phrase", "Machine learning", "Pattern recognition", "Algorithm", "Computer science", "Artificial intelligence"]}
{"id": "2125838338", "references": ["1991133427", "2142384583", "2022554507", "2105594594", "1877570817", "1966812932", "2049633694", "1966264494", "2002182716", "2171850596"], "title": "A tutorial on hidden Markov models and selected applications in speech recognition", "abstract": "This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described. &gt;", "citation_count": "62", "reference_count": "31,278", "date": "1989", "authors": ["L.R. Rabiner"], "related_topics": ["Layered hidden Markov model", "Hidden semi-Markov model", "Hierarchical hidden Markov model", "Markov model", "Hidden Markov model", "Markov chain", "Forward algorithm", "Maximum-entropy Markov model", "Speech recognition", "Mathematics"]}
{"id": "2165698076", "references": ["2142827986", "2113606819", "1977970897", "2048679005", "2136504847", "2148603752", "2097089247", "2115403315", "2158108973", "2163302275"], "title": "A Survey on Transfer Learning", "abstract": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.", "citation_count": "87", "reference_count": "13,144", "date": "2010", "authors": ["Sinno Jialin Pan", "Qiang Yang"], "related_topics": ["Semi-supervised learning", "Inductive transfer", "Multi-task learning", "Online machine learning", "Active learning (machine learning)", "Instance-based learning", "Stability (learning theory)", "Unsupervised learning", "Transfer of learning", "Algorithmic learning theory", "Proactive learning", "Computational learning theory", "Cluster analysis", "Feature vector", "Machine learning", "Computer science", "Probability distribution", "Artificial intelligence", "Training set"]}
{"id": "1995903777", "references": ["2132984323", "2217896605", "2067191022", "2125838338", "2151103935", "2132103241", "2177274842", "2148603752", "2121947440", "3124955340"], "title": "Object tracking: A survey", "abstract": "The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.", "citation_count": "157", "reference_count": "6,768", "date": "2006", "authors": ["Alper Yilmaz", "Omar Javed", "Mubarak Shah"], "related_topics": ["Video tracking", "Motion detection", "Object detection", "Motion estimation", "Object (computer science)", "Motion (physics)", "Context (language use)", "Tracking (particle physics)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2104290444", "references": ["2154455818", "1578099820", "2001141328", "1479807131", "2139823104", "2053186076", "2097308346", "2048679005", "2148603752", "3023786531"], "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples", "abstract": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.", "citation_count": "50", "reference_count": "4,178", "date": "2006", "authors": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Transduction (machine learning)", "Regularization perspectives on support vector machines", "Representer theorem", "Online machine learning", "Supervised learning", "Support vector machine", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1512387364", "references": ["197270748", "2914746235", "2136518234", "2048679005", "2785349534", "2132655161", "2140243223", "2068737686", "2101210369", "2903158431"], "title": "Toward an architecture for never-ending language learning", "abstract": "We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.", "citation_count": "27", "reference_count": "1,965", "date": "2010", "authors": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka", "Tom M. Mitchell"], "related_topics": ["Knowledge base", "Software agent", "Information extraction", "Semi-supervised learning", "Task (project management)", "Web mining", "Never-Ending Language Learning", "Set (psychology)", "Human\u2013computer interaction", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "2150341604", "references": ["2102605133", "2618530766", "1614298861", "2136922672", "1904365287", "2153579005", "2310919327", "2100495367", "1849277567", "2064675550"], "title": "Deep Learning: Methods and Applications", "abstract": "This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.", "citation_count": "430", "reference_count": "3,282", "date": "2014", "authors": ["Li Deng", "Dong Yu"], "related_topics": ["Unsupervised learning", "Multi-task learning", "Deep learning", "Information processing", "Language model", "Text processing", "Artificial neural network", "Supervised learning", "Multimedia", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2114315281", "references": ["1534477342", "2052684427", "2153635508", "2146241755", "2096175520", "1999954155", "2157825442", "2053463056", "2105842272", "2133990480"], "title": "A Review On Multi-Label Learning Algorithms", "abstract": "Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.", "citation_count": "104", "reference_count": "2,073", "date": "2014", "authors": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "related_topics": ["Algorithmic learning theory", "Learning sciences", "Inductive transfer", "Supervised learning", "Multi-label classification", "Classifier chains", "Semantics", "Data science", "Computer science", "Algorithm"]}
{"id": "2098411764", "references": ["2134270519", "2108082645", "2161969291", "2154422044", "2914746235", "2120419212", "2171188998", "2004915807", "2149489787", "2131846894"], "title": "Describing objects by their attributes", "abstract": "We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (\u201cspotty dog\u201d, not just \u201cdog\u201d); to say something about unfamiliar objects (\u201chairy and four-legged\u201d, not just \u201cunknown\u201d); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (\u201cspotty\u201d) or discriminative (\u201cdogs have it but sheep do not\u201d). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.", "citation_count": "22", "reference_count": "2,016", "date": "2009", "authors": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "related_topics": ["Identity (object-oriented programming)", "3D single-object recognition", "Object (philosophy)", "Cognitive neuroscience of visual object recognition", "Object model", "Method", "Feature extraction", "Discriminative model", "Feature selection", "Natural language processing", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "2184188583", "references": ["2136922672", "2100235303", "2161969291", "2913932916", "2122922389", "2133257461", "2914746235", "2100495367", "2116064496", "2025768430"], "title": "Multimodal Deep Learning", "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.", "citation_count": "26", "reference_count": "2,685", "date": "2011", "authors": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng"], "related_topics": ["Feature learning", "Multimodal learning", "Multi-task learning", "Deep learning", "Classifier (UML)", "Speech recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2550821151", "references": ["2964308564", "2402144811", "2130942839", "2187089797", "1902237438", "2525778437", "2962784628", "2100664567", "1753482797", "2157331557"], "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model. On the WMT\u201914 benchmarks, a single multilingual model achieves comparable performance for English\u2192French and surpasses state-of-the-art results for English\u2192German. Similarly, a single multilingual model surpasses state-of-the-art results for French\u2192English and German\u2192English on WMT\u201914 and WMT\u201915 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages.", "citation_count": "31", "reference_count": "1,189", "date": "2017", "authors": ["Melvin Johnson", "Mike Schuster", "Quoc V. Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda B. Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "related_topics": ["Computer-assisted translation", "Transfer-based machine translation", "Machine translation", "Rule-based machine translation", "Interlingua", "Vocabulary", "Bridging (programming)", "Natural language processing", "Sentence", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "3115898234", "references": ["2587150483", "2142384583", "1524333225", "2293634267", "2046932483", "2094342469", "2173629880", "2150769028", "2890964092", "1631260214"], "title": "A unified system for multilingual speech recognition and language identification", "abstract": "Abstract   In this paper, a multilingual automatic speech recognition (ASR) and language identification (LID) system is designed. In contrast to conventional multilingual ASR systems, this paper takes advantage of the complementarity of the ASR and LID modules. First, the LID module contributes to the language adaptive training of the multilingual acoustic model. Then, the ASR decoding information acts as the confidence metric to balance the LID results. To simulate complex multilingual speech recognition situations, two types of LID strategies are conducted. For a multilingual speech recognition task in which only one language is contained in the speech stream, the language information can be directly determined based on utterance-level judgment. Under this condition, a segment-level statistical component and a two-stage update strategy are designed to assist in the utterance-level language classification. In another multilingual speech recognition task, where the speech stream contains multiple languages simultaneously, the Viterbi language state retrieval method based on neural network (NN) classification is used to perform dynamic detection of the language state. In both cases, the ASR decoding information is used to adjust the language classification results. Without prior knowledge of language identity information, the enhanced LID module achieves an accuracy of 99.3% for utterance-level language judgment and 92.4% for dynamic language detection, and the multilingual ASR system also provides performance comparable to that of monolingual ASR systems.", "citation_count": "45", "reference_count": "0", "date": "2021", "authors": ["Danyang Liu", "Ji Xu", "Pengyuan Zhang", "Yonghong Yan"], "related_topics": ["Language identification", "Acoustic model", "Viterbi algorithm", "Component (UML)", "Speech recognition", "Artificial neural network", "Identity (object-oriented programming)", "Computer science", "Decoding methods", "Task (project management)"]}
{"id": "3007502375", "references": ["2120605154", "2402146185", "1524333225", "1494198834", "2514741789", "2914746235", "2173629880", "2471520273", "2150769028", "2624871570"], "title": "A Modularized Neural Network with Language-Specific Output Layers for Cross-Lingual Voice Conversion", "abstract": "This paper presents a cross-lingual voice conversion framework that adopts a modularized neural network. The modularized neural network has a common input structure that is shared for both languages, and two separate output modules, one for each language. The idea is motivated by the fact that phonetic systems of languages are similar because humans share a common vocal production system, but acoustic renderings, such as prosody and phonotactic, vary a lot from language to language. The modularized neural network is trained to map Phonetic PosteriorGram (PPG) to acoustic features for multiple speakers. It is conditioned on a speaker i-vector to generate the desired target voice. We validated the idea between English and Mandarin languages in objective and subjective tests. In addition, mixed-lingual PPG derived from a unified English-Mandarin acoustic model is proposed to capture the linguistic information from both languages. It is found that our proposed modularized neural network significantly outperforms the baseline approaches in terms of speech quality and speaker individuality, and mixed-lingual PPG representation further improves the conversion performance.", "citation_count": "48", "reference_count": "4", "date": "2019", "authors": ["Yi Zhou", "Xiaohai Tian", "Emre Yilmaz", "Rohan Kumar Das", "Haizhou Li"], "related_topics": ["Acoustic model", "Rule-based machine translation", "Prosody", "Artificial neural network", "Speech recognition", "Phonotactics", "Mandarin Chinese", "Representation (mathematics)", "Computer science", "Structure (mathematical logic)"]}
{"id": "2098694627", "references": ["2154642048", "2157041604", "1652505363", "2103983685", "2113076747", "1485268022", "2135346934", "1562895369", "2173629880", "2330022088"], "title": "Modeling the Global Solar Radiation on the Earth\u2019s Surface Using Atmospheric Deterministic and Intelligent Data-Driven Techniques", "abstract": "Abstract Three methods for analyzing and modeling the global shortwave radiation reaching the earth\u2019s surface are presented in this study. Solar radiation is a very important input for many aspects of climatology, hydrology, atmospheric sciences, and energy applications. The estimation methods consist of an atmospheric deterministic model and two data-driven intelligent methods. The deterministic method is a broadband atmospheric model, developed for predicting the global and diffuse solar radiation incident on the earth\u2019s surface. The intelligent data-driven methods are a new neural network approach in which the hourly values of global radiation for several years are calculated and a new fuzzy logic method based on fuzzy sets theory. The two data-driven models, calculating the global solar radiation on a horizontal surface, are based on measured data of several meteorological parameters such as the air temperature, the relative humidity, and the sunshine duration. The three methods are tested and compare...", "citation_count": "38", "reference_count": "72", "date": "1999", "authors": ["M. Santamouris", "G. Mihalakakou", "B. Psiloglou", "G. Eftaxias", "D. N. Asimakopoulos"], "related_topics": ["Atmospheric model", "Shortwave radiation", "Fuzzy logic", "Fuzzy set", "Meteorology", "Sunshine duration", "Remote sensing", "Radiation", "Artificial neural network", "Energy (signal processing)", "Environmental science"]}
{"id": "2973127116", "references": ["2989925457", "3094821064", "3160044950", "3095822285", "3161000786", "3134568285", "3143843080", "3097643313", "3162354890"], "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction", "abstract": "Speech applications dealing with conversations require not only recognizing the spoken words, but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. The two systems are trained independently with different objective functions. Often the SD systems operate directly on the acoustics and are not constrained to respect word boundaries and this deficiency is overcome in an ad hoc manner. Motivated by recent advances in sequence to sequence learning, we propose a novel approach to tackle the two tasks by a joint ASR and SD system using a recurrent neural network transducer. Our approach utilizes both linguistic and acoustic cues to infer speaker roles, as opposed to typical SD systems, which only use acoustic cues. We evaluated the performance of our approach on a large corpus of medical conversations between physicians and patients. Compared to a competitive conventional baseline, our approach improves word-level diarization error rate from 15.8% to 2.2%.", "citation_count": "0", "reference_count": "33", "date": "2019", "authors": ["Laurent El Shafey", "Hagen Soltau", "Izhak Shafran"], "related_topics": ["Speaker diarisation", "Recurrent neural network", "Sequence learning", "Speech recognition", "Transduction (psychology)", "Task (project management)", "Word (computer architecture)", "Joint (audio engineering)", "Computer science", "Sequence"]}
{"id": "3016138882", "references": ["2239141610", "2153822685", "2313339984", "2963403868", "2751214333", "2173629880", "2346454595", "2143350951", "2009059481", "2085662862"], "title": "Multimodal Transformer Fusion for Continuous Emotion Recognition", "abstract": "Multimodal fusion increases the performance of emotion recognition because of the complementarity of different modalities. Compared with decision level and feature level fusion, model level fusion makes better use of the advantages of deep neural networks. In this work, we utilize the Transformer model to fuse audio-visual modalities on the model level. Specifically, the multi-head attention produces multimodal emotional intermediate representations from common semantic feature space after encoding audio and visual modalities. Meanwhile, it also can learn long-term temporal dependencies with self-attention mechanism effectively. The experiments, on the AVEC 2017 database, shows the superiority of model level fusion than other fusion strategies. Moreover, we combine the Transformer model and LSTM to further improve the performance, which achieves better results than other methods.", "citation_count": "21", "reference_count": "3", "date": "2020", "authors": ["Jian Huang", "Jianhua Tao", "Bin Liu", "Zheng Lian", "Mingyue Niu"], "related_topics": ["Transformer (machine learning model)", "Semantic feature", "Speech recognition", "Computer science", "Fusion", "Emotion recognition"]}
{"id": "2982413405", "references": ["2964308564", "2964265128", "2102113734", "179875071", "2147768505", "1494198834", "2963341956", "2962835968", "2064675550", "2127141656"], "title": "Transformer-Transducer: End-to-End Speech Recognition with Self-Attention", "abstract": "We explore options to use Transformer networks in neural transducer for end-to-end speech recognition. Transformer networks use self-attention for sequence modeling and comes with advantages in parallel computation and capturing contexts. We propose 1) using VGGNet with causal convolution to incorporate positional information and reduce frame rate for efficient inference 2) using truncated self-attention to enable streaming for Transformer and reduce computational complexity. All experiments are conducted on the public LibriSpeech corpus. The proposed Transformer-Transducer outperforms neural transducer with LSTM/BLSTM networks and achieved word error rates of 6.37 % on the test-clean set and 15.30 % on the test-other set, while remaining streamable, compact with 45.7M parameters for the entire system, and computationally efficient with complexity of O(T), where T is input sequence length.", "citation_count": "22", "reference_count": "65", "date": "2019", "authors": ["Ching-Feng Yeh", "Jay Mahadeokar", "Kaustubh Kalgaonkar", "Yongqiang Wang", "Duc Le", "Mahaveer Jain", "Kjell Schubert", "Christian Fuegen", "Michael L. Seltzer"], "related_topics": ["Transformer (machine learning model)", "Computational complexity theory", "Transducer", "Frame rate", "Speech recognition", "End-to-end principle", "Inference", "Computer science", "Self attention", "Sequence modeling"]}
{"id": "1606209288", "references": ["1989477529", "2110485445", "2112332687", "2173629880", "2974832207", "1994851566", "2146670093", "1980862600", "1554576613", "1983578042"], "title": "The Neuroscience of Language: On Brain Circuits of Words and Serial Order", "abstract": "How is language organized in the human brain? The Neuroscience of Language, published in 2003, puts forth a systematic model of language to bridge the gap between linguistics and neuroscience. Neuronal models of word and serial order processing are presented in the form of a computational, connectionist neural network. The linguistic emphasis is on words and elementary syntactic rules. Introductory chapters focus on neuronal structure and function, cognitive brain processes, the basics of classical aphasia research and modern neuroimaging of language, neural network approaches to language, and the basics of syntactic theories. The essence of the work is contained in chapters on neural algorithms and networks, basic syntax, serial order mechanisms, and neuronal grammar. Throughout, excursuses illustrate the functioning of brain models of language, some of which are accessible as animations on the book's accompanying web site. It will appeal to graduate students and researchers in neuroscience, psychology, linguistics, and computational modeling.", "citation_count": "313", "reference_count": "808", "date": "2003", "authors": ["Friedemann Pulverm\u00fcller"], "related_topics": ["Syntax", "Cognitive neuroscience", "Grammar", "Connectionism", "Aphasia", "Cognition", "Cognitive science", "Focus (linguistics)", "Artificial neural network", "Computer science", "Neuroscience"]}
{"id": "2970350205", "references": ["3136525061", "3126620582", "3126235073", "3121588208"], "title": "Chirality Nets for Human Pose Regression", "abstract": "We propose Chirality Nets, a family of deep nets that is equivariant to the \u201cchirality transform,\u201d i.e., the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings.", "citation_count": "0", "reference_count": "6", "date": "2019", "authors": ["Raymond A. Yeh", "Yuan-Ting Hu", "Alexander G. Schwing"], "related_topics": ["3D pose estimation", "Equivariant map", "Transformation (function)", "Reduction (complexity)", "Pattern recognition", "Activity recognition", "Representation (mathematics)", "Symmetry (geometry)", "Computer science", "Artificial intelligence"]}
{"id": "2729906263", "references": ["2153677638", "2162010436", "1981706894", "2128634885", "2163568299", "2797583228", "158057341", "89857650", "2251044566", "2151295812"], "title": "Linguistic Data Consortium", "abstract": "", "citation_count": "0", "reference_count": "1,022", "date": "1999", "authors": ["Treebank Penn"], "related_topics": ["Linguistic Data Consortium", "Computer science", "Linguistics"]}
{"id": "2037894654", "references": ["1632114991", "2151170651", "2146574666", "301824129", "2092654472", "2159080219", "2116410915", "1535015163", "2152263452", "3145128584"], "title": "Better k-best Parsing", "abstract": "We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.", "citation_count": "42", "reference_count": "404", "date": "2005", "authors": ["Liang Huang", "David Chiang"], "related_topics": ["Bottom-up parsing", "Top-down parsing", "S-attributed grammar", "Parsing", "Scalability", "Phrase", "Relevance (information retrieval)", "Natural language processing", "Hypergraph", "Computer science", "Artificial intelligence"]}
{"id": "2048679005", "references": ["2128221272", "2167044614", "2103555337", "2150516767", "2049633694", "1995897489", "1550302919", "3017143921", "2020764470", "2101210369"], "title": "Combining labeled and unlabeled data with co-training", "abstract": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu", "citation_count": "14", "reference_count": "6,444", "date": "1998", "authors": ["Avrim Blum", "Tom Mitchell"], "related_topics": ["Semi-supervised learning", "Co-training", "Web page", "Hyperlink", "Set (abstract data type)", "Permission", "Server", "Task (project management)", "Information retrieval", "Computer science"]}
{"id": "2098379588", "references": ["2150218618", "1508165687", "28766783", "1976711294", "160680375", "2159080219", "2096175520", "2114220616", "1533001652", "1734853756"], "title": "Estimators for Stochastic \"Unification-Based\" Grammars", "abstract": "Log-linear models provide a statistically sound framework for Stochastic \"Unification-Based\" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar.", "citation_count": "11", "reference_count": "258", "date": "1999", "authors": ["Mark Johnson", "Stuart Geman", "Stephen Canon", "Zhiyi Chi", "Stefan Riezler"], "related_topics": ["Tree-adjoining grammar", "Phrase structure grammar", "Context-sensitive grammar", "L-attributed grammar", "Indexed grammar", "Stochastic context-free grammar", "Context-free grammar", "Deterministic context-free grammar", "Synchronous context-free grammar", "Ambiguous grammar", "Stochastic grammar", "Parsing expression grammar", "Extended Affix Grammar", "Adaptive grammar", "Mildly context-sensitive grammar formalism", "Link grammar", "Syntax", "Rule-based machine translation", "Grammar", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2125712079", "references": ["2143458716", "1985754308", "2097606805", "2102667697", "2037894654", "1986543644", "1535015163", "3021452258", "2098379588", "2123893795"], "title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "abstract": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.", "citation_count": "18", "reference_count": "1,350", "date": "2005", "authors": ["Eugene Charniak", "Mark Johnson"], "related_topics": ["Statistical parsing", "Parsing", "Discriminative model", "Sentence", "Set (abstract data type)", "Natural language processing", "Pattern recognition", "Generative grammar", "Computer science", "Artificial intelligence", "Coarse to fine"]}
{"id": "2151170651", "references": ["1632114991", "2127314673", "2160842254", "2092654472", "1567570606", "1986543644", "2038721957", "2039217078", "2038248725", "2115792525"], "title": "Automatic labeling of semantic roles", "abstract": "We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.", "citation_count": "41", "reference_count": "2,460", "date": "2002", "authors": ["Daniel Gildea", "Daniel Jurafsky"], "related_topics": ["Semantic role labeling", "FrameNet", "Semantic similarity", "Semantic property", "Semantic equivalence", "Syntactic predicate", "Phrase", "Sentence", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2154626406", "references": ["2147880316", "12836875", "2151170651", "1934019294", "1567570606", "1986543644", "1667964228", "2115792525", "2098921539", "2163918411"], "title": "The Necessity of Parsing for Predicate Argument Recognition", "abstract": "Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification.", "citation_count": "14", "reference_count": "316", "date": "2002", "authors": ["Daniel Gildea", "Martha Palmer"], "related_topics": ["Semantic role labeling", "Parsing", "Predicate (grammar)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1567277581", "references": ["2798490496", "2111907245", "2075322664", "1519554123", "2038141781"], "title": "The syntactic process", "abstract": "In this book Mark Steedman argues that the surface syntax of natural languages maps spoken and written forms directly to a compositional semantic representation that includes predicate-argument structure, quantification and information structure without constructing any intervening structural representation. His purpose is to construct a principle theory of natural grammar that is directly compatible with both expalantory linguistic accounts of a number of problematic syntactic phenomena and a straightforward computational account of the way sentences are mapped onto represenations of meaning. The radical nature of Steedman's proposal stems from his claim that much of the apparent complexity of syntax, prosody and processing follows from the lexical specification of the grammar and from the involvement of a small number of universal rule-types for combining predicates and arguments. These syntactic operations are related to the combinators of Combinatory Logic, engendering a much freer definition of derivational constituency than is traditionally assumed. This property allows Combinatory Categorical Grammar to capture elegantly the structure and interpretation of coordination and intonation contour in English as well as some well-known interactions between word order, coordination and relativization across a number of other languages. It also allows more direct compatibility with incremental semantic interpretation during parsing. The book covers topics in formal linguistics, intonational phonology, computational linguistics and experimental psycholinguistics, presenting them as an integrated theory of the language faculty in a form accessible to readers from any of those fields.", "citation_count": "5", "reference_count": "2,386", "date": "2000", "authors": ["Mark Steedman"], "related_topics": ["Combinatory categorial grammar", "Relational grammar", "Syntax", "Categorial grammar", "Parsing", "Grammar", "Natural language", "Computational linguistics", "Linguistics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2126851059", "references": ["1632114991", "2144578941", "2116786260", "2138043057", "204260652", "2151170651", "2145310422", "2092654472", "1535015163", "2158847908"], "title": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling", "abstract": "In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.", "citation_count": "37", "reference_count": "955", "date": "2005", "authors": ["Xavier Carreras", "Llu'is M`arquez"], "related_topics": ["Semantic computing", "Semantic role labeling", "Task (project management)", "PropBank", "Natural language processing", "Information retrieval", "Computer science", "Artificial intelligence", "Evaluation methods"]}
{"id": "2039217078", "references": ["2151170651", "2000196122", "2166957049", "2088622183", "2600702321", "2158847908", "2128870637", "2075123415"], "title": "English Verb Classes and Alternations: A Preliminary Investigation", "abstract": "In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.", "citation_count": "0", "reference_count": "3,708", "date": "1993", "authors": ["Beth Levin"], "related_topics": ["Modal verb", "English verbs", "Verb", "Reflexive verb", "Verb phrase ellipsis", "Causative alternation", "VerbNet", "Syntax", "Linguistics", "Computer science"]}
{"id": "2115792525", "references": ["2154890447", "3088277579"], "title": "The Berkeley FrameNet Project", "abstract": "FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.", "citation_count": "2", "reference_count": "3,730", "date": "1998", "authors": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe"], "related_topics": ["FrameNet", "Semantic role labeling", "PropBank", "Lexicon", "VerbNet", "Phrase", "Frame semantics", "Noun", "Syntax", "Natural language processing", "Linguistics", "Computer science", "Artificial intelligence", "Lexicography"]}
{"id": "1480376833", "references": ["3102641634", "2076063813", "2884561390", "1641498739", "2516809705", "2132914434", "2115709314"], "title": "The Elements of Statistical Learning", "abstract": "", "citation_count": "0", "reference_count": "15,839", "date": "2001", "authors": ["Trevor Hastie", "Robert Tibshirani", "Jerome H. Friedman"], "related_topics": ["Algorithmic learning theory", "Semi-supervised learning", "Ensemble learning", "Active learning (machine learning)", "Unsupervised learning", "Instance-based learning", "Learning classifier system", "Competitive learning", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2154455818", "references": ["1578099820", "2165874743", "2001141328", "2139823104", "1966949944", "2053186076", "2159737176", "2148603752", "2122837498", "1708874574"], "title": "Learning with Local and Global Consistency", "abstract": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.", "citation_count": "19", "reference_count": "4,414", "date": "2003", "authors": ["Dengyong Zhou", "Olivier Bousquet", "Thomas N. Lal", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Machine learning", "Pattern recognition", "Function (mathematics)", "Structure (category theory)", "SIMPLE algorithm", "Mathematics", "Artificial intelligence", "General problem", "Global consistency", "Manifold ranking"]}
{"id": "2010353172", "references": ["2802739963"], "title": "Weak Convergence and Empirical Processes", "abstract": "", "citation_count": "1", "reference_count": "6,681", "date": "1996", "authors": ["Thomas Mikosch", "Aad W. Van der Vaart", "Jon A. Wellner"], "related_topics": ["Weak convergence", "Mathematics", "Applied mathematics", "Bernstein\u2013von Mises theorem", "Additive hazards"]}
{"id": "2097089247", "references": ["2114535528", "2117853077", "1550206324", "2138745909", "2099111195", "2140785063", "2149684865", "2049633694", "2048679005", "2435251607"], "title": "Text Classification from Labeled and Unlabeled Documents using EM", "abstract": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.\r\n\r\nWe introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.", "citation_count": "47", "reference_count": "3,934", "date": "2000", "authors": ["Kamal Nigam", "Andrew Kachites McCallum", "Sebastian Thrun", "Tom Mitchell"], "related_topics": ["Semi-supervised learning", "Co-training", "Naive Bayes classifier", "Unsupervised learning", "Classifier (linguistics)", "Bayesian inference", "Expectation\u2013maximization algorithm", "Pattern recognition", "Machine learning", "Class (biology)", "Computer science", "Artificial intelligence"]}
{"id": "2101210369", "references": ["2129139611", "2156202195", "2049633694", "2040004971", "2099247782", "2102381086", "1977182536", "2428981601", "1971220772", "1554031433"], "title": "UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS", "abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.", "citation_count": "23", "reference_count": "2,982", "date": "1995", "authors": ["David Yarowsky"], "related_topics": ["SemEval", "Unsupervised learning", "Bootstrapping (linguistics)", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence", "Word-sense disambiguation", "Word-sense induction"]}
{"id": "2150203234", "references": ["2138745909", "12836875", "2156909104", "2151170651", "1520377376", "2154626406", "2145310422", "2166776180", "2115792525", "2885050925"], "title": "Semantic role parsing: adding semantic structure to unstructured text", "abstract": "There is an ever-growing need to add structure in the form of semantic markup to the huge amounts of unstructured text data now available. We present the technique of shallow semantic parsing, the process of assigning a simple WHO did WHAT to WHOM, etc., structure to sentences in text, as a useful tool in achieving this goal. We formulate the semantic parsing problem as a classification problem using support vector machines. Using a hand-labeled training set and a set of features drawn from earlier work together with some feature enhancements, we demonstrate a system that performs better than all other published results on shallow semantic parsing.", "citation_count": "17", "reference_count": "120", "date": "2003", "authors": ["Sameer Pradhan", "K. Hacioglu", "W. Ward", "J.H. Martin", "D. Jurafsky"], "related_topics": ["Semantic computing", "S-attributed grammar", "Bottom-up parsing", "Parsing", "Semantic HTML", "Rule-based machine translation", "Feature (linguistics)", "Set (abstract data type)", "Natural language processing", "Computational linguistics", "Computer science", "Text mining", "Artificial intelligence"]}
{"id": "1520377376", "references": ["1991133427", "2033209333", "1971763646", "2117400858", "2125838338", "1718065290", "2080856991", "2099247782", "1568095077", "1568620938"], "title": "An Algorithm that Learns What\u2018s in a Name", "abstract": "In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder\u2018s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.", "citation_count": "14", "reference_count": "1,107", "date": "1999", "authors": ["Daniel M. Bikel", "Richard Schwartz", "Ralph M. Weischedel"], "related_topics": ["Hidden Markov model", "Class (computer programming)", "Machine learning", "Task (project management)", "Computer science", "Algorithm", "Artificial intelligence", "Controlled experiment", "Speech input", "Training set"]}
{"id": "2145310422", "references": ["2124634352", "2151170651", "2117400858", "2154626406", "1986543644", "2085606725", "1667964228", "1971563386", "2039217078", "2115792525"], "title": "Using Predicate-Argument Structures for Information Extraction", "abstract": "In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.", "citation_count": "13", "reference_count": "514", "date": "2003", "authors": ["Mihai Surdeanu", "Sanda Harabagiu", "John Williams", "Paul Aarseth"], "related_topics": ["Predicate (grammar)", "Decision tree learning", "Information extraction", "PropBank", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2040909025", "references": ["2156909104", "2119821739", "1574862351", "2149684865", "2153104898", "1490175418"], "title": "Use of support vector learning for chunk identification", "abstract": "In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling.", "citation_count": "6", "reference_count": "419", "date": "2000", "authors": ["Taku Kudoh", "Yuji Matsumoto"], "related_topics": ["Support vector machine", "Margin (machine learning)", "Identification (information)", "Machine learning", "Task (project management)", "Pattern recognition", "Generalization", "Computer science", "Large numbers", "Artificial intelligence"]}
{"id": "2166776180", "references": ["2127314673", "2123489126", "2163953154", "2109779030", "1997841190", "2102997946", "2123084125", "2102381086", "1940278502", "2016001305"], "title": "Automatic Retrieval and Clustering of Similar Words", "abstract": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.", "citation_count": "18", "reference_count": "2,471", "date": "1998", "authors": ["Dekang Lin"], "related_topics": ["Thesaurus (information retrieval)", "WordNet", "Similarity measure", "Bootstrapping (linguistics)", "Natural language", "Semantics", "Cluster analysis", "Natural language processing", "Parsing", "Information retrieval", "Computer science", "Artificial intelligence"]}
{"id": "1988995507", "references": ["2112076978", "1676820704", "2119821739", "2117400858", "2149684865", "1975846642", "2153104898", "2148603752", "2101276256", "2107008379"], "title": "Chunking with support vector machines", "abstract": "We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches.", "citation_count": "24", "reference_count": "729", "date": "2001", "authors": ["Taku Kudo", "Yuji Matsumoto"], "related_topics": ["Relevance vector machine", "Support vector machine", "Kernel (linear algebra)", "Curse of dimensionality", "Chunking (division)", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2093647425", "references": ["1632114991", "1859173823", "2334801970", "2167434254", "2102924265", "2087165009", "2796493717", "1483126227", "2021758792"], "title": "The Penn Treebank: annotating predicate argument structure", "abstract": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.", "citation_count": "11", "reference_count": "1,048", "date": "1994", "authors": ["Mitchell Marcus", "Grace Kim", "Mary Ann Marcinkiewicz", "Robert MacIntyre", "Ann Bies", "Mark Ferguson", "Karen Katz", "Britta Schasberger"], "related_topics": ["Treebank", "Semantic role labeling", "Predicate (grammar)", "Annotation", "Tag system", "Natural language processing", "Computer science", "Artificial intelligence", "Syntactic annotation"]}
{"id": "2099111195", "references": ["2032372805", "2151795416", "2106248279", "2790166049", "2148963518", "2132103241", "2165232124", "2071707134"], "title": "Elements of information theory", "abstract": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.", "citation_count": "0", "reference_count": "62,337", "date": "1991", "authors": ["Thomas M. Cover", "Joy A. Thomas"], "related_topics": ["Shannon's source coding theorem", "Entropy rate", "Joint entropy", "Differential entropy", "Maximum entropy probability distribution", "Entropy power inequality", "Entropy (information theory)", "R\u00e9nyi entropy", "Applied mathematics", "Engineering"]}
{"id": "2123084125", "references": ["2034274945", "2052690453", "1981724541", "2111108424", "1593045043", "1528321674", "2099247782", "1496719572"], "title": "NOUN CLASSIFICATION FROM PREDICATE-ARGUMENT STRUCTURES", "abstract": "A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.", "citation_count": "8", "reference_count": "796", "date": "1990", "authors": ["Donald Hindle"], "related_topics": ["Object (grammar)", "Predicate (grammar)", "Noun", "Subject (grammar)", "Argument (linguistics)", "Verb", "Text corpus", "Automatic indexing", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2059800182", "references": ["2996160789", "2168938909", "2082092506", "1990438144", "2099247782", "2134237567", "2159782014"], "title": "A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams", "abstract": "Abstract   In principle,  n -gram probabilities can be estimated from a large sample of text by counting the number of occurrences of each  n -gram of interest and dividing by the size of the training sample. This method, which is known as maximum likelihood estimator (MLE), is very simple. However, it is unsuitable because  n -grams which do not occur in the training sample are assigned zero probability. This is qualitatively wrong for use as a prior model, because it would never allow the  n -gram, while clearly some of the unseen  n -grams will occur in other texts. For non-zero frequencies, the MLE is quantitatively wrong. Moreover, at all frequencies, the MLE does not separate bigrams with the same frequency.  We study two alternative methods. The first method is an enhanced version of the method due to Good and Turing (I. J. Good [1953].  Biometrika ,  40 , 237\u2013264). Under the modest assumption that the distribution of each bigram is binomial, Good provided a theoretical result that increases estimation accuracy. The second method is an enhanced version of the deleted estimation method (F. Jelinek &amp; R. Mercer [1985].  IBM Technical Disclosure Bulletin ,  28 , 2591\u20132594). It assumes even less, merely that the training and test corpora are generated by the same process.  We emphasize three points about these methods. First, by using a second predictor of the probability in addition to the observed frequency, it is possible to estimate different probabilities for bigrams with the same frequency. We refer to this use of a second predictor as \u201cenhancement.\u201d With enhancement, we find 1200 significantly different probabilities (with a range of five orders of magnitude) for the group of bigrams not observed in the training text; the MLE method would not be able to distinguish any one of these bigrams from any other. The probabilities found by the enhanced methods agree quite closely in qualitative comparisons with the standard calculated from the test corpus.  Second, the enhanced Good-Turing method provides accurate predictions of the variances of the standard probabilities estimated from the test corpus. Third, we introduce a refined testing method that enables us to measure the prediction errors directly and accurately and thus to study small differences between methods. We find that while the errors of both methods are small due to the large amount of data that we use, the enhanced Good-Turing method is three to four times as efficient in its use of data as the enhanced deleted estimate method. Good-Turing method is preferable to the enhanced deleted estimate method. Both methods are much better than MLE.", "citation_count": "7", "reference_count": "356", "date": "1991", "authors": ["Kenneth W. Church", "William A. Gale"], "related_topics": ["Bigram", "Range (statistics)", "Measure (mathematics)", "Sample (material)", "Orders of magnitude (bit rate)", "Statistics", "Algorithm", "Gram", "Zero (linguistics)", "Distribution (mathematics)", "Mathematics"]}
{"id": "2099247782", "references": ["2124479173", "2155818555", "1571096757", "1526508180", "2134237567", "2055438451"], "title": "A stochastic parts program and noun phrase parser for unrestricted text", "abstract": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct. &gt;", "citation_count": "6", "reference_count": "3,376", "date": "1989", "authors": ["K.W. Church"], "related_topics": ["Speech synthesis", "Part of speech", "Noun phrase", "Sentence", "Parsing", "Natural language", "Word (computer architecture)", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "1982944197", "references": ["2439178139", "2153198088", "2077302143", "2047706513", "1995875735", "1541301615", "1526927911", "1978470410", "1504046386", "2131986285"], "title": "Stochastic lexicalized tree-adjoining grammars", "abstract": "The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars.", "citation_count": "16", "reference_count": "202", "date": "1992", "authors": ["Yves Schabes"], "related_topics": ["Stochastic context-free grammar", "Tree-adjoining grammar", "L-attributed grammar", "Stochastic grammar", "Hidden Markov model", "Rule-based machine translation", "Data-oriented parsing", "Tree (data structure)", "Theoretical computer science", "Sentence", "Natural language processing", "Computer science", "Grammar", "Artificial intelligence"]}
{"id": "2016001305", "references": ["2137638032", "2034274945", "1715319291", "2123084125", "2441154163", "2134237567"], "title": "Contextual word similarity and estimation from sparse data", "abstract": "Abstract   In recent years there is much interest in word co-occurrence relations, such as n-grams, verb\u2013object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co-occurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models. A background survey is included, covering issues of lexical co-occurrence, data sparseness and smoothing, word similarity and clustering, and mutual information.", "citation_count": "6", "reference_count": "412", "date": "1995", "authors": ["Ido Dagan", "Shaul Marcus", "Shaul Markovitch"], "related_topics": ["Mutual information", "Word (computer architecture)", "Smoothing", "Similarity (network science)", "Cluster analysis", "Metric (mathematics)", "Context (language use)", "Natural language processing", "Pattern recognition", "Sparse matrix", "Computer science", "Artificial intelligence"]}
{"id": "2025887562", "references": ["2127314673", "2964184826", "2116801843", "2141012957", "2161877964", "2077990749", "2166322089", "2087684630", "2022686119"], "title": "Statistical mechanics and phase transitions in clustering.", "abstract": "A new approach to clustering based on statistical physics is presented. The problem is formulated as fuzzy clustering and the association probability distribution is obtained by maximizing the entropy at a given average variance. The corresponding Lagrange multiplier is related to the ``temperature'' and motivates a deterministic annealing process where the free energy is minimized at each temperature. Critical temperatures are derived for phase transitions when existing clusters split. It is a hierarchical clustering estimating the most probable cluster parameters at various average variances.", "citation_count": "0", "reference_count": "626", "date": "1990", "authors": ["Kenneth Rose", "Eitan Gurewitz", "Geoffrey C. Fox"], "related_topics": ["Fuzzy clustering", "Correlation clustering", "Cluster analysis", "Hierarchical clustering", "Entropy (statistical thermodynamics)", "Probability distribution", "Statistical mechanics", "Lagrange multiplier", "Statistical physics", "Mathematics"]}
{"id": "2611071497", "references": ["2158195707", "2244729984", "2138662031", "2088386938", "2053691921", "2161439181", "1993803315"], "title": "Text Compression", "abstract": "", "citation_count": "0", "reference_count": "2,244", "date": "1990", "authors": ["Timothy C. Bell", "John G. Cleary", "Ian H. Witten"], "related_topics": ["Computer science", "Compressed pattern matching", "Speech recognition", "Text compression"]}
{"id": "2075201173", "references": ["2136542423", "2162630660", "2117278770", "1972594981", "2158195707", "2142416747", "1984052055", "1934041838", "2100506586"], "title": "On structuring probabilistic dependences in stochastic language modelling", "abstract": "Abstract   In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.", "citation_count": "0", "reference_count": "850", "date": "1994", "authors": ["Hermann Ney", "Ute Essen", "Reinhard Kneser"], "related_topics": ["Bigram", "Word (computer architecture)", "Linear interpolation", "Conditional probability", "Trigram", "Probabilistic logic", "Cluster analysis", "Optimal estimation", "Algorithm", "Computer science"]}
{"id": "2097333193", "references": ["1966812932", "2334801970", "2021021293", "2049633694", "2029825515", "2008506796", "1575431606", "1597533204"], "title": "A statistical approach to machine translation", "abstract": "In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.", "citation_count": "8", "reference_count": "2,549", "date": "1990", "authors": ["Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "related_topics": ["Example-based machine translation", "Machine translation software usability", "Rule-based machine translation", "Machine translation", "Evaluation of machine translation", "Synchronous context-free grammar", "Translation (geometry)", "Natural language processing", "Computer science", "ROUGE", "Artificial intelligence"]}
{"id": "2170120409", "references": ["1746819321", "2047205370", "2121016876", "2103546861", "2132103241", "1595159159", "2171268876", "1976969221", "2183707334", "2161406034"], "title": "Numerical recipes in C", "abstract": "Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08", "citation_count": "0", "reference_count": "16,021", "date": "1994", "authors": ["William H. Press", "Saul A. Teukolsky", "William T. Vetterling", "Brian P. Flannery"], "related_topics": ["IBM PC compatible", "Computer graphics (images)", "Computer science", "Algorithm"]}
{"id": "2166637769", "references": ["1643320849", "2077302143", "1976349544"], "title": "SWITCHBOARD: telephone speech corpus for research and development", "abstract": "SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording. &gt;", "citation_count": "3", "reference_count": "2,531", "date": "1992", "authors": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "related_topics": ["Speech corpus", "Speech processing", "VoxForge", "Speaker recognition", "Speech technology", "Transcription (software)", "Dialog act", "Vocabulary", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2134237567", "references": ["2170967986", "2168938909", "1507770639", "2082092506", "1590636096", "1597533204", "2159782014"], "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "abstract": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.", "citation_count": "7", "reference_count": "2,741", "date": "1987", "authors": ["S. Katz"], "related_topics": ["Cache language model", "Speech processing", "Language model", "Sparse matrix", "Factored language model", "Natural language", "Component (UML)", "Data-oriented parsing", "Speech recognition", "Computer science"]}
{"id": "2595741664", "references": ["2053101950", "2973707709", "2091746061", "2962782102", "2127429655", "2478503558", "1982106473", "2111305191", "2189089430", "2061068689"], "title": "IEEE International Conference on Acoustics Speech and Signal Processing", "abstract": "", "citation_count": "0", "reference_count": "2,256", "date": "2001", "authors": ["S. Chen", "A. K. Samingan", "Bernie Mulgrew", "L. Hanzo"], "related_topics": ["Signal processing", "Computer science", "Speech recognition", "Acoustics"]}
{"id": "47415966", "references": ["2100506586", "100623710", "88081813", "2142416747", "2111305191", "2156985047", "2132339004", "36903255"], "title": "Improved Clustering Techniques for Class-Based Statistical Language Modelling", "abstract": "", "citation_count": "0", "reference_count": "270", "date": "1993", "authors": ["Reinhard Kneser", "Hermann Ney"], "related_topics": ["Conceptual clustering", "Cluster analysis", "Class (computer programming)", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence", "Language modelling"]}
{"id": "1996764654", "references": ["2041565863", "2084048649", "1965080174", "2145036943", "1956559956", "2066667100", "2093377061", "1971784203", "2120636855", "2045370125"], "title": "Scatter/Gather: a cluster-based approach to browsing large document collections", "abstract": "Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval.We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm.", "citation_count": "12", "reference_count": "2,564", "date": "1992", "authors": ["Douglass R. Cutting", "David R. Karger", "Jan O. Pedersen", "John W. Tukey"], "related_topics": ["Cluster analysis", "Document clustering", "Fuzzy clustering", "Canopy clustering algorithm", "Data stream clustering", "CURE data clustering algorithm", "Conceptual clustering", "Brown clustering", "Cluster labeling", "Clustering high-dimensional data", "Consensus clustering", "Information retrieval", "Data mining", "Computer science"]}
{"id": "2016243284", "references": ["1549285799", "3765491", "2100969003", "2090861223", "1547083988", "1484181928", "1482605500", "2146871184", "2024490156", "2081323593"], "title": "The LIMSI Broadcast News transcription system", "abstract": "Abstract   This paper reports on activites at LIMSI over the last few years directed at the transcription of broadcast news data. We describe our development work in moving from laboratory read speech data to real-world or `found' speech data in preparation for the DARPA evaluations on this task from 1996 to 1999. Two main problems needed to be addressed to deal with the continuous flow of inhomogenous data. These concern the varied acoustic nature of the signal (signal quality, environmental and transmission noise, music) and different linguistic styles (prepared and spontaneous speech on a wide range of topics, spoken by a large variety of speakers).  The problem of partitioning the continuous stream of data is addressed using an iterative segmentation and clustering algorithm with Gaussian mixtures. The speech recognizer makes use of continuous density HMMs with Gaussian mixture for acoustic modeling and 4-gram statistics estimated on large text corpora. Word recognition is performed in multiple passes, where current hypotheses are used for cluster-based acoustic model adaptation prior to the next decoding pass. The overall word transcription error of the LIMSI evaluation systems were 27.1% (Nov96, partitioned test data), 18.3% (Nov97, unpartitioned data), 13.6% (Nov98, unpartitioned data) and 17.1% (Fall99, unpartitioned data with computation time under 10\u00d7 real-time).", "citation_count": "24", "reference_count": "539", "date": "2002", "authors": ["Jean-Luc Gauvain", "Lori Lamel", "Gilles Adda"], "related_topics": ["Acoustic model", "Transcription (software)", "Language model", "Cluster analysis", "Test data", "Word recognition", "Text corpus", "Speech recognition", "Transcription error", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "82490022", "references": ["11920722", "3765491", "1977434607", "2042759801", "1482958488", "2146871184", "2098162425", "2081323593", "1579752022", "2917519822"], "title": "Fast decoding for indexation of broadcast data.", "abstract": "", "citation_count": "10", "reference_count": "22", "date": "2000", "authors": ["Jean-Luc Gauvain", "Lori Lamel"], "related_topics": ["Decoding methods", "Indexation", "Speech recognition", "Information retrieval", "Computer science", "Broadcast data"]}
{"id": "17500809", "references": ["2099111195", "2108321481", "2125838338", "2160842254", "1996903695", "2049633694", "2441154163", "3017143921", "2134237567", "2116780029"], "title": "Language Model Adaptation", "abstract": "This paper reviews methods for language model adaptation. Paradigms and basic methods are first introduced. Basic theory is presented for maximum a-posteriori estimation, mixture based adaptation, and minimum discrimination information. Models to cope with long distance dependencies are also introduced. Applications and results from the recent literature are finally surveyed.", "citation_count": "43", "reference_count": "53", "date": "1999", "authors": ["Renato DeMori", "Marcello Federico"], "related_topics": ["Language model", "Adaptation (computer science)", "Natural language", "Artificial intelligence", "Word error rate", "Genetic algorithm", "Systems architecture", "Speech processing", "Document processing", "Art"]}
{"id": "145476170", "references": ["1539686131", "2013239224", "1652505363", "2073257493"], "title": "Learning distributed representations of concepts.", "abstract": "", "citation_count": "4", "reference_count": "1,274", "date": "1989", "authors": ["Geoffrey E. Hinton"], "related_topics": ["Concept learning", "Cognition", "Cognitive science", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2147010501", "references": ["1813659000", "2136922672", "1575388622", "1528056001", "116210019", "1516111018", "2100495367", "2116064496", "66838807", "1481420047"], "title": "Learning Multilevel Distributed Representations for High-Dimensional Sequences", "abstract": "We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box.", "citation_count": "14", "reference_count": "271", "date": "2007", "authors": ["Ilya Sutskever", "Geoffrey E. Hinton"], "related_topics": ["Approximate inference", "Hidden Markov model", "Markov model", "Linear dynamical system", "Sequence", "Pattern recognition", "Pixel", "Simple (abstract algebra)", "Generative grammar", "Mathematics", "Artificial intelligence"]}
{"id": "2437096199", "references": ["2156909104", "2158195707", "2148603752", "2070534370", "2056590938", "2121227244", "2134237567", "2132339004", "1631260214"], "title": "Training Neural Network Language Models on Very Large Corpora", "abstract": "During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.", "citation_count": "20", "reference_count": "162", "date": "2005", "authors": ["Holger Schwenk", "Jean-Luc Gauvain"], "related_topics": ["Time delay neural network", "Cache language model", "Language model", "Text corpus", "Artificial neural network", "Word (computer architecture)", "Natural language processing", "Machine learning", "Computer science", "Contrast (statistics)", "Space (commercial competition)", "Artificial intelligence", "Training set"]}
{"id": "2751862591", "references": ["2136913207", "1506281249", "1660562555", "2099040451", "2149959815", "2106864314", "2117756735", "2180060173"], "title": "An introduction to probability theory and its applications", "abstract": "", "citation_count": "0", "reference_count": "56,325", "date": "1950", "authors": ["William Feller"], "related_topics": ["Probability and statistics", "Imprecise probability", "Tree diagram", "Inverse probability", "Law of total probability", "Empirical probability", "Conditional mutual information", "Frequentist probability", "Mathematics", "Statistics"]}
{"id": "2016871293", "references": ["2109469864", "2066792529", "1966812932", "2002938383", "2065533756"], "title": "Context based spelling correction", "abstract": "Abstract   Some mistakes in spelling and typing produce correct words, such as typing \u201cfig\u201d when \u201cfog\u201d was intended. These errors are undetectable by traditional spelling correction techniques. In this paper we present a statistical technique capable of detecting and correcting some of these errors when they occur in sentences. Experimental results show that this technique is capable of detecting 76% of simple spelling errors and correcting 73%.", "citation_count": "5", "reference_count": "401", "date": "1991", "authors": ["Eric Mays", "Fred J. Damerau", "Robert L. Mercer"], "related_topics": ["Spelling", "Context (language use)", "Orthography", "Error detection and correction", "Sentence", "Phrase", "Speech recognition", "Natural language processing", "Syntax", "Computational linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2142901448", "references": ["2151795416", "2129766733", "2150498905", "2098567664", "2159080219", "2106864314", "2127490352", "1667950888", "2133401839"], "title": "Information Theory and Reliable Communication", "abstract": "Communication Systems and Information Theory. A Measure of Information. Coding for Discrete Sources. Discrete Memoryless Channels and Capacity. The Noisy-Channel Coding Theorem. Techniques for Coding and Decoding. Memoryless Channels with Discrete Time. Waveform Channels. Source Coding with a Fidelity Criterion. Index.", "citation_count": "0", "reference_count": "9,038", "date": "1968", "authors": ["Robert G. Gallager"], "related_topics": ["Variable-length code", "Shannon\u2013Fano coding", "Information theory", "Decoding methods", "Binary symmetric channel", "Discrete time and continuous time", "Coding (social sciences)", "Communications system", "Theoretical computer science", "Mathematics"]}
{"id": "1628850721", "references": ["1901281023", "2056809970", "1521239006", "2170967986", "1966812932", "1507770639", "2134587001", "2089661020", "2134237567", "1990005915"], "title": "Experiments with the Tangora 20,000 word speech recognizer", "abstract": "The Speech Recognition Group at IBM Research in Yorktown Heights has developed a real-time, isolated-utterance speech recognizer for natural language based on the IBM Personal Computer AT and IBM Signal Processors. The system has recently been enhanced by expanding the vocabulary from 5,000 words to 20,000 words and by the addition of a speech workstation to support usability studies on document creation by voice. The system supports spelling and interactive personalization to augment the vocabularies. This paper describes the implementation, user interface, and comparative performance of the recognizer.", "citation_count": "10", "reference_count": "148", "date": "1987", "authors": ["A. Averbuch", "L. Bahl", "R. Bakis", "P. Brown", "G. Daggett", "S. Das", "K. Davies", "S. De Gennaro", "P. de Souza", "E. Epstein", "D. Fraleigh", "F. Jelinek", "B. Lewis", "R. Mercer", "J. Moorhead", "A. Nadas", "D. Nahamoo", "M. Picheny", "G. Shichman", "P. Spinelli", "D. Van Compernolle", "H. Wilkens"], "related_topics": ["Speech synthesis", "Speech processing", "Speech corpus", "Speech technology", "Speech analytics", "Speech enhancement", "Word (computer architecture)", "Audio mining", "Personal computer", "Vocabulary", "Natural language", "User interface", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2007780422", "references": ["2168979204", "2436001372", "1984251878", "2158195707", "2103018059", "569787612", "1997161938", "2121227244", "1983578042"], "title": "Computational analysis of present-day American English", "abstract": "", "citation_count": "0", "reference_count": "11,207", "date": "1967", "authors": ["Henry Ku\u010dera", "W. Nelson Francis", "W. F. Twaddell", "Mary Lois Marckworth", "Laura M. Bell", "John Bissell Carroll"], "related_topics": ["American English", "Psychology", "Brown Corpus", "Present day", "Linguistics", "Library science", "Computational analysis", "Visual word recognition"]}
{"id": "1575431606", "references": ["1916559533", "2125838338", "2166501262", "2158195707", "2006969979", "2049633694", "2146871184", "2121227244", "2156985047"], "title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process", "abstract": "", "citation_count": "0", "reference_count": "2,336", "date": "1972", "authors": ["L. Baum"], "related_topics": ["Markov process", "Markov chain", "Markov renewal process", "Markov model", "Variable-order Markov model", "Markov property", "Markov kernel", "Partially observable Markov decision process", "Mathematical optimization", "Computer science", "Econometrics"]}
{"id": "1989705153", "references": ["1632114991", "1508165687", "2108321481", "1567570606", "2049633694", "2096175520", "2949237929", "2166637769", "2121227244", "2110882317"], "title": "Structured language modeling", "abstract": "This paper presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood re-estimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal and Switchboard corpora show improvement in both perplexity and word error rate?word lattice rescoring?over the standard 3-gram language model.", "citation_count": "32", "reference_count": "397", "date": "2000", "authors": ["Ciprian Chelba", "Frederick Jelinek"], "related_topics": ["Language model", "Language identification", "Cache language model", "Modeling language", "Perplexity", "Natural language", "Parsing", "Word error rate", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2056029990", "references": ["2025172185", "2147152072", "2058616517", "2106365165", "2078875869", "1956559956", "2032840958", "2083605078", "2000672666", "2165978089"], "title": "Personalized information delivery: an analysis of information filtering methods", "abstract": "", "citation_count": "19", "reference_count": "956", "date": "1992", "authors": ["Peter W. Foltz", "Susan T. Dumais"], "related_topics": ["Information filtering system", "Information system", "Information source", "Information retrieval", "Data mining", "Computer science", "Information delivery", "Latent semantic indexing"]}
{"id": "2072773380", "references": ["2074965064", "2093432797", "2147152072", "1990215053", "2106365165", "30999966", "2099708568", "2032840958", "2000672666", "2149671658"], "title": "Using linear algebra for intelligent information retrieval", "abstract": "Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users\u2019 requests and those in or assigned to documents in a database. ...", "citation_count": "12", "reference_count": "2,407", "date": "1995", "authors": ["Michael W. Berry", "Susan T. Dumais", "Gavin W. O'Brien"], "related_topics": ["Document retrieval", "Latent semantic analysis", "Automatic indexing", "Search engine indexing", "Semantic analysis (machine learning)", "Information retrieval", "Computer science", "Linear algebra", "Systems design", "Latent semantic indexing"]}
{"id": "2928502135", "references": ["2042276900", "1990948551", "2040187703", "1982914430", "2153076044", "2092771413", "2090639711", "2002654918", "2409145188", "2039107287"], "title": "The role of knowledge in discourse comprehension : a construction-integration model", "abstract": "Publisher Summary   This chapter discusses data concerning the time course of word identification in a discourse context. A simulation of arithmetic word-problem understanding provides a plausible account for some well-known phenomena. The current theories use representations with several mutually constraining layers. There is typically a linguistic level of representation, conceptual levels to represent both the local and global meaning and structure of a text, and a level at which the text itself has lost its individuality and its information content. Knowledge provides part of the context within which a discourse interpreted. The integration phase is the price the model pays for the necessary flexibility in the construction process.", "citation_count": "26", "reference_count": "5,530", "date": "1991", "authors": ["Walter Kintsch"], "related_topics": ["Context (language use)", "Meaning (linguistics)", "Process (engineering)", "Representation (arts)", "Flexibility (engineering)", "Structure (mathematical logic)", "Comprehension", "Cognitive science", "Psychology", "Content (Freudian dream analysis)"]}
{"id": "1674947250", "references": ["1125023678", "2099907817", "2158997610", "1536274117", "2137299080", "2127076710", "2136518234", "2807813414", "2106779500", "2106695994"], "title": "Comprehension: A Paradigm for Cognition", "abstract": "Preface Acknowledgements 1. Introduction Part I. The Theory: 2. Cognition and representation 3. Propositional representations 4. Modeling comprehension processes: the construction-integration model Part II. Models of Comprehension: 5. Word identification in discourse 6. Textbases and situation models 7. The role of working memory in comprehension 8. Memory for text 9. Learning from text 10. Word problems 11. Beyond text References name index Subject index.", "citation_count": "0", "reference_count": "4,937", "date": "1998", "authors": ["Walter Kintsch"], "related_topics": ["Comprehension", "Cognition", "Coh-Metrix", "Working memory", "Representation (arts)", "Cognitive psychology", "Index (publishing)", "Metacomprehension", "Subject (grammar)", "Computer science"]}
{"id": "2059086756", "references": ["2091785129", "2121773050", "2073257493", "1581218697", "2153076044", "2134199742", "1582594879", "2264742718", "2021625970", "2157368609"], "title": "Strategies of discourse comprehension", "abstract": "rhetorical schemata to be discussed in what follows. Finally, schemata are descriptions, not definitions. The \u2018bus\u2019 schema contains information that is nor-", "citation_count": "110", "reference_count": "11,710", "date": "1983", "authors": ["Teun Adrianus van Dijk", "Walter Kintsch"], "related_topics": ["Schema (psychology)", "Rhetorical question", "Comprehension", "Coh-Metrix", "Linguistics", "Psychology", "Discourse processing", "Narrative comprehension", "Situation model", "Text comprehension"]}
{"id": "2092919341", "references": ["2158997610", "2962785568", "2080972498", "2115095583", "2136518234", "2115281393", "1928882148", "1983578042", "2084335597"], "title": "The Adaptive Character of Thought", "abstract": "Contents: Part I:Introduction. Preliminaries. Levels of a Cognitive Theory. Current Formulation of the Levels Issues. The New Theoretical Framework. Is Human Cognition Rational? The Rest of This Book. Appendix: Non-Identifiability and Response Time. Part II:Memory. Preliminaries. A Rational Analysis of Human Memory. The History Factor. The Contextual Factor. Relationship of Need and Probability to Probability and Latency of Recall. Combining Information From Cues. Implementation in the ACT Framework. Effects of Subject Strategy. Conclusions. Part III:Categorization. Preliminaries. The Goal of Categorization. The Structure of the Environment. Recapitulation of Goals and Environment. The Optimal Solution. An Iterative Algorithm for Categorization. Application of the Algorithm. Survey of the Experimental Literature. Conclusion. Appendix: The Ideal Algorithm. Part IV:Causal Inference. Preliminaries. Basic Formulation of the Causal Inference Problem. Causal Estimation. Cues for Causal Inference. Integration of Statistical and Temporal Cues. Discrimination. Abstraction of Causal Laws. Implementation in a Production System. Conclusion. Appendix. Part V:Problem Solving. Preliminaries. Making a Choice Among Simple Actions. Combining Steps. Studies of Hill Climbing. Means-Ends Analysis. Instantiation of Indefinite Objects. Conclusions on Rational Analysis of Problem Solving. Implementation in ACT. Appendix: Problem Solving and Clotheslines. Part VI:Retrospective. Preliminaries. Twelve Questions About Rational Analysis.", "citation_count": "0", "reference_count": "3,585", "date": "1990", "authors": ["John R. Anderson"], "related_topics": ["Causal inference", "Rational analysis", "Inference", "Categorization", "Structure (mathematical logic)", "Hill climbing", "Cognition", "Artificial intelligence", "Abstraction (linguistics)", "Psychology"]}
{"id": "1981617416", "references": ["1550028225", "2074657344", "1986010321", "3149550307", "1974351977", "2116411927", "1568757443", "2074439471", "2086220442", "2069736034"], "title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "abstract": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).", "citation_count": "13", "reference_count": "2,076", "date": "1996", "authors": ["Kevin Lund", "Curt Burgess"], "related_topics": ["Explicit semantic analysis", "Semantic similarity", "SemEval", "Latent semantic analysis", "Semantic computing", "Semantic compression", "Random indexing", "Basis (linear algebra)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1983578042", "references": ["2059975159", "2163953154", "2147152072", "1898014694", "1593045043", "2001467963", "1540136915", "2152632951", "1594369375", "2293063825"], "title": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.", "abstract": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched.", "citation_count": "106", "reference_count": "7,840", "date": "1997", "authors": ["Thomas K. Landauer", "Susan T. Dumais"], "related_topics": ["Probabilistic latent semantic analysis", "Similarity (psychology)", "Latent semantic analysis", "Knowledge representation and reasoning", "Vocabulary", "Verbal learning", "Distributional semantics", "Statistical semantics", "Natural language processing", "Artificial intelligence", "Psychology"]}
{"id": "2063392856", "references": ["2147152072", "2058616517", "2013737143", "2072773380", "2138621811", "1956559956", "2106285343", "1979750072", "2983896310", "2798909945"], "title": "Latent semantic indexing: a probabilistic analysis", "abstract": "", "citation_count": "20", "reference_count": "1,583", "date": "1998", "authors": ["Christos H. Papadimitriou", "Hisao Tamaki", "Prabhakar Raghavan", "Santosh Vempala"], "related_topics": ["Probabilistic latent semantic analysis", "Latent Dirichlet allocation", "Document-term matrix", "Probabilistic analysis of algorithms", "Computer science", "Information retrieval", "Latent semantic indexing"]}
{"id": "2334889010", "references": ["2158997610", "1612003148", "2130416410", "3143596294", "2134731454", "1880262756", "1983578042", "2001082470", "2158266063", "2098126593"], "title": "Probabilistic Topic Models", "abstract": "", "citation_count": "23", "reference_count": "2,963", "date": "2007", "authors": ["Mark Steyvers", "Tom Griffiths"], "related_topics": ["Probabilistic logic", "Topic model", "Semantics (computer science)", "Frequentist probability", "Natural language processing", "Computer science", "Cognition", "Artificial intelligence"]}
{"id": "2143017621", "references": ["1549285799", "118140219", "1625780798", "2136248378", "2042978285", "2045784978", "2161160885", "1548991402", "2038248725", "2143017621"], "title": "NLTK: The Natural Language Toolkit", "abstract": "The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simplified toolkit and explains how it is used in teaching NLP.", "citation_count": "15", "reference_count": "2,495", "date": "2006", "authors": ["Steven Bird"], "related_topics": ["Natural language programming", "Language technology", "Natural language user interface", "Natural language", "Python (programming language)", "Computational linguistics", "Temporal annotation", "Question answering", "Programming language", "Computer science"]}
{"id": "2159426623", "references": ["2147152072", "2108598243", "2100935296", "2334889010", "2049633694", "1970381522", "1612003148", "1880262756", "1983578042", "2143017621"], "title": "Reading Tea Leaves: How Humans Interpret Topic Models", "abstract": "Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.", "citation_count": "23", "reference_count": "2,170", "date": "2009", "authors": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-graber", "David M. Blei"], "related_topics": ["Probabilistic latent semantic analysis", "Topic model", "Probabilistic logic", "Natural language processing", "Reading (process)", "Space (commercial competition)", "Meaning (linguistics)", "Representation (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "2001082470", "references": ["1997063559", "2104924585", "2165554837", "1574901103", "2130416410", "2069739265", "2134731454", "578760377", "2753533763", "1666636243"], "title": "Finding scientific topics", "abstract": "A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. &amp; Jordan, M. I. (2003)  J. Machine Learn. Res.  3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying \u201chot topics\u201d by examining temporal dynamics and tagging abstracts to illustrate semantic content.", "citation_count": "10", "reference_count": "6,872", "date": "2004", "authors": ["Thomas L. Griffiths", "Mark Steyvers"], "related_topics": ["Topic model", "Latent Dirichlet allocation", "Dynamic topic model", "Generative model", "Pachinko allocation", "Bayesian inference", "Inference", "Information retrieval", "Structure (mathematical logic)", "Computer science"]}
{"id": "2158266063", "references": ["2115979064", "2125838338", "2152664025", "1956559956", "2890040444", "1880262756", "2009570821", "2011832962", "2110755408", "2098126593"], "title": "Hierarchical Dirichlet Processes", "abstract": "We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes ...", "citation_count": "50", "reference_count": "4,521", "date": "2006", "authors": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "related_topics": ["Hierarchical Dirichlet process", "Dirichlet process", "Latent Dirichlet allocation", "Dirichlet distribution", "Mixture model", "Pitman\u2013Yor process", "Chinese restaurant process", "Dynamic topic model", "Pure mathematics", "Mathematics", "Econometrics"]}
{"id": "2047804403", "references": ["2156838815", "2136922672", "2102724320", "1511812886", "2167217202", "2148856562", "1554663460", "2138754805", "2156141384", "1663973292"], "title": "Modular toolkit for Data Processing (MDP): a Python data processing framework", "abstract": "Modular toolkit for Data Processing (MDP) is a data processing framework written in Python. From the user's perspective, MDP is a collection of supervised and unsupervised learning algorithms and other data processing units that can be combined into data processing sequences and more complex feed-forward network architectures. Computations are performed efficiently in terms of speed and memory requirements. From the scientific developer's perspective, MDP is a modular framework, which can easily be expanded. The implementation of new algorithms is easy and intuitive. The new implemented units are then automatically integrated with the rest of the library. MDP has been written in the context of theoretical research in neuroscience, but it has been designed to be helpful in any context where trainable data processing algorithms are used. Its simplicity on the user's side, the variety of readily available algorithms, and the reusability of the implemented units make it also a useful educational tool.", "citation_count": "11", "reference_count": "145", "date": "2008", "authors": ["Tiziano Zito", "Niko Wilbert", "Laurenz Wiskott", "Pietro Berkes"], "related_topics": ["Python (programming language)", "Modular design", "Data processing", "Unsupervised learning", "Network architecture", "Programming language", "Computer science", "Reusability", "Computational neuroscience", "Computation"]}
{"id": "2962735828", "references": ["2147880316", "2156909104", "2160842254", "2167044614", "2138621811", "2126185296", "2159080219", "2098678088", "2076008912", "2107008379"], "title": "Discriminative probabilistic models for relational data", "abstract": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.", "citation_count": "19", "reference_count": "828", "date": "2002", "authors": ["Ben Taskar", "Pieter Abbeel", "Daphne Koller"], "related_topics": ["Bayesian network", "Discriminative model", "Relational database", "Probabilistic logic", "Supervised learning", "Statistical model", "Markov chain", "Representation (mathematics)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1995945562", "references": ["1570448133", "2155965977", "2140190241", "2038702827", "1995945562", "2000042664", "2135046866", "1964357740", "3102641634"], "title": "An introduction to the bootstrap", "abstract": "This article presents bootstrap methods for estimation, using simple arguments. Minitab macros for implementing these methods are given.", "citation_count": "0", "reference_count": "48,411", "date": "1993", "authors": ["Bradley Efron", "Robert J Tibshirani"], "related_topics": ["Bootstrap aggregating", "Macro", "Computer science", "Data mining", "Inference", "Simple (abstract algebra)", "Simar", "Bootstrap analysis", "Bootstrap confidence interval", "Sampling theory"]}
{"id": "2020842694", "references": ["2093390569", "2172085063", "2108346334", "1516111018", "2137471889", "3016843226", "2137918516", "2121947440", "1880262756", "2109868644"], "title": "Modeling annotated data", "abstract": "We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval.", "citation_count": "17", "reference_count": "1,466", "date": "2003", "authors": ["David M. Blei", "Michael I. Jordan"], "related_topics": ["Latent Dirichlet allocation", "Image retrieval", "Automatic image annotation", "Probabilistic latent semantic analysis", "Latent variable model", "Mixture model", "Graphical model", "Conditional probability distribution", "Data mining", "Computer science"]}
{"id": "2107743791", "references": ["2127314673", "2063089147", "2567948266", "2147152072", "2140842551", "1956559956", "2049633694", "1612003148", "1718512272", "2143144851"], "title": "Probabilistic latent semantic indexing", "abstract": "Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.", "citation_count": "15", "reference_count": "6,791", "date": "1999", "authors": ["Thomas Hofmann"], "related_topics": ["Probabilistic latent semantic analysis", "Latent Dirichlet allocation", "Document-term matrix", "Latent semantic analysis", "Latent class model", "Topic model", "Search engine indexing", "Probabilistic logic", "Dynamic topic model", "Pachinko allocation", "Natural language processing", "Data mining", "Computer science", "Synonym", "Artificial intelligence", "Latent semantic indexing"]}
{"id": "2144578941", "references": ["1553244859", "2041614298", "1623072288", "2056451646", "2141099517", "1505083828", "2075635421", "1522263329", "2004384146", "2915429162"], "title": "Introduction to the CoNLL-2003 shared task: language-independent named entity recognition", "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.", "citation_count": "23", "reference_count": "3,212", "date": "2003", "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "related_topics": ["Entity linking", "Named-entity recognition", "Task (project management)", "Sequence labeling", "German", "Natural language processing", "Computer science", "Artificial intelligence", "Background information"]}
{"id": "2128634885", "references": ["1632114991", "2147880316", "2008652694", "194033037", "2122922578", "2027979924", "2139621418", "2116410915", "2121227244", "1773803948"], "title": "Simple Semi-supervised Dependency Parsing", "abstract": "We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.", "citation_count": "30", "reference_count": "560", "date": "2008", "authors": ["Terry Koo", "Xavier Carreras", "Michael Collins"], "related_topics": ["Treebank", "Parsing", "Dependency grammar", "Dependency (UML)", "Natural language processing", "Word (computer architecture)", "Range (mathematics)", "Computer science", "Baseline (configuration management)", "Focus (optics)", "Czech", "Artificial intelligence"]}
{"id": "2096765155", "references": ["1997063559", "2147880316", "2171776966", "2135194391", "2125838338", "2160842254", "2962735828", "2581275558", "2129712609", "1513861746"], "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.", "citation_count": "23", "reference_count": "3,704", "date": "2005", "authors": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning"], "related_topics": ["Approximate inference", "Information extraction", "Gibbs sampling", "Inference", "Simulated annealing", "Probabilistic logic", "Consistency (database systems)", "Monte Carlo method", "Algorithm", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2148540243", "references": ["3146306708", "2140785063", "2048679005", "2097089247", "2168625136", "2068737686", "1567365482", "202303397", "2103931177"], "title": "Unsupervised named-entity extraction from the Web: An experimental study", "abstract": "The KnowItAll system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an unsupervised, domain-independent, and scalable manner. The paper presents an overview of KnowItAll's novel architecture and design principles, emphasizing its distinctive ability to extract information without any hand-labeled training examples. In its first major run, KnowItAll extracted over 50,000 class instances, but suggested a challenge: How can we improve KnowItAll's recall and extraction rate without sacrificing precision? This paper presents three distinct ways to address this challenge and evaluates their performance. Pattern Learning learns domain-specific extraction rules, which enable additional extractions. Subclass Extraction automatically identifies sub-classes in order to boost recall (e.g., ''chemist'' and ''biologist'' are identified as sub-classes of ''scientist''). List Extraction locates lists of class instances, learns a ''wrapper'' for each list, and extracts elements of each list. Since each method bootstraps from KnowItAll's domain-independent methods, the methods also obviate hand-labeled training examples. The paper reports on experiments, focused on building lists of named entities, that measure the relative efficacy of each method and demonstrate their synergy. In concert, our methods gave KnowItAll a 4-fold to 8-fold increase in recall at precision of 0.90, and discovered over 10,000 cities missing from the Tipster Gazetteer.", "citation_count": "54", "reference_count": "1,455", "date": "2005", "authors": ["Oren Etzioni", "Michael Cafarella", "Doug Downey", "Ana-Maria Popescu", "Tal Shaked", "Stephen Soderland", "Daniel S. Weld", "Alexander Yates"], "related_topics": ["Information extraction", "Unsupervised learning", "Pointwise mutual information", "Question answering", "Class (computer programming)", "Scalability", "World Wide Web", "Computer science", "Bootstrapping (linguistics)", "The Internet"]}
{"id": "1996430422", "references": ["1632114991", "2147880316", "2008652694", "2117400858", "1860991815", "2099247782", "2135843243", "1529196404", "1513861746", "1773803948"], "title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "abstract": "We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "citation_count": "19", "reference_count": "3,939", "date": "2003", "authors": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "related_topics": ["Treebank", "Dependency network", "Feature (machine learning)", "Natural language processing", "Word (computer architecture)", "Representation (mathematics)", "Computer science", "Artificial intelligence", "Part-of-speech tagging"]}
{"id": "2116877738", "references": ["2147880316", "2131686571", "2067191022", "2143516773", "2101309634", "1528789833", "2121947440", "2124351162", "2169551590", "1999478155"], "title": "Robust higher order potentials for enforcing label consistency", "abstract": "This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner. Our method is based on higher order conditional random fields and uses potentials defined on sets of pixels (image segments) generated using unsupervised segmentation algorithms. These potentials enforce label consistency in image regions and can be seen as a strict generalization of the commonly used pairwise contrast sensitive smoothness potentials. The higher order potential functions used in our framework take the form of the robust Pn model. This enables the use of powerful graph cut based move making algorithms for performing inference in the framework [14 ]. We test our method on the problem of multi-class object segmentation by augmenting the conventional CRF used for object segmentation with higher order potentials defined on image regions. Experiments on challenging data sets show that integration of higher order potentials quantitatively and qualitatively improves results leading to much better definition of object boundaries. We believe that this method can be used to yield similar improvements for many other labelling problems.", "citation_count": "49", "reference_count": "854", "date": "2008", "authors": ["P. Kohli", "L. Ladicky", "P. Torr"], "related_topics": ["Image segmentation", "Conditional random field", "Segmentation", "Cut", "Robustness (computer science)", "Image processing", "Unsupervised learning", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2132947399", "references": ["2104974755", "2161969291", "2119823327", "2156598602", "1508960934", "1677409904", "2146352414", "2296319761", "1999478155", "1663973292"], "title": "Make3D: Learning 3D Scene Structure from a Single Still Image", "abstract": "We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models that are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of \"plane parametersrdquo that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9 percent of 588 images downloaded from the Internet. We have also extended our model to produce large-scale 3D models from a few images.", "citation_count": "42", "reference_count": "1,864", "date": "2009", "authors": ["A. Saxena", "Min Sun", "A.Y. Ng"], "related_topics": ["Image-based modeling and rendering", "Image processing", "Rendering (computer graphics)", "Image segmentation", "Markov random field", "Supervised learning", "Iterative reconstruction", "Computer vision", "Markov process", "Depth perception", "Artificial intelligence", "Monocular vision", "Computer science", "Scale model"]}
{"id": "2112301665", "references": ["2124386111", "2110764733", "2107034620", "2154422044", "1625255723", "2121947440", "1880262756", "2107743791", "2001082470", "2131846894"], "title": "Using Multiple Segmentations to Discover Objects and their Extent in Image Collections", "abstract": "Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe.", "citation_count": "28", "reference_count": "854", "date": "2006", "authors": ["B.C. Russell", "W.T. Freeman", "A.A. Efros", "J. Sivic", "A. Zisserman"], "related_topics": ["Segmentation-based object categorization", "LabelMe", "Image segmentation", "Scale-space segmentation", "Image texture", "Cognitive neuroscience of visual object recognition", "Object (computer science)", "Segmentation", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2159128898", "references": ["1687797484", "2033009866", "2135346934", "2140235142", "1874027545", "2168682262", "2914885528", "2161406034", "1964443764", "204885769"], "title": "Real-time tracking of non-rigid objects using mean shift", "abstract": "A new method for real time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module is based on the mean shift iterations and finds the most probable target position in the current frame. The dissimilarity between the target model (its color distribution) and the target candidates is expressed by a metric derived from the Bhattacharyya coefficient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and efficient solution. The capability of the tracker to handle in real time partial occlusions, significant clutter, and target scale variations, is demonstrated for several image sequences.", "citation_count": "24", "reference_count": "4,836", "date": "2000", "authors": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "related_topics": ["Bhattacharyya distance", "Mean-shift", "Condensation algorithm", "Metric (mathematics)", "Clutter", "Kernel (image processing)", "Position (vector)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2999729612", "references": ["2140190241", "2153233077", "2067191022", "2032230795", "2162088497", "2155707112", "2165835468", "1673310716", "2011430131", "1966327575"], "title": "Finding Groups in Data: An Introduction to Cluster Analysis", "abstract": "1. Introduction. 2. Partitioning Around Medoids (Program PAM). 3. Clustering large Applications (Program CLARA). 4. Fuzzy Analysis. 5. Agglomerative Nesting (Program AGNES). 6. Divisive Analysis (Program DIANA). 7. Monothetic Analysis (Program MONA). Appendix 1. Implementation and Structure of the Programs. Appendix 2. Running the Programs. Appendix 3. Adapting the Programs to Your Needs. Appendix 4. The Program CLUSPLOT. References. Author Index. Subject Index.", "citation_count": "0", "reference_count": "20,628", "date": "1990", "authors": ["Leonard Kaufman", "Peter J. Rousseeuw"], "related_topics": ["Medoid", "Complete-linkage clustering", "Cluster analysis", "Nesting (computing)", "Hierarchical clustering", "OPTICS algorithm", "Index (publishing)", "k-medoids", "Information retrieval", "Computer science"]}
{"id": "2099244020", "references": ["1667165204", "2067681708", "1526351017", "2051826135", "2150134853", "1602550945", "2064347832", "2101248405", "2104763670", "2008014451"], "title": "Bilateral filtering for gray and color images", "abstract": "Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.", "citation_count": "15", "reference_count": "10,324", "date": "1998", "authors": ["C. Tomasi", "R. Manduchi"], "related_topics": ["Color histogram", "Color balance", "Color quantization", "Color space", "RGB color model", "Color image", "Bilateral filter", "Edge-preserving smoothing", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2129905273", "references": ["2093390569", "2159805452", "2067191022", "3125367243", "1585939719", "2112315008", "2098613108", "1991566301", "1504943474"], "title": "Density estimation for statistics and data analysis", "abstract": "Introduction. Survey of Existing Methods. The Kernel Method for Univariate Data. The Kernel Method for Multivariate Data. Three Important Methods. Density Estimation in Action.", "citation_count": "0", "reference_count": "27,530", "date": "1986", "authors": ["Bernard. W. Silverman"], "related_topics": ["Multivariate kernel density estimation", "Variable kernel density estimation", "Kernel (statistics)", "Kernel density estimation", "Density estimation", "Univariate", "Kernel method", "Kernel Bandwidth", "Statistics", "Mathematics"]}
{"id": "2140235142", "references": ["2140487300", "2119444142", "2069356045", "2144573889", "2034836133", "2157548127", "2178278515", "2042850474", "1635989058", "2131938593"], "title": "Pfinder: real-time tracking of the human body", "abstract": "Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10 Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding.", "citation_count": "14", "reference_count": "7,685", "date": "1997", "authors": ["C.R. Wren", "A. Azarbayejani", "T. Darrell", "A.P. Pentland"], "related_topics": ["Gesture recognition", "Foreground detection", "Background subtraction", "Data compression", "Image segmentation", "Statistical model", "Computer vision", "Real-time operating system", "Segmentation", "Virtual reality", "Computer science", "Artificial intelligence"]}
{"id": "1971784203", "references": ["2913066018", "1978616828", "2018388286", "2118587067", "1975152892", "1533790012", "1572134371", "2086943813", "2120636855", "1576534100"], "title": "Algorithms for clustering data", "abstract": "", "citation_count": "16", "reference_count": "15,559", "date": "1988", "authors": ["Anil K. Jain", "Richard C. Dubes"], "related_topics": ["Cluster analysis", "Correlation clustering", "CURE data clustering algorithm", "Fuzzy clustering", "Biclustering", "Single-linkage clustering", "Canopy clustering algorithm", "Clustering high-dimensional data", "Data mining", "Computer science"]}
{"id": "2482402870", "references": ["2139479830", "2154278880", "2154053567", "2153233077", "2130660124", "2075647286", "2067191022", "2074788634", "2015245929", "2161160262"], "title": "Statistical Pattern Recognition", "abstract": "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical ap...", "citation_count": "0", "reference_count": "7,074", "date": "2000", "authors": ["K JainAnil", "P W DuinRobert", "MaoJianchang"], "related_topics": ["Feature (machine learning)", "Feature extraction", "Pattern recognition (psychology)", "Cluster analysis", "Artificial neural network", "Feature selection", "Pattern recognition", "Artificial intelligence", "Computer science", "Statistical pattern"]}
{"id": "182831726", "references": ["1591706642", "2436001372", "2169818249", "2147272182", "1574901103", "1984251878", "1536274117", "2166183437", "2126399065", "2438667436"], "title": "Speech and Language Processing", "abstract": "is one of the most recognizablecharacters in 20th century cinema. HAL is an arti\ufb01cial agent capable of such advancedlanguage behavior as speaking and understanding English, and at a crucial moment inthe plot, even reading lips. It is now clear that HAL\u2019s creator, Arthur C. Clarke, wasa little optimistic in predicting when an arti\ufb01cial agent such as HAL would be avail-able. But just how far off was he? What would it take to create at least the language-relatedpartsofHAL?WecallprogramslikeHALthatconversewithhumansinnatural", "citation_count": "0", "reference_count": "3,374", "date": "1999", "authors": ["Dan Jurafsky", "James H. Martin"], "related_topics": ["Cued speech", "Speech processing", "Language technology", "Speech technology", "Telegraphic speech", "Universal Networking Language", "Reading (process)", "Natural language", "Linguistics", "Psychology"]}
{"id": "1736036918", "references": ["2165476957", "2142786899", "24765167", "1574901103", "2000775051", "1492896593", "1859582638", "2162102207", "144593342", "2139865360"], "title": "Information Retrieval: A Health Care Perspective", "abstract": "As the health care industry becomes increasingly dependent on electronic information, the need for sophisticated information retrieval systems and for knowledgeable people to design, purchase and use them also increases. This book provides an overview of the theory, practical applications, evaluation and research directions of these systems. In addition to bibliographic and full-text literature retrieval, the author discusses clinical records, multimedia and networked applications.", "citation_count": "0", "reference_count": "127", "date": "2013", "authors": ["William R. Hersh"], "related_topics": ["Human\u2013computer information retrieval", "Cognitive models of information retrieval", "Relevance (information retrieval)", "Health care", "Perspective (graphical)", "Information retrieval", "Medicine", "Clinical record", "Electronic information"]}
{"id": "2132549764", "references": [], "title": "Statistical pattern recognition: a review", "abstract": "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.", "citation_count": "178", "reference_count": "8,688", "date": "2000", "authors": ["A.K. Jain", "R.P.W. Duin", "Jianchang Mao"], "related_topics": ["Feature (machine learning)", "Intelligent character recognition", "Feature extraction", "Statistical learning theory", "Facial recognition system", "Cluster analysis", "Artificial neural network", "Feature selection", "Classifier (UML)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1549026077", "references": ["1508471544", "2121184547", "1574901103", "2024228866", "2114388055", "2124436241", "1995907191", "1506845741", "2163107094", "2106695994"], "title": "Natural Language Understanding", "abstract": "From the Publisher:\r\nIn addition, this title offers coverage of two entirely new subject areas. First, the text features a new chapter on statistically-based methods using large corpora. Second, it includes an appendix on speech recognition and spoken language understanding. Also, the information on semantics that was covered in the first edition has been largely expanded in this edition to include an emphasis on compositional interpretation.", "citation_count": "0", "reference_count": "4,077", "date": "1987", "authors": ["James Allen"], "related_topics": ["Spoken language", "Language identification", "Comprehension approach", "Natural language understanding", "Semantics", "Linguistics", "Interpretation (philosophy)", "Natural language processing", "Emphasis (typography)", "Computer science", "Artificial intelligence", "Subject areas"]}
{"id": "1746620543", "references": ["2045929671", "2121184547", "1574901103", "2543927704", "1513248649", "2106277773", "2251291469"], "title": "Text Information Retrieval Systems", "abstract": "From the Publisher:\r\nThis book's purpose is to teach people who will be searching or designing text retrieval systems how the systems work. For designers, it covers problems they will face and reviews currently available solutions to provide a basis for more advanced study. For the searcher its purpose is to describe why such systems work as they do. The book is primarily about computer-based retrieval systems, but the principles apply to nonmechanized ones as well.. \"The book covers the nature of information, how it is organized for use by a computer, how search functions are carried out, and some of the theory underlying these functions. As well, it discusses the interaction between user and system and how retrieved items, users, and complete systems are evaluated. A limited knowledge of mathematics and of computing is assumed.", "citation_count": "0", "reference_count": "684", "date": "1992", "authors": ["Charles T. Meadow", "Donald H. Kraft", "Bert R. Boyce"], "related_topics": ["Information retrieval", "Computer science", "Face (sociological concept)", "Work (electrical)", "Basis (linear algebra)", "Text retrieval"]}
{"id": "1994851566", "references": ["2080950999", "1984251878", "1574901103", "1579838312", "2024228866", "1977995219", "1981264193", "2159398820", "2048658075", "2166667242"], "title": "Statistical Language Learning", "abstract": "From the Publisher:\r\nEugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background.\r\nNew, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises.\r\nCharniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\"\r\nLanguage, Speech, and Communication", "citation_count": "0", "reference_count": "1,732", "date": "1994", "authors": ["Eugene Charniak"], "related_topics": ["Language identification", "Cache language model", "Computational linguistics", "Machine translation", "Language technology", "Grammar induction", "Parsing", "Knowledge representation and reasoning", "Natural language processing", "Mathematics", "Artificial intelligence"]}
{"id": "2108321481", "references": [], "title": "Exploiting Syntactic Structure for Language Modeling", "abstract": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.", "citation_count": "7", "reference_count": "254", "date": "1998", "authors": ["Ciprian Chelba", "Frederick Jelinek"], "related_topics": ["Language model", "Trigram", "Headword", "Probabilistic logic", "Word (computer architecture)", "Set (abstract data type)", "Natural language processing", "Speech recognition", "Sequence", "Computer science", "Artificial intelligence", "Syntactic structure"]}
{"id": "2949237929", "references": ["1632114991", "2049633694", "1606548921", "1607229519", "2110882317"], "title": "Expoiting Syntactic Structure for Language Modeling", "abstract": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words--binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.", "citation_count": "5", "reference_count": "202", "date": "1998", "authors": ["Ciprian Chelba", "Frederick Jelinek"], "related_topics": ["Language model", "Trigram", "Headword", "Probabilistic logic", "Word (computer architecture)", "Set (abstract data type)", "Natural language processing", "Sequence", "Computer science", "Artificial intelligence", "Syntactic structure"]}
{"id": "2130259898", "references": ["3040267042", "1535031115", "1587863748", "2135346934"], "title": "Low-dimensional procedure for the characterization of human faces", "abstract": "A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose.", "citation_count": "4", "reference_count": "3,443", "date": "1987", "authors": ["L. Sirovich", "M. Kirby"], "related_topics": ["Face (geometry)", "Eigenface", "Representation (mathematics)", "Ideal (set theory)", "Facial recognition system", "Image processing", "Pattern recognition (psychology)", "Algorithm", "Fourier transform", "Computer science", "Optics"]}
{"id": "1524408959", "references": ["1533169541", "2008297189", "2118783153", "2102475035", "2238624099", "2049633694", "2068272887", "2160066518", "2151135734", "2134814621"], "title": "Blobworld: A System for Region-Based Image Indexing and Retrieval", "abstract": "Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (\"blobs\") with associated color and texture descriptors. Queryingi s based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions usinga tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both queryinga nd indexing.", "citation_count": "17", "reference_count": "1,248", "date": "1999", "authors": ["Chad Carson", "Megan Thomas", "Serge Belongie", "Joseph M. Hellerstein", "Jitendra Malik"], "related_topics": ["Image texture", "Visual Word", "Automatic image annotation", "Feature detection (computer vision)", "Image retrieval", "Image processing", "Color histogram", "Search engine indexing", "Feature vector", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2156406284", "references": ["2059975159", "1513966746", "2073257493", "2149095485", "2059799772", "2032533296", "1501418839", "2081519360", "2125756925", "2740373864"], "title": "Recognition-by-Components: A Theory of Human Image Understanding.", "abstract": "The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N \u00a3 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensiona l image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. Any single object can project an infinity of image configurations to the retina. The orientation of the object to the viewer can vary continuously, each giving rise to a different two-dimensional projection. The object can be occluded by other objects or texture fields, as when viewed behind foliage. The object need not be presented as a full-colored textured image but instead can be a simplified line drawing. Moreover, the object can even be missing some of its parts or be a novel exemplar of its particular category. But it is only with rare exceptions that an image fails to be rapidly and readily classified, either as an instance of a familiar object category or as an instance that cannot be so classified (itself a form of classification).", "citation_count": "73", "reference_count": "7,661", "date": "1987", "authors": ["Irving Biederman"], "related_topics": ["3D single-object recognition", "Form perception", "Object model", "Cognitive neuroscience of visual object recognition", "Object permanence", "Image quality", "Perceptual Closure", "Invariant (mathematics)", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2142796031", "references": ["2780103611", "2127859399", "2086532736", "1772101126", "657075062", "2106289661", "2015634454", "2150142674", "2000700912", "2122896223"], "title": "To See or not to See: The Need for Attention to Perceive Changes in Scenes", "abstract": "When looking at a scene, observers feel that they see its entire structure in great detail and can immediately notice any changes in it However, when brief blank fields are placed between alternating displays of an original and a modified scene, a striking failure of perception is induced Identification of changes becomes extremely difficult, even when changes are large and made repeatedly Identification is much faster when a verbal cue is provided showing that poor visibility is not the cause of this difficulty Identification is also faster for objects considered to be important in the scene These results support the idea that observers never form a complete, detailed representation of their surroundings In addition, the results indicate that attention is required to perceive change, and that in the absence of localized motion signals attention is guided on the basis of high-level interest", "citation_count": "29", "reference_count": "3,082", "date": "1997", "authors": ["Ronald A. Rensink", "J. Kevin O'Regan", "James J. Clark"], "related_topics": ["Change blindness", "Inattentional blindness", "Transsaccadic memory", "Perception", "Identification (information)", "Visual perception", "Visibility (geometry)", "Motion (physics)", "Cognitive psychology", "Communication", "Psychology"]}
{"id": "2104825706", "references": ["2127370936", "2010560725", "1982948355", "2091503252", "2914369697", "2021751319", "2066610120", "2160066518", "2914885528", "2025653905"], "title": "Indoor-outdoor image classification", "abstract": "We show how high-level scene properties can be inferred from classification of low-level image features, specifically for the indoor-outdoor scene retrieval problem. We systematically studied the features of: histograms in the Ohta color space; multiresolution, simultaneous autoregressive model parameters; and coefficients of a shift-invariant DCT. We demonstrate that performance is improved by computing features on subblocks, classifying these subblocks, and then combining these results in a way reminiscent of stacking. State of the art single-feature methods are shown to result in about 75-86% performance, while the new method results in 90.3% correct classification, when evaluated on a diverse database of over 1300 consumer images provided by Kodak.", "citation_count": "13", "reference_count": "998", "date": "1998", "authors": ["M. Szummer", "R.W. Picard"], "related_topics": ["Contextual image classification", "Color histogram", "Image retrieval", "Color space", "Histogram", "Image resolution", "Pattern recognition", "Autoregressive model", "Discrete cosine transform", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2128716185", "references": ["2099111195", "2098947662", "2159686933", "2113341759", "2049633694", "2148694408", "2123977795", "2798909945", "2104095591", "2138451337"], "title": "Probabilistic visual learning for object representation", "abstract": "We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a mixture-of-Gaussians model (for multimodal distributions). Those probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands.", "citation_count": "38", "reference_count": "2,344", "date": "1997", "authors": ["B. Moghaddam", "A. Pentland"], "related_topics": ["Unsupervised learning", "Visual modeling", "Visual learning", "Cognitive neuroscience of visual object recognition", "Face detection", "Object detection", "Density estimation", "Visual search", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2012352340", "references": ["2135463994", "2101055828", "2798461040", "2098947662", "1770825568", "2135346934", "2122111042", "2138451337", "2148694408", "2159173611"], "title": "Using discriminant eigenfeatures for image retrieval", "abstract": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects presented as \"well-framed\" views, and compare it with that of the principal component analysis.", "citation_count": "20", "reference_count": "2,332", "date": "1996", "authors": ["D.L. Swets", "J.J. Weng"], "related_topics": ["Linear discriminant analysis", "Optimal discriminant analysis", "Content-based image retrieval", "Image retrieval", "Facial recognition system", "Principal component analysis", "Feature selection", "Discriminant", "Pattern recognition", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2167034998", "references": ["2138100172", "2078498116", "2003370853", "1999908130", "2006500012", "1995875735", "2105672294", "1499486838", "2116360511", "2135587681"], "title": "Relations between the statistics of natural images and the response properties of cortical cells.", "abstract": "The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.", "citation_count": "34", "reference_count": "4,010", "date": "1987", "authors": ["David J. Field"], "related_topics": ["Efficient coding hypothesis", "Orientation (computer vision)", "Redundancy (engineering)", "Image processing", "Log Gabor filter", "Edge detection", "Pixel", "Coding (social sciences)", "Computer science", "Statistics"]}
{"id": "2180838288", "references": ["2097074225", "2037090920", "2140499889", "2124486835", "2124731682", "2113466552", "2151035455", "2167804690", "2142940228", "2133180260"], "title": "What is the goal of sensory coding", "abstract": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a \"compact\" coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of \"sparse distributed\" coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling \"wavelet transforms\" are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented.", "citation_count": "0", "reference_count": "1,662", "date": "1999", "authors": ["David J. Field"], "related_topics": ["Neural coding", "Code (cryptography)", "Wavelet", "Redundancy (engineering)", "Wavelet transform", "Histogram", "Coding (social sciences)", "Kurtosis", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2076526090", "references": ["2122214306", "2083607646", "2064910499", "2006002820", "2099247782", "2112323378", "2127836646"], "title": "Studies in part of speech labelling", "abstract": "We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words. Part of speech tagging, per se, is a well understood problem. Our paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag. We describe the algorithms that we used and the specific results of our experiments on Wall Street Journal articles and on MUC terrorist messages.", "citation_count": "7", "reference_count": "23", "date": "1991", "authors": ["Marie Meteer", "Richard Schwartz", "Ralph Weischedel"], "related_topics": ["Part of speech", "Ambiguity", "Set (psychology)", "Natural language processing", "Speech recognition", "Word (computer architecture)", "Computer science", "Artificial intelligence", "Limiting", "Part-of-speech tagging", "Training set"]}
{"id": "2439178139", "references": ["2125838338", "2077302143", "2047706513", "2012837062", "1541301615", "2047067573", "1978470410", "2110190189", "2087165009", "1982944197"], "title": "INSIDE-OUTSIDE REESTIMATION FROM PARTIALLY BRACKETED CORPORA", "abstract": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.", "citation_count": "11", "reference_count": "671", "date": "1992", "authors": ["Fernando Pereira", "Yves Schabes"], "related_topics": ["Parsing", "Natural language", "Rule-based machine translation", "Data-oriented parsing", "Spoken language", "Grammar", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2334801970", "references": ["1632114991", "2140887277", "2081687495", "2168579166", "2097333193", "1551773846", "2093647425", "2044599851", "1940278502"], "title": "The computational analysis of English : a corpus-based approach", "abstract": "", "citation_count": "0", "reference_count": "313", "date": "1989", "authors": ["Roger Garside", "Geoffrey N. Leech", "Geoffrey Sampson"], "related_topics": ["Computer science", "Natural language processing", "Artificial intelligence", "Computational analysis", "Corpus based"]}
{"id": "2012837062", "references": ["2034274945", "3044664353", "1593045043", "1528321674", "2099247782", "2110190189", "2126477387", "2134237567", "1483126227", "2065585771"], "title": "Deducing linguistic structure from the statistics of large corpora", "abstract": "Within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4% error rate, when trained on moderate sized (500K word) corpora of English text (e.g. Church, 1988; Hindle, 1989). The success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s.", "citation_count": "12", "reference_count": "224", "date": "1990", "authors": ["Eric Brill", "David Magerman", "Mitchell Marcus", "Beatrice Santorini"], "related_topics": ["Word error rate", "Natural language processing", "Structure (mathematical logic)", "Word (computer architecture)", "Resolution (logic)", "Speech recognition", "Linguistics", "Computer science", "Artificial intelligence", "Grammatical structure", "Lexical ambiguity"]}
{"id": "2110190189", "references": ["2034274945", "1593045043", "2017580301", "2099247782", "2126477387", "2134237567", "1483126227"], "title": "Parsing a natural language using mutual information statistics", "abstract": "The purpose of this paper is to characterize a constituent boundary parsing algorithm, using an information-theoretic measure called generalized mutual information, which serves as an alternative to traditional grammar-based parsing methods. This method is based on the hypothesis that constituent boundaries can be extracted from a given sentence (or word sequence) by analyzing the mutual information values of the part of speech n-grams within the sentence. This hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted English text with a relatively low error rate. This paper derives the generalized mutual information statistic, describes the parsing algorithm, and presents results and sample output from the parser.", "citation_count": "7", "reference_count": "224", "date": "1990", "authors": ["David M. Magerman", "Mitchell P. Marcus"], "related_topics": ["Top-down parsing", "Bottom-up parsing", "Parser combinator", "S-attributed grammar", "Parsing", "Top-down parsing language", "Pointwise mutual information", "Parsing expression grammar", "Word error rate", "Sentence", "Natural language", "Part of speech", "Natural language processing", "Speech recognition", "Traditional grammar", "Computer science", "Artificial intelligence"]}
{"id": "900993354", "references": ["1632114991", "2127002961", "1996665163", "1482328859", "1515847863", "2108646579", "1988931981", "2542192908"], "title": "Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision)", "abstract": "This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech (\"tagging\"). Section 2 is an alphabetical list of the parts of speech encoded in the annotation systems of the Penn Treebank Project, along with their corresponding abbreviations (\"tags\") and some information concerning their definition. This section allows you to find an unfamiliar tag by looking up a familiar part of speech. Section 3 recapitulates the information in Section 2, but this time the information is alphabetically ordered by tags. This is the section to consult in order to find out what an unfamiliar tag means. Since the parts of speech are probably familiar to you from high school English, you should have little difficulty in assimilating the tags themselves. However, it is often quite difficult to decide which tag is appropriate in a particular context. The two sections 4 and 5 therefore include examples and guidelines on how to tag problematic cases. If you are uncertain about whether a given tag is correct or not, refer to these sections in order to ensure a consistently annotated text. Section 4 discusses parts of speech that are easily confused and gives guidelines on how to tag such cases, while Section 5 contains an alphabetical list of specific problematic words and collocations. Finally, Section 6 discusses some general tagging conventions. One general rule, however, is so important that we state it here. Many texts are not models of good prose, and some contain outright errors and slips of the pen. Do not be tempted to correct a tag to what it would be if the text were correct; rather, it is the incorrect word that should be tagged correctly. Disciplines Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-47. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/570 Part-of-S peech Tagging Guidelines For The Penn Treebank Project (3rd Revision) MS-CIS-90-47 LINC LAB 178", "citation_count": "0", "reference_count": "620", "date": "1990", "authors": ["Beatrice Santorini"], "related_topics": ["Treebank", "Section (typography)", "Part of speech", "Context (language use)", "Information and Computer Science", "Natural language processing", "Linguistics", "Annotation", "Technical report", "Computer science", "Word (computer architecture)", "Artificial intelligence"]}
{"id": "2166675302", "references": ["2055460448", "3044664353", "2052262800", "1541301615", "1483126227"], "title": "Discovering the Lexical Features of a Language", "abstract": "This paper examines the possibility of automatically discovering the lexieal features of a language. There is strong evidence that the set of possible lexical features which can be used in a language is unbounded, and thus not innate. Lakoff [Lakoff 87] describes a language in which the feature -I-woman-or-fire-ordangerons-thing exists. This feature is based upon ancient folklore of the society in which it is used. If the set of possible lexieal features is indeed unbounded, then it cannot be par t of the innate Universal Grammar and must be learned. Even if the set is not unbounded, the child is still left with the challenging task of determining which features are used in her language. If a child does not know a priori what lexical features are used in her language, there are two sources for acquiring this information: semantic and syntactic cues. A learner using semantic cues could recognize that words often refer to objects, actions, and properties, and from this deduce the lexical features: noun, verb and adjective. Pinker [Pinker 89] proposes that a combination of semantic cues and innate semantic primitives could account for the acquisition of verb features. He believes that the child can discover semantic properties of a verb by noticing the types of actions typically taking place when the verb is uttered. Once these properties are known, says Pinker, they can be used to reliably predict the distributional behavior of the verb. However, Gleitman [Gleitman 90] presents evidence that semantic cues axe not sufficient for a child to acquire verb features and believes that the use of this semantic information in conjunction with information about the subcategorization properties of the verb may be sufficient for learning verb features. This paper takes Glei tman's suggestion to the extreme, in hope of determining whether syntactic cues may not just aid in feature discovery, but may be all tha t is necessary. We present evidence for the sufficiency of a strictly syntax-based model for discovering", "citation_count": "5", "reference_count": "20", "date": "1991", "authors": ["Eric Brill"], "related_topics": ["Verb", "Lexical item", "Syntax", "Semantic property", "Noun", "Feature (linguistics)", "Lexical choice", "Subcategorization", "Lexical functional grammar", "Adjective", "Universal grammar", "Linguistics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2121407024", "references": ["2124102576", "2015773474", "1590656471", "1981724541", "1571096757", "2158652440", "2017580301", "2099247782", "1487155516"], "title": "ACQUIRING DISAMBIGUATION RULES FROM TEXT", "abstract": "An effective procedure for automatically acquiring a new set of disambiguation rules for an existing deterministic parser on the basis of tagged text is presented. Performance of the automatically acquired rules is much better than the existing hand-written disambiguation rules. The success of the acquired rules depends on using the linguistic information encoded in the parser; enhancements to various components of the parser improves the acquired rule set. This work suggests a path toward more robust and comprehensive syntactic analyzers.", "citation_count": "9", "reference_count": "145", "date": "1989", "authors": ["Donald Hindle"], "related_topics": ["Parsing", "Rule-based machine translation", "Syntax", "Set (abstract data type)", "Natural language processing", "Information retrieval", "Computer science", "Path (graph theory)", "Basis (linear algebra)", "Artificial intelligence"]}
{"id": "1964262399", "references": ["1556800872", "2147152072", "2609978083", "2157575588", "1979905340", "1989270637", "2016448024", "1963623641", "1980746611"], "title": "Computer methods for mathematical computations", "abstract": "", "citation_count": "0", "reference_count": "4,322", "date": "1977", "authors": ["George E. Forsythe", "Michael A. Malcolm", "Cleve B. Moler"], "related_topics": ["Computational resource", "Computational number theory", "Computational model", "Computation", "Computational science", "Computer science", "Programming method"]}
{"id": "2096411881", "references": ["2947000318", "1966365186", "2107668593", "2123838014", "2098057602", "2156273867", "2043909051", "3017143921", "149585942", "2163166770"], "title": "A THEORETICAL BASIS FOR THE USE OF CO\u2010OCCURRENCE DATA IN INFORMATION RETRIEVAL", "abstract": "This paper provides a foundation for a practical way of improving the effectiveness of an automatic retrieval system. Its main concern is with the weighting of index terms as a device for increasing retrieval effectiveness. Previously index terms have been assumed to be independent for the good reason that then a very simple weighting scheme can be used. In reality index terms are most unlikely to be independent. This paper explores one way of removing the independence assumption. Instead the extent of the dependence between index terms is measured and used to construct a non\u2010linear weighting function. In a practical situation the values of some of the parameters of such a function must be estimated from small samples of documents. So a number of estimation rules are discussed and one in particular is recommended. Finally the feasibility of the computations required for a non\u2010linear weighting scheme is examined.", "citation_count": "21", "reference_count": "652", "date": "1977", "authors": ["C.J. Van Rijsbergen"], "related_topics": ["Weighting", "Statistical assumption", "Function (mathematics)", "Basis (linear algebra)", "Data mining", "Index (economics)", "Simple (abstract algebra)", "Information retrieval", "Computer science", "Computation", "Scheme (programming language)"]}
{"id": "2024683548", "references": ["2060488060", "2091273188", "2058616517", "2147152072", "2118376336", "2535871509", "1980746611", "2153654879", "2028173473", "2134057677"], "title": "Forsythe, G. E. / Malcolm, M. A. / Moler, C. B., Computer Methods for Mathematical Computations. Englewood Cliffs, New Jersey 07632. Prentice Hall, Inc., 1977. XI, 259 S", "abstract": "", "citation_count": "0", "reference_count": "663", "date": "1979", "authors": ["F. Grund"], "related_topics": ["Mathematics", "Discrete mathematics", "Engineering physics", "Programming method"]}
{"id": "2000215628", "references": ["2056640768", "2056930330", "1963826206", "2083795909", "1572268863", "2024971339", "1997153468", "1977063224", "2185761757", "1979317087"], "title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of 'eckart-young' decomposition", "abstract": "An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common \u201cpsychological space\u201d. A corresponding method of analyzing similarities data is proposed, involving a generalization of \u201cEckart-Young analysis\u201d to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.", "citation_count": "15", "reference_count": "5,989", "date": "1970", "authors": ["J. Douglas Carroll", "Jih-Jie Chang"], "related_topics": ["Multidimensional scaling", "Tucker decomposition", "Matricization", "Statistical theory", "Multilinear subspace learning", "Tensor product network", "Applied mathematics", "Perception", "Discrete mathematics", "Mathematics", "Auditory stimuli"]}
{"id": "3012395598", "references": ["2172195418", "2085946009", "1539811621", "2147152072", "2089079408", "2119741678", "2001964924", "2155844971", "1986326495", "2024165284"], "title": "Data preprocessing and the extended PARAFAC model", "abstract": "", "citation_count": "0", "reference_count": "215", "date": "1984", "authors": ["R. A. Harshman"], "related_topics": ["Data pre-processing", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "1984565341", "references": ["1997042879", "2104115112", "3090556797", "50190571", "1988337648", "2051279338", "2046760690"], "title": "The vocabulary problem in human-system communication", "abstract": "In almost all computer applications, users must enter correct words for the desired objects or actions. For success without extensive training, or in first-tries for new targets, the system must recognize terms that will be chosen spontaneously. We studied spontaneous word choice for objects in five application-related domains, and found the variability to be surprisingly large. In every case two people favored the same term with probability", "citation_count": "7", "reference_count": "2,069", "date": "1987", "authors": ["G. W. Furnas", "T. K. Landauer", "L. M. Gomez", "S. T. Dumais"], "related_topics": ["Vocabulary", "Vocabulary mismatch", "Term (time)", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence", "Human system", "Word choice"]}
{"id": "1965061793", "references": ["2049577316", "2074876593", "2024472735", "1956559956", "1974406477", "2004913349", "1985414707", "2052842312"], "title": "A critical analysis of vector space model for information retrieval", "abstract": "Notations and definitions necessary to identify the concepts and relationships that are important in modelling information retrieval objects and processes in the context of vector spaces are presented. Earlier work on the use of vector model is evaluated in terms of the concepts introduced and certain problems and inconsistencies are identified. More importantly, this investigation should lead to a clear understanding of the issues and problems in using the vector space model in information retrieval. \u00a9 1986 John Wiley &amp; Sons, Inc.", "citation_count": "8", "reference_count": "523", "date": "1986", "authors": ["Vijay V. Raghavan", "S. K. M. Wong"], "related_topics": ["Vector space model", "Relevance (information retrieval)", "Document retrieval", "Context (language use)", "Information retrieval", "Computer science", "Weighting", "Mathematical model"]}
{"id": "2114804204", "references": ["2041565863", "2068632118", "1980849928", "73128518", "2007154686", "1546473629", "2045584315", "2065938637", "120261275", "2053256916"], "title": "The cluster hypothesis revisited", "abstract": "A new means of evaluating the cluster hypothesis is introduced and the results of such an evaluation are presented for four collections. The results of retrieval experiments comparing a sequential search, a cluster-based search, and a search of the clustered collection in which individual documents are scored against the query are also presented. These results indicate that while the absolute performance of a search on a particular collection is dependent on the pairwise similarity of the relevant documents, the relative effectiveness of clustered retrieval versus sequential retrieval is independent of this factor. However, retrieval of entire clusters in response to a query usually results in a poorer performance than retrieval of individual documents from clusters.", "citation_count": "11", "reference_count": "275", "date": "1985", "authors": ["Ellen M. Vdorhees"], "related_topics": ["Cluster hypothesis", "Ranking (information retrieval)", "Concept search", "Linear search", "Information retrieval", "Computer science"]}
{"id": "2151561903", "references": ["2011386395", "1521517187", "2993383518", "2110416104", "2315179971", "1530533107", "2111030512", "2006423836", "1964653871", "1969340322"], "title": "Subject access in online catalogs: A design model", "abstract": "A model based on strikingly different philosophical as. sumptions from those currently popular is proposed for the design of online subject catalog access. Three design principles are presented and discussed: uncertainty (subject indexing is indeterminate and probabilistic beyond a certain point), variety (by Ashby\u2019s law of requisite variety, variety of searcher query must equal variety of document indexing), and complexity (the search process, particularly during the entry and orientation phases, is subtler and more complex, on several grounds, than current models assume). Design features presented are an access phase, including entry and orientation, a hunting phase, and a selection phase. An end-user thesaurus and a front-end system mind are presented as examples of online catalog system components to improve searcher success during entry and orientation. The proposed model is \u201cwrapped around\u201d existing Library of Congress subject-heading indexing in such a way as to enhance access greatly without requiring reindexing. It is argued that both for cost reasons and in principle this is a superior approach to other design philosophies.", "citation_count": "56", "reference_count": "569", "date": "1986", "authors": ["Marcia J. Bates"], "related_topics": ["Subject access", "Subject indexing", "Search engine indexing", "Thesaurus (information retrieval)", "User assistance", "Relevance (information retrieval)", "Process (engineering)", "Information retrieval", "Computer science"]}
{"id": "2292896937", "references": ["2107878631", "1959983357", "2110485445", "2151834591", "2468203291", "3036751298", "1979684610", "2121553911", "2339378878", "2016589492"], "title": "A guide to recurrent neural networks and backpropagation", "abstract": "This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks.", "citation_count": "20", "reference_count": "297", "date": "2001", "authors": ["Mikael Boden"], "related_topics": ["Deep learning", "Feedforward neural network", "Types of artificial neural networks", "Recurrent neural network", "Catastrophic interference", "Backpropagation", "Probabilistic logic", "Artificial intelligence", "Machine learning", "Feed forward", "Computer science"]}
{"id": "2027499299", "references": ["1569447338", "2037740282", "2100969003", "2046317813", "2093225945", "1571931074", "2150907703", "2125336414", "1591607137", "2468573742"], "title": "The AMI System for the Transcription of Speech in Meetings", "abstract": "This paper describes the AMI transcription system for speech in meetings developed in collaboration by five research groups. The system includes generic techniques such as discriminative and speaker adaptive training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, maximum likelihood linear regression, and phone posterior based features, as well as techniques specifically designed for meeting data. These include segmentation and cross-talk suppression, beam-forming, domain adaptation, Web-data collection, and channel adaptive training. The system was improved by more than 20% relative in word error rate compared to our previous system and was used in the NIST RT106 evaluations where it was found to yield competitive performance.", "citation_count": "32", "reference_count": "143", "date": "2007", "authors": ["T. Hain", "V. Wan", "L. Burget", "M. Karafiat", "J. Dines", "J. Vepa", "G. Garau", "M. Lincoln"], "related_topics": ["Speech processing", "Word error rate", "Linear predictive coding", "Acoustic model", "Speech technology", "Transcription (software)", "Discriminative model", "Vocal tract", "Speech recognition", "NIST", "Phone", "Computer science"]}
{"id": "2152808281", "references": ["1574901103", "1802356529", "2096175520", "2069739265", "2116064496", "1934041838", "2134237567", "2132339004", "1985093013"], "title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model", "abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.", "citation_count": "32", "reference_count": "235", "date": "2008", "authors": ["Y. Bengio", "J.-S. Senecal"], "related_topics": ["Time delay neural network", "Probabilistic neural network", "Feedforward neural network", "Language model", "Artificial neural network", "Importance sampling", "Probabilistic logic", "Context model", "Conditional probability distribution", "Markov chain", "Machine learning", "Artificial intelligence", "Computer science", "Monte Carlo method", "Maximum likelihood"]}
{"id": "2468573742", "references": ["1569447338", "2037740282", "2100969003", "2046317813", "2093225945", "1571931074", "2150907703", "2094971681", "2125336414", "1591607137"], "title": "The 2005 AMI system for the transcription of speech in meetings", "abstract": "In this paper we describe the 2005 AMI system for the transcription of speech in meetings used in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve competitive performance.", "citation_count": "21", "reference_count": "75", "date": "2005", "authors": ["Thomas Hain", "Lukas Burget", "John Dines", "Giulia Garau", "Martin Karafiat", "Mike Lincoln", "Iain McCowan", "Darren Moore", "Vincent Wan", "Roeland Ordelman", "Steve Renals"], "related_topics": ["Speech synthesis", "Word error rate", "Transcription (software)", "NIST", "Linear discriminant analysis", "Speech recognition", "Discriminative model", "Vocal tract", "Microphone", "Computer science"]}
{"id": "2096072088", "references": ["2099345940", "2116625254", "2143866356", "2155280192", "2158195707", "2080018251", "1924403233", "2056250865", "2121227244", "2113691817"], "title": "A Joint Language Model With Fine-grain Syntactic Tags", "abstract": "We present a scalable joint language model designed to utilize fine-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees - a combination of properties that allows easy adoption of this model for new languages. We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions.", "citation_count": "17", "reference_count": "35", "date": "2009", "authors": ["Denis Filimonov", "Mary Harper"], "related_topics": ["Language model", "Parsing", "Syntax", "Natural language processing", "Computer science", "Task (project management)", "Scale (map)", "Joint (audio engineering)", "Artificial intelligence", "Fine grain"]}
{"id": "2109722477", "references": ["2173213060", "1530699444", "1924689489", "1985690171", "2017977879", "2019363670", "1512098439", "2147898188", "2108384452", "2148694408"], "title": "Map-Reduce for Machine Learning on Multicore", "abstract": "We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.", "citation_count": "27", "reference_count": "1,793", "date": "2006", "authors": ["Cheng-tao Chu", "Sang K. Kim", "Yi-an Lin", "Yuanyuan Yu", "Gary Bradski", "Kunle Olukotun", "Andrew Y. Ng"], "related_topics": ["Active learning (machine learning)", "Supervised learning", "Support vector machine", "Speedup", "Naive Bayes classifier", "Linear discriminant analysis", "Multi-core processor", "Backpropagation", "Theoretical computer science", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2045271686", "references": ["2143462372", "1989582918", "1555673550", "2103012681", "1544480906", "2069489095", "2052207834", "2107997203", "1969008575", "2137239103"], "title": "A bridging model for parallel computation", "abstract": "The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware.", "citation_count": "29", "reference_count": "5,035", "date": "1990", "authors": ["Leslie G. Valiant"], "related_topics": ["Bulk synchronous parallel", "Bridging model", "List ranking", "Software", "Von Neumann architecture", "Parallel processing (DSP implementation)", "Parallel random-access machine", "Multiprocessing", "Parallel computing", "Computer science"]}
{"id": "1510543252", "references": ["2081612620", "2173213060", "1982478920", "2622263826", "2109065830", "2111996486", "2154010459", "90568776", "1995017064"], "title": "Using MPI: Portable Parallel Programming with the Message-Passing Interface", "abstract": "This book offers a thoroughly updated guide to the MPI (Message-Passing Interface) standard library for writing programs for parallel computers. Since the publication of the previous edition of Using MPI, parallel computing has become mainstream. Today, applications run on computers with millions of processors; multiple processors sharing memory and multicore processors with multiple hardware threads per core are common. The MPI-3 Forum recently brought the MPI standard up to date with respect to developments in hardware capabilities, core language evolution, the needs of applications, and experience gained over the years by vendors, implementers, and users. This third edition of Using MPI reflects these changes in both text and example code. The book takes an informal, tutorial approach, introducing each concept through easy-to-understand examples, including actual code in C and Fortran. Topics include using MPI in simple programs, virtual topologies, MPI datatypes, parallel libraries, and a comparison of MPI with sockets. For the third edition, example code has been brought up to date; applications have been updated; and references reflect the recent attention MPI has received in the literature. A companion volume, Using Advanced MPI, covers more advanced topics, including hybrid programming and coping with large data.", "citation_count": "0", "reference_count": "6,041", "date": "1994", "authors": ["William Gropp", "Ewing Lusk", "Anthony Skjellum"], "related_topics": ["Message Passing Interface", "Multi-core processor", "Interface (Java)", "Fortran", "Computer science", "Parallel computing", "Volume (computing)", "Code (cryptography)", "SIMPLE (military communications protocol)", "Network topology"]}
{"id": "2148317584", "references": ["2439240014", "2091257550", "2985178474", "1809943808", "1973501242", "2146749986", "2152526229", "2083469471", "2120510885", "2131053137"], "title": "Distributed computing in practice: the Condor experience", "abstract": "SUMMARY Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational Grid. In this paper, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course travelled by research ideas as they grow into production systems. Copyright c \ufffd 2005 John Wiley &amp; Sons, Ltd.", "citation_count": "53", "reference_count": "2,562", "date": "2005", "authors": ["Douglas Thain", "Todd Tannenbaum", "Miron Livny"], "related_topics": ["Many-task computing", "Grid computing", "High-throughput computing", "Grid", "Computer science", "Social structure", "Distributed computing", "Scheduling (computing)"]}
{"id": "2073965851", "references": ["2338343669", "2155350341", "3013264884", "2114421447", "1971851724"], "title": "Web search for a planet: The Google cluster architecture", "abstract": "Amenable to extensive parallelization, Google's web search application lets different queries run on different processors and, by partitioning the overall index, also lets a single query use multiple processors. to handle this workload, Googless architecture features clusters of more than 15,000 commodity-class PCs with fault tolerant software. This architecture achieves superior performance at a fraction of the cost of a system built from fewer, but more expensive, high-end servers.", "citation_count": "5", "reference_count": "1,566", "date": "2003", "authors": ["L.A. Barroso", "J. Dean", "U. Holzle"], "related_topics": ["Web search engine", "Web search query", "Server", "Web crawler", "Web page", "Software fault tolerance", "File server", "The Internet", "Site map", "Operating system", "Search engine", "Computer science"]}
{"id": "2044534358", "references": ["1527961683", "3121531027", "1779735989", "2008793926", "2134342348", "2135131646", "2114728910", "2150048640", "2105818147", "2096322909"], "title": "Cluster-based scalable network services", "abstract": "We identify three fundamental requirements for scalable network services: incremental scalability and overflow growth provisioning, 24x7 availability through fault masking, and cost-effectiveness. We argue that clusters of commodity workstations interconnected by a high-speed SAN are exceptionally well-suited to meeting these challenges for Internet-server workloads, provided the software infrastructure for managing partial failures and administering a large cluster does not have to be reinvented for each new service. To this end, we propose a general, layered architecture for building cluster-based scalable network services that encapsulates the above requirements for reuse, and a service-programming model based on composable workers that perform transformation, aggregation, caching, and customization (TACC) of Internet content. For both performance and implementation simplicity, the architecture and TACC programming model exploit BASE, a weaker-than-ACID data semantics that results from trading consistency for availability and relying on soft state for robustness in failure management. Our architecture can be used as an off the shelf infrastructural platform for creating new network services, allowing authors to focus on the content of the service (by composing TACC building blocks) rather than its implementation. We discuss two real implementations of services based on this architecture: TranSend, a Web distillation proxy deployed to the UC Berkeley dialup IP population, and HotBot, the commercial implementation of the Inktomi search engine. We present detailed measurements of TranSend's performance based on substantial client traces, as well as anecdotal evidence from the TranSend and HotBot experience, to support the claims made for the architecture.", "citation_count": "50", "reference_count": "911", "date": "1997", "authors": ["Armando Fox", "Steven D. Gribble", "Yatin Chawathe", "Eric A. Brewer", "Paul Gauthier"], "related_topics": ["Systems architecture", "Multitier architecture", "Scalability", "Load balancing (computing)", "Provisioning", "Server", "Soft state", "The Internet", "Distributed computing", "Operating system", "Computer science"]}
{"id": "1988243929", "references": ["2150676586", "2100406636", "2439240014", "2199552016", "2152526229", "2158714788", "2104210894", "2154010459", "2123820820", "2119565742"], "title": "Explicit control a batch-aware distributed file system", "abstract": "We present the design, implementation, and evaluation of the Batch-Aware Distributed File System (BAD-FS), a system designed to orchestrate large, I/O-intensive batch workloads on remote computing clusters distributed across the wide area. BAD-FS consists of two novel components: a storage layer that exposes control of traditionally fixed policies such as caching, consistency, and replication; and a scheduler that exploits this control as necessary for different workloads. By extracting control from the storage layer and placing it within an external scheduler, BAD-FS manages both storage and computation in a coordinated way while gracefully dealing with cache consistency, fault-tolerance, and space management issues in a workload-specific manner. Using both microbenchmarks and real workloads, we demonstrate the performance benefits of explicit control, delivering excellent end-to-end performance across the wide-area.", "citation_count": "62", "reference_count": "171", "date": "2004", "authors": ["John Bent", "Douglas Thain", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau", "Miron Livny"], "related_topics": ["Replication (computing)", "Distributed File System", "Consistency (database systems)", "Distributed computing", "Operating system", "Layer (object-oriented design)", "Computer science", "Exploit", "Control (management)", "Computation", "Wide area"]}
{"id": "2104644701", "references": ["2173213060", "2073965851", "2022740893", "2072725684", "2116059696", "2109065830", "2108204150", "2134408405", "2162465831", "2119565742"], "title": "Evaluating MapReduce for Multi-core and Multiprocessor Systems", "abstract": "This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automatically parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix runtime automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code", "citation_count": "25", "reference_count": "1,397", "date": "2007", "authors": ["C. Ranger", "R. Raghuraman", "A. Penmetsa", "G. Bradski", "C. Kozyrakis"], "related_topics": ["Runtime system", "Server", "Scalability", "Multi-core processor", "Fault tolerance", "Multiprocessing", "Multithreading", "Thread (computing)", "Parallel computing", "Operating system", "Computer science"]}
{"id": "2119565742", "references": ["2157737097", "2073965851", "2133338501", "1566984846", "2131645490", "2025413686", "2007807439", "2147504831", "2137808089", "2005373714"], "title": "The Google file system", "abstract": "We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.", "citation_count": "14", "reference_count": "9,611", "date": "2003", "authors": ["Sanjay Ghemawat", "Howard Gobioff", "Shun-Tak Leung"], "related_topics": ["Self-certifying File System", "File system", "Distributed File System", "Global Namespace", "Distributed lock manager", "BitTorrent tracker", "Replication (computing)", "Geo-replication", "Operating system", "Database", "Computer science"]}
{"id": "1550206324", "references": ["2153283265", "2096152098", "2099111195", "2140785063", "2167044614", "2149684865", "1924689489", "2118234018", "1817561967", "1648885110"], "title": "A comparison of event models for naive bayes text classification", "abstract": "Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size.", "citation_count": "32", "reference_count": "5,029", "date": "1998", "authors": ["Andrew McCallum", "Kamal Nigam"], "related_topics": ["Naive Bayes classifier", "Multinomial distribution", "Vocabulary", "Bayesian network", "Language model", "Event (probability theory)", "Text corpus", "Probabilistic logic", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2140785063", "references": ["1912123407", "2125055259", "1587718046", "1678889691", "1817561967", "2136000097", "1840338487", "2084812512", "3017143921", "1625504505"], "title": "On the Optimality of the Simple Bayesian Classifier under Zero-One Loss", "abstract": "The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier\u2018s probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article\u2018s results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.", "citation_count": "34", "reference_count": "4,007", "date": "1997", "authors": ["Pedro Domingos", "Michael Pazzani"], "related_topics": ["Margin classifier", "Bayesian average", "Quadratic classifier", "Naive Bayes classifier", "Bayes classifier", "Bayesian statistics", "Classifier (UML)", "Statistical assumption", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2149684865", "references": ["2114535528", "2096152098", "740415", "2156909104", "2119821739", "2125055259", "1978394996", "2435251607", "1504694836", "2087614174"], "title": "Text Categorization with Suport Vector Machines: Learning with Many Relevant Features", "abstract": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.", "citation_count": "12", "reference_count": "11,689", "date": "1998", "authors": ["Thorsten Joachims"], "related_topics": ["Support vector machine", "Categorization", "Machine learning", "Computer science", "Document processing", "Variety (cybernetics)", "Task (project management)", "Artificial intelligence", "Boosting methods for object categorization", "Text categorization"]}
{"id": "2199803028", "references": ["2127314673", "1528905581", "2797692640", "2102381086", "3017143921", "2099247782", "1987971958", "2121227244", "2011039300", "103650626"], "title": "Predicting the Semantic Orientation of Adjectives", "abstract": "We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus.", "citation_count": "19", "reference_count": "3,502", "date": "1997", "authors": ["Vasileios Hatzivassiloglou", "Kathleen R. McKeown"], "related_topics": ["Orientation (computer vision)", "Conjunction (grammar)", "Natural language processing", "Task (project management)", "Computer science", "Artificial intelligence"]}
{"id": "1924689489", "references": ["2140785063", "2149684865", "1997841190", "1969572066", "2014415866", "1956559956", "2000569744", "3017143921", "2085989833", "2000672666"], "title": "Naive (Bayes) at forty: the independence assumption in information retrieval", "abstract": "The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.", "citation_count": "50", "reference_count": "2,906", "date": "1998", "authors": ["David D. Lewis"], "related_topics": ["Naive Bayes classifier", "Binary Independence Model", "Relevance feedback", "Statistical assumption", "Information science", "Information theory", "Machine learning", "Information retrieval", "Word (computer architecture)", "Computer science", "Artificial intelligence", "The Renaissance"]}
{"id": "2132267493", "references": ["2128978199", "1981663184", "2027559251", "290527292", "1509568713", "2018282388", "2039748980", "1561337879", "2013912476", "1981745143"], "title": "Tensor Rank and the Ill-Posedness of the Best Low-Rank Approximation Problem", "abstract": "There has been continued interest in seeking a theorem describing optimal low-rank approximations to tensors of order 3 or higher that parallels the Eckart-Young theorem for matrices. In this paper, we argue that the naive approach to this problem is doomed to failure because, unlike matrices, tensors of order 3 or higher can fail to have best rank-$r$ approximations. The phenomenon is much more widespread than one might suspect: examples of this failure can be constructed over a wide range of dimensions, orders, and ranks, regardless of the choice of norm (or even Bregman divergence). Moreover, we show that in many instances these counterexamples have positive volume: they cannot be regarded as isolated phenomena.\u00a0 In one extreme case, we exhibit a tensor space in which no rank-3 tensor has an optimal rank-2 approximation. The notable exceptions to this misbehavior are rank-1 tensors and order-2 tensors (i.e., matrices). In a more positive spirit, we propose a natural way of overcoming the ill-posedness of the low-rank approximation problem, by using weak solutions when true solutions do not exist. For this to work, it is necessary to characterize the set of weak solutions, and we do this\u00a0 in the case of rank 2, order 3 (in arbitrary dimensions). In our work we emphasize the importance of closely studying concrete low-dimensional examples as a first step toward more general results. To this end, we present a detailed analysis of equivalence classes of $2 \\times 2 \\times 2$ tensors, and we develop methods for extending results upward to higher orders and dimensions. Finally, we link our work to existing studies of tensors from an algebraic geometric point of view. The rank of a tensor can in theory be given a semialgebraic description; in other words, it can be determined by a system of polynomial inequalities. We study some of these polynomials in cases of interest to us; in particular, we make extensive use of the hyperdeterminant $\\Delta$ on $\\mathbb{R}^{2\\times 2 \\times 2}$.", "citation_count": "70", "reference_count": "892", "date": "2008", "authors": ["Vin de Silva", "Lek-Heng Lim"], "related_topics": ["Invariants of tensors", "Tensor", "Hyperdeterminant", "Low-rank approximation", "Bregman divergence", "Polynomial", "Norm (mathematics)", "Counterexample", "Pure mathematics", "Combinatorics", "Mathematics"]}
{"id": "2752853835", "references": ["1506281249", "1996360405", "2156030242", "2540924152", "2095595785", "2156186849", "2137147061", "2024165284", "1736726159", "1992419399"], "title": "The Art of Computer Programming", "abstract": "", "citation_count": "0", "reference_count": "27,471", "date": "1968", "authors": ["Donald Ervin Knuth"], "related_topics": ["Computer network programming", "Symbolic programming", "Computer programming", "Imperative programming", "Programming language implementation", "Computer literacy", "Internal sort", "Computer science", "Timsort", "Programming language"]}
{"id": "2090208105", "references": ["2070769763", "1602307574", "217710249", "2044269663", "1483218536", "2090799283", "1599833186", "1977415556", "2154335661", "2115955567"], "title": "Eigenvalues of a real supersymmetric tensor", "abstract": "In this paper, we define the symmetric hyperdeterminant, eigenvalues and E-eigenvalues of a real supersymmetric tensor. We show that eigenvalues are roots of a one-dimensional polynomial, and when the order of the tensor is even, E-eigenvalues are roots of another one-dimensional polynomial. These two one-dimensional polynomials are associated with the symmetric hyperdeterminant. We call them the characteristic polynomial and the E-characteristic polynomial of that supersymmetric tensor. Real eigenvalues (E-eigenvalues) with real eigenvectors (E-eigenvectors) are called H-eigenvalues (Z-eigenvalues). When the order of the supersymmetric tensor is even, H-eigenvalues (Z-eigenvalues) exist and the supersymmetric tensor is positive definite if and only if all of its H-eigenvalues (Z-eigenvalues) are positive. An mth-order n-dimensional supersymmetric tensor where m is even has exactly n(m-1)^n^-^1 eigenvalues, and the number of its E-eigenvalues is strictly less than n(m-1)^n^-^1 when m&gt;=4. We show that the product of all the eigenvalues is equal to the value of the symmetric hyperdeterminant, while the sum of all the eigenvalues is equal to the sum of the diagonal elements of that supersymmetric tensor, multiplied by (m-1)^n^-^1. The n(m-1)^n^-^1 eigenvalues are distributed in n disks in C. The centers and radii of these n disks are the diagonal elements, and the sums of the absolute values of the corresponding off-diagonal elements, of that supersymmetric tensor. On the other hand, E-eigenvalues are invariant under orthogonal transformations.", "citation_count": "17", "reference_count": "1,097", "date": "2005", "authors": ["Liqun Qi"], "related_topics": ["Symmetric tensor", "Tensor density", "Hyperdeterminant", "Tensor product of Hilbert spaces", "Tensor contraction", "Weyl tensor", "Tensor product of algebras", "Exact solutions in general relativity", "Pure mathematics", "Mathematical analysis", "Mathematics"]}
{"id": "2013912476", "references": ["2610857016", "2004406940", "2123879611", "1981745143", "2020062309", "66060913", "32702425", "2615836285", "1995963238", "2099741732"], "title": "A Multilinear Singular Value Decomposition", "abstract": "We discuss a multilinear generalization of the singular value decomposition. There is a strong analogy between several properties of the matrix and the higher-order tensor decomposition; uniqueness, link with the matrix eigenvalue decomposition, first-order perturbation effects, etc., are analyzed. We investigate how tensor symmetries affect the decomposition and propose a multilinear generalization of the symmetric eigenvalue decomposition for pair-wise symmetric tensors.", "citation_count": "28", "reference_count": "4,093", "date": "2000", "authors": ["Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle"], "related_topics": ["Higher-order singular value decomposition", "Singular value decomposition", "Multilinear map", "Multilinear principal component analysis", "Matrix decomposition", "Polar decomposition", "LU decomposition", "Multilinear subspace learning", "Pure mathematics", "Mathematical analysis", "Mathematics"]}
{"id": "2113722075", "references": ["1602307574", "2000204283", "1993670676", "1973652906", "2074447841", "2090208105", "1509568713", "2018282388", "1500921805", "2013912476"], "title": "Singular values and eigenvalues of tensors: a variational approach", "abstract": "We propose a theory of eigenvalues, eigenvectors, singular values, and singular vectors for tensors based on a constrained variational approach much like the Rayleigh quotient for symmetric matrix eigenvalues. These notions are particularly useful in generalizing certain areas where the spectral theory of matrices has traditionally played an important role. For illustration, we will discuss a multilinear generalization of the Perron-Frobenius theorem", "citation_count": "10", "reference_count": "821", "date": "2005", "authors": ["Lek-Heng Lim"], "related_topics": ["Singular value", "Singular solution", "Invariants of tensors", "Rayleigh quotient", "Spectrum of a matrix", "Spectral theory", "Symmetric matrix", "Eigenvalues and eigenvectors", "Pure mathematics", "Mathematical analysis", "Mathematics"]}
{"id": "2798909945", "references": ["1746819321", "2145096794", "2164278908", "2167667767", "2138621811", "2121947440", "2127271355", "3102641634"], "title": "Matrix computations", "abstract": "", "citation_count": "0", "reference_count": "74,397", "date": "1983", "authors": ["Gene H. Golub"], "related_topics": ["Matrix (mathematics)", "LU decomposition", "Matrix exponential", "Interpolative decomposition", "Bidiagonal matrix", "Eigenvalue algorithm", "QR decomposition", "Sylvester matrix", "Computer science", "Applied mathematics"]}
{"id": "1521626219", "references": ["2129765547", "2751318774", "2024228866", "1967461618", "1496357020", "1592805114", "1711163617", "1532325895", "2013833248", "2148212498"], "title": "Natural Language Processing with Python", "abstract": "This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify \"named entities\" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.", "citation_count": "57", "reference_count": "3,107", "date": "2009", "authors": ["Steven Bird", "Ewan Klein", "Edward Loper"], "related_topics": ["Language identification", "Natural language programming", "Language technology", "Universal Networking Language", "Natural language user interface", "Scripting language", "Information extraction", "Deep linguistic processing", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2122369144", "references": ["2022204871", "1508206474", "3146166473", "3126053622", "2044503966", "2129294185", "2148709190", "1532325895", "2048658075", "2097726431"], "title": "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series", "abstract": "We connect measures of public opinion measured from polls with sentiment measured from text. We analyze several surveys on consumer confidence and political opinion over the 2008 to 2009 period, and find they correlate to sentiment word frequencies in contemporaneous Twitter messages. While our results vary across datasets, in several cases the correlations are as high as 80%, and capture important large-scale trends. The results highlight the potential of text streams as a substitute and supplement for traditional polling.", "citation_count": "20", "reference_count": "2,436", "date": "2010", "authors": ["Brendan O'Connor", "Ramnath Balasubramanyan", "Bryan R. Routledge", "Noah A. Smith"], "related_topics": ["Sentiment analysis", "Consumer confidence index", "Public opinion", "Social media", "Word lists by frequency", "Advertising", "Polling", "Text mining", "Internet privacy", "Psychology", "Political Elections"]}
{"id": "2962965405", "references": ["2964308564", "1904365287", "2130942839", "2123442489", "1753482797", "2157331557", "1532325895", "2124807415", "2132339004", "2118434577"], "title": "A Neural Attention Model for Abstractive Sentence Summarization", "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.", "citation_count": "25", "reference_count": "1,862", "date": "2015", "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "related_topics": ["Automatic summarization", "Sentence", "Natural language processing", "Word (computer architecture)", "Task (project management)", "Computer science", "Artificial intelligence", "Attention model", "Training set"]}
{"id": "2089554624", "references": ["2122646361", "2752099845", "2138621811", "2112090702", "2121947440", "2095293504", "3013264884", "2008620264", "2244663144", "2798909945"], "title": "Graph based anomaly detection and description: a survey", "abstract": "Detecting anomalies in data is a vital task, with numerous high-impact applications in areas such as security, finance, health care, and law enforcement. While numerous techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points, with graph data becoming ubiquitous, techniques for structured graph data have been of focus recently. As objects in graphs have long-range correlations, a suite of novel technology has been developed for anomaly detection in graph data. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for anomaly detection in data represented as graphs. As a key contribution, we give a general framework for the algorithms categorized under various settings: unsupervised versus (semi-)supervised approaches, for static versus dynamic graphs, for attributed versus plain graphs. We highlight the effectiveness, scalability, generality, and robustness aspects of the methods. What is more, we stress the importance of anomaly attribution and highlight the major techniques that facilitate digging out the root cause, or the `why', of the detected anomalies for further analysis and sense-making. Finally, we present several real-world applications of graph-based anomaly detection in diverse domains, including financial, auction, computer traffic, and social networks. We conclude our survey with a discussion on open theoretical and practical challenges in the field.", "citation_count": "265", "reference_count": "961", "date": "2015", "authors": ["Leman Akoglu", "Hanghang Tong", "Danai Koutra"], "related_topics": ["Anomaly detection", "Change detection", "Scalability", "Data mining", "Machine learning", "Visual analytics", "Root cause", "Computer science", "Generality", "Outlier", "Suite", "Artificial intelligence"]}
{"id": "2048562911", "references": ["2046166587", "1854214752", "2173213060", "2138621811", "2015191210", "1812636409", "1981420413", "3013264884", "2108991785", "102708294"], "title": "Searching and browsing Linked Data with SWSE: The Semantic Web Search Engine", "abstract": "In this paper, we discuss the architecture and implementation of the Semantic Web Search Engine (SWSE). Following traditional search engine architecture, SWSE consists of crawling, data enhancing, indexing and a user interface for search, browsing and retrieval of information; unlike traditional search engines, SWSE operates over RDF Web data - loosely also known as Linked Data - which implies unique challenges for the system design, architecture, algorithms, implementation and user interface. In particular, many challenges exist in adopting Semantic Web technologies for Web data: the unique challenges of the Web - in terms of scale, unreliability, inconsistency and noise - are largely overlooked by the current Semantic Web standards. Herein, we describe the current SWSE system, initially detailing the architecture and later elaborating upon the function, design, implementation and performance of each individual component. In so doing, we also give an insight into how current Semantic Web standards can be tailored, in a best-effort manner, for use on Web data. Throughout, we offer evaluation and complementary argumentation to support our design choices, and also offer discussion on future directions and open research questions. Later, we also provide candid discussion relating to the difficulties currently faced in bringing such a search engine into the mainstream, and lessons learnt from roughly six years working on the Semantic Web Search Engine project.", "citation_count": "123", "reference_count": "296", "date": "2011", "authors": ["Aidan Hogan", "Andreas Harth", "J\u00fcrgen Umbrich", "Sheila Kinsella", "Axel Polleres", "Stefan Decker"], "related_topics": ["Semantic Web Stack", "Social Semantic Web", "Data Web", "Semantic Web", "Semantic search", "Web modeling", "Web standards", "Web design", "World Wide Web", "Information retrieval", "Computer science"]}
{"id": "2037959956", "references": ["3100307207", "2151259137", "2404098708", "1601432161", "2126631960", "1987380777", "1483313504", "1660390307", "2306119308"], "title": "Selected Studies of the Principle of Relative Frequency in Language", "abstract": "", "citation_count": "0", "reference_count": "1,103", "date": "2014", "authors": ["George Kingsley Zipf"], "related_topics": ["Frequency", "Linguistics", "Natural language processing", "Mathematics", "Artificial intelligence"]}
{"id": "2007541434", "references": ["2136931666", "2144885342", "2065304353", "2112090702", "1976969221", "3013264884", "2008620264", "2769133055", "2038195874", "2124637492"], "title": "Growing network with local rules: preferential attachment, clustering hierarchy, and degree correlations.", "abstract": "The linear preferential attachment hypothesis has been shown to be quite successful in explaining the existence of networks with power-law degree distributions. It is then quite important to determine if this mechanism is the consequence of a general principle based on local rules. In this work it is claimed that an effective linear preferential attachment is the natural outcome of growing network models based on local rules. It is also shown that the local models offer an explanation for other properties like the clustering hierarchy and degree correlations recently observed in complex networks. These conclusions are based on both analytical and numerical results for different local rules, including some models already proposed in the literature.", "citation_count": "79", "reference_count": "769", "date": "2003", "authors": ["Alexei V\u00e1zquez"], "related_topics": ["Preferential attachment", "Complex network", "Triadic closure", "Cluster analysis", "Hierarchy (mathematics)", "Network model", "Degree (graph theory)", "Outcome (game theory)", "Statistical physics", "Mathematics"]}
{"id": "166263196", "references": ["2023624732", "2038526807", "2340309946", "255396808", "2154627749", "2132285835", "1483313504", "1660390307", "2020795663", "2104588805"], "title": "Collection Selection via Lexicon Inspection", "abstract": "", "citation_count": "0", "reference_count": "24", "date": "1997", "authors": ["Justin Zobel"], "related_topics": ["Lexicon", "Information retrieval", "Computer science", "Collection selection", "Text database"]}
{"id": "1499900670", "references": ["2159591770", "585519466", "2114851258", "2074394031", "115846232", "2144125830", "1483313504", "1660390307", "2339276216", "2104353650"], "title": "Unsupervised audio stream segmentation and clustering via the Bayesian information criterion", "abstract": "", "citation_count": "0", "reference_count": "157", "date": "2000", "authors": ["Bowen Zhou", "John H. L. Hansen"], "related_topics": ["Determining the number of clusters in a data set", "Cluster analysis", "Bayesian information criterion", "Segmentation", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2122962290", "references": ["2136864990", "2533058580", "2142901448", "1993690617", "2077770566", "2107745473"], "title": "Compression of individual sequences via variable-rate coding", "abstract": "Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequence x a quantity \\rho(x) is defined, called the compressibility of x , which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved for x by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of \\rho(x) allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences.", "citation_count": "6", "reference_count": "5,350", "date": "1978", "authors": ["J. Ziv", "A. Lempel"], "related_topics": ["Shannon\u2013Fano coding", "Variable-length code", "Tunstall coding", "Context-adaptive binary arithmetic coding", "Information theory", "Harmonic Vector Excitation Coding", "Sequence", "Upper and lower bounds", "Discrete mathematics", "Mathematics"]}
{"id": "2107745473", "references": ["1992371956", "2141463438", "1995000717", "2109227390", "2131024393", "2142901448", "2094396702", "2126590220", "2077770566", "2159253410"], "title": "A universal algorithm for sequential data compression", "abstract": "A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.", "citation_count": "10", "reference_count": "8,276", "date": "1977", "authors": ["J. Ziv", "A. Lempel"], "related_topics": ["Data compression ratio", "Lossless compression", "Data compression", "Universal code", "Lempel\u2013Ziv\u2013Stac", "Incremental encoding", "Move-to-front transform", "Lempel\u2013Ziv\u2013Welch", "Prediction by partial matching", "Context tree weighting", "Dictionary coder", "Burrows\u2013Wheeler transform", "DEFLATE", "PackBits", "Algorithm", "LZ77 and LZ78", "Computer science"]}
{"id": "2138745909", "references": ["1553696291", "1601529450", "1523293200", "3162744413", "1610836425", "3017143921", "1547515408", "1513282898", "1520252399", "1602329118"], "title": "Data mining and knowledge discovery: making sense out of data", "abstract": "Current computing and storage technology is rapidly outstripping society's ability to make meaningful use of the torrent of available data. Without a concerted effort to develop knowledge discovery techniques, organizations stand to forfeit much of the value from the data they currently collect and store.", "citation_count": "14", "reference_count": "7,003", "date": "1996", "authors": ["U.M. Feyyad"], "related_topics": ["Knowledge extraction", "Software mining", "Data warehouse", "Data stream mining", "Knowledge acquisition", "Relational data mining", "Data science", "Computer science", "Data mining", "Value (ethics)", "Meaningful use"]}
{"id": "2095897464", "references": ["2999729612", "1634005169", "1493454437", "2049631158", "1577072181", "2073308541", "2024868117", "2101616188", "3017143921", "1575476631"], "title": "BIRCH: an efficient data clustering method for very large databases", "abstract": "Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle \"noise\" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.", "citation_count": "11", "reference_count": "6,437", "date": "1996", "authors": ["Tian Zhang", "Raghu Ramakrishnan", "Miron Livny"], "related_topics": ["CURE data clustering algorithm", "Data stream clustering", "Correlation clustering", "Cluster analysis", "Single-linkage clustering", "Canopy clustering algorithm", "Fuzzy clustering", "Clustering high-dimensional data", "DBSCAN", "Affinity propagation", "k-medians clustering", "Brown clustering", "FLAME clustering", "Consensus clustering", "OPTICS algorithm", "Data mining", "Computer science", "Database"]}
{"id": "2912565176", "references": ["2186428165", "2153233077", "2342408547", "2140190241", "2038420319", "2151498684", "2143451122", "2069754181", "1992419399"], "title": "Fuzzy sets", "abstract": "", "citation_count": "0", "reference_count": "67,237", "date": "1996", "authors": ["Lotfi A. Zadeh"], "related_topics": ["Fuzzy set operations", "Type-2 fuzzy sets and systems", "Defuzzification", "Fuzzy set", "Membership function", "Fuzzy classification", "Fuzzy number", "Vague set", "Computer science", "Data mining"]}
{"id": "2133671888", "references": ["137941959", "2128499899", "1547224907", "2286699414", "308480622"], "title": "Introduction To The Theory Of Neural Computation", "abstract": "From the Publisher:\r\nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest.", "citation_count": "5", "reference_count": "12,113", "date": "1991", "authors": ["John Hertz", "Anders Krogh", "Richard G. Palmer"], "related_topics": ["Nervous system network models", "Artificial neural network", "Unsupervised learning", "Content-addressable memory", "Models of neural computation", "Artificial intelligence", "Variety (cybernetics)", "Feed forward", "Attractor neural network"]}
{"id": "2046432185", "references": ["2017257315", "2110485445", "2001141328", "2025054170", "2095301394", "2173629880", "2070665556", "1553004968", "2078626246", "2160699933"], "title": "Learning the hidden structure of speech", "abstract": "In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95%. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.", "citation_count": "0", "reference_count": "429", "date": "1988", "authors": ["Jeffery Locke Elman", "David Zipser"], "related_topics": ["Feature (machine learning)", "Phonetics", "Pattern recognition (psychology)", "Backpropagation", "Set (psychology)", "Speech recognition", "Segmentation", "Structure (mathematical logic)", "Computer science"]}
{"id": "3036751298", "references": ["2105883124", "2555121662"], "title": "Parallel Networks that Learn to Pronounce English Text", "abstract": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations.", "citation_count": "0", "reference_count": "2,693", "date": "1989", "authors": ["T. J. Sejnowski"], "related_topics": ["NETtalk", "Speech synthesis", "Network model", "Invariant (computer science)", "Task (computing)", "Speech recognition", "Class (computer programming)", "Hierarchical clustering", "Computer science"]}
{"id": "2122988375", "references": ["1997063559", "2154642048", "1652505363", "1991848143", "2266946488", "2177721432", "2002089154", "1708874574", "2293063825", "22297218"], "title": "On the proper treatment of connectionism", "abstract": "A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models.", "citation_count": "150", "reference_count": "3,095", "date": "1993", "authors": ["Paul Smolensky"], "related_topics": ["Cognitive model", "Connectionism", "Dynamicism", "Dynamical systems theory", "Language of thought hypothesis", "Information processing", "Massively parallel", "Set (psychology)", "Cognitive science", "Psychology"]}
{"id": "2118373646", "references": ["2144862731", "2912225506", "1652505363", "2112325651", "2158365276", "2122988375", "1529681538", "2094249282", "2170716495", "2083137466"], "title": "Connectionism and cognitive architecture: a critical analysis", "abstract": "Abstract   This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a \u2018language of thought\u2019: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the \u2018systematicity\u2019 of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or \u2018abstract neurological\u2019) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation.", "citation_count": "50", "reference_count": "5,487", "date": "1988", "authors": ["Jerry A. Fodor", "Zenon W. Pylyshyn"], "related_topics": ["Cognitive architecture", "Language of thought hypothesis", "Cognitive model", "Connectionism", "Mental representation", "Dynamicism", "Cognition", "Cognitive science", "Representation (arts)", "Cognitive psychology", "Psychology"]}
{"id": "2094249282", "references": ["2152824855", "2110485445", "2067551521", "1562911371", "2016429292", "2161070585", "1498878034", "1928882148", "1732736211", "2002103405"], "title": "Language learnability and language development", "abstract": "Language learnability and language devlopment revisited the acquisition theory - assumptions and postulates phrase structure rules phrase stucture rules - developmental considerations inflection complementation and control auxiliaries lexical entries and lexical rules.", "citation_count": "0", "reference_count": "2,926", "date": "1984", "authors": ["Steven Pinker"], "related_topics": ["Learnability", "Phrase structure rules", "Object language", "Language construct", "Developmental linguistics", "Phrase", "Universal Networking Language", "Language identification", "Linguistics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1964564149", "references": ["2137902979", "2127218465", "1600264885", "1537293292", "2026896213", "2106879254", "1966285605", "2018802931", "2110256578", "351767304"], "title": "Monitors, messages, and clusters: the p4 parallel programming system", "abstract": "p4 is a portable library of C and Fortran subroutines for programming parallel computers. It is the current version of a system that has been in use since 1984. It includes features for explicit parallel programming of shared-memory machines, distributed-memory machines (including heterogeneous networks of workstations), and clusters, by which we mean shared-memory multiprocessors communicating via message passing. We discuss here the design goals, history, and system architecture of p4 and describe briefly a diverse collection of applications that have demonstrated the utility of p4.", "citation_count": "17", "reference_count": "386", "date": "1994", "authors": ["Ralph M. Butler", "Ewing L. Lusk"], "related_topics": ["Procedural programming", "Subroutine", "Message passing", "Fortran", "Systems architecture", "Workstation", "Heterogeneous network", "Operating system", "Parallel computing", "Computer science"]}
{"id": "2083200599", "references": ["2108000325", "1998657709", "607187418", "3161155143", "2037535386"], "title": "Improved parallel I/O via a two-phase run-time access strategy", "abstract": "As scientists expand their models to describe physical phenomena of increasingly large extent, I/O becomes crucial and a system with limited I/O capacity can severely constrain the performance of the entire program.We provide experimental results, performed on an lntel Touchtone Delta and nCUBE 2 I/O system, to show that the performance of existing parallel I/O systems can vary by several orders of magnitude as a function of the data access pattern of the parallel program. We then propose a two-phase access strategy, to be implemented in a runtime system, in which the data distribution on computational nodes is decoupled from storage distribution. Our experimental results show that performance improvements of several orders of magnitude over direct access based data distribution methods can be obtained, and that performance for most data access patterns can be improved to within a factor of 2 of the best performance. Further, the cost of redistribution is a very small fraction of the overall access cost.", "citation_count": "5", "reference_count": "385", "date": "1993", "authors": ["Juan Miguel del Rosario", "Rajesh Bordawekar", "Alok Choudhary"], "related_topics": ["Parallel I/O", "Data access", "Runtime system", "Orders of magnitude (bit rate)", "Parallel computing", "Function (mathematics)", "Computer science", "Fraction (mathematics)", "Factor (programming language)", "Phase (waves)"]}
{"id": "2090683636", "references": ["2156359540", "1567735309", "2080413856", "2117645881", "1978513924", "2001948415", "2115394931", "2144413105", "3141437129", "2131610641"], "title": "Server-Directed Collective I/O in Panda", "abstract": "We present the architecture and implementation results for Panda 2.0, a library for input and output of multidimensional arrays on parallel and sequential platforms. Panda achieves remarkable performance levels on the IBM SP2, showing excellent scalability as data size increases and as the number of nodes increases, and provides throughputs close to the full capacity of the AIX file system on the SP2 we used. We argue that this good performance can be traced to Panda's use of server-directed i/o (a logical-level version of disk-directed i/o [Kotz94b]) to perform array i/o using sequential disk reads and writes, a very high level interface for collective i/o requests, and built-in facilities for arbitrary rearrangements of arrays during i/o. Other advantages of Panda's approach are ease of use, easy application portability, and a reliance on commodity system software.", "citation_count": "16", "reference_count": "325", "date": "1995", "authors": ["K. E. Seamons", "Y. Chen", "P. Jones", "J. Jozwiak", "M. Winslett"], "related_topics": ["Parallel I/O", "File system", "Interface (computing)", "Scalability", "Supercomputer", "Application software", "System software", "Concurrent computing", "Operating system", "Parallel computing", "Computer science"]}
{"id": "2010269868", "references": ["2086507055", "2111820660", "2001946794", "2026896213", "90802191", "1971077560", "2155066383", "2051975225", "2106918918", "2099159950"], "title": "The design and evolution of Zipcode", "abstract": "Abstract   Zipcode is a message-passing and process-management system that was designed for multicomputers and homogeneous networks of computers in order to support libraries and large-scale multicomputer software. The system has evolved significantly over the last five years, based on our experiences and identified needs. Features of Zipcode that were originally unique to it, were its simulataneous support of static process groups, communication contexts, and virtual topologies, forming the \u2018mailer\u2019 data structure. Point-to-point and collective operations reference the underlying group, and use contexts to avoid mixing up messages. Recently, we have added \u2018gather-send\u2019 and \u2018receive-scatter\u2019 semantics, based on persistent Zipcode \u2018invoices\u2019, both as a means to simplify message passing, and as a means to reveal more potential runtime optimizations. Key features in Zipcode appear in the forthcoming MPI standard.", "citation_count": "21", "reference_count": "75", "date": "1994", "authors": ["Anthony Skjellum", "Steven G. Smith", "Nathan E. Doss", "Alvin P. Leung", "Manfred Morari"], "related_topics": ["Message passing", "Context (language use)", "Software portability", "Data structure", "Data type", "Interface (Java)", "Semantics (computer science)", "Computer science", "Programming language", "Distributed computing", "Network topology"]}
{"id": "2010542899", "references": ["2902573062", "2132572305", "2156704873", "2793414965", "2167536864", "2149538934", "1575350781", "2072339136", "2024586441", "2098228070"], "title": "Integrated Pvm Framework Supports Heterogeneous Network Computing", "abstract": "", "citation_count": "0", "reference_count": "276", "date": "1993", "authors": ["Jack Dongarra", "G. A. Geist", "Robert Manchek", "V. S. Sunderam"], "related_topics": ["Grid computing", "Network architecture", "Intelligent computer network", "Network simulation", "Heterogeneous network", "Data diffusion machine", "Distributed computing", "Computer architecture", "Computer science"]}
{"id": "1978513924", "references": ["2168395296", "1978513924", "1967141605", "2098815550", "2150864656", "2041330544", "2147504831", "2114167330", "1496940124", "2157219191"], "title": "Disk-directed I/O for MIMD multiprocessors", "abstract": "Many scientific applications that run on today's multiprocessors, such as weather forecasting and seismic analysis, are bottlenecked by their file-I/O needs. Even if the multiprocessor is configured with sufficient I/O hardware, the file system software often fails to provide the available bandwidth to the application. Although libraries and enhanced file system interfaces can make a significant improvement, we believe that fundamental changes are needed in the file server software. We propose a new technique, disk-directed I/O, to allow the disk servers to determine the flow of data for maximum performance. Our simulations show that tremendous performance gains are possible both for simple reads and writes and for an out-of-core application. Indeed, our disk-directed I/O technique provided consistent high performance that was largely independent of data distribution and obtained up to 93% of peak disk bandwidth. It was as much as 18 times faster than either a typical parallel file system or a two-phase-I/O library.", "citation_count": "73", "reference_count": "714", "date": "1997", "authors": ["David Kotz"], "related_topics": ["Parallel I/O", "Device file", "File system", "File server", "Server", "MIMD", "Bandwidth (computing)", "Multiprocessing", "Parallel computing", "Computer science"]}
{"id": "1521571223", "references": ["2090409324", "2121082877", "50182133", "3003257820", "1575350781", "1850405760", "2148542244", "1573548168", "2138180780", "2140300123"], "title": "The High Performance Fortran Handbook", "abstract": "From the Publisher:\r\nHigh Performance Fortran (HPF) is a set of extensions to Fortran expressing parallel execution at a relatively high level. For the thousands of scientists, engineers, and others who wish to take advantage of the power of both vector and parallel supercomputers, five of the principal authors of HPF have teamed up here to write a tutorial for the language.\r\nThere is an increasing need for a common parallel Fortran that can serve as a programming interface with the new parallel machines that are appearing on the market. While HPF does not solve all the problems of parallel programming, it does provide a portable, high-level expression for data- parallel algorithms that brings the convenience of sequential Fortran a step closer to today's complex parallel machines.", "citation_count": "0", "reference_count": "1,078", "date": "1993", "authors": ["Charles H. Koelbel", "David B. Loveman", "Robert S. Schreiber", "Guy L. Steele", "Mary E. Zosel"], "related_topics": ["High Performance Fortran", "Parallel programming model", "Fortran", "Parallel algorithm", "Interface (Java)", "Programming language", "Parallel computing", "Expression (computer science)", "Set (abstract data type)", "Principal (computer security)", "Computer science"]}
{"id": "2294265735", "references": ["2152054146", "2116729357", "2274874950", "2159449639", "2134507521", "1993957940", "2037043104", "1931895810", "2103940294", "1972981988"], "title": "Visualization and debugging in a heterogeneous environment", "abstract": "The authors' experiences with visualization and debugging of parallel virtual machine (PVM) applications and two of the tools they have devised to facilitate these tasks are described. One of the tools is a graphical monitoring package called Xab that can visually display PVM activities inside an application running across a network. The other is a graphical programming environment called Hence, which helps the user write, compile, execute, and trace heterogeneous distributed programs. The authors discuss their early work, the present research, and the future directions of these experimental projects. &gt;", "citation_count": "0", "reference_count": "212", "date": "1995", "authors": ["Adam Beguelin", "Jack Dongarra", "Al Geist", "Vaidy Sunderam"], "related_topics": ["Debugging", "TRACE (psycholinguistics)", "Visualization", "Virtual machine", "Visual programming language", "Compiler", "Programming language", "Computer science"]}
{"id": "1480928214", "references": ["2116773335", "2102653059", "2128853364", "2035080386", "2124007994", "2842089854", "2124313187", "1575350781", "2120145199", "2141704677"], "title": "Lapack Users' Guide", "abstract": "Preface to the third edition Preface to the secondedition Part 1. Guide. 1. Essentials 2. Contents of LAPACK 3. Performance of LAPACK 4. Accuracy and Stability 5. Documentation and Software Conventions 6. Installing LAPACK Routines 7. Troubleshooting Appendix A. Index of Driver and Computational Routines Appendix B. Index of Auxiliary Routines Appendix C. Quick Reference Guide to the BLAS Appendix D. Converting from LINPACK or EISPACK Appendix E. LAPACK Working Notes Part 2. Specifications of Routines. Bibliography Index by Keyword Index by Routine Name.", "citation_count": "0", "reference_count": "9,659", "date": "1995", "authors": ["Ed Anderson"], "related_topics": ["ScaLAPACK", "EISPACK", "Index (publishing)", "Troubleshooting", "Software", "Documentation", "Programming language", "Computer science", "Operations research"]}
{"id": "1843937266", "references": ["2111820660", "2108167744", "90802191", "1532915129", "2142471664", "2006774019", "2161307885", "2099159950", "1538115971", "2116524044"], "title": "A Proposal for a User-Level, Message-Passing Interface in a Distributed Memory Environment", "abstract": "This paper describes Message Passing Interface 1 (MPI1), a proposed library interface standard for supporting point-to-point message passing. The intended standard will be provided with Fortran 77 and C interfaces, and will form the basis of a standard high level communication environment featuring collective communication and data distribution transformations. The standard proposed here provides blocking and nonblocking message passing between pairs of processes, with message selectivity by source process and message type. Provision is made for noncontiguous messages. Context control provides a convenient means of avoiding message selectivity conflicts between different phases of an application. The ability to form and manipulate process groups permit task parallelism to be exploited, and is a useful abstraction in controlling certain types of collective communication.", "citation_count": "14", "reference_count": "135", "date": "1993", "authors": ["Jack J. Dongarra", "Rolf Hempel", "Anthony J.G. Hey", "David W. Walker"], "related_topics": ["Message broker", "Message passing", "Message Passing Interface", "Interface standard", "Task parallelism", "Distributed memory", "Context (language use)", "Blocking (computing)", "Distributed computing", "Computer science"]}
{"id": "2099345940", "references": ["1521239006", "193579291", "1990822176", "1966812932", "2581275558", "1995564620", "2079145130", "2120234416", "1990005915"], "title": "A tree-based statistical language model for natural language speech recognition", "abstract": "The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior. &gt;", "citation_count": "9", "reference_count": "484", "date": "1989", "authors": ["L.R. Bahl", "P.F. Brown", "P.V. de Souza", "R.L. Mercer"], "related_topics": ["Incremental decision tree", "Language model", "Vocabulary", "Tree (data structure)", "Trigram", "Word (computer architecture)", "Decision tree", "Binary decision diagram", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "1976241232", "references": ["62444100", "2152428909", "2055528812", "1571096757", "2109621219", "2093099215"], "title": "Tagging text with a probabilistic model", "abstract": "Experiments on the use of a probabilistic model to tag English text, that is, to assign to each word the correct tag (part of speech) in the context of the sentence, are presented. A simple triclass Markov model is used, and the best way to estimate the parameters of this model, depending on the kind and amount of training data that is provided, is found. Two approaches are compared: the use of text that has been tagged by hand and comparing relative frequency counts; and use text without tags and training the model as a hidden Markov process, according to a maximum likelihood principle. Experiments show that the best training is obtained by using as much tagged text as is available, a maximum likelihood training may improve the accuracy of the tagging. &gt;", "citation_count": "6", "reference_count": "117", "date": "1991", "authors": ["B. Merialdo"], "related_topics": ["Markov model", "Hidden Markov model", "Markov process", "Statistical model", "Estimation theory", "Viterbi algorithm", "Context model", "Context (language use)", "Sentence", "Pattern recognition", "Computer science", "Artificial intelligence", "Maximum likelihood", "Training set"]}
{"id": "2167434254", "references": ["1535681052", "1973021928", "1859173823", "2127009519", "2012837062", "2029825515", "2008506796", "2099247782", "2121227244", "2021758792"], "title": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing", "abstract": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.", "citation_count": "12", "reference_count": "227", "date": "1993", "authors": ["Ezra Black", "Fred Jelinek", "John Lafrerty", "David M. Magerman", "Robert Mercer", "Salim Roukos"], "related_topics": ["Parse tree", "Top-down parsing", "Parsing", "Treebank", "Rule-based machine translation", "Generative grammar", "Probabilistic logic", "Syntax", "Natural language", "Sentence", "Grammar", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1904457459", "references": ["1586176709"], "title": "A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)", "abstract": "Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a \"voting\" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or \"voting\" process that selects the output sequence with the lowest score.", "citation_count": "1", "reference_count": "1,792", "date": "1997", "authors": ["J.G. Fiscus"], "related_topics": ["Word error rate", "NIST", "Language model", "Word (computer architecture)", "Process (computing)", "Speech recognition", "Dynamic programming", "Voting", "Computer science", "Sequence"]}
{"id": "1528470941", "references": ["1990387894", "2144920829", "1966812932", "2166637769", "3017143921"], "title": "Explicit word error minimization in N-Best list rescoring", "abstract": "We show that the standard hypothesis scoring paradigm used in maximum-likelihood-based speech recognition systems is not optimal with regard to minimizing the word error rate, the commonly used performance metric in speech recognition. This can lead to sub-optimal performance, especially in high-error-rate environments where word error and sentence error are not necessarily monotonically related. To address this discrepancy, we developed a new algorithm that explicitly minimizes expected word error for recognition hypotheses. First, we approximate the posterior hypothesis probabilities using N-best lists. We then compute the expected word error for each hypothesis with respect to the posterior distribution, and choose the hypothesis with the lowest error. Experiments show improved recognition rates on two spontaneous speech corpora.", "citation_count": "5", "reference_count": "227", "date": "1997", "authors": ["Andreas Stolcke", "Yochai Konig", "Mitchel Weintraub"], "related_topics": ["Word error rate", "Word (computer architecture)", "Posterior probability", "Sentence", "Speech recognition", "Minification", "Computer science"]}
{"id": "2594610113", "references": ["2125529971", "173561343", "1904457459", "1571931074", "1966812932", "1528470941", "2097978681", "3017143921", "2129334286", "55333121"], "title": "Finding consensus in speech recognition: word error minimization and other applications of confusion networks\u2606", "abstract": "We describe a new framework for distilling information from word lattices to improve the accuracy of the speech recognition output and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of a set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.", "citation_count": "17", "reference_count": "889", "date": "2000", "authors": ["Lidia Mangu", "Eric Brill", "Andreas Stolcke"], "related_topics": ["Word error rate", "Language model", "Posterior probability", "Sentence", "Decoding methods", "Speech recognition", "Small set", "Natural language processing", "Minification", "Performance metric", "Computer science", "Artificial intelligence"]}
{"id": "2097978681", "references": ["173561343", "2144920829", "1904457459", "2121464381", "1528470941", "2104663520", "2113641473", "1797288984", "2121227244", "2082474452"], "title": "THE SRI MARCH 2000 HUB-5 CONVERSATIONAL SPEECH TRANSCRIPTION SYSTEM", "abstract": "We describe SRI\u2019s large vocabulary conversational speech r ecognition system as used in the March 2000 NIST Hub-5E evaluation. The system performs four recognition passes: (1) bigram recognition with phone-loop-adapted, within-word triphone acoustic models, (2) lattice generation with transcription-mod e-adapted models, (3) trigram lattice recognition with adapted cross -word triphone models, and (4) N-best rescoring and reranking with various additional knowledge sources. The system incorporates two new kinds of acoustic model: triphone models conditioned on speaking rate, and an explicit joint model of within-word phone durations. We also obtained an unusually large improvement from modeling crossword pronunciation variants in \u201cmultiword\u201d vocabulary items. The language model (LM) was enhanced with an \u201canti-LM\u201d representing acoustically confusable word sequences. Finally, we applied a generalized ROVER algorithm to combine the N-best hypotheses from several systems based on different acoustic models.", "citation_count": "18", "reference_count": "189", "date": "2000", "authors": ["A. Stolcke", "H. Bratt", "J. Butzberger", "H. Franco", "V. R. Rao Gadde", "C. Richey", "E. Shriberg", "F. Weng", "J. Zheng"], "related_topics": ["Triphone", "Acoustic model", "Bigram", "Language model", "Trigram", "Vocabulary", "Pronunciation", "Transcription (software)", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1797288984", "references": ["2611071497", "1757803263", "32217939", "2099111195", "2082092506", "2162995740", "2134237567", "1903115690", "2157140289"], "title": "Entropy-based Pruning of Backoff Language Models", "abstract": "A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance.", "citation_count": "9", "reference_count": "424", "date": "2000", "authors": ["Andreas Stolcke"], "related_topics": ["Maximum entropy probability distribution", "Entropy (energy dispersal)", "Entropy (information theory)", "Kullback\u2013Leibler divergence", "Binary entropy function", "Entropy (classical thermodynamics)", "Entropy (statistical thermodynamics)", "Perplexity", "Entropy (arrow of time)", "Language model", "Algorithm", "Pattern recognition", "Computer science", "Entropy (order and disorder)", "Artificial intelligence", "Training set"]}
{"id": "2127836646", "references": ["2076639289", "1521239006", "2105594594", "2751601659", "1966812932", "340893908", "2055528812", "1597533204", "2159782014", "1507680813"], "title": "A cache-based natural language model for speech recognition", "abstract": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. &gt;", "citation_count": "10", "reference_count": "757", "date": "1990", "authors": ["R. Kuhn", "R. De Mori"], "related_topics": ["Cache language model", "Language model", "Cache", "Natural language", "Markov model", "CPU cache", "String (computer science)", "Word (computer architecture)", "Vocabulary", "Terminology", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2100506586", "references": ["1632114991", "1508165687", "2093390569", "2147152072", "2160842254", "2158195707", "2096175520", "2038721957", "2121227244", "1594031697"], "title": "Two decades of statistical language modeling: where do we go from here?", "abstract": "Statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.", "citation_count": "77", "reference_count": "919", "date": "2000", "authors": ["R. Rosenfeld"], "related_topics": ["Language identification", "Language model", "Computational linguistics", "Natural language", "Natural language processing", "Bayesian probability", "Computer science", "Point (typography)", "State (computer science)", "Artificial intelligence", "Statistical analysis"]}
{"id": "2155653793", "references": ["2154642048", "2125055259", "2117897510", "1587362683", "1770825568", "2135346934", "1784695092", "2102150307", "2157825442", "1594031697"], "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms", "abstract": "In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six ''real world'' medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for ''single number'' evaluation of machine learning algorithms.", "citation_count": "34", "reference_count": "5,679", "date": "1997", "authors": ["Andrew P. Bradley"], "related_topics": ["Receiver operating characteristic", "Perceptron", "Cross-validation", "Standard error", "Machine learning", "Invariant (mathematics)", "Algorithm", "Classifier (UML)", "Computer science", "Analysis of variance", "Artificial intelligence"]}
{"id": "2002016471", "references": ["2042264548", "1652505363", "23758216", "26450609", "1554576613", "2147800946", "2293063825", "3121926921", "2133671888", "22297218"], "title": "Artificial neural networks: a tutorial", "abstract": "Artificial neural nets (ANNs) are massively parallel systems with large numbers of interconnected simple processors. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model. It outlines network architectures and learning processes, and presents some of the most commonly used ANN models. It concludes with character recognition, a successful ANN application.", "citation_count": "20", "reference_count": "3,454", "date": "1996", "authors": ["A.K. Jain", "Jianchang Mao", "K.M. Mohiuddin"], "related_topics": ["Artificial neural network", "Network architecture", "Artificial intelligence", "Machine learning", "Concurrent computing", "Computer science", "Parallel processing (DSP implementation)"]}
{"id": "2148138104", "references": ["2042264548", "2154642048", "1825077972", "2110485445", "3102923851", "2155482699", "2148694408", "2029949252", "1554576613", "1498436455"], "title": "Neural Network Toolbox\u2122 User's Guide", "abstract": "", "citation_count": "72", "reference_count": "1,480", "date": "2015", "authors": ["Mark Hudson Beale", "Martin T. Hagan", "Howard B. Demuth"], "related_topics": ["Time delay neural network", "Nervous system network models", "Physical neural network", "Computer science", "Artificial intelligence", "Neural network toolbox"]}
{"id": "1873332500", "references": ["1570448133", "1563088657", "2119479037", "2154642048", "2156909104", "2125055259", "2139212933", "1594031697", "2912934387", "1992419399"], "title": "Supervised Machine Learning: A Review of Classification Techniques", "abstract": "The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.", "citation_count": "111", "reference_count": "10,179", "date": "2007", "authors": ["Sotiris B. Kotsiantis"], "related_topics": ["Semi-supervised learning", "Linear classifier", "Supervised learning", "Online machine learning", "Unsupervised learning", "Learning to rank", "Learning classifier system", "Instance-based learning", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2158940042", "references": ["3132971798", "654435104", "1489213177", "2102201073", "1969423031", "2024599390", "2098914003", "2797502950", "2140832824", "2062024414"], "title": "Ideal spatial adaptation by wavelet shrinkage", "abstract": "SUMMARY With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log 2 n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone.", "citation_count": "15", "reference_count": "13,625", "date": "1994", "authors": ["David L Donoho", "Iain M Johnstone"], "related_topics": ["Piecewise", "Oracle", "Spline (mathematics)", "Wavelet", "Estimator", "Wavelet noise", "Multivariate normal distribution", "Decision theory", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2132680427", "references": ["59771946", "2035737465", "2031299600", "2109504624", "2033367330", "2158576618", "2066462711", "2134929491", "1485280399", "2107790757"], "title": "The curvelet transform for image denoising", "abstract": "We describe approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and the curvelet transform. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. A central tool is Fourier-domain computation of an approximate digital Radon transform. We introduce a very simple interpolation in the Fourier space which takes Cartesian samples and yields samples on a rectopolar grid, which is a pseudo-polar sampling set based on a concentric squares geometry. Despite the crudeness of our interpolation, the visual performance is surprisingly good. Our ridgelet transform applies to the Radon transform a special overcomplete wavelet pyramid whose wavelets have compact support in the frequency domain. Our curvelet transform uses our ridgelet transform as a component step, and implements curvelet subbands using a filter bank of a/spl grave/ trous wavelet filters. Our philosophy throughout is that transforms should be overcomplete, rather than critically sampled. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with \"state of the art\" techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features. Existing theory for curvelet and ridgelet transforms suggests that these new approaches can outperform wavelet methods in certain image reconstruction problems. The empirical results reported here are in encouraging agreement.", "citation_count": "26", "reference_count": "3,209", "date": "2002", "authors": ["Jean-Luc Starck", "E.J. Candes", "D.L. Donoho"], "related_topics": ["Curvelet", "Discrete wavelet transform", "Wavelet", "Harmonic wavelet transform", "Wavelet transform", "Stationary wavelet transform", "Constant Q transform", "Radon transform", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2151693816", "references": ["2165878107", "3132971798", "1970352604", "2156447271", "2913399920", "2080563952", "2091886411", "1996021349", "2798909945", "2062024414"], "title": "Matching pursuits with time-frequency dictionaries", "abstract": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). &gt;", "citation_count": "13", "reference_count": "11,971", "date": "1993", "authors": ["S.G. Mallat", "Zhifeng Zhang"], "related_topics": ["Matching pursuit", "Basis pursuit", "K-SVD", "Basis pursuit denoising", "Gabor atom", "Time\u2013frequency analysis", "Matching (statistics)", "Orthonormal basis", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2079724595", "references": ["191129667", "2158940042", "2132984323", "2102201073", "2054640142", "2098914003", "2024081693", "162854683", "2166982406", "2062024414"], "title": "Adapting to Unknown Smoothness via Wavelet Shrinkage", "abstract": "Abstract We attempt to recover a function of unknown smoothness from noisy sampled data. We introduce a procedure, SureShrink, that suppresses noise by thresholding the empirical wavelet coefficients. The thresholding is adaptive: A threshold level is assigned to each dyadic resolution level by the principle of minimizing the Stein unbiased estimate of risk (Sure) for threshold estimates. The computational effort of the overall procedure is order N \u00b7 log(N) as a function of the sample size N. SureShrink is smoothness adaptive: If the unknown function contains jumps, then the reconstruction (essentially) does also; if the unknown function has a smooth piece, then the reconstruction is (essentially) as smooth as the mother wavelet will allow. The procedure is in a sense optimally smoothness adaptive: It is near minimax simultaneously over a whole interval of the Besov scale; the size of this interval depends on the choice of mother wavelet. We know from a previous paper by the authors that traditional smoot...", "citation_count": "29", "reference_count": "6,855", "date": "1995", "authors": ["David L. Donoho", "Iain M. Johnstone"], "related_topics": ["Wavelet", "Thresholding", "Smoothness", "Function (mathematics)", "Minimax", "James\u2013Stein estimator", "Wavelet noise", "White noise", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2160547390", "references": ["1634005169", "2151693816", "2097323375", "2099641086", "2050834445", "2049633694", "2078204800", "2154332973", "2108384452", "2116148865"], "title": "$rm K$ -SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "abstract": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data", "citation_count": "44", "reference_count": "10,199", "date": "2006", "authors": ["M. Aharon", "M. Elad", "A. Bruckstein"], "related_topics": ["K-SVD", "Sparse approximation", "Matching pursuit", "Basis pursuit", "Vector quantization", "k-means clustering", "Cluster analysis", "Feature extraction", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2113945798", "references": ["2129276048", "2158940042", "2132984323", "2132680427", "2145889472", "2134929491", "2053691921", "2137234026", "2127006916", "1991605728"], "title": "Image denoising using scale mixtures of Gaussians in the wavelet domain", "abstract": "We describe a method for removing noise from digital images, based on a statistical model of the coefficients of an overcomplete multiscale oriented basis. Neighborhoods of coefficients at adjacent positions and scales are modeled as the product of two independent random variables: a Gaussian vector and a hidden positive scalar multiplier. The latter modulates the local variance of the coefficients in the neighborhood, and is thus able to account for the empirically observed correlation between the coefficient amplitudes. Under this model, the Bayesian least squares estimate of each coefficient reduces to a weighted average of the local linear estimates over all possible values of the hidden multiplier variable. We demonstrate through simulations with images contaminated by additive white Gaussian noise that the performance of this method substantially surpasses that of previously published methods, both visually and in terms of mean squared error.", "citation_count": "60", "reference_count": "2,959", "date": "2003", "authors": ["J. Portilla", "V. Strela", "M.J. Wainwright", "E.P. Simoncelli"], "related_topics": ["Mean squared error", "Additive white Gaussian noise", "Estimation theory", "Least squares", "Wavelet transform", "Wavelet", "Noise reduction", "Statistical model", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2146842127", "references": ["191129667", "3005363104", "654435104", "2109246257", "2050880896", "2107790757", "2033484654", "2098914003", "2079724595", "2152328854"], "title": "De-noising by soft-thresholding", "abstract": "Donoho and Johnstone (1994) proposed a method for reconstructing an unknown function f on [0,1] from noisy data d/sub i/=f(t/sub i/)+/spl sigma/z/sub i/, i=0, ..., n-1,t/sub i/=i/n, where the z/sub i/ are independent and identically distributed standard Gaussian random variables. The reconstruction f/spl circ/*/sub n/ is defined in the wavelet domain by translating all the empirical wavelet coefficients of d toward 0 by an amount /spl sigma//spl middot//spl radic/(2log (n)/n). The authors prove two results about this type of estimator. [Smooth]: with high probability f/spl circ/*/sub n/ is at least as smooth as f, in any of a wide variety of smoothness measures. [Adapt]: the estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. The present proof of these results develops new facts about abstract statistical inference and its connection with an optimal recovery model. &gt;", "citation_count": "32", "reference_count": "13,464", "date": "1995", "authors": ["D.L. Donoho"], "related_topics": ["Independent and identically distributed random variables", "Estimator", "Smoothness (probability theory)", "Random variable", "Function (mathematics)", "Gaussian", "Sigma", "Wavelet transform", "Discrete mathematics", "Mathematics", "Statistics"]}
{"id": "2117853077", "references": ["2049633694", "3129711340"], "title": "The EM algorithm and extensions", "abstract": "The first unified account of the theory, methodology, and applications of the EM algorithm and its extensionsSince its inception in 1977, the Expectation-Maximization (EM) algorithm has been the subject of intense scrutiny, dozens of applications, numerous extensions, and thousands of publications. The algorithm and its extensions are now standard tools applied to incomplete data problems in virtually every field in which statistical methods are used. Until now, however, no single source offered a complete and unified treatment of the subject.The EM Algorithm and Extensions describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. Employing numerous examples, Geoffrey McLachlan and Thriyambakam Krishnan examine applications both in evidently incomplete data situations-where data are missing, distributions are truncated, or observations are censored or grouped-and in a broad variety of situations in which incompleteness is neither natural nor evident. They point out the algorithm's shortcomings and explain how these are addressed in the various extensions.Areas of application discussed include: Regression Medical imaging Categorical data analysis Finite mixture analysis Factor analysis Robust statistical modeling Variance-components estimation Survival analysis Repeated-measures designs For theoreticians, practitioners, and graduate students in statistics as well as researchers in the social and physical sciences, The EM Algorithm and Extensions opens the door to the tremendous potential of this remarkably versatile statistical tool.", "citation_count": "2", "reference_count": "9,097", "date": "1996", "authors": ["Geoffrey J. McLachlan", "Thriyambakam Krishnan"], "related_topics": ["MM algorithm", "Statistical model", "Expectation\u2013maximization algorithm", "Field (computer science)", "Theoretical computer science", "Data mining", "Variety (cybernetics)", "Point (typography)", "Factor (programming language)", "Mathematics", "Finite mixture"]}
{"id": "581152777", "references": ["2505725817", "2567948266", "2092636373"], "title": "Recent results in estimation theory and related topics", "abstract": "", "citation_count": "0", "reference_count": "3", "date": "1984", "authors": ["Edward J. Dudewicz", "Detlef Plachky", "Pranab Kumar Sen"], "related_topics": ["Estimation theory", "Mathematics", "Econometrics"]}
{"id": "2024476015", "references": ["2117853077", "2045656233", "2610857016", "2106706098", "2033482494", "2017899835", "2154744699", "2049633694", "2082246284", "2148534890"], "title": "The EM Algorithm\u2014an Old Folk\u2010song Sung to a Fast New Tune", "abstract": "Celebrating the 20th anniversary of the presentation of the paper by Dempster, Laird and Rubin which popularized the EM algorithm, we investigate, after a brief historical account, strategies that aim to make the EM algorithm converge faster while maintaining its simplicity and stability (e.g. automatic monotone convergence in likelihood). First we introduce the idea of a \u2018working parameter\u2019 to facilitate the search for efficient data augmentation schemes and thus fast EM implementations. Second, summarizing various recent extensions of the EM algorithm, we formulate a general alternating expectation\u2013conditional maximization algorithm AECM that couples flexible data augmentation schemes with model reduction schemes to achieve efficient computations. We illustrate these methods using multivariate t-models with known or unknown degrees of freedom and Poisson models for image reconstruction. We show, through both empirical and theoretical evidence, the potential for a dramatic reduction in computational time with little increase in human effort. We also discuss the intrinsic connection between EM-type algorithms and the Gibbs sampler, and the possibility of using the techniques presented here to speed up the latter. The main conclusion of the paper is that, with the help of statistical considerations, it is possible to construct algorithms that are simple, stable and fast.", "citation_count": "110", "reference_count": "905", "date": "1997", "authors": ["Xiao-Li Meng", "David Van Dyk"], "related_topics": ["Stability (learning theory)", "Expectation\u2013maximization algorithm", "Gibbs sampling", "Reduction (complexity)", "Maximization", "Rate of convergence", "Convergence (routing)", "Algorithm", "Monotone polygon", "Mathematical optimization", "Mathematics"]}
{"id": "1991278573", "references": ["2038614136", "2010140249", "1981367467", "2113076747", "2043803464", "2049633694", "2131034055", "2118570622"], "title": "Another interpretation of the EM algorithm for mixture distributions", "abstract": "Abstract   The EM algorithm for mixture problems can be interpreted as a method of coordinate descent on a particular objective function. This view of the iteration partially illuminates the relationship of EM to certain clustering techniques and explains global convergence properties of the algorithm without direct reference to an incomplete data framework.", "citation_count": "8", "reference_count": "312", "date": "1986", "authors": ["Richard J. Hathaway"], "related_topics": ["Coordinate descent", "Expectation\u2013maximization algorithm", "Cluster analysis", "Convergence (routing)", "Interpretation (model theory)", "Algorithm", "Mathematical optimization", "Mathematics", "Maximum likelihood"]}
{"id": "1580495158", "references": ["2570764145", "1693345094", "2567948266", "2115415549", "1573546770", "1686266550", "2125587358", "2025653905"], "title": "Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures", "abstract": "In this thesis, we consider learning algorithms for neural networks which are based on fitting a mixture probability density to a set of data.\r\nWe begin with an unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms. Rather than updating only the parameters of the \"winner\" on each case, the parameters of all competitors are updated in proportion to their relative responsibility for the case. Use of such a \"soft\" competitive algorithm is shown to give better performance than the more traditional algorithms, with little additional cost.\r\nWe then consider a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task. A soft competitive mechanism is used to determine how much an expert learns on a case, based on how well the expert performs relative to the other expert networks. At the same time, a separate gating network learns to weight the output of each expert according to a prediction of its relative performance based on the input to the system. Experiments on a number of tasks illustrate that this architecture is capable of uncovering interesting task decompositions and of generalizing better than a single network with small training sets.\r\nFinally, we consider learning algorithms in which we assume that the actual output of the network should fall into one of a small number of classes or clusters. The objective of learning is to make the variance of these classes as small as possible. In the classical decision-directed algorithm, we decide that an output belongs to the class it is closest to and minimize the squared distance between the output and the center (mean) of this closest class. In the \"soft\" version of this algorithm, we minimize the squared distance between the actual output and a weighted average of the means of all of the classes. The weighting factors are the relative probability that the output belongs to each class. This idea may also be used to model the weights of a network, to produce networks which generalize better from small training sets.", "citation_count": "0", "reference_count": "180", "date": "1991", "authors": ["Steven J. Nowlan"], "related_topics": ["Competitive analysis", "Competitive learning", "Artificial neural network", "Weighting", "Set (abstract data type)", "Class (philosophy)", "Artificial intelligence", "Variance (accounting)", "Machine learning", "Algorithm", "Probability density function", "Computer science"]}
{"id": "2116013899", "references": ["132940180", "2141080522", "2096355125", "2042371054", "2150920547", "1490632837", "1971719398", "1995875735", "2134849909", "2162413715"], "title": "Texture synthesis by non-parametric sampling", "abstract": "A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.", "citation_count": "10", "reference_count": "4,212", "date": "1999", "authors": ["A.A. Efros", "T.K. Leung"], "related_topics": ["Texture atlas", "Image texture", "Texture synthesis", "Texture filtering", "Texture compression", "Bidirectional texture function", "Markov random field", "Feature detection (computer vision)", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2295936755", "references": ["2100415658", "2116013899", "2103559027", "2150134853", "1991113069", "1490632837", "2140865211", "2023292900", "2132363464", "2146052399"], "title": "Image inpainting", "abstract": "Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.", "citation_count": "16", "reference_count": "4,731", "date": "2000", "authors": ["Marcelo Bertalmio", "Guillermo Sapiro", "Vincent Caselles", "Coloma Ballester"], "related_topics": ["Inpainting", "Image restoration", "Texture synthesis", "Computer vision", "Anisotropic diffusion", "Computer graphics (images)", "Image (mathematics)", "Computer science", "Contrast (music)", "Artificial intelligence", "Isophote"]}
{"id": "2149760002", "references": ["1997063559", "2145889472", "2103504761", "2159080219", "1554663460", "2137234026", "1988520084", "2911709767", "1481420047", "1746680969"], "title": "Learning Low-Level Vision", "abstract": "We describe a learning-based method for low-level vision problems\u2014estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA\u2014Vision by Image/Scene TrAining.\r\n\r\nWe apply VISTA to the \u201csuper-resolution\u201d problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a \u201cblobs world\u201d, we show figure/ground discrimination, solution of the aperture problem, and filling-in arising from application of the same probabilistic machinery.", "citation_count": "47", "reference_count": "2,156", "date": "2000", "authors": ["William T. Freeman", "Egon C. Pasztor", "Owen T. Carmichael"], "related_topics": ["Motion estimation", "Probabilistic logic", "Belief propagation", "Posterior probability", "Bayesian probability", "Pattern recognition (psychology)", "Markov chain", "Computer vision", "Aperture", "Computer science", "Artificial intelligence"]}
{"id": "2138448681", "references": ["2057612871", "2126174118", "2160842254", "2146672645", "2098821691", "1654789990", "2119938170", "2116064496", "2143421693"], "title": "Learning Sparse Topographic Representations with Products of Student-t Distributions", "abstract": "We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the \"iterated Wiener filter\" for the purpose of denoising images.", "citation_count": "9", "reference_count": "192", "date": "2002", "authors": ["Max Welling", "Simon Osindero", "Geoffrey E. Hinton"], "related_topics": ["Filter (video)", "Wiener filter", "Orientation (computer vision)", "Topographic map", "Noise reduction", "Pattern recognition", "Product (mathematics)", "Spatial frequency", "Data mining", "Computer science", "Artificial intelligence"]}
{"id": "1813659000", "references": ["2145094598", "2163922914", "3096831136", "2147768505", "44815768", "189596042", "2099471712", "2116064496", "2072128103"], "title": "Information processing in dynamical systems: foundations of harmony theory", "abstract": "Abstract : At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or cognitive psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these methodologies combined. The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis. The success of cognitive science, like that of many other sciences, will, I believe, depend upon the construction of a solid body of theoretical results: results that express in a mathematical language the conceptual insights of the field; results that squeeze all possible implications out of those insights by exploiting powerful mathematical techniques. This body of results, which I will call the theory of information processing, exists because information is a concept that lends itself to mathematical formalization. One part of the theory of information processing is already well-developed. The classical theory of computation provides powerful and elegant results about the notion of effective procedure, including languages for precisely expressing them and theoretical machines for realizing them.", "citation_count": "0", "reference_count": "2,373", "date": "1986", "authors": ["P. Smolensky"], "related_topics": ["Information processing theory", "Cognitive models of information retrieval", "Information processing", "Language of mathematics", "Cognition", "Harmony (color)", "Dynamical systems theory", "Cognitive science", "Mathematical logic", "Management science", "Computer science"]}
{"id": "2140124448", "references": ["2104924585", "2108346334", "2130416410", "2154498027", "1880262756", "2057592151", "2001082470", "2107743791"], "title": "The author-topic model for authors and documents", "abstract": "We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output.", "citation_count": "8", "reference_count": "1,441", "date": "2004", "authors": ["Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth"], "related_topics": ["Dynamic topic model", "Latent Dirichlet allocation", "Topic model", "Dirichlet-multinomial distribution", "Pachinko allocation", "Generative model", "Multinomial distribution", "Gibbs sampling", "Information retrieval", "Computer science"]}
{"id": "145818128", "references": ["2946776431", "2116064496", "2102432043", "2108384452", "2137969290", "2138451337"], "title": "Diffusion Networks, Products of Experts, and Factor Analysis", "abstract": "Hinton (in press) recently proposed a learning algorithm called contrastive divergence learning for a class of probabilistic models called product of experts (PoE). Whereas in standard mixture models the \u201cbeliefs\u201d of individual experts are averaged, in PoEs the \u201cbeliefs\u201d are multiplied together and then renormalized. One advantage of this approach is that the combined beliefs can be much sharper than the individual beliefs of each expert. It has been shown that a restricted version of the Boltzmann machine, in which there are no lateral connections between hidden units or between observation units, is a PoE. In this paper we generalize these results to diffusion networks, a continuous-time, continuous-state version of the Boltzmann machine. We show that when the unit activation functions are linear, this PoE architecture is equivalent to a factor analyzer. This result suggests novel non-linear generalizations of factor analysis and independent component analysis that could be implemented using interactive neural circuitry.", "citation_count": "6", "reference_count": "29", "date": "2001", "authors": ["Tim K. Marks", "Javier R. Movellan"], "related_topics": ["Product of experts", "Boltzmann machine", "Mixture model", "Probabilistic logic", "Theoretical computer science", "Independent component analysis", "Machine learning", "Class (set theory)", "Computer science", "Factor (programming language)", "Diffusion (acoustics)", "Artificial intelligence"]}
{"id": "2109720450", "references": ["3121531027", "1669104078", "2567948266", "2148124601", "2110325612", "1806731464", "2151052953", "2118079529", "2116064496", "2070786785"], "title": "The multiple multiplicative factor model for collaborative filtering", "abstract": "We describe a class of causal, discrete latent variable models called Multiple Multiplicative Factor models (MMFs). A data vector is represented in the latent space as a vector of factors that have discrete, non-negative expression levels. Each factor proposes a distribution over the data vector. The distinguishing feature of MMFs is that they combine the factors' proposed distributions multiplicatively, taking into account factor expression levels. The product formulation of MMFs allow factors to specialize to a subset of the items, while the causal generative semantics mean MMFs can readily accommodate missing data. This makes MMFs distinct from both directed models with mixture semantics and undirected product models. In this paper we present empirical results from the collaborative filtering domain showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users.", "citation_count": "11", "reference_count": "97", "date": "2004", "authors": ["Benjamin Marlin", "Richard S. Zemel"], "related_topics": ["Collaborative filtering", "Latent variable", "Latent class model", "Missing data", "Expression (mathematics)", "Factor analysis", "Multinomial distribution", "Feature (machine learning)", "Theoretical computer science", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1934021597", "references": ["1575388622", "1560512119", "1515272691", "2046584898", "2141274633", "2084812512", "2098678088", "2120294885", "1559536185", "2137813581"], "title": "Expectation propagation for approximate Bayesian inference", "abstract": "This paper presents a new deterministic approximation technique in Bayesian networks. This method, \"Expectation Propagation,\" unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.", "citation_count": "15", "reference_count": "1,334", "date": "2001", "authors": ["Thomas P. Minka"], "related_topics": ["Expectation propagation", "Belief propagation", "Bayesian network", "Bayesian statistics", "Bayesian inference", "Variational message passing", "Mixture model", "Kalman filter", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2146766088", "references": ["2019357902"], "title": "Spline models for observational data", "abstract": "This book serves well as an introduction into the more theoretical aspects of the use of spline models. It develops a theory and practice for the estimation of functions from noisy data on functionals. The simplest example is the estimation of a smooth curve, given noisy observations on a finite number of its values. Convergence properties, data based smoothing parameter selection, confidence intervals, and numerical methods are established which are appropriate to a number of problems within this framework. Methods for including side conditions and other prior information in solving ill posed inverse problems are provided. Data which involves samples of random variables with Gaussian, Poisson, binomial, and other distributions are treated in a unified optimization context. Experimental design questions, i.e., which functionals should be observed, are studied in a general context. Extensions to distributed parameter system identification problems are made by considering implicitly defined functionals.", "citation_count": "1", "reference_count": "9,365", "date": "1990", "authors": ["Grace Wahba"], "related_topics": ["Smoothing spline", "Thin plate spline", "Smoothing", "Spline (mathematics)", "Additive smoothing", "Mathematical statistics", "Box spline", "Poisson distribution", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2038952578", "references": ["2042695730", "2151105625", "1863232835", "2007895855", "2096600681", "1696786548", "1512148629", "2132396702", "2128294289", "2160225702"], "title": "Active shape models\u2014their training and application", "abstract": "!, Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply modelbased methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our Active Shape Models can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images. Q 199s A&amp;&amp; prrss, IN.", "citation_count": "13", "reference_count": "9,968", "date": "1995", "authors": ["T. F. Cootes", "C. J. Taylor", "D. H. Cooper", "J. Graham"], "related_topics": ["Active shape model", "Active appearance model", "Active contour model", "Point distribution model", "Image processing", "Iterative refinement", "Robustness (computer science)", "Statistical shape analysis", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "2101522199", "references": ["1676820704", "2102734279", "2168228682", "2154579312", "2087347434", "1530454533", "2120240539", "2100659887", "1594031697", "2912934387"], "title": "Joint induction of shape features and tree classifiers", "abstract": "We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent.", "citation_count": "26", "reference_count": "267", "date": "1997", "authors": ["Y. Amit", "D. Geman", "K. Wilder"], "related_topics": ["Affine transformation", "Feature extraction", "Invariant (mathematics)", "Contextual image classification", "Word error rate", "Binary number", "Pattern recognition", "Salient", "Nonlinear system", "Mathematics", "Artificial intelligence"]}
{"id": "1593793857", "references": ["1997063559", "1549872659", "2143474538", "2143075689", "2166325326", "1516964807", "2138162238", "2797148637", "2155322595", "1498436455"], "title": "Local computations with probabilities on graphical structures and their application to expert systems", "abstract": "", "citation_count": "130", "reference_count": "5,409", "date": "1990", "authors": ["S. L. Lauritzen", "D. J. Spiegelhalter"], "related_topics": ["Expert system", "Variable elimination", "Probabilistic logic", "Computer science", "Polytree", "Theoretical computer science", "Junction tree algorithm", "Markov random field", "Computation", "Bayesian probability"]}
{"id": "158727920", "references": ["1965740984", "1980054641", "1972320590", "2079199322", "2016377072", "2035782089", "2013224592", "2018507693", "1976624377", "1965761421"], "title": "Judgment Under Uncertainty: Heuristics and Biases", "abstract": "This article described three heuristics that are employed in making judgements under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgements and decisions in situations of uncertainty.", "citation_count": "13", "reference_count": "45,112", "date": "1974", "authors": ["A. Tversky", "D. Kahneman"], "related_topics": ["Heuristics", "Social heuristics", "Representativeness heuristic", "Attribute substitution", "Cognitive Reflection Test", "Availability heuristic", "Affect heuristic", "Recognition heuristic", "Machine learning", "Econometrics", "Psychology", "Artificial intelligence"]}
{"id": "2108309071", "references": ["1549872659", "2038569232", "1983382292", "1562964770", "2144386448", "1998363560", "2032511848", "2155322595", "2140627345", "1520443109"], "title": "A theory of diagnosis from first principles", "abstract": "Suppose one is given a description of a system, together with an observation of the system's behaviour which conflicts with the way the system is meant to behave. The diagnostic problem is to determine those components of the system which, when assumed to be functioning abnormally, will explain the discrepancy between the observed and correct system behaviour. We propose a general theory for this problem. The theory requires only that the system be described in a suitable logic. Moreover, there are many such suitable logics, e.g. first-order, temporal, dynamic, etc. As a result, the theory accommodates diagnostic reasoning in a wide variety of practical settings, including digital and analogue circuits, medicine, and database updates. The theory leads to an algorithm for computing all diagnoses, and to various results concerning principles of measurement for discriminating among competing diagnoses. Finally, the theory reveals close connections between diagnostic reasoning and nonmonotonic reasoning.", "citation_count": "15", "reference_count": "4,582", "date": "1987", "authors": ["Raymond Reiter"], "related_topics": ["Non-monotonic logic", "Automated theorem proving", "Variety (cybernetics)", "Knowledge-based configuration", "Artificial intelligence", "Medical diagnosis", "Mathematics", "Analogue circuits", "Diagnostic reasoning", "General theory"]}
{"id": "2797148637", "references": ["1604936042", "1986721142", "2038420319", "2019950953", "2101840010", "2159080219", "2013093146", "2116817369", "2100235918", "2151376743"], "title": "A mathematical theory of evidence", "abstract": "Both in science and in practical affairs we reason by combining facts only inconclusively supported by evidence. Building on an abstract understanding of this process of combination, this book constructs a new theory of epistemic probability. The theory draws on the work of A. P. Dempster but diverges from Depster's viewpoint by identifying his \"lower probabilities\" as epistemic probabilities and taking his rule for combining \"upper and lower probabilities\" as fundamental. The book opens with a critique of the well-known Bayesian theory of epistemic probability. It then proceeds to develop an alternative to the additive set functions and the rule of conditioning of the Bayesian theory: set functions that need only be what Choquet called \"monotone of order of infinity.\" and Dempster's rule for combining such set functions. This rule, together with the idea of \"weights of evidence,\" leads to both an extensive new theory and a better understanding of the Bayesian theory. The book concludes with a brief treatment of statistical inference and a discussion of the limitations of epistemic probability. Appendices contain mathematical proofs, which are relatively elementary and seldom depend on mathematics more advanced that the binomial theorem.", "citation_count": "0", "reference_count": "20,987", "date": "1976", "authors": ["Glenn Shafer"], "related_topics": ["Dempster\u2013Shafer theory", "Upper and lower probabilities", "Mathematical theory", "Transferable belief model", "Probability interpretations", "Imprecise probability", "Statistical inference", "Possibility theory", "Mathematical economics", "Computer science"]}
{"id": "2138162238", "references": ["3149025087", "1964011508", "2145482038", "1684304711", "2075277371", "205043379", "3144889436", "2119409989", "2041833446", "2124141583"], "title": "Some philosophical problems from the standpoint of artificial intelligence", "abstract": "Abstract   A computer program capable of acting intelligently in the world must have a general representation of the world in terms of which its inputs are interpreted. Designing such a program requires commitments about what knowledge is and how it is obtained. Thus, some of the major traditional problems of philosophy arise in artificial intelligence.  More specifically, we want a computer program that decides what to do by inferring in a formal language that a certain strategy will achieve its assigned goal. This requires formalizing concepts of causality, ability, and knowledge. Such formalisms are also considered in philosophical logic.  The first part of the paper begins with a philosophical point of view that seems to arise naturally once we take seriously the idea of actually making an intelligent machine. We go on to the notions of metaphysically and epistemo-logically adequate representations of the world and then to an explanation of can, causes, and knows in terms of a representation of the world by a system of interacting automata. A proposed resolution of the problem of freewill in a deterministic universe and of counterfactual conditional sentences is presented.  The second part is mainly concerned with formalisms within which it can be proved that a strategy will achieve a goal. Concepts of situation, fluent, future operator, action, strategy, result of a strategy and knowledge are formalized. A method is given of constructing a sentence of first-order logic which will be true in all models of certain axioms if and only if a certain strategy will achieve a certain goal.  The formalism of this paper represents an advance over McCarthy (1963) and Green (1969) in that it permits proof of the correctness of strategies that contain loops and strategies that involve the acquisition of knowledge; and it is also somewhat more concise.  The third part discusses open problems in extending the formalism of part 2.  The fourth part is a review of work in philosophical logic in relation to problems of artificial intelligence and a discussion of previous efforts to program \u2018general intelligence\u2019 from the point of view of this paper.", "citation_count": "46", "reference_count": "5,488", "date": "1987", "authors": ["J. McCarthy", "P. J. Hayes"], "related_topics": ["Frame problem", "Action description language", "Situation calculus", "Philosophical logic", "Ramification problem", "Qualification problem", "Correctness", "Fluent calculus", "Computer science", "Artificial intelligence"]}
{"id": "2155322595", "references": ["1996347293", "2212036697", "2121773050", "1766332311", "1541540802", "2157368609", "2138162238", "2105486835", "2530006810", "2100738443"], "title": "A logic for default reasoning", "abstract": "The need to make default assumptions is frequently encountered in reasoning about incompletely specified worlds. Inferences sanctioned by default are best viewed as beliefs which may well be modified or rejected by subsequent observations. It is this property which leads to the non-monotonicity of any logic of defaults.\r\n\r\nIn this paper we propose a logic for default reasoning. We then specialize our treatment to a very large class of commonly occuring defaults. For this class we develop a complete proof theory and show how to interface it with a top down resolution theorem prover. Finally, we provide criteria under which the revision of derived beliefs must be effected.", "citation_count": "26", "reference_count": "6,744", "date": "1987", "authors": ["Raymond Reiter"], "related_topics": ["Default logic", "Non-monotonic logic", "Default rule", "Deductive reasoning", "Default argument", "Defeasible reasoning", "Autoepistemic logic", "Proof theory", "Mathematical economics", "Algorithm", "Mathematics"]}
{"id": "1986808060", "references": ["2129327240", "1588282782", "1977655452", "1939407039", "2033401964", "2101857176", "2159080219", "2042063943", "2133469585", "1484750607"], "title": "Decisions with Multiple Objectives: Preferences and Value Trade-Offs", "abstract": "Many of the complex problems faced by decision makers involve multiple conflicting objectives. This book describes how a confused decision maker, who wishes to make a reasonable and responsible choice among alternatives, can systematically probe his true feelings in order to make those critically important, vexing trade-offs between incommensurable objectives. The theory is illustrated by many real concrete examples taken from a host of disciplinary settings. The standard approach in decision theory or decision analysis specifies a simplified single objective like monetary return to maximise. By generalising from the single objective case to the multiple objective case, this book considerably widens the range of applicability of decision analysis.", "citation_count": "0", "reference_count": "19,552", "date": "1976", "authors": ["R. L. Keeney", "H. Raiffa", "David W. Rajala"], "related_topics": ["Decision analysis", "Decision engineering", "Decision theory", "Preference elicitation", "Management by objectives", "Multi-attribute utility", "Multicriteria classification", "Risk management", "Management science", "Computer science"]}
{"id": "1512098439", "references": ["1746819321", "2098941887", "2172000360", "2132870739", "2153635508", "2035720976", "2132549764", "2118978333", "1964357740", "2108995755"], "title": "Fast training of support vector machines using sequential minimal optimization", "abstract": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm.", "citation_count": "0", "reference_count": "7,780", "date": "1999", "authors": ["John C. Platt"], "related_topics": ["Sequential minimal optimization", "Support vector machine", "Relevance vector machine", "MNIST database", "Quadratic programming", "Structured support vector machine", "Vector optimization", "Optimization problem", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "1604938182", "references": ["2132870739", "2147238549", "2149298154", "2076063813", "2165966284", "1648445109", "2161920802", "1964357740", "2108995755", "2072128103"], "title": "Advances in kernel methods: support vector learning", "abstract": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.", "citation_count": "0", "reference_count": "5,767", "date": "1999", "authors": ["Bernhard Sch\u00f6lkopf", "Christopher J. C. Burges", "Alexander J. Smola"], "related_topics": ["Sequential minimal optimization", "Relevance vector machine", "Least squares support vector machine", "Margin classifier", "Kernel method", "Structured support vector machine", "Polynomial kernel", "Support vector machine", "Artificial intelligence", "Mathematics"]}
{"id": "2140095548", "references": ["2135463994", "2156909104", "2150796457", "2119821739", "2149298154", "2008056655", "2147800946", "2087347434", "2132549764", "2148694408"], "title": "Nonlinear component analysis as a kernel eigenvalue problem", "abstract": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map\u2014for instance, the space of all possible five-pixel products in 16 \u00d7 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.", "citation_count": "25", "reference_count": "9,387", "date": "1998", "authors": ["Bernhard Sch\u00f6lkopf", "Alexander Smola", "Klaus-Robert M\u00fcller"], "related_topics": ["Kernel principal component analysis", "Kernel method", "Polynomial kernel", "Kernel embedding of distributions", "k-nearest neighbors algorithm", "Kernel (statistics)", "Principal component regression", "Radial basis function kernel", "Algorithm", "Mathematics"]}
{"id": "2151040995", "references": ["2138907228", "2156909104", "2133458583", "1574862351", "2139212933", "1512098439", "2166282318", "2047542122", "2108995755", "2100038678"], "title": "Improvements to Platt's SMO Algorithm for SVM Classifier Design", "abstract": "This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried.", "citation_count": "16", "reference_count": "2,198", "date": "2001", "authors": ["S. S. Keerthi", "S. K. Shevade", "C. Bhattacharyya", "K. R. K. Murthy"], "related_topics": ["Sequential minimal optimization", "Support vector machine", "Karush\u2013Kuhn\u2013Tucker conditions", "Vector optimization", "Quadratic programming", "Minification", "Algorithm", "Lagrange multiplier", "Goal programming", "Mathematics"]}
{"id": "2172000360", "references": ["2119821739", "1576520375", "2124351082", "1512098439", "2153635508", "1602492977", "2084812512", "2148603752", "1543810117", "2282078507"], "title": "A comparison of methods for multiclass support vector machines", "abstract": "Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such \"all-together\" methods. We then compare their performance with three methods based on binary classifications: \"one-against-all,\" \"one-against-one,\" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the \"one-against-one\" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.", "citation_count": "24", "reference_count": "9,542", "date": "2002", "authors": ["Chih-Wei Hsu", "Chih-Jen Lin"], "related_topics": ["Multiclass classification", "Structured support vector machine", "Support vector machine", "Binary classification", "Directed acyclic graph", "Optimization problem", "Classifier (UML)", "Machine learning", "Learning automata", "Data mining", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2132870739", "references": ["2099111195", "2156909104", "1576520375", "1512098439", "1604938182", "2148603752", "2087347434", "2161920802", "2798766386", "2108995755"], "title": "Estimating the Support of a High-Dimensional Distribution", "abstract": "Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a \"simple\" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.", "citation_count": "56", "reference_count": "5,456", "date": "2001", "authors": ["Bernhard Sch\u00f6lkopf", "John C. Platt", "John C. Shawe-Taylor", "Alex J. Smola", "Robert C. Williamson"], "related_topics": ["Probability distribution", "Function (mathematics)", "Kernel (statistics)", "Quadratic programming", "Complement (set theory)", "Support vector machine", "Probability theory", "Feature vector", "Algorithm", "Mathematics"]}
{"id": "2109943925", "references": ["2156909104", "2119821739", "2118585731", "91932901", "2118286367", "2153635508", "2129191766", "2087347434", "2160072419", "2142334564"], "title": "A Practical Guide to Support Vector Classication", "abstract": "Support vector machine (SVM) is a popular technique for classication. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signicant steps. In this guide, we propose a simple procedure, which usually gives reasonable results.", "citation_count": "10", "reference_count": "7,927", "date": "2008", "authors": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"], "related_topics": ["Structured support vector machine", "Relevance vector machine", "Support vector machine", "Simple (abstract algebra)", "Machine learning", "Computer science", "Computer vision", "Artificial intelligence"]}
{"id": "169539560", "references": ["56903235", "19621276", "2606594511", "2173629880", "2153988646"], "title": "Generalization and network design strategies", "abstract": "", "citation_count": "5", "reference_count": "1,113", "date": "1989", "authors": ["Yann Lecun"], "related_topics": ["Generalization", "Network planning and design", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "56903235", "references": ["1666015432", "2154642048", "2137224975", "1991848143", "1964849666", "2581275558", "1505652865", "2011039300", "2895674046", "1978909760"], "title": "Large Automatic Learning, Rule Extraction, and Generalization.", "abstract": "", "citation_count": "12", "reference_count": "505", "date": "1987", "authors": ["John S. Denker", "Daniel B. Schwartz", "Ben S. Wittner", "Sara A. Solla", "Richard E. Howard", "Lawrence D. Jackel", "John J. Hopfield"], "related_topics": ["Generalization", "Computer science", "Machine learning", "Extraction (chemistry)", "Artificial intelligence", "Automatic learning"]}
{"id": "1965770722", "references": ["56903235", "1968908999", "2012903341", "2123838014", "2043014754", "2161278885", "1593125407", "2581275558", "2293063825", "2476694670"], "title": "Consistent inference of probabilities in layered networks: predictions and generalizations", "abstract": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example. &gt;", "citation_count": "12", "reference_count": "202", "date": "1989", "authors": ["Tishby", "Levin", "Solla"], "related_topics": ["Supervised learning", "Generalization", "Artificial neural network", "Inference", "Canonical ensemble", "Boltzmann distribution", "Algorithm", "Minification", "Learning curve", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2606594511", "references": ["169539560", "2141504882", "2971225054", "103531544", "2147800946", "3120582545", "2982377460", "753012316", "2962727772"], "title": "Sn: A simulator for connectionist models", "abstract": "", "citation_count": "0", "reference_count": "12", "date": "1988", "authors": ["Leon Bottou", "Yann Lecun"], "related_topics": ["Computer science", "Connectionism", "Simulation"]}
{"id": "2165758113", "references": ["2010029425", "1530699444", "3036751298", "2019363670", "2147800946", "3017143921", "2154952480", "2129113961", "2020246210", "2176028050"], "title": "What Size Net Gives Valid Generalization", "abstract": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 &lt; e \u2264 1/8. We show that if m \u2265 O(W/e log N/e) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 - e/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 - e of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than \u03a9(W/e) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 - e fraction of the future test examples.", "citation_count": "32", "reference_count": "2,218", "date": "1988", "authors": ["Eric B. Baum", "David Haussler"], "related_topics": ["Probability distribution", "Fraction (mathematics)", "Generalization", "Distribution (mathematics)", "Binary logarithm", "Discrete mathematics", "Combinatorics", "Sample (statistics)", "Net (mathematics)", "Mathematics", "Feed forward"]}
{"id": "2116360511", "references": ["2110121211", "2103212315", "2253776861", "2111624873", "2166025442", "2010554296", "2212384750", "2037316494", "2418763445", "2136325353"], "title": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex", "abstract": "", "citation_count": "19", "reference_count": "23,698", "date": "1962", "authors": ["D. H. Hubel", "T. N. Wiesel"], "related_topics": ["Binocular neurons", "Visual cortex", "Receptive field", "Orientation column", "Binocular summation", "Visual system", "Surround suppression", "Ocular dominance column", "Neuroscience", "Psychology"]}
{"id": "2157475639", "references": ["1655554306", "2155818555", "2058841211", "131259011", "2044630651", "3017143921", "2002448074", "2116360511", "2062361515", "1501657095"], "title": "Neural Network Recognizer for Hand-Written Zip Code Digits", "abstract": "This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.", "citation_count": "11", "reference_count": "178", "date": "1988", "authors": ["John S. Denker", "W. R. Gardner", "Hans Peter Graf", "Donnie Henderson", "R. E. Howard", "W. Hubbard", "L. D. Jackel", "Henry S. Baird", "Isabelle Guyon"], "related_topics": ["Word error rate", "Artificial neural network", "Speech recognition", "Task (computing)", "Computer science", "Range (mathematics)", "Zip code"]}
{"id": "1533169541", "references": ["1993845689", "2125527601", "2154496743", "2114766824", "2132549764", "1501500081", "2153638435", "2135705692", "2098152234", "2072128103"], "title": "Stochastic Complexity In Statistical Inquiry", "abstract": "", "citation_count": "0", "reference_count": "2,304", "date": "1989", "authors": ["Jorma Rissanen"], "related_topics": ["Stochastic optimization", "Minimum description length", "Theoretical computer science", "Computer science", "Stochastic complexity"]}
{"id": "94647076", "references": ["1997063559", "1993845689", "1558012644", "1998563636", "3022628558", "2093976044", "1983649039", "2962701695", "1992192543", "85976583"], "title": "Lectures in pattern theory", "abstract": "", "citation_count": "0", "reference_count": "86", "date": "1978", "authors": ["Ulf Grenander"], "related_topics": ["Pattern theory", "Computer science", "Calculus"]}
{"id": "2044875682", "references": ["1969884751", "2160356704", "155080488", "2073055671", "1991848143", "2027066860", "2037944537", "2115121624", "2126974689", "2129090497"], "title": "Acetylcholine and memory", "abstract": "Acetylcholine may set the dynamics of cortical networks to those appropriate for learning of new information, while decreased cholinergic modulation may set the appropriate dynamics for recall. In slice preparations of the olfactory cortex, acetylcholine selectively suppresses intrinsic but not afferent fiber synaptic transmission, while decreasing the adaptation of pyramidal cells. In biologically realistic models of this region, the selective suppression of synaptic transmission prevents recall of previously learned memories from interfering with the learning of new memories, while the decrease in adaptation enhances the response to afferent input and the modification of synapses. This theoretical framework may serve to guide future studies linking neuromodulators to cortical memory function.", "citation_count": "40", "reference_count": "460", "date": "1993", "authors": ["Michael E. Hasselmo", "James M. Bower"], "related_topics": ["Acetylcholine", "Neurotransmission", "Recall", "Olfactory system", "Memoria", "Neuroscience", "Set (psychology)", "Psychology", "Afferent", "Future studies"]}
{"id": "2177040213", "references": ["2065134213", "1977867644", "2119141384", "2012611887", "2154102142", "2015857587", "2153858306", "2166280719", "2116424792", "2145747704"], "title": "A massively parallel architecture for a self-organizing neural pattern recognition machine", "abstract": "A neural network architecture for the learning of recognition categories is derived. Real-time network dynamics are completely characterized through mathematical analysis and computer simulations. The architecture self-organizes and self-stabilizes its recognition codes in response to arbitrary orderings of arbitrarily many and arbitrarily complex binary input patterns. Top-down attentional and matching mechanisms are critical in self-stabilizing the code learning process. The architecture embodies a parallel search scheme which updates itself adaptively as the learning process unfolds. After learning self-stabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time does not grow as a function of code complexity. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. These invariant properties emerge in the form of learned critical feature patterns, or prototypes. The architecture possesses a context-sensitive self-scaling property which enables its emergent critical feature patterns to form. They detect and remember statistically predictive configurations of featural elements which are derived from the set of all input patterns that are ever experienced. Four types of attentional process\u2014priming, gain control, vigilance, and intermodal competition\u2014are mechanistically characterized. Top\u2014down priming and gain control are needed for code matching and self-stabilization. Attentional vigilance determines how fine the learned categories will be. If vigilance increases due to an environmental disconfirmation, then the system automatically searches for and learns finer recognition categories. A new nonlinear matching law (the \u2154 Rule) and new nonlinear associative laws (the Weber Law Rule, the Associative Decay Rule, and the Template Learning Rule) are needed to achieve these properties. All the rules describe emergent properties of parallel network interactions. The architecture circumvents the noise, saturation, capacity, orthogonality, and linear predictability constraints that limit the codes which can be stably learned by alternative recognition models.", "citation_count": "0", "reference_count": "4,070", "date": "1988", "authors": ["G. A. Carpenter", "S. Grossberg"], "related_topics": ["Learning rule", "Feature (machine learning)", "Pattern recognition (psychology)", "Network dynamics", "Set (abstract data type)", "Associative property", "Property (programming)", "Orthogonality (programming)", "Theoretical computer science", "Computer science", "Parallel computing"]}
{"id": "2740373864", "references": ["2518281301", "2167501464", "2121647436", "1995903777", "2565516711", "2123921160", "2124351162", "1518138188", "2169805405"], "title": "Computer vision", "abstract": "", "citation_count": "0", "reference_count": "5,558", "date": "1982", "authors": ["Dana Harry Ballard", "Christopher M. Brown"], "related_topics": ["Computer science", "Computer vision", "Artificial intelligence"]}
{"id": "2127385318", "references": ["2034099719", "2171277043", "1993740947", "2153709524", "3036751298", "2094631910", "2052207834", "1594031697", "2066366061", "2103504761"], "title": "Fast Learning in Multi-Resolution Hierarchies", "abstract": "A class of fast, supervised learning algorithms is presented. They use local representations, hashing, and multiple scales of resolution to approximate functions which are piece-wise continuous. Inspired by Albus's CMAC model, the algorithms learn orders of magnitude more rapidly than typical implementations of back propagation, while often achieving comparable qualities of generalization. Furthermore, unlike most traditional function approximation methods, the algorithms are well suited for use in real time adaptive signal processing. Unlike simpler adaptive systems, such as linear predictive coding, the adaptive linear combiner, and the Kalman filter, the new algorithms are capable of efficiently capturing the structure of complicated non-linear systems. As an illustration, the algorithm is applied to the prediction of a chaotic timeseries.", "citation_count": "18", "reference_count": "193", "date": "1988", "authors": ["John Moody"], "related_topics": ["Adaptive filter", "Function approximation", "Kalman filter", "Adaptive system", "Linear predictive coding", "Backpropagation", "Hash function", "Chaotic", "Algorithm", "Theoretical computer science", "Time series", "Computer science"]}
{"id": "50076749", "references": ["2109779438", "2123060977", "2012611887", "2164276496", "2153382583", "2002193345", "2099579348", "2010150441", "2100211715", "2166280719"], "title": "Learning to tell two spirals apart", "abstract": "", "citation_count": "0", "reference_count": "1,035", "date": "1989", "authors": ["K. Lang"], "related_topics": ["Computer science"]}
{"id": "2169163929", "references": ["1541007220", "2154642048", "2120383790", "3108739439", "2189011649", "2173629880", "1997547570", "2306005020", "2167462730"], "title": "Consonant recognition by modular construction of large phonemic time-delay neural networks", "abstract": "It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants and 94.7% for all phonemes. &gt;", "citation_count": "9", "reference_count": "182", "date": "1989", "authors": ["A. Waibel", "H. Sawai", "K. Shikano"], "related_topics": ["Artificial neural network", "Speech recognition", "Consonant", "Computer science", "Structure (mathematical logic)", "Incremental learning", "Modular construction"]}
{"id": "2160208155", "references": ["423115584", "2154642048", "1573427320", "2078057316", "1535810436", "2129113961", "3147765605", "2895674046", "1686514609", "1492221128"], "title": "Increased Rates of Convergence Through Learning Rate Adaptation", "abstract": "WHILE THERE EXIST MANY TECHNIQUES FOR FINDING THE PARAMETERS THAT MINI- MIZE AN ERROR FUNCTION, ONLY THOSE METHODS THAT SOLELY PERFORM LOCAL COMPU- TATIONS ARE USED IN CONNECTIONIST NETWORKS. THE MOST POPULAR LEARNING ALGO RITHM FOR CONNECTIONIST NETWORKS IS THE BACK-PROPOGATION PROCEDURE [13], WHICH CAN BE USED TO UPDATE THE WEIGHTS BY THE METHOD OF STEEPEST DESCENT. IN THIS PAPER, WE EXAMINE STEEPEST DESCENT AND ANALYZE WHY IT CAN BE SLOW TO CONVERGE. WE THEN PROPOSE FOUR HEURISTICS FOR ACHIEVING FASTER RATES OF CONVERGENCE WHILE ADHERING TO THE LOCALITY CONSTRAINT. THESE HEURISTICS SUGGEST THAT EVERY WEIGHT OF A NETWORK SHOULD BE GIVEN ITS OWN LEARNING RATE AND THAT THESE RATES SHOULD BE ALLOWED TO VARY OVER TIME. ADDITIONALLY THE HEURISTICS SUGGEST HOW THE LEARNING RATES SHOULD BE ADJUSTED. TWO IMPLEMENTATIONS OF THESE HEURISTICS, NAMELY MOMENTUM AND AN ALGORITHM CALLED THE DELTA-BAR-DELTA RULE, ARE STUDIED AND SIMULATION RESULTS ARE PRESENTED.", "citation_count": "14", "reference_count": "2,753", "date": "1987", "authors": ["Robert A. Jacobs"], "related_topics": ["Heuristics", "Method of steepest descent", "Gradient descent", "Quickprop", "Rprop", "Convergence (routing)", "Error function", "Connectionism", "Mathematical optimization", "Computer science"]}
{"id": "3121126077", "references": ["2109779438", "2106525823", "2019207321", "2124290836", "1586335931", "1998442441", "2166280719", "2144499799", "2122538988", "2133321814"], "title": "Fast-learning variations on back propagation: an empirical study.", "abstract": "", "citation_count": "0", "reference_count": "1,134", "date": "1989", "authors": ["S. E. Fahlman"], "related_topics": ["Computer science", "Empirical research", "Algorithm"]}
{"id": "2160699933", "references": ["2154642048", "2046432185", "2171074980", "2077658674", "2266946488", "2022772618", "1526055535", "2078409719", "2293063825", "1686514609"], "title": "Learning Algorithms for Connectionist Networks: Applied Gradient Methods of Nonlinear Optimization", "abstract": "The problem of learning using connectionist networks, in which network connection strengths are modified systematically so that the response of the network increasingly approximates the desired response can be structured as an optimization problem. The widely used back propagation method of connectionist learning [19, 21, 18] is set in the context of nonlinear optimization. In this framework, the issues of stability, convergence and parallelism are considered. As a form of gradient descent with fixed step size, back propagation is known to be unstable, which is illustrated using Rosenbrock's function. This is contrasted with stable methods which involve a line search in the gradient direction. The convergence criterion for connectionist problems involving binary functions is discussed relative to the behavior of gradient descent in the vicinity of local minima. A minimax criterion is compared with the least squares criterion. The contribution of the momentum term [19, 18] to more rapid convergence is interpreted relative to the geometry of the weight space. It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentum term. In valley regions with steep sides, the momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-62. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/597 LEARNING ALGORITHMS FOR CONNECTIONIST NETWORKS: APPLIED GRADIENT METHODS OF NONLINEAR OPTIMIZATION", "citation_count": "18", "reference_count": "351", "date": "1988", "authors": ["Raymond L. Watrous"], "related_topics": ["Gradient descent", "Backpropagation", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Online machine learning", "Optimization problem", "Stability (learning theory)", "Line search", "Nonlinear programming", "Algorithm", "Computer science"]}
{"id": "2154952480", "references": ["1655990431", "2752853835", "1530699444", "2019363670", "2611147814", "2165758113", "3017143921", "2129113961", "2011039300", "2117362057"], "title": "Learnability and the Vapnik-Chervonenkis dimension", "abstract": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability.", "citation_count": "64", "reference_count": "2,283", "date": "1989", "authors": ["Anselm Blumer", "A. Ehrenfeucht", "David Haussler", "Manfred K. Warmuth"], "related_topics": ["Learnability", "Probably approximately correct learning", "VC dimension", "Concept class", "Closure (topology)", "Sample exclusion dimension", "Teaching dimension", "Dimension (graph theory)", "Discrete mathematics", "Combinatorics", "Mathematics"]}
{"id": "2159047538", "references": ["2021332478", "19427811", "1605775535", "2074584355", "2067642555", "2134148799", "1543584403", "15464195"], "title": "Learning Efficient Classification Procedures and Their Application to Chess End Games", "abstract": "A series of experiments dealing with the discovery of efficient classification procedures from large numbers of examples is described, with a case study from the chess end game king-rook versus king-knight. After an outline of the inductive inference machinery used, the paper reports on trials leading to correct and very fast attribute-based rules for the relations lost 2-ply and lost 3-ply. On another tack, a model of the performance of an idealized induction system is developed and its somewhat surprising predictions compared with observed results. The paper ends with a description of preliminary work on the automatic specification of relevant attributes.", "citation_count": "8", "reference_count": "1,653", "date": "1983", "authors": ["J. Ross Quinlan"], "related_topics": ["Game tree", "Classification rule", "Inductive reasoning", "Decision tree", "Artificial intelligence", "Machine learning", "Computer science", "Series (mathematics)", "Work (electrical)", "Induction system"]}
{"id": "2108263314", "references": ["2024046085", "2112076978", "1605688901", "1966280301", "2099968818", "2032210760", "1975846642", "2084812512", "2912934387", "3124955340"], "title": "Boosting Algorithms as Gradient Descent", "abstract": "We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.", "citation_count": "17", "reference_count": "873", "date": "1999", "authors": ["Llew Mason", "Jonathan Baxter", "Peter L. Bartlett", "Marcus R. Frean"], "related_topics": ["Stochastic gradient descent", "Gradient descent", "Boosting (machine learning)", "AdaBoost", "Gradient boosting", "Overfitting", "Algorithm", "Mathematical optimization", "Computer science"]}
{"id": "2075887074", "references": ["2152882024", "2171502450", "2596682183", "2503147665", "1995994736", "2048741758", "2087576060", "1494436834", "624054654", "2798065041"], "title": "Semi-infinite programming: theory, methods, and applications", "abstract": "Starting from a number of motivating and abundant applications in \u00a72, including control of robots, eigenvalue computations, mechanical stress of materials, and statistical design, the authors describe a class of optimization problems which are referred to as semi-infinite, because their constraints bound functions of a finite number of variables on a whole region. In \u00a7\u00a73\u20135, first- and second-order optimality conditions are derived for general nonlinear problems as well as a procedure for reducing the problem locally to one with only finitely many constraints. Another main effort for achieving simplification is through duality in \u00a76. There, algebraic properties of finite linear programming are brought to bear on duality theory in semi-infinite programming. Section 7 treats numerical methods based on either discretization or local reduction with the emphasis on the design of superlinearly convergent (SQP-type) methods. Taking this differentiable point of view, this paper can be considered to be complementar...", "citation_count": "19", "reference_count": "1,293", "date": "1993", "authors": ["R. Hettich", "K. O. Kortanek"], "related_topics": ["Semi-infinite programming", "Generalized semi-infinite programming", "Nonlinear programming", "Duality (mathematics)", "Linear programming", "Optimization problem", "Discretization", "Finite set", "Algebra", "Mathematics"]}
{"id": "1678356000", "references": ["2024046085", "2112076978", "2151693816", "2156909104", "740415", "2102201073", "2797583072", "2117812871", "1594031697", "1498436455"], "title": "Greedy function approximation: A gradient boosting machine.", "abstract": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent \u201cboosting\u201d paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such \u201cTreeBoost\u201d models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.", "citation_count": "20", "reference_count": "13,655", "date": "2001", "authors": ["Jerome H. Friedman"], "related_topics": ["Gradient boosting", "BrownBoost", "LogitBoost", "Boosting (machine learning)", "Gradient descent", "Function approximation", "Least absolute deviations", "Regression analysis", "Algorithm", "Statistics", "Mathematics"]}
{"id": "2109405055", "references": ["2154455818", "2165874743", "2139823104", "1574877594", "2113592823", "2112545207", "1491300635", "1514707997", "2122837498", "2520931985"], "title": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning", "abstract": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor. The advantage of the extension is that it allows predicting the label for a new example without having to solve again a linear system of dimension n (the number of unlabeled and labeled training examples), which can cost O(n 3 ). Experiments show that the extension works well, in the sense of predicting a label close to the one that would have been obtained if the test example had been included in the unlabeled set. This relatively efficient function induction procedure can also be used when n is large to approximate the solution by writing it only in terms of a kernel expansion with m n terms, and reducing the linear system to m equations in m unknowns.", "citation_count": "17", "reference_count": "230", "date": "2004", "authors": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "related_topics": ["Semi-supervised learning", "Linear system", "Regularization (mathematics)", "Minification", "Algorithm", "Nonparametric statistics", "Pattern recognition", "Mathematics", "Artificial intelligence", "Induction procedure"]}
{"id": "2091886411", "references": ["2146434287", "2319794630", "2082612735", "2059507684", "1979519992", "1994753884", "2478937241", "2024081693", "2026258334", "168150807"], "title": "Projection Pursuit Regression", "abstract": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation.", "citation_count": "12", "reference_count": "2,972", "date": "1981", "authors": ["Jerome H. Friedman", "Werner Stuetzle"], "related_topics": ["Nonparametric regression", "Proper linear model", "Linear predictor function", "Regression diagnostic", "Local regression", "Segmented regression", "Polynomial regression", "Projection pursuit regression", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2151907713", "references": ["1533169541", "2102409316", "2051719061", "2150065638", "2146465837", "2107053213", "1578739277", "2040739363", "2132747970", "65738273"], "title": "Developing Population Codes by Minimizing Description Length", "abstract": "The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs.", "citation_count": "11", "reference_count": "65", "date": "1993", "authors": ["Richard S. Zemel", "Geoffrey E. Hinton"], "related_topics": ["Autoencoder", "Population", "Minimum description length", "Artificial neural network", "Representation (mathematics)", "Algorithm", "Nonlinear system", "Space (mathematics)", "Flexibility (engineering)", "Mathematics"]}
{"id": "1578739277", "references": ["2102409316", "2093135704", "1541680420", "2105464873", "2962897886", "3022628558", "2076063813", "2071709160", "1528056001", "2111842831"], "title": "A minimum description length framework for unsupervised learning", "abstract": "A fundamental problem in learning and reasoning about a set of information is finding the right representation. The primary goal of an unsupervised learning procedure is to optimize the quality of a system's internal representation. In this thesis, we present a general framework for describing unsupervised learning procedures based on the Minimum Description Length (MDL) principle. The MDL principle states that the best model is one that minimizes the summed description length of the model and the data with respect to the model. Applying this approach to the unsupervised learning problem makes explicit a key trade off between the accuracy of a representation (i.e., how concise a description of the input may be generated from it) and its succinctness (i.e., how compactly the representation itself can be described).\r\nViewing existing unsupervised learning procedures in terms of the framework exposes their implicit assumptions about the type of structure assumed to underlie the data. While these existing algorithms typically minimize the data description using a fixed length representation, we use the framework to derive a class of objective functions for training self-supervised neural networks, where the goal is to minimize the description length of the representation simultaneously with that of the data. Formulating a description of the representation forces assumptions about the structure of the data to be made explicit, which in turn leads to a particular network configuration as well as an objective function that can be used to optimize the network parameters. We describe three new learning algorithms derived in this manner from the MDL framework. Each algorithm embodies a different scheme for describing the internal representation, and is therefore suited to a range of datasets based on the structure underlying the data. Simulations demonstrate the applicability of these algorithms on some simple computational vision tasks.", "citation_count": "0", "reference_count": "134", "date": "1994", "authors": ["Richard Stanley Zemel"], "related_topics": ["Unsupervised learning", "Minimum description length", "Artificial neural network", "Representation (mathematics)", "Succinctness", "Set (abstract data type)", "Range (mathematics)", "Theoretical computer science", "Structure (mathematical logic)", "Mathematics"]}
{"id": "2133069808", "references": ["1996355918", "1520168181", "1977067929", "2022261649", "2108384452", "2099741732"], "title": "A New Learning Algorithm for Blind Signal Separation", "abstract": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations.", "citation_count": "6", "reference_count": "3,042", "date": "1995", "authors": ["Shun-ichi Amari", "Andrzej Cichocki", "Howard Hua Yang"], "related_topics": ["Blind signal separation", "Wake-sleep algorithm", "Artificial neural network", "Infomax", "Mutual information", "Edgeworth series", "Dependency (UML)", "Activation function", "Algorithm", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2145012779", "references": ["2122925692", "2120838001", "2117731089", "1971027050", "2167034998", "1993197592", "2108384452", "2180838288", "2135587681", "1914401667"], "title": "Natural image statistics and efficient coding.", "abstract": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex.", "citation_count": "15", "reference_count": "758", "date": "1996", "authors": ["B A Olshausen", "D J Field"], "related_topics": ["Hebbian theory", "Coding (social sciences)", "Pairwise comparison", "Principal component analysis", "Receptive field", "Statistics", "Mathematics", "Striate cortex"]}
{"id": "2109863423", "references": ["2117900366", "1968245656", "1995756857", "1530383550", "2044972977", "2824445807", "2059376358", "2524793274", "1965555058", "1870339432"], "title": "Scale-space filtering", "abstract": "The extrema in a signal and its first few derivatives provide a useful general-purpose qualitative description for many kinds of signals. A fundamental problem in computing such descriptions is scale: a derivative must be taken over some neighborhood, but there is seldom a principled basis for choosing its size. Scale-space filtering is a method that describes signals qualitatively, managing the ambiguity of scale in an organized and natural way. The signal is first expanded by convolution with gaussian masks over a continuum of sizes. This \"scale-space\" image is then collapsed, using its qualitative structure, into a tree providing a concise but complete qualitative description covering all scales of observation. The description is further refined by applying a stability criterion, to identify events that persist of large changes in scale.", "citation_count": "16", "reference_count": "4,020", "date": "1987", "authors": ["Andrew P. Witkin"], "related_topics": ["Scale-space axioms", "Scale space", "Scale (ratio)", "Convolution", "Basis (linear algebra)", "Maxima and minima", "Gaussian", "Tree (data structure)", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2146474141", "references": ["2133069808", "2145889472", "2124101779", "2098947662", "1996355918", "1997011019", "2108384452", "1902027874", "2138451337", "2099741732"], "title": "Face recognition by independent component analysis", "abstract": "A number of current face recognition algorithms use face representations found by unsupervised statistical methods. Typically these methods find a set of basis images and represent faces as a linear combination of those images. Principal component analysis (PCA) is a popular example of such methods. The basis images found by PCA depend only on pairwise relationships between pixels in the image database. In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to these high-order statistics. Independent component analysis (ICA), a generalization of PCA, is one such method. We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons. ICA was performed on face images in the FERET database under two different architectures, one which treated the images as random variables and the pixels as outcomes, and a second which treated the pixels as random variables and the images as outcomes. The first architecture found spatially local basis images for the faces. The second architecture produced a factorial face code. Both ICA representations were superior to representations based on PCA for recognizing faces across days and changes in expression. A classifier that combined the two ICA representations gave the best performance.", "citation_count": "56", "reference_count": "2,826", "date": "2002", "authors": ["M.S. Bartlett", "J.R. Movellan", "T.J. Sejnowski"], "related_topics": ["FERET database", "Facial recognition system", "Independent component analysis", "Principal component analysis", "Pixel", "Unsupervised learning", "Pattern recognition", "Classifier (UML)", "Computer science", "Artificial intelligence"]}
{"id": "2134929491", "references": ["1493163583", "2101789093", "2125838338", "2159080219", "2049633694", "2053691921", "2079724595", "2152328854", "2004217976", "2062024414"], "title": "Wavelet-based statistical signal processing using hidden Markov models", "abstract": "Wavelet-based statistical signal processing techniques such as denoising and detection typically model the wavelet coefficients as independent or jointly Gaussian. These models are unrealistic for many real-world signals. We develop a new framework for statistical signal processing based on wavelet-domain hidden Markov models (HMMs) that concisely models the statistical dependencies and non-Gaussian statistics encountered in real-world signals. Wavelet-domain HMMs are designed with the intrinsic properties of the wavelet transform in mind and provide powerful, yet tractable, probabilistic signal models. Efficient expectation maximization algorithms are developed for fitting the HMMs to observational signal data. The new framework is suitable for a wide range of applications, including signal estimation, detection, classification, prediction, and even synthesis. To demonstrate the utility of wavelet-domain HMMs, we develop novel algorithms for signal denoising, classification, and detection.", "citation_count": "39", "reference_count": "2,410", "date": "1998", "authors": ["M.S. Crouse", "R.D. Nowak", "R.G. Baraniuk"], "related_topics": ["Step detection", "Statistical signal processing", "Wavelet", "Wavelet transform", "Hidden Markov model", "Signal processing", "White noise", "Statistical model", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2137234026", "references": ["2133069808", "2099111195", "2003370853", "1996355918", "2006500012", "1977067929", "2167034998", "2108384452", "2180838288", "2099741732"], "title": "The \"independent components\" of natural scenes are edge filters.", "abstract": "It has previously been suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and it has been reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that a new unsupervised learning algorithm based on information maximization, a nonlinear \u201cinfomax\u201d network, when applied to an ensemble of natural scenes produces sets of visual filters that are localized and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximization network. In addition, the outputs of these filters are as independent as possible, since this infomax network performs Independent Components Analysis or ICA, for sparse (super-gaussian) component distributions. We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, information-theoretic coordinate system for natural images.", "citation_count": "48", "reference_count": "2,771", "date": "1997", "authors": ["Anthony J. Bell", "Terrence J. Sejnowski"], "related_topics": ["Independent component analysis", "Infomax", "Efficient coding hypothesis", "Factorial code", "Artificial neural network", "Principal component analysis", "Information theory", "Visual cortex", "Pattern recognition", "Computer science", "Communication", "Artificial intelligence"]}
{"id": "2127006916", "references": ["1997063559", "2132984323", "2103384342", "2116013899", "2124731682", "2167034998", "2137234026", "2098914003", "2107790757", "1490632837"], "title": "A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients", "abstract": "We present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We develop an efficient algorithm for synthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying each constraint, and we use this to test the perceptual validity of the model. In particular, we demonstrate the necessity of subgroups of the parameter set by showing examples of texture synthesis that fail when those parameters are removed from the set. We also demonstrate the power of our model by successfully synthesizing examples drawn from a diverse collection of artificial and natural textures.", "citation_count": "64", "reference_count": "2,091", "date": "2000", "authors": ["Javier Portilla", "Eero P. Simoncelli"], "related_topics": ["Parametric statistics", "Complex wavelet transform", "Wavelet", "Texture synthesis", "Statistical model", "Markov random field", "Set (abstract data type)", "Basis function", "Pattern recognition", "Statistics", "Mathematics", "Artificial intelligence"]}
{"id": "2103504761", "references": ["2074163268", "118993750", "1622620102", "2078498116", "2089975134", "2024397673", "1486071255", "2099590965", "2978983090", "2125311604"], "title": "The Laplacian Pyramid as a Compact Image Code", "abstract": "We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding.", "citation_count": "11", "reference_count": "8,776", "date": "1983", "authors": ["P. Burt", "E. Adelson"], "related_topics": ["Image compression", "Image processing", "Image texture", "Binary image", "Digital image processing", "Scale space", "Image gradient", "Image warping", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2042422091", "references": ["1956327946", "2145889472", "2131803903", "2053268711", "266383608", "2163886718", "2090306035", "1976738367", "2038584209", "1603661052"], "title": "An Energy Budget for Signaling in the Grey Matter of the Brain", "abstract": "Anatomic and physiologic data are used to analyze the energy expenditure on different components of excitatory signaling in the grey matter of rodent brain. Action potentials and postsynaptic effects of glutamate are predicted to consume much of the energy (47% and 34%, respectively), with the resting potential consuming a smaller amount (13%), and glutamate recycling using only 3%. Energy usage depends strongly on action potential rate\u2014an increase in activity of 1 action potential/cortical neuron/s will raise oxygen consumption by 145 mL/100 g grey matter/h. The energy expended on signaling is a large fraction of the total energy used by the brain; this favors the use of energy efficient neural codes and wiring patterns. Our estimates of energy usage predict the use of distributed codes, with \u226415% of neurons simultaneously active, to reduce energy consumption and allow greater computing power from a fixed number of neurons. Functional magnetic resonance imaging signals are likely to be dominated by chang...", "citation_count": "71", "reference_count": "3,018", "date": "2001", "authors": ["David Attwell", "Simon B. Laughlin"], "related_topics": ["Energy consumption", "Energy budget", "Efficient energy use", "Grey matter", "Excitatory postsynaptic potential", "Postsynaptic potential", "Neurotransmission", "Membrane potential", "Neuroscience", "Physics"]}
{"id": "43284170", "references": ["2042422091", "2147546041", "2169714384", "2022616811", "1986098213", "2075187489", "2016927879", "2105220251", "2164682760", "2112845172"], "title": "Circulation and Energy Metabolism of the Brain", "abstract": "", "citation_count": "0", "reference_count": "791", "date": "1999", "authors": ["Donald D Clarke", "Louis Sokoloff"], "related_topics": ["Circulation (currency)", "Cell biology", "Biology", "Energy metabolism"]}
{"id": "2110208125", "references": ["2170120409", "2151721316", "2099290282", "2113319997", "1994875376", "2110437310", "3127623214", "2104095591", "1238092070", "2103504761"], "title": "Measuring the thickness of the human cerebral cortex from magnetic resonance images", "abstract": "Accurate and automated methods for measuring the thickness of human cerebral cortex could provide powerful tools for diagnosing and studying a variety of neurodegenerative and psychiatric disorders. Manual methods for estimating cortical thickness from neuroimaging data are labor intensive, requiring several days of effort by a trained anatomist. Furthermore, the highly folded nature of the cortex is problematic for manual techniques, frequently resulting in measurement errors in regions in which the cortical surface is not perpendicular to any of the cardinal axes. As a consequence, it has been impractical to obtain accurate thickness estimates for the entire cortex in individual subjects, or group statistics for patient or control populations. Here, we present an automated method for accurately measuring the thickness of the cerebral cortex across the entire brain and for generating cross-subject statistics in a coordinate system based on cortical anatomy. The intersubject standard deviation of the thickness measures is shown to be less than 0.5 mm, implying the ability to detect focal atrophy in small populations or even individual subjects. The reliability and accuracy of this new method are assessed by within-subject test-retest studies, as well as by comparison of cross-subject regional thickness measures with published values.", "citation_count": "41", "reference_count": "4,973", "date": "2000", "authors": ["Bruce Fischl", "Anders M. Dale"], "related_topics": ["Cortex (anatomy)", "Gyrification", "Cerebral cortex", "Neuroimaging", "Image processing", "Magnetic resonance imaging", "Standard deviation", "Radiography", "Biomedical engineering", "Computer science"]}
{"id": "1993303421", "references": ["1991707478", "1898723177", "1982774084", "2170652634", "2070972951", "2013885616", "2080949017", "1989650300", "1574637310", "1967009686"], "title": "Nonoxidative glucose consumption during focal physiologic neural activity", "abstract": "Brain glucose uptake, oxygen metabolism, and blood flow in humans were measured with positron emission tomography, and a resting-state molar ratio of oxygen to glucose consumption of 4.1:1 was obtained. Physiological neural activity, however, increased glucose uptake and blood flow much more (51 and 50 percent, respectively) than oxygen consumption (5 percent) and produced a molar ratio for the increases of 0.4:1. Transient increases in neural activity cause a tissue uptake of glucose in excess of that consumed by oxidative metabolism, acutely consume much less energy than previously believed, and regulate local blood flow for purposes other than oxidative metabolism.", "citation_count": "15", "reference_count": "2,022", "date": "1988", "authors": ["Peter T. Fox", "Marcus E. Raichle", "Mark A. Mintun", "Carmen Dence"], "related_topics": ["Glucose uptake", "Carbohydrate metabolism", "Metabolism", "Blood flow", "Oxygen", "Lactate shuttle hypothesis", "Functional magnetic resonance spectroscopy of the brain", "Oxygene", "Endocrinology", "Chemistry", "Internal medicine"]}
{"id": "1991233288", "references": ["2082622165", "2107098820", "1996773027", "1672439543", "2163630896", "2008240154", "2177721432", "2096519870", "2116360511", "2038718814"], "title": "Recurrent excitation in neocortical circuits.", "abstract": "The majority of synapses in the mammalian cortex originate from cortical neurons. Indeed, the largest input to cortical cells comes from neighboring excitatory cells. However, most models of cortical development and processing do not reflect the anatomy and physiology of feedback excitation and are restricted to serial feedforward excitation. This report describes how populations of neurons in cat visual cortex can use excitatory feedback, characterized as an effective \"network conductance\", to amplify their feedforward input signals and demonstrates how neuronal discharge can be kept proportional to stimulus strength despite strong, recurrent connections that threaten to cause runaway excitation. These principles are incorporated into models of cortical direction and orientation selectivity that emphasize the basic design principles of cortical architectures.", "citation_count": "50", "reference_count": "1,064", "date": "1995", "authors": ["Rodney J. Douglas", "Christof Koch", "Misha Mahowald", "Kevan A. C. Martin", "Humbert H. Suarez"], "related_topics": ["Visual cortex", "Cortex (anatomy)", "Neural Inhibition", "Neural Conduction", "Electrophysiology", "Excitatory postsynaptic potential", "Feed forward", "Neuroscience", "Central nervous system", "Anatomy", "Biology"]}
{"id": "2096519870", "references": ["1926219845", "2082769538", "97067864", "1953088445", "2042431572", "2096428903", "1864836097", "1939037149", "2116360511", "1971866692"], "title": "Comparative electrophysiology of pyramidal and sparsely spiny stellate neurons of the neocortex.", "abstract": "Slices of sensorimotor and anterior cingulate cortex from guinea pigs were maintained in vitro and bathed in a normal physiological medium. Electrophysiological properties of neurons were assessed with intracellular recording techniques. Some neurons were identified morphologically by intracellular injection of the fluorescent dye Lucifer yellow CH. Three distinct neuronal classes of electrophysiological behavior were observed; these were termed regular spiking, bursting, and fast spiking. The physiological properties of neurons from sensorimotor and anterior cingulate areas did not differ significantly. Regular-spiking cells were characterized by action potentials with a mean duration of 0.80 ms at one-half amplitude, a ratio of maximum rate of spike rise to maximum rate of fall of 4.12, and a prominent afterhyperpolarization following a train of spikes. The primary slope of initial spike frequency versus injected current intensity was 241 Hz/nA. During prolonged suprathreshold current pulses the frequency of firing adapted strongly. When local synaptic pathways were activated, all cells were transiently excited and then strongly inhibited. Bursting cells were distinguished by their ability to generate endogenous, all-or-none bursts of three to five action potentials. Their properties were otherwise very similar to regular-spiking cells. The ability to generate a burst was eliminated when the membrane was depolarized to near the firing threshold with tonic current. By contrast, hyperpolarization of regular-spiking (i.e., nonbursting) cells did not uncover latent bursting tendencies. The action potentials of fast-spiking cells were much briefer (mean of 0.32 ms) than those of the other cell types.(ABSTRACT TRUNCATED AT 250 WORDS)", "citation_count": "59", "reference_count": "2,101", "date": "1985", "authors": ["D. A. McCormick", "B. W. Connors", "J. W. Lighthall", "D. A. Prince"], "related_topics": ["Bursting", "Electrophysiology", "Afterhyperpolarization", "Hyperpolarization (biology)", "Neocortex", "Neurotransmission", "Cingulate cortex", "Cerebral cortex", "Neuroscience", "Chemistry"]}
{"id": "1976738367", "references": ["1971405760", "2053794249", "2013070880", "1997862361", "2090584164", "1974804836", "2087758815", "2063542323", "2053980736", "2056661320"], "title": "Physiology and anatomy of synaptic connections between thick tufted pyramidal neurones in the developing rat neocortex.", "abstract": "1. Dual voltage recordings were made from pairs of adjacent, synaptically connected thick tufted layer 5 pyramidal neurones in brain slices of young rat (14-16 days) somatosensory cortex to examine the physiological properties of unitary EPSPs. Pre- and postsynaptic neurones were filled with biocytin and examined in the light and electron microscope to quantify the morphology of axonal and dendritic arbors and the number and location of synaptic contacts on the target neurone. 2. In 138 synaptic connections between pairs of pyramidal neurones 96 (70%) were unidirectional and 42 (30%) were bidirectional. The probability of finding a synaptic connection in dual recordings was 0.1. Unitary EPSPs evoked by a single presynaptic action potential (AP) had a mean peak amplitude ranging from 0.15 to 5.5 mV in different connections with a mean of 1.3 +/- 1.1 mV, a latency of 1.7 +/- 0.9 ms, a 20-80% rise time of 2.9 +/- 2.3 ms and a decay time constant of 40 +/- 18 ms at 32-24 degrees C and -60 +/- 2 mV membrane potential. 3. Peak amplitudes of unitary EPSPs fluctuated randomly from trial to trial. The coefficient of variation (c.v.) of the unitary EPSP amplitudes ranged from 0.13 to 2.8 in different synaptic connections (mean, 0.52; median, 0.41). The percentage of failures of single APs to evoke a unitary EPSP ranged from 0 to 73% (mean, 14%; median, 7%). Both c.v. and percentage of failures decreased with increasing mean EPSP amplitude. 4. Postsynaptic glutamate receptors which mediate unitary EPSPs at -60 mV were predominantly of the L-alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionate (AMPA) receptor type. Receptors of the N-methyl-D-aspartate (NMDA) type contributed only a small fraction (&lt; 20%) to the voltage-time integral of the unitary EPSP at -60 mV, but their contribution increased at more positive membrane potentials. 5. Branching patterns of dendrites and axon collaterals of forty-five synaptically connected neurones, when examined in the light microscope, indicated that the axonal and dendritic anatomy of both projecting and target neurones and of uni- and bidirectionally connected neurones was uniform. 6. The number of potential synaptic contacts formed by a presynaptic neurone on a target neurone varied between four and eight (mean, 5.5 +/- 1.1 contacts; n = 19 connections). Synaptic contacts were preferentially located on basal dendrites (63%, 82 +/- 35 microns from the soma, n = 67) and apical oblique dendrites (27%, 145 +/- 59 microns, n = 29), and 35% of all contacts were located on tertiary basal dendritic branches. The mean geometric distances (from the soma) of the contacts of a connection varied between 80 and 585 microns (mean, 147 microns; median, 105 microns). The correlation between EPSP amplitude and the number of morphologically determined synaptic contacts or the mean geometric distances from the soma was only weak (correlation coefficients were 0.2 and 0.26, respectively). 7. Compartmental models constructed from camera lucida drawings of eight target neurones showed that synaptic contacts were located at mean electrotonic distances between 0.07 and 0.33 from the soma (mean, 0.13). Simulations of unitary EPSPs, assuming quantal conductance changes with fast rise time and short duration, indicated that amplitudes of quantal EPSPs at the soma were attenuated, on average, to &lt; 10% of dendritic EPSPs and varied in amplitude up to 10-fold depending on the dendritic location of synaptic contacts. The inferred quantal peak conductance increase varied between 1.5 and 5.5 nS (mean, 3 nS). 8. The combined physiological and morphological measurements in conjunction with EPSP simulations indicated that the 20-fold range in efficacy of the synaptic connections between thick tufted pyramidal neurones, which have their synaptic contacts preferentially located on basal and apical oblique dendrites, was due to differences in transmitter release probability of the projecting neurones and, to a lesser extent, to differenc", "citation_count": "47", "reference_count": "966", "date": "1997", "authors": ["Henry Markram", "Joachim H. R. L\u00fcbke", "Michael Frotscher", "Arnd Roth", "Bert Sakmann"], "related_topics": ["Excitatory postsynaptic potential", "Postsynaptic potential", "Biocytin", "NMDA receptor", "Neocortex", "AMPA receptor", "Membrane potential", "Axon", "Anatomy", "Chemistry"]}
{"id": "1907121963", "references": ["1939596644", "2028310026", "2118615399", "2053315601", "1565561704", "2080530802", "2013755742", "1569395231", "2033118251", "2012521891"], "title": "Neural Mechanisms of Visual Working Memory in Prefrontal Cortex of the Macaque", "abstract": "Prefrontal (PF) cells were studied in monkeys performing a delayed matching to sample task, which requires working memory. The stimuli were complex visual patterns and to solve the task, the monkeys had to discriminate among the stimuli, maintain a memory of the sample stimulus during the delay periods, and evaluate whether a test stimulus matched the sample presented earlier in the trial. PF cells have properties consistent with a role in all three of these operations. Approximately 25% of the cells responded selectively to different visual stimuli. Half of the cells showed heightened activity during the delay after the sample and, for many of these cells, the magnitude of delay activity was selective for different samples. Finally, more than half of the cells responded differently to the test stimuli depending on whether they matched the sample. Because inferior temporal (IT) cortex also is important for working memory, we compared PF cells with IT cells studied in the same task. Compared with IT cortex, PF responses were less often stimulus-selective but conveyed more information about whether a given test stimulus was a match to the sample. Furthermore, sample-selective delay activity in PF cortex was maintained throughout the trial even when other test stimuli intervened during the delay, whereas delay activity in IT cortex was disrupted by intervening stimuli. The results suggest that PF cortex plays a primary role in working memory tasks and may be a source of feedback inputs to IT cortex, biasing activity in favor of behaviorally relevant stimuli.", "citation_count": "58", "reference_count": "1,681", "date": "1996", "authors": ["Earl K. Miller", "Cynthia A. Erickson", "Robert Desimone"], "related_topics": ["Working memory", "Interference theory", "Echoic memory", "Visual memory", "Prefrontal cortex", "Levels-of-processing effect", "Stimulus (physiology)", "Short-term memory", "Neuroscience", "Cognitive psychology", "Psychology"]}
{"id": "1603661052", "references": ["2042422091", "1993577976", "2142875089", "2040073285", "2072522618", "2033708181", "2126462474", "2022235653", "1513082520"], "title": "Cortex: Statistics and Geometry of Neuronal Connectivity", "abstract": "", "citation_count": "0", "reference_count": "1,121", "date": "1998", "authors": ["Braitenberg", "A Sch\u00fcz"], "related_topics": ["Cortex (anatomy)", "Neuroscience", "Computer science"]}
{"id": "2003739479", "references": ["2042422091", "2002660165", "2114104729", "2071714163", "2123586737", "2170117095", "2111609296", "2050717100", "2025283285", "2170814877"], "title": "What does fMRI tell us about neuronal activity", "abstract": "In recent years, cognitive neuroscientists have taken great advantage of functional magnetic resonance imaging (fMRI) as a non-invasive method of measuring neuronal activity in the human brain. But what exactly does fMRI tell us? We know that its signals arise from changes in local haemodynamics that, in turn, result from alterations in neuronal activity, but exactly how neuronal activity, haemodynamics and fMRI signals are related is unclear. It has been assumed that the fMRI signal is proportional to the local average neuronal activity, but many factors can influence the relationship between the two. A clearer understanding of how neuronal activity influences the fMRI signal is needed if we are correctly to interpret functional imaging data.", "citation_count": "116", "reference_count": "1,138", "date": "2002", "authors": ["David J. Heeger", "David Ress"], "related_topics": ["Resting state fMRI", "Functional magnetic resonance imaging", "Functional imaging", "Premovement neuronal activity", "Human brain", "Neuroscience", "Nerve net", "Cognition", "Magnetic resonance imaging", "Psychology"]}
{"id": "1996355918", "references": ["2248478808", "1996773027", "46701496", "2068447332", "2215995846", "1481696337", "2051385641", "2090736342"], "title": "Blind separation of sources, Part 1: an adaptive algorithm based on neuromimetic architecture", "abstract": "Abstract   The separation of independent sources from an array of sensors is a classical but difficult problem in signal processing. Based on some biological observations, an adaptive algorithm is proposed to separate simultaneously all the unknown independent sources. The adaptive rule, which constitutes an independence test using non-linear functions, is the main original point of this blind identification procedure. Moreover, a new concept, that of INdependent Components Analysis (INCA), more powerful than the classical Principal Components Analysis (in decision tasks) emerges from this work.", "citation_count": "8", "reference_count": "3,738", "date": "1991", "authors": ["Christian Jutten", "Jeanny Herault"], "related_topics": ["Blind signal separation", "Adaptive algorithm", "Independent component analysis", "Adaptive filter", "Signal processing", "Artificial neural network", "Identification (information)", "Linear filter", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2963012544", "references": ["1904365287", "2158899491", "1832693441", "2149684865", "2310919327", "2153579005", "104184427", "1689711448", "1665214252", "2064675550"], "title": "Character-level convolutional networks for text classification", "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "citation_count": "31", "reference_count": "2,296", "date": "2015", "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "related_topics": ["Deep learning", "Recurrent neural network", "Bag-of-words model", "Character (mathematics)", "tf\u2013idf", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2102113734", "references": ["2618530766", "3023071679", "1524333225", "2143612262", "2184045248", "2144499799", "2131774270", "2100495367", "2064675550", "2127141656"], "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks", "abstract": "This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.", "citation_count": "23", "reference_count": "2,000", "date": "2014", "authors": ["Alex Graves", "Navdeep Jaitly"], "related_topics": ["Word error rate", "Recurrent neural network", "Language model", "Phonetic representation", "Lexicon", "Transcription (software)", "Connectionism", "Rule-based machine translation", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2963674932", "references": ["2618530766", "2158899491", "2964299589", "2097117768", "2145287260", "2310919327", "2095705004", "2149933564", "2962835968", "2155893237"], "title": "Learning both weights and connections for efficient neural networks", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "citation_count": "29", "reference_count": "3,477", "date": "2015", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "related_topics": ["Artificial neural network", "Machine learning", "Computer science", "Factor (programming language)", "Artificial intelligence"]}
{"id": "1689711448", "references": ["1947481528", "2097998348", "2143612262", "1810943226", "2131241448", "104184427", "2157331557", "1924770834", "2064675550", "2127141656"], "title": "LSTM: A Search Space Odyssey", "abstract": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs (   $\\approx 15$    years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.", "citation_count": "45", "reference_count": "3,387", "date": "2017", "authors": ["Klaus Greff", "Rupesh K. Srivastava", "Jan Koutnik", "Bas R. Steunebrink", "Jurgen Schmidhuber"], "related_topics": ["Recurrent neural network", "Handwriting recognition", "Random search", "Activation function", "Speech recognition", "Machine learning", "Hyperparameter", "Computer science", "Task (project management)", "Artificial intelligence"]}
{"id": "2285660444", "references": ["2155893237", "2618530766", "2964299589", "2097117768", "2108598243", "1536680647", "2310919327", "1665214252", "2962835968", "2064675550"], "title": "EIE: efficient inference engine on compressed deep neural network", "abstract": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120\u00d7 energy saving; Exploiting sparsity saves 10\u00d7; Weight sharing gives 8\u00d7; Skipping zero activations from ReLU saves another 3\u00d7. Evaluated on nine DNN benchmarks, EIE is 189\u00d7 and 13\u00d7 faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88\u00d7104 frames/sec with a power dissipation of only 600mW. It is 24,000\u00d7 and 3,400\u00d7 more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9\u00d7, 19\u00d7 and 3\u00d7 better throughput, energy efficiency and area efficiency.", "citation_count": "43", "reference_count": "1,825", "date": "2016", "authors": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A. Horowitz", "William J. Dally"], "related_topics": ["Hardware acceleration", "Static random-access memory", "Dram", "Deep learning", "Efficient energy use", "System on a chip", "Throughput (business)", "Uncompressed video", "Artificial neural network", "Parallel computing", "Computer science", "Artificial intelligence", "Random access memory"]}
{"id": "2127141656", "references": ["2150355110", "2147880316", "2147568880", "3023071679", "2079735306", "2125838338", "2131774270", "1554663460", "1553004968", "2064675550"], "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "abstract": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.", "citation_count": "16", "reference_count": "3,154", "date": "2006", "authors": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "related_topics": ["Recurrent neural network", "TIMIT", "Speech corpus", "Sequence learning", "Hidden Markov model", "Connectionism", "Speech recognition", "Pattern recognition", "Sequence", "SIGNAL (programming language)", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2155273149", "references": ["2134557905", "2394932179", "1538131130", "2103359087", "2107789863", "2310919327", "1993882792", "2002342963", "2076794394", "2504108613"], "title": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition", "abstract": "Convolutional Neural Networks (CNN) have showed success in achieving translation invariance for many image processing tasks. The success is largely attributed to the use of local filtering and max-pooling in the CNN architecture. In this paper, we propose to apply CNN to speech recognition within the framework of hybrid NN-HMM model. We propose to use local filtering and max-pooling in frequency domain to normalize speaker variance to achieve higher multi-speaker speech recognition performance. In our method, a pair of local filtering layer and max-pooling layer is added at the lowest end of neural network (NN) to normalize spectral variations of speech signals. In our experiments, the proposed CNN architecture is evaluated in a speaker independent speech recognition task using the standard TIMIT data sets. Experimental results show that the proposed CNN method can achieve over 10% relative error reduction in the core TIMIT test sets when comparing with a regular NN using the same number of hidden layers and weights. Our results also show that the best result of the proposed CNN model is better than previously published results on the same TIMIT test sets that use a pre-trained deep NN model.", "citation_count": "12", "reference_count": "961", "date": "2012", "authors": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Gerald Penn"], "related_topics": ["TIMIT", "Convolutional neural network", "Artificial neural network", "Hidden Markov model", "Speech recognition", "Pattern recognition", "Image processing", "Reduction (complexity)", "Frequency domain", "Normalization (statistics)", "Convolution", "Computer science", "Artificial intelligence"]}
{"id": "2116825644", "references": ["2099866409", "2136922672", "2096192494", "1994197834", "2124914669", "2110798204", "2120340025", "2100495367", "2116064496", "66838807"], "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "abstract": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.", "citation_count": "20", "reference_count": "1,010", "date": "2008", "authors": ["Tijmen Tieleman"], "related_topics": ["Boltzmann machine", "Restricted Boltzmann machine", "Algorithm", "Data type", "Simple (abstract algebra)", "Distribution (mathematics)", "Mathematics", "Training (meteorology)", "Contrastive divergence"]}
{"id": "2029949252", "references": ["2186428165", "2561981131", "44815768", "2148138104", "1570411240", "2153791616", "2167839676", "2132424367", "2072128103"], "title": "Parallel &amp; distributed processing", "abstract": "", "citation_count": "0", "reference_count": "4,043", "date": "2005", "authors": ["Philipp Slusallek", "Peter Shirley", "William Mark", "Gordon Stoll", "Ingo Wald"], "related_topics": ["Computational science", "Theoretical computer science", "Connectionism", "Mathematics"]}
{"id": "2150218618", "references": ["1496450597", "1538192045", "2126185296", "2090761873", "2159080219", "1965552673"], "title": "Graphical Models for Machine Learning and Digital Communication", "abstract": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions.", "citation_count": "6", "reference_count": "780", "date": "1998", "authors": ["Brendan J. Frey"], "related_topics": ["Unsupervised learning", "Active learning (machine learning)", "Computational learning theory", "Online machine learning", "Artificial neural network", "Statistical relational learning", "Graphical model", "Data compression", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2170942820", "references": ["2147568880", "2167898728", "2149194912", "2131774270", "2310919327", "2142069714", "2163614729", "2144499799", "2064675550", "2127141656"], "title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks", "abstract": "Offline handwriting recognition\u2014the automatic transcription of images of handwritten text\u2014is a challenging task that combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks\u2014multidimensional recurrent neural networks and connectionist temporal classification\u2014this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.", "citation_count": "20", "reference_count": "1,019", "date": "2008", "authors": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "related_topics": ["Intelligent character recognition", "Recurrent neural network", "Handwriting recognition", "Time delay neural network", "Handwriting", "Connectionism", "Sequence learning", "Feature (computer vision)", "Speech recognition", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2129652681", "references": ["2119047110", "2107927941", "2079729471", "2165564574", "1990653637", "1995875735", "1975965284", "2122962290", "2161628678", "2911940095"], "title": "Arithmetic coding for data compression", "abstract": "The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding.", "citation_count": "23", "reference_count": "4,114", "date": "1987", "authors": ["Ian H. Witten", "Radford M. Neal", "John G. Cleary"], "related_topics": ["Context-adaptive binary arithmetic coding", "Arithmetic coding", "Huffman coding", "Tunstall coding", "Range encoding", "Data compression", "Context-adaptive variable-length coding", "Lossless compression", "Arithmetic", "Algorithm", "Mathematics"]}
{"id": "3161062409", "references": ["2110723321", "2057222388", "1480553938", "3093116395", "2514926921", "2116192590", "2554917868", "2135792119", "2981812042", "2403192338"], "title": "A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm.", "abstract": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques. &gt;", "citation_count": "0", "reference_count": "5,854", "date": "1993", "authors": ["Riedmiller M", "Braun H"], "related_topics": ["Rprop", "Backpropagation", "Error function", "Feed forward", "Pattern recognition", "Process (computing)", "Adaptation (computer science)", "Computer science", "Adaptive method", "Artificial intelligence", "Rprop algorithm"]}
{"id": "2114766824", "references": ["1533169541", "169539560", "56903235", "19621276", "2169163929", "2147800946", "2165758113", "2029538739", "2134273960", "2154579312"], "title": "Optimal Brain Damage", "abstract": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.", "citation_count": "17", "reference_count": "3,992", "date": "1989", "authors": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "related_topics": ["Network complexity", "Artificial neural network", "Generalization", "Machine learning", "Artificial intelligence", "Computer science", "Class (computer programming)", "Training set"]}
{"id": "2054658115", "references": ["2158208985", "2549568718", "2123838014", "2168175751", "2110381504", "2046419776", "2798453649", "2005097301", "2058815839", "171686316"], "title": "Paper: Modeling by shortest data description", "abstract": "The number of digits it takes to write down an observed sequence x\"1, ..., x\"N of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.", "citation_count": "11", "reference_count": "8,821", "date": "1978", "authors": ["J. Rissanen"], "related_topics": ["Minimum description length", "Minimum message length", "Series (mathematics)", "Sequence", "Estimation theory", "Algorithm", "Structure (category theory)", "Identification (information)", "Statistics", "Computer science", "Data description"]}
{"id": "2097998348", "references": ["1746819321", "1533861849", "2025768430", "2136922672", "44815768", "2310919327", "2153635508", "1554663460", "2581275558", "2072128103"], "title": "Random search for hyper-parameter optimization", "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.", "citation_count": "35", "reference_count": "5,895", "date": "2012", "authors": ["James Bergstra", "Yoshua Bengio"], "related_topics": ["Beam search", "Best-first search", "Random search", "Iterative deepening depth-first search", "Search algorithm", "Beam stack search", "Combinatorial search", "Guided Local Search", "Data mining", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2138204974", "references": ["1480376833", "2145889472", "2116825644", "2133665775", "1554663460", "2152790380", "1548802052", "2116064496", "130710483", "1985093013"], "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "abstract": "We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.", "citation_count": "29", "reference_count": "597", "date": "2012", "authors": ["Michael U. Gutmann", "Aapo Hyv\u00e4rinen"], "related_topics": ["Estimator", "Statistical model", "Probability density function", "Normalization (statistics)", "Markov chain", "Spline (mathematics)", "Parameterized complexity", "Sample size determination", "Statistics", "Mathematics"]}
{"id": "196761320", "references": ["3029645440", "2006903949", "2914484425", "2110798204", "3023533631", "2100495367", "2138857742", "2166347285", "2130984546"], "title": "Deep learning via Hessian-free optimization", "abstract": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton &amp; Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.", "citation_count": "10", "reference_count": "920", "date": "2010", "authors": ["James Martens"], "related_topics": ["Hessian matrix", "Deep learning", "Class (philosophy)", "Machine learning", "Artificial intelligence", "Computer science", "Curvature"]}
{"id": "2110575115", "references": ["1480376833", "2147880316", "2125838338", "2160337655", "2159080219", "1554663460", "2581275558", "1483307070", "2137813581", "2122410182"], "title": "Dynamic bayesian networks: representation, inference and learning", "abstract": "", "citation_count": "383", "reference_count": "3,528", "date": "2002", "authors": ["Kevin Patrick Murphy", "Stuart Russell"], "related_topics": ["Frequentist inference", "Bayesian statistics", "Inference", "Variable-order Bayesian network", "Dynamic Bayesian network", "Influence diagram", "Bayesian network", "Graphical model", "Artificial intelligence", "Computer science"]}
{"id": "1408639475", "references": ["2107878631", "3029645440", "3023071679", "2136848157", "196761320", "2118706537", "2110575115", "2100495367", "2064675550", "1498436455"], "title": "Learning Recurrent Neural Networks with Hessian-Free Optimization", "abstract": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.", "citation_count": "16", "reference_count": "670", "date": "2011", "authors": ["James Martens", "Ilya Sutskever"], "related_topics": ["Recurrent neural network", "Hessian matrix", "Sequence", "Machine learning", "Artificial intelligence", "Computer science", "Scheme (programming language)", "Matrix (mathematics)", "Interpretation (logic)"]}
{"id": "2103179919", "references": ["1982370770", "2468203291", "1991194182", "2148603752", "2162019295", "2014339193", "2054930781", "1564514837", "2133671888", "2154890045"], "title": "Real-time computing without stable states: a new framework for neural computation based on perturbations", "abstract": "A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.", "citation_count": "27", "reference_count": "3,212", "date": "2002", "authors": ["Wolfgang Maass", "Thomas Natschl\u00e4ger", "Henry Markram"], "related_topics": ["Cellular neural network", "Recurrent neural network", "Reservoir computing", "Artificial neural network", "Turing machine", "Liquid state machine", "Echo state network", "Dynamical systems theory", "Algorithm", "Computer science"]}
{"id": "1943433854", "references": ["2044474088", "1959983357", "2887242076", "3081834340", "2031170958", "1828538530", "2099652807", "1513126258", "182048401", "1939596644"], "title": "Model of Cortical-Basal Ganglionic Processing: Encoding the Serial Order of Sensory Events", "abstract": "Beiser, David G. and James C. Houk. Model of cortical-basal ganglionic processing: encoding the serial order of sensory events. J. Neurophysiol. 79: 3168\u20133188, 1998. Several lines of evidence sugge...", "citation_count": "111", "reference_count": "328", "date": "1998", "authors": ["David G. Beiser", "James C. Houk"], "related_topics": ["Sensory system", "Neuroscience", "Basal (phylogenetics)", "Encoding (semiotics)", "Psychology"]}
{"id": "2058580716", "references": ["2065134213", "2154642048", "2103496339", "1977106683", "2007431958", "2137983211", "2143503258", "1971735090", "2016589492"], "title": "Original Contribution: Approximation of dynamical systems by continuous time recurrent neural networks", "abstract": "In this paper, we prove that any finite time trajectory of a given n-dimensional dynamical system can be approximately realized by the internal state of the output units of a continuous time recurrent neural network with n output units, some hidden units, and an appropriate initial condition. The essential idea of the proof is to embed the n-dimensional dynamical system into a higher dimensional one which defines a recurrent neural network. As a corollary, we also show that any continuous curve can be approximated by the output of a recurrent neural network.", "citation_count": "13", "reference_count": "1,033", "date": "1993", "authors": ["Ken-ichi Funahashi", "Yuichi Nakamura"], "related_topics": ["Recurrent neural network", "Feedforward neural network", "Dynamical systems theory", "Artificial neural network", "Dynamical system (definition)", "Initial value problem", "Multidimensional systems", "Corollary", "Topology", "Algorithm", "Mathematics"]}
{"id": "1543237449", "references": ["1483156930", "2080119716", "2168120373", "2003209004", "2043721804", "2040739363", "2018742520", "2039619955", "2098301339", "2090257483"], "title": "Real-time prediction of hand trajectory by ensembles of cortical neurons in primates", "abstract": "Signals derived from the rat motor cortex can be used for controlling one-dimensional movements of a robot arm1. It remains unknown, however, whether real-time processing of cortical signals can be employed to reproduce, in a robotic device, the kind of complex arm movements used by primates to reach objects in space. Here we recorded the simultaneous activity of large populations of neurons, distributed in the premotor, primary motor and posterior parietal cortical areas, as non-human primates performed two distinct motor tasks. Accurate real-time predictions of one- and three-dimensional arm movement trajectories were obtained by applying both linear and nonlinear algorithms to cortical neuronal ensemble activity recorded from each animal. In addition, cortically derived signals were successfully used for real-time control of robotic devices, both locally and through the Internet. These results suggest that long-term control of complex prosthetic robot arm movements can be achieved by simple real-time transformations of neuronal population signals derived from multiple cortical areas in primates.", "citation_count": "30", "reference_count": "1,769", "date": "2000", "authors": ["Johan Wessberg", "Christopher R. Stambaugh", "Jerald D. Kralik", "Pamela D. Beck", "Mark Laubach", "John K. Chapin", "Jung Kim", "S. James Biggs", "Mandayam A. Srinivasan", "Miguel A. L. Nicolelis"], "related_topics": ["Premotor cortex", "Robotic arm", "Body movement", "Motor cortex", "Neurophysiology", "Experimental Brain Research", "Parietal lobe", "Cerebral cortex", "Neuroscience", "Bioinformatics", "Biology"]}
{"id": "2143879519", "references": ["2097560155", "1984931175", "2145889472", "2162489171", "1589549454", "1570751785", "2167034998", "2137234026", "2116360511", "1993197592"], "title": "Reconstruction of Natural Scenes from Ensemble Responses in the Lateral Geniculate Nucleus", "abstract": "A major challenge in studying sensory processing is to understand the meaning of the neural messages encoded in the spiking activity of neurons. From the recorded responses in a sensory circuit, what information can we extract about the outside world? Here we used a linear decoding technique to reconstruct spatiotemporal visual inputs from ensemble responses in the lateral geniculate nucleus (LGN) of the cat. From the activity of 177 cells, we have reconstructed natural scenes with recognizable moving objects. The quality of reconstruction depends on the number of cells. For each point in space, the quality of reconstruction begins to saturate at six to eight pairs of on and off cells, approaching the estimated coverage factor in the LGN of the cat. Thus, complex visual inputs can be reconstructed with a simple decoding algorithm, and these analyses provide a basis for understanding ensemble coding in the early visual pathway.", "citation_count": "41", "reference_count": "308", "date": "1999", "authors": ["Garrett B. Stanley", "Fei F. Li", "Yang Dan"], "related_topics": ["Neural decoding", "Lateral geniculate nucleus", "Sensory system", "Sensory processing", "Decoding methods", "Coding (social sciences)", "Pattern recognition", "Basis (linear algebra)", "Neuroscience", "Natural (music)", "Computer science", "Artificial intelligence"]}
{"id": "2094631910", "references": ["2325337867", "2320365816", "2014096963", "2102892532", "2470860349", "2008536883", "117960354", "2047857069", "2286035120", "2056942998"], "title": "Oscillation and chaos in physiological control systems", "abstract": "First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases.", "citation_count": "26", "reference_count": "4,910", "date": "1977", "authors": ["Michael C. Mackey", "Leon Glass"], "related_topics": ["Dynamical systems theory", "Nonlinear system", "Oscillation (cell signaling)", "Chaotic", "Aperiodic graph", "Statistical physics", "Physics", "Chaos (genus)", "Limit cycle oscillation", "Physiological control"]}
{"id": "2045182040", "references": ["2034099719", "2129287653", "2496842803", "2136038769", "2171987742", "1549386224", "2024668293", "1689445748", "3036383388", "1599681710"], "title": "WINNING ENTRY OF THE K. U. LEUVEN TIME-SERIES PREDICTION COMPETITION", "abstract": "In this paper we describe the winning entry of the time-series prediction competition which was part of the International Workshop on Advanced Black-Box Techniques for Nonlinear Modeling, held at K. U. Leuven, Belgium on July 8\u201310, 1998. We also describe the source of the data set, a nonlinear transform of a 5-scroll generalized Chua's circuit. Participants were given 2000 data points and were asked to predict the next 200 points in the series. The winning entry exploited symmetry that was discovered during exploratory data analysis and a method of local modeling designed specifically for the prediction of chaotic time-series. This method includes an exponentially weighted metric, a nearest trajectory algorithm, integrated local averaging, and a novel multistep ahead cross-validation estimation of model error for the purpose of parameter optimization.", "citation_count": "42", "reference_count": "60", "date": "1999", "authors": ["J McNames", "Johan Suykens", "Joos Vandewalle"], "related_topics": ["Time series", "Metric (mathematics)", "Exploratory data analysis", "Data point", "Series (mathematics)", "Chaotic", "Errors-in-variables models", "Algorithm", "Data set", "Artificial intelligence", "Computer science"]}
{"id": "2166322089", "references": ["1997063559", "1998367480", "2150593711", "2171277043", "2127218421", "2094631910", "2098929365", "65738273", "2913399920", "2002182716"], "title": "'Neural-gas' network for vector quantization and its application to time-series prediction", "abstract": "A neural network algorithm based on a soft-max adaptation rule is presented. This algorithm exhibits good performance in reaching the optimum minimization of a cost function for vector quantization data compression. The soft-max rule employed is an extension of the standard K-means clustering procedure and takes into account a neighborhood ranking of the reference (weight) vectors. It is shown that the dynamics of the reference (weight) vectors during the input-driven adaptation procedure are determined by the gradient of an energy function whose shape can be modulated through a neighborhood determining parameter and resemble the dynamics of Brownian particles moving in a potential determined by the data point density. The network is used to represent the attractor of the Mackey-Glass equation and to predict the Mackey-Glass time series, with additional local linear mappings for generating output values. The results obtained for the time-series prediction compare favorably with the results achieved by backpropagation and radial basis function networks. &gt;", "citation_count": "28", "reference_count": "1,862", "date": "1993", "authors": ["T.M. Martinetz", "S.G. Berkovich", "K.J. Schulten"], "related_topics": ["Neural gas", "Vector quantization", "Backpropagation", "k-means clustering", "Artificial neural network", "Radial basis function", "Cluster analysis", "Attractor", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2134514463", "references": ["2034099719", "2109779438", "2154642048", "2138784882", "2171277043", "1978970913", "2145085734", "2166322089", "177635913", "2142334564"], "title": "A new evolutionary system for evolving artificial neural networks", "abstract": "This paper presents a new evolutionary system, i.e., EPNet, for evolving artificial neural networks (ANNs). The evolutionary algorithm used in EPNet is based on Fogel's evolutionary programming (EP). Unlike most previous studies on evolving ANN's, this paper puts its emphasis on evolving ANN's behaviors. Five mutation operators proposed in EPNet reflect such an emphasis on evolving behaviors. Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting. EPNet evolves ANN's architectures and connection weights (including biases) simultaneously in order to reduce the noise in fitness evaluation. The parsimony of evolved ANN's is encouraged by preferring node/connection deletion to addition. EPNet has been tested on a number of benchmark problems in machine learning and ANNs, such as the parity problem, the medical diagnosis problems, the Australian credit card assessment problem, and the Mackey-Glass time series prediction problem. The experimental results show that EPNet can produce very compact ANNs with good generalization ability in comparison with other algorithms.", "citation_count": "48", "reference_count": "1,212", "date": "1997", "authors": ["X. Yao", "Y. Liu"], "related_topics": ["Evolutionary programming", "Evolutionary algorithm", "Evolutionary computation", "Genetic programming", "Artificial neural network", "Feedforward neural network", "Node (networking)", "Artificial intelligence", "Machine learning", "Computer science", "Offspring"]}
{"id": "2061939373", "references": ["2076977756", "1630264553", "2030572719", "1540903397", "3005347330", "1550301432", "2141013982"], "title": "IPython: A System for Interactive Scientific Computing", "abstract": "Python offers basic facilities for interactive work and a comprehensive library on top of which more sophisticated systems can be built. The IPython project provides on enhanced interactive environment that includes, among other features, support for data visualization and facilities for distributed and parallel computation", "citation_count": "7", "reference_count": "3,362", "date": "2007", "authors": ["F. Perez", "B.E. Granger"], "related_topics": ["Python (programming language)", "Data visualization", "Object-oriented programming", "High-level programming language", "Computer science", "Computational science"]}
{"id": "2110114082", "references": ["2912827267"], "title": "Python for Scientific Computing", "abstract": "Python is an excellent \"steering\" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.", "citation_count": "1", "reference_count": "3,298", "date": "2007", "authors": ["Travis E. Oliphant"], "related_topics": ["Scripting language", "High-level programming language", "Python (programming language)", "Fourth-generation programming language", "Programming language", "Computer science", "Computational science"]}
{"id": "2006903949", "references": ["2111406701", "19621276", "2125389748", "2170120409", "2111051539", "2007431958", "2114766824", "1507849272", "2176028050", "2051812123"], "title": "Fast exact multiplication by the Hessian", "abstract": "Just storing the Hessian H (the matrix of second derivatives \u03b42E/\u03b4wi\u03b4 wj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv{f(w)} = (\u03b4/\u03b4r)f(w + rv)|r=0, note that Rv{\u2207w} = Hv and Rv{w} = v, and then apply Rv{\u00b7} to the equations used to compute \u2207w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.", "citation_count": "37", "reference_count": "582", "date": "1994", "authors": ["Barak A. Pearlmutter"], "related_topics": ["Hessian matrix", "Gradient method", "Stochastic gradient descent", "Backpropagation", "Differential operator", "Second derivative", "Stochastic approximation", "Computation", "Applied mathematics", "Algorithm", "Mathematics"]}
{"id": "2254715784", "references": ["2025768430", "2136922672", "2310919327", "1498436455", "2072128103"], "title": "Theano: Deep Learning on GPUs with Python", "abstract": "In this paper, we present Theano 1 , a framework in the Python programming language for defining, optimizing and evaluating expressions involving high-level operations on tensors. Theano offers most of NumPy\u2019s functionality, but adds automatic symbolic differentiation, GPU support, and faster expression evaluation. Theano is a general mathematical tool, but it was developed with the goal of facilitating research in deep learning. The Deep Learning Tutorials 2 introduce recent advances in deep learning, and showcase how Theano", "citation_count": "5", "reference_count": "300", "date": "2012", "authors": ["James Bergstra", "Frederic Bastien", "Olivier Breuleux", "Pascal Lamblin", "Razvan Pascanu", "Olivier Delalleau", "Guillaume Desjardins", "David Warde-Farley", "Ian Goodfellow", "Arnaud Bergeron", "Yoshua Bengio"], "related_topics": ["Theano", "NumPy", "Python (programming language)", "Deep learning", "Compiler", "CUDA", "Programming language", "Computer science", "Expression (mathematics)", "Artificial intelligence"]}
{"id": "3005347330", "references": ["2061939373", "2166562121", "3003257820", "1606347560", "2964010366", "2384495648", "2015159529", "2970971581", "3101479050", "2800392236"], "title": "SciPy: Open Source Scientific Tools for Python", "abstract": "", "citation_count": "0", "reference_count": "6,150", "date": "2001", "authors": ["E Jones", "T Oliphant", "P Peterson"], "related_topics": ["Python (programming language)", "Scientific instrument", "Computer science", "Programming language", "Open source"]}
{"id": "2185726469", "references": ["2143719855", "2171645483", "179875071", "1970689298", "2158195707", "2171928131", "10704533", "2091812280", "2132339004", "2096072088"], "title": "Empirical Evaluation and Combination of Advanced Language Modeling Techniques.", "abstract": "We present results obtained with several advanced language modeling techniques, including class based model, cache model, maximum entropy model, structured language model, random forest language model and several types of neural network based language models. We show results obtained after combining all these models by using linear interpolation. We conclude that for both small and moderately sized tasks, we obtain new state of the art results with combination of models, that is significantly better than performance of any individual model. Obtained perplexity reductions against Good-Turing trigram baseline are over 50% and against modified Kneser-Ney smoothed 5-gram over 40%.", "citation_count": "18", "reference_count": "352", "date": "2011", "authors": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Luk\u00e1s Burget", "Jan Cernock\u00fd"], "related_topics": ["Language model", "Perplexity", "Trigram", "Principle of maximum entropy", "Random forest", "Artificial neural network", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2011301426", "references": ["3039901154", "2132022337", "3003257820", "1606347560", "2244729984", "3099878876", "2015159529", "2432815617", "2798336535"], "title": "Matplotlib: A 2D Graphics Environment", "abstract": "Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems", "citation_count": "0", "reference_count": "18,762", "date": "2007", "authors": ["J.D. Hunter"], "related_topics": ["2D computer graphics", "Computer graphics", "Python (programming language)", "Scripting language", "User interface", "NumPy", "Object-oriented programming", "Computer graphics (images)", "Programming language", "Computer science", "Image generation"]}
{"id": "1578856370", "references": ["2154278880", "1951216520", "2140897751", "2122585011", "1989314204", "1790231888", "3088653102", "1992475611", "2963206148"], "title": "Spoken Language Processing: A Guide to Theory, Algorithm, and System Development", "abstract": "From the Publisher:\r\n\r\nNew advances in spoken language processing: theory and practice\r\nIn-depth coverage of speech processing, speech recognition, speech synthesis, spoken language understanding, and speech interface design\r\nMany case studies from state-of-the-art systems, including examples from Microsoft's advanced research labs\r\n\r\n\r\nSpoken Language Processing draws on the latest advances and techniques from multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, and beyond. Starting with the fundamentals, it presents all this and more: \r\n\r\nEssential background on speech production and perception, probability and information theory, and pattern recognition\r\nExtracting information from the speech signal: useful representations and practical compression solutions\r\nModern speech recognition techniques: hidden Markov models, acoustic and language modeling, improving resistance to environmental noises, search algorithms, and large vocabulary speech recognition\r\nText-to-speech: analyzing documents, pitch and duration controls; trainable synthesis, and more\r\nSpoken language understanding: dialog management, spoken language applications, and multimodal interfaces \r\n\r\n\r\nTo illustrate the book's methods, the authors present detailed case studies based on state-of-the-art systems, including Microsoft's Whisper speech recognizer, Whistler text-to-speech system, Dr. Who dialog system, and the MiPad handheld device. Whether you're planning, designing, building, or purchasing spoken language technology, this is the state of the art\u0097fromalgorithms through business productivity.", "citation_count": "0", "reference_count": "4,281", "date": "2001", "authors": ["Xuedong Huang", "Alex Acero", "Hsiao-Wuen Hon", "Raj Reddy"], "related_topics": ["Speech processing", "Speech corpus", "Spoken language", "Language technology", "Speech analytics", "Speech synthesis", "Language model", "Cache language model", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2131774270", "references": ["1980501707", "2974222084", "3161062409", "2110871230", "2173629880", "183625566", "1554663460", "1988520084", "2143503258", "2095539364"], "title": "Bidirectional recurrent neural networks", "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.", "citation_count": "15", "reference_count": "5,215", "date": "1997", "authors": ["M. Schuster", "K.K. Paliwal"], "related_topics": ["Recurrent neural network", "Posterior probability", "Frame (networking)", "Pattern recognition", "Speech processing", "Symbol (chemistry)", "Computer science", "Structure (category theory)", "Distribution (mathematics)", "Artificial intelligence", "Recurrent neural nets"]}
{"id": "2142069714", "references": ["2046079134", "1549285799", "2102734279", "1970800786", "2125838338", "2178432768", "2133059825", "1554663460", "2010595692", "1885639605"], "title": "Online and off-line handwriting recognition: a comprehensive survey", "abstract": "Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered.", "citation_count": "215", "reference_count": "3,475", "date": "2000", "authors": ["R. Plamondon", "S.N. Srihari"], "related_topics": ["Intelligent character recognition", "Handwriting", "Handwriting recognition", "Word recognition", "Reading (process)", "Written language", "Character (computing)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1659842140", "references": ["2143560894", "2124823351", "2111935653", "2017337590", "2171865010", "2118331730", "2134389439", "1483736078", "1501347617"], "title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence", "abstract": "From the Publisher:\r\nGenetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.\r\nIn its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.\r\nInitially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.\r\nJohn H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.", "citation_count": "0", "reference_count": "17,161", "date": "1992", "authors": ["John H. Holland"], "related_topics": ["Complex adaptive system", "Adaptation (computer science)", "Game theory", "Field (computer science)", "Artificial intelligence", "Natural (music)", "Research program", "Engineering", "Coevolution", "Range (mathematics)"]}
{"id": "2112090702", "references": ["2079948225", "111157985", "2905110430", "2017444605", "2062663664", "2088678566", "2061901927", "2026552514", "2025490132", "3049667020"], "title": "Collective dynamics of small-world networks", "abstract": "Networks of coupled dynamical systems have been used to model biological oscillators, Josephson junction arrays, excitable media, neural networks, spatial games, genetic control networks and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon (popularly known as six degrees of separation. The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.", "citation_count": "28", "reference_count": "47,081", "date": "1998", "authors": ["Duncan J. Watts", "Steven H. Strogatz"], "related_topics": ["Complex network", "Evolving networks", "Network motif", "Synchronization networks", "Barab\u00e1si\u2013Albert model", "Degree distribution", "Network science", "Interdependent networks", "Topology", "Computer science"]}
{"id": "2008620264", "references": ["2905110430", "3125161049", "2147164982", "1643412971", "2121821841", "2062021443", "2107252390", "2112090702", "1971788485", "2769133055"], "title": "Emergence of Scaling in Random Networks", "abstract": "Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.", "citation_count": "16", "reference_count": "40,132", "date": "1999", "authors": ["Albert L\u00e1szl\u00f3 Barab\u00e1si", "R\u00e9ka Albert"], "related_topics": ["Evolving networks", "Complex network", "Network motif", "Barab\u00e1si\u2013Albert model", "Interdependent networks", "Degree distribution", "Hierarchical network model", "Biological network", "Topology", "Computer science"]}
{"id": "2113651538", "references": ["2147880316", "2156515921", "1530699444", "2150102617", "2019363670", "2129191766", "2035720976", "3017143921", "2142623206", "2068484625"], "title": "The Tradeoffs of Large Scale Learning", "abstract": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.", "citation_count": "28", "reference_count": "1,611", "date": "2007", "authors": ["Olivier Bousquet", "L\u00e9on Bottou"], "related_topics": ["Computational learning theory", "Stability (learning theory)", "Algorithmic learning theory", "Instance-based learning", "Active learning (machine learning)", "Empirical risk minimization", "Proactive learning", "Online machine learning", "Sample exclusion dimension", "Mathematical optimization", "Theoretical computer science", "Computer science", "Generalization error"]}
{"id": "2156779765", "references": ["2113651538", "1510073064", "2205628031", "1992208280", "2142623206", "3141595720", "2799002609", "1499021337", "3023786531"], "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning", "abstract": "We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a. Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a. Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.", "citation_count": "31", "reference_count": "586", "date": "2011", "authors": ["Eric Moulines", "Francis R. Bach"], "related_topics": ["Stochastic gradient descent", "Stochastic approximation", "Online machine learning", "Rate of convergence", "Convex function", "Asymptotic analysis", "Convexity", "Convergence (routing)", "Mathematical optimization", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1598497354", "references": ["2113651538", "3029645440", "1970789124", "2137515395", "2164273299", "1491622225"], "title": "A fast natural Newton method", "abstract": "Nowadays, for many tasks such as object recognition or language modeling, data is plentiful. As such, an important challenge has become to find learning algorithms which can make use of all the available data. In this setting, called \"large-scale learning\" by Bottou &amp; Bousquet (2008), learning and optimization become different and powerful optimization algorithms are suboptimal learning algorithms. While most efforts are focused on adapting optimization algorithms for learning by efficiently using the information contained in the Hessian, Le Roux et al. (2008) exploited the special structure of the learning problem to achieve faster convergence. In this paper, we investigate a natural way of combining these two directions to yield fast and robust learning algorithms.", "citation_count": "6", "reference_count": "61", "date": "2010", "authors": ["Nicolas L. Roux", "Andrew W. Fitzgibbon"], "related_topics": ["Semi-supervised learning", "Instance-based learning", "Active learning (machine learning)", "Online machine learning", "Computational learning theory", "Stability (learning theory)", "Multi-task learning", "Algorithmic learning theory", "Artificial intelligence", "Machine learning", "Computer science", "Generalization error"]}
{"id": "2137515395", "references": ["2113651538", "2914484425", "2150102617", "2135106139", "2165966284", "2166347285", "2142623206", "2051669046", "1491622225", "1568229137"], "title": "SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent", "abstract": "The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the \"Wild Track\" of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008).", "citation_count": "14", "reference_count": "373", "date": "2009", "authors": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "related_topics": ["Stochastic gradient descent", "Gradient descent", "Gradient method", "Neighbourhood components analysis", "Nonlinear conjugate gradient method", "Backpropagation", "Descent direction", "Iterated function", "Algorithm", "Mathematics"]}
{"id": "2130984546", "references": ["2129142203", "2006903949", "1520168181", "1970789124", "1708197474", "2011395874", "1554663460", "2112462566", "2986444355", "2137269967"], "title": "Fast curvature matrix-vector products for second-order gradient descent", "abstract": "We propose a generic method for iteratively approximating various second-order gradient steps--Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient--in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.", "citation_count": "26", "reference_count": "313", "date": "2002", "authors": ["Nicol N. Schraudolph"], "related_topics": ["Gradient method", "Gradient descent", "Curvature", "Jacobian matrix and determinant", "Matrix (mathematics)", "Acceleration", "Matrix method", "Gauss\u2013Seidel method", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "1568229137", "references": ["2112447569", "2158899491", "2005876975", "2071707134", "2963433607", "2117812871", "2963606038"], "title": "Adaptive Algorithms and Stochastic Approximations", "abstract": "Adaptive systems are widely encountered in many applications ranging through adaptive filtering and more generally adaptive signal processing, systems identification and adaptive control, to pattern recognition and machine intelligence: adaptation is now recognised as keystone of \"intelligence\" within computerised systems. These diverse areas echo the classes of models which conveniently describe each corresponding system. Thus although there can hardly be a \"general theory of adaptive systems\" encompassing both the modelling task and the design of the adaptation procedure, nevertheless, these diverse issues have a major common component: namely the use of adaptive algorithms, also known as stochastic approximations in the mathematical statistics literature, that is to say the adaptation procedure (once all modelling problems have been resolved). The juxtaposition of these two expressions in the title reflects the ambition of the authors to produce a reference work, both for engineers who use these adaptive algorithms and for probabilists or statisticians who would like to study stochastic approximations in terms of problems arising from real applications. Hence the book is organised in two parts, the first one user-oriented, and the second providing the mathematical foundations to support the practice described in the first part. The book covers the topcis of convergence, convergence rate, permanent adaptation and tracking, change detection, and is illustrated by various realistic applications originating from these areas of applications.", "citation_count": "0", "reference_count": "2,688", "date": "1990", "authors": ["Albert Benveniste", "Pierre Priouret", "Michel M\u00e9tivier"], "related_topics": ["Adaptive system", "Adaptive control", "Adaptive filter", "Adaptation (computer science)", "Stochastic approximation", "Component (UML)", "Pattern recognition (psychology)", "Convergence (routing)", "Computer science", "Artificial intelligence", "Algorithm"]}
{"id": "1526055535", "references": ["1597286183", "2154642048", "1980658026", "413857758", "2157629899", "2581275558", "1507849272", "2293063825", "2155487652"], "title": "Experiments on Learning by Back Propagation.", "abstract": "Abstract : Rumelhart, Hinton and Williams (Rumelhart 86) describe a learning procedure for layered networks of deterministic, neuron-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in weight space . We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. We give some preliminary results on scaling and show how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results illustrate the effects on learning speed of the amount of interaction between the weights. A variation of the learning procedure that back-propagates desired state information rather than error gradients is developed and compared with the standard procedure. Finally, we discuss the relationship between our iterative networks and the analog networks described by Hopefield and Tank (Hopfield 85). The learning procedure can discover appropriate weights in their kind of network, as well as determine an optimal schedule for varying the nonlinearity of the units during a search.", "citation_count": "9", "reference_count": "547", "date": "1986", "authors": ["David C Plaut", "Steven J Nowlan", "Geoffrey E Hinton"], "related_topics": ["Active learning (machine learning)", "Artificial neural network", "Backpropagation", "Discrimination learning", "Schedule", "Algorithm", "Measure (mathematics)", "Noise (video)", "Set (abstract data type)", "Artificial intelligence", "Computer science"]}
{"id": "2225156818", "references": ["2963977107", "1997063559", "2962897886", "2334889010", "2159080219", "1959608418", "2049633694", "2166851633", "2107107106", "1880262756"], "title": "Variational Inference: A Review for Statisticians", "abstract": "ABSTRACTOne of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback\u2013Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data...", "citation_count": "147", "reference_count": "2,109", "date": "2017", "authors": ["David M. Blei", "Alp Kucukelbir", "Jon D. McAuliffe"], "related_topics": ["Frequentist inference", "Fiducial inference", "Bayesian inference", "Bayesian statistics", "Inference", "Bayesian probability", "Exponential family", "Computational statistics", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "1491468723", "references": ["2102017903", "1904365287", "1872489089", "2146502635", "196761320", "1815076433", "1970789124", "2294059674", "2108384452", "2296319761"], "title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods", "abstract": "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages.", "citation_count": "42", "reference_count": "107", "date": "2014", "authors": ["Jascha Sohl-Dickstein", "Ben Poole", "Surya Ganguli"], "related_topics": ["Stochastic gradient descent", "Hessian matrix", "Optimization problem", "Subspace topology", "Quadratic equation", "Algorithm", "Mathematical optimization", "MATLAB", "Hyperparameter", "Inverse", "Mathematics"]}
{"id": "607505555", "references": ["1480376833", "2136922672", "2147880316", "2156909104", "2072128103", "2911964244", "2296616510", "2148603752", "2296319761", "1663973292"], "title": "Understanding Machine Learning: From Theory to Algorithms", "abstract": "Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.", "citation_count": "191", "reference_count": "3,469", "date": "2015", "authors": ["Shai Shalev-Shwartz", "Shai Ben-David"], "related_topics": ["Algorithmic learning theory", "Computational learning theory", "Active learning (machine learning)", "Stability (learning theory)", "Inductive transfer", "Artificial neural network", "Stochastic gradient descent", "Field (computer science)", "Computer science", "Theoretical computer science", "Machine learning", "Algorithm", "Artificial intelligence"]}
{"id": "2523246573", "references": ["1836465849", "2271840356", "2951781666", "2146502635", "2903382683", "6908809", "2964121744", "2250539671", "2296073425", "2168231600"], "title": "An overview of gradient descent optimization algorithms", "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.", "citation_count": "18", "reference_count": "2,549", "date": "2016", "authors": ["Sebastian Ruder"], "related_topics": ["Gradient descent", "Artificial intelligence", "Computer science", "Optimization algorithm"]}
{"id": "2251098065", "references": ["2158899491", "2131462252", "2146574666", "1970689298", "2171928131", "2006969979", "2158195707", "2124807415", "2121227244", "2132339004"], "title": "Continuous Space Translation Models with Neural Networks", "abstract": "The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.", "citation_count": "33", "reference_count": "143", "date": "2012", "authors": ["Hai-Son Le", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "related_topics": ["Translation (geometry)", "Context (language use)", "Artificial neural network", "Representation (mathematics)", "Phrase", "Machine learning", "Scale (descriptive set theory)", "Computer science", "Space (commercial competition)", "Artificial intelligence", "Training set"]}
{"id": "2143719855", "references": ["1632114991", "179875071", "2130917146", "2131462252", "1970689298", "2056590938", "2121227244", "2132339004", "83522546"], "title": "Structured Output Layer neural network language model", "abstract": "This paper introduces a new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM. This model is able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs. Several softmax layers replace the standard output layer in this model. The output structure depends on the word clustering which uses the continuous word representation induced by a NNLM. The GALE Mandarin data was used to carry out the speech-to-text experiments and evaluate the NNLMs. On this data the well tuned baseline system has a character error rate under 10%. Our model achieves consistent improvements over the combination of an n-gram model and classical short-list NNLMs both in terms of perplexity and recognition accuracy.", "citation_count": "13", "reference_count": "175", "date": "2011", "authors": ["Hai-Son Le", "Ilya Oparin", "Alexandre Allauzen", "Jean-Luc Gauvain", "Francois Yvon"], "related_topics": ["Word error rate", "Language model", "Cluster analysis", "Word (computer architecture)", "Softmax function", "Artificial neural network", "Perplexity", "Context (language use)", "Vocabulary", "Speech recognition", "Speech synthesis", "Computer science", "Text mining"]}
{"id": "2250379827", "references": ["2134800885", "179875071", "2131462252", "1970689298", "2158195707", "2171928131", "2124807415", "2132339004", "36903255", "2072128103"], "title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "abstract": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build.", "citation_count": "27", "reference_count": "126", "date": "2012", "authors": ["Holger Schwenk", "Anthony Rousseau", "Mohammed Attik"], "related_topics": ["Machine translation", "Language model", "Machine learning", "Natural language processing", "Computer science", "Space (commercial competition)", "Task (computing)", "BLEU", "Artificial intelligence"]}
{"id": "2146574666", "references": ["2078861931", "1517947178", "2101105183", "1603508585", "2170120409", "1529616844", "2154124206", "1979102019", "2018482254", "3017143921"], "title": "Minimum Error Rate Training in Statistical Machine Translation", "abstract": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.", "citation_count": "18", "reference_count": "3,527", "date": "2003", "authors": ["Franz Josef Och"], "related_topics": ["Interactive machine translation", "Hybrid machine translation", "Example-based machine translation", "Transfer-based machine translation", "Machine translation", "Evaluation of machine translation", "Word error rate", "Machine translation software usability", "Synchronous context-free grammar", "Machine learning", "Computer science", "BLEU", "Artificial intelligence"]}
{"id": "2103078213", "references": ["2437096199", "1970689298", "2158195707", "2006969979", "2154124206", "2096175520", "2124807415", "2109664771", "2132339004", "1631260214"], "title": "Continuous Space Language Models for Statistical Machine Translation", "abstract": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems.\r\n\r\nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data.\r\n\r\nWe also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks.", "citation_count": "25", "reference_count": "115", "date": "2006", "authors": ["Holger Schwenk", "Daniel Dechelotte", "Jean-Luc Gauvain"], "related_topics": ["Machine translation", "Cache language model", "Rule-based machine translation", "Transfer-based machine translation", "Machine translation software usability", "Language model", "Example-based machine translation", "Universal Networking Language", "Language identification", "Evaluation of machine translation", "Object language", "Vocabulary", "Natural language processing", "Speech recognition", "Computer science", "BLEU", "Artificial intelligence"]}
{"id": "2109664771", "references": ["2101105183", "2173213060", "222053410", "2158195707", "2006969979", "2111305191", "2838046082", "1934041838", "2134237567", "2119168550"], "title": "Large Language Models in Machine Translation", "abstract": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.", "citation_count": "44", "reference_count": "672", "date": "2007", "authors": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "related_topics": ["Machine translation", "Language model", "Factor (programming language)", "Speech recognition", "Function (mathematics)", "Computer science"]}
{"id": "2156985047", "references": ["1916559533", "1517947178", "2116316001", "2006969979", "2049633694", "47415966", "1538023239", "2119168550", "2038698865", "2139403546"], "title": "A systematic comparison of various statistical alignment models", "abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.", "citation_count": "37", "reference_count": "4,803", "date": "2003", "authors": ["Franz Josef Och", "Hermann Ney"], "related_topics": ["Tree alignment", "Statistical model", "Hidden Markov model", "Viterbi algorithm", "Heuristic", "Smoothing", "Hybrid machine translation", "Word (computer architecture)", "Algorithm", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2138584836", "references": ["1489181569", "2011008859", "2066873261", "2097333193", "1600795850", "2752853835", "2117652747", "2111173177", "1593045043", "2799137445"], "title": "Text-translation alignment", "abstract": "We present an algorithm for aligning texts with their translations that is based only on internal evidence. The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions. It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate. The algorithm appears to converge to the correct sentence alignment in only a few iterations.", "citation_count": "12", "reference_count": "657", "date": "1993", "authors": ["Martin Kay", "Martin R\u00f6scheisen"], "related_topics": ["Word (computer architecture)", "Sentence", "Similarity (geometry)", "Translation (geometry)", "Algorithm", "Computer science", "Maximum likelihood", "Partial alignment", "Relaxation process"]}
{"id": "1489181569", "references": ["2048390999", "2120062331", "2097333193", "1501400124", "2028770325", "2117652747", "2099247782", "2801179766", "78534123", "1973948212"], "title": "A program for aligning sentences in bilingual corpora", "abstract": "Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language.This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences.It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English-French subcorpus than on the English-German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs.To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program.", "citation_count": "11", "reference_count": "1,962", "date": "1993", "authors": ["William A. Gale", "Kenneth W. Church"], "related_topics": ["Sentence", "Machine translation", "Computational linguistics", "Phrase", "Thesaurus", "Natural language processing", "German", "Character (computing)", "Word error rate", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2048390999", "references": ["1991133427", "1966812932", "2142901448", "2334801970", "2021021293", "2049633694", "1575431606", "2035227369", "1594031697"], "title": "A statistical approach to language translation", "abstract": "An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French.Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence.We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences.", "citation_count": "9", "reference_count": "369", "date": "1988", "authors": ["P. Brown", "J. Cocke", "S. Della Pietra", "V. Della Pietra", "F. Jelinek", "R. Mercer", "P. Roossin"], "related_topics": ["Language translation", "Glossary", "Source text", "Information extraction", "Sentence", "Set (abstract data type)", "Natural language processing", "Word (computer architecture)", "Translation (geometry)", "Information retrieval", "Computer science", "Artificial intelligence", "Automatic translation"]}
{"id": "2154384676", "references": ["2097333193", "2028770325", "2117652747", "2099247782"], "title": "Identifying word correspondence in parallel texts", "abstract": "Researchers in both machine translation (e.g., Brown et al, 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts (also known as bilingual corpora), bodies of text such as the Canadian Hansards (parliamentary debates) which are available in multiple languages (such as French and English). Much of the current excitement surrounding parallel texts was initiated by Brown et al. (1990), who outline a self-organizing method for using these parallel texts to build a machine translation system.", "citation_count": "4", "reference_count": "475", "date": "1991", "authors": ["William A. Gale", "Kenneth W. Church"], "related_topics": ["Machine translation", "Linguistics", "Natural language processing", "Word (computer architecture)", "Computer science", "Artificial intelligence", "Lexicography", "Machine translation system"]}
{"id": "2129139611", "references": ["2048390999", "2097333193", "2099345940", "2169528353", "195809013", "1976241232", "2120234416", "1594031697", "1971220772"], "title": "WORD-SENSE DISAMBIGUATION USING STATISTICAL METHODS", "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.", "citation_count": "9", "reference_count": "613", "date": "1991", "authors": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "related_topics": ["SemEval", "Word error rate", "Statistical semantics", "Word (computer architecture)", "Context (language use)", "Natural language processing", "Mutual information", "Speech recognition", "Computer science", "Translation (geometry)", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "2117652747", "references": ["1489181569", "2048390999", "2341171179", "2097333193", "2028770325", "2049633694", "1880089301", "1575431606"], "title": "ALIGNING SENTENCES IN PARALLEL CORPORA", "abstract": "In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried.", "citation_count": "8", "reference_count": "846", "date": "1991", "authors": ["Peter F. Brown", "Jennifer C. Lai", "Robert L. Mercer"], "related_topics": ["Sentence", "Set (abstract data type)", "Natural language processing", "Variety (linguistics)", "Speech recognition", "Computer science", "Artificial intelligence", "Parallel corpora"]}
{"id": "2162911105", "references": ["2154642048", "2097998348", "2072128103", "1408639475", "1828163288", "2116064496", "2135181320", "2962968839", "2064675550", "2025768430"], "title": "High-dimensional sequence transduction", "abstract": "We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of- the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.", "citation_count": "24", "reference_count": "51", "date": "2013", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "related_topics": ["Recurrent neural network", "Audio signal processing", "Statistical model", "Transduction (machine learning)", "Speech recognition", "Computer science", "Polyphony", "High dimensional"]}
{"id": "2108563286", "references": ["2618530766", "2136922672", "2156387975", "2097998348", "196761320", "2110798204", "1665214252", "2064675550", "1498436455", "2072128103"], "title": "Advances in optimizing recurrent networks", "abstract": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.", "citation_count": "31", "reference_count": "516", "date": "2013", "authors": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "related_topics": ["Deep learning", "Recurrent neural network", "Artificial neural network", "Types of artificial neural networks", "Machine learning", "Data mining", "Computer science", "Clipping (computer graphics)", "Artificial intelligence", "Recurrent neural nets"]}
{"id": "2962968839", "references": ["2107878631", "2154642048", "2096192494", "2135181320", "2124914669", "1408639475", "2135341757", "2116064496", "2158164339", "2072128103"], "title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription", "abstract": "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.", "citation_count": "25", "reference_count": "466", "date": "2012", "authors": ["Nicolas Boulanger-lewandowski", "Yoshua Bengio", "Pascal Vincent"], "related_topics": ["Recurrent neural network", "Transcription (software)", "Polyphony", "Speech recognition", "Representation (mathematics)", "Computer science", "Musical language", "High dimensional"]}
{"id": "2117278770", "references": ["2075201173", "1974967573", "22168010", "265531733", "9155011", "1926502259", "2109664771", "2123958887"], "title": "Intelligent Selection of Language Model Training Data", "abstract": "We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.", "citation_count": "8", "reference_count": "516", "date": "2010", "authors": ["Robert C. Moore", "William Lewis"], "related_topics": ["Language identification", "Cache language model", "Data control language", "Universal Networking Language", "Language model", "Data manipulation language", "Specification language", "Language primitive", "Low-level programming language", "High-level programming language", "Object language", "First-generation programming language", "Machine translation", "Modeling language", "Programming language specification", "Sentence", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2130450156", "references": ["2101105183", "1574901103", "2146574666", "2006969979", "2049633694", "2098162425", "1773803948", "2152263452", "2153653739", "1631260214"], "title": "Factored Language Models for Statistical Machine Translation", "abstract": "Machine translation systems, as a whole, are currently not able to use the output of linguistic tools, such as part-of-speech taggers, to effectively improve translation performance. However, a new language modeling technique, Factored Language Models can incorporate the additional linguistic information that is produced by these tools. In the field of automatic speech recognition, Factored Language Models smoothed with Generalized Parallel Backoff have been shown to significantly reduce language model perplexity. However, Factored Language Models have previously only been applied to statistical machine translation as part of a second-pass rescoring system. In this thesis, we show that a state-of-the-art phrase-based system using factored language models with generalized parallel backoff can improve performance over an identical system using trigram language models. These improvements can be seen both with the use of additional word features and without. The relative gain from the Factored Language Models increases with smaller training corpora, making this approach especially useful for domains with limited data.", "citation_count": "30", "reference_count": "52", "date": "2006", "authors": ["Amittai E. Axelrod"], "related_topics": ["Cache language model", "Machine translation", "Language model", "Universal Networking Language", "Rule-based machine translation", "Transfer-based machine translation", "Example-based machine translation", "Computer-assisted translation", "Natural language processing", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2148861208", "references": ["2101105183", "2100969003", "2125838338", "1506806321", "2146574666", "2006969979", "1973923101", "2109664771", "2153653739", "1663973292"], "title": "Using Word-Dependent Transition Models in HMM-Based Word Alignment for Statistical Machine Translation", "abstract": "In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment. We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.", "citation_count": "47", "reference_count": "93", "date": "2007", "authors": ["Xiaodong He"], "related_topics": ["Word error rate", "Word (computer architecture)", "Machine translation", "Hidden Markov model", "Phrase", "Speech recognition", "Natural language processing", "Bayesian inference", "Reduction (complexity)", "Computer science", "Transition (fiction)", "Artificial intelligence"]}
{"id": "2132001515", "references": ["1480376833", "1508165687", "2147152072", "2101105183", "2170120409", "2105891181", "2006969979", "2143564602", "24102868", "2153653739"], "title": "Mixture-Model Adaptation for SMT", "abstract": "We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.", "citation_count": "21", "reference_count": "300", "date": "2007", "authors": ["George Foster", "Roland Kuhn"], "related_topics": ["Machine translation", "Mixture model", "Log-linear model", "Granularity", "Percentage point", "Translation (geometry)", "Adaptation (computer science)", "Pattern recognition", "BLEU", "Computer science", "Artificial intelligence"]}
{"id": "2124807415", "references": ["2101105183", "2130450156", "1498238796", "2146574666", "2153653739", "2105891181", "2056250865", "2113788796", "2156985047", "1631260214"], "title": "Moses: Open Source Toolkit for Statistical Machine Translation", "abstract": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.", "citation_count": "13", "reference_count": "6,107", "date": "2007", "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst"], "related_topics": ["Hybrid machine translation", "Computer-assisted translation", "Interactive machine translation", "Example-based machine translation", "Postediting", "Transfer-based machine translation", "Machine translation", "Rule-based machine translation", "Machine translation software usability", "Synchronous context-free grammar", "Evaluation of machine translation", "Speech translation", "Language model", "Pivot language", "Natural language processing", "Computer science", "BLEU", "Artificial intelligence"]}
{"id": "2137387514", "references": ["2146574666", "2280403519", "2597684388", "2401082558", "2162245945", "2124807415", "2153653739", "1631260214"], "title": "Experiments in Domain Adaptation for Statistical Machine Translation", "abstract": "The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task.", "citation_count": "8", "reference_count": "362", "date": "2007", "authors": ["Philipp Koehn", "Josh Schroeder"], "related_topics": ["Example-based machine translation", "Machine translation", "Machine translation software usability", "Domain (software engineering)", "Task (project management)", "Natural language processing", "Computer science", "Artificial intelligence", "Parliament", "Domain adaptation", "Training set"]}
{"id": "2123301721", "references": ["2078861931", "2101105183", "1489409710", "2118021410", "1588719663"], "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.", "citation_count": "5", "reference_count": "2,714", "date": "2005", "authors": ["Satanjeev Banerjee", "Alon Lavie"], "related_topics": ["Hybrid machine translation", "Evaluation of machine translation", "Machine translation", "Metric (mathematics)", "Matching (statistics)", "Meteor (satellite)", "Translation (geometry)", "Pattern recognition", "Algorithm", "Computer science", "Closed captioning", "Arabic", "Artificial intelligence"]}
{"id": "2078861931", "references": ["2604799547", "2150824314", "2123301721", "2146574666", "2149327368", "1489409710", "2963463964", "2996403597"], "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "abstract": "Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&amp;D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an \"evaluation understudy\", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research.", "citation_count": "0", "reference_count": "2,135", "date": "2002", "authors": ["George Doddington"], "related_topics": ["Machine translation", "Evaluation of machine translation", "Hybrid machine translation", "NIST", "Word (computer architecture)", "IBM", "n-gram", "Language technology", "Natural language processing", "Information retrieval", "Computer science", "Statistics", "Artificial intelligence"]}
{"id": "222053410", "references": ["2097333193", "2101105183", "2111798208", "2797563284", "2170120409", "1498238796", "2136657878", "1995945562", "2088781183", "2153653739"], "title": "Statistical Significance Tests for Machine Translation Evaluation.", "abstract": "If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.", "citation_count": "15", "reference_count": "1,462", "date": "2004", "authors": ["Philipp Koehn"], "related_topics": ["Machine translation", "Test set", "Test (assessment)", "Natural language processing", "Speech recognition", "BLEU", "Translation (geometry)", "Computer science", "Statistical significance", "Artificial intelligence"]}
{"id": "1489525520", "references": ["2150824314", "2123301721", "2078861931", "1916559533", "2101105183", "1498238796", "23077562", "2154124206", "22168010", "2088781183"], "title": "Re-evaluating the Role of Bleu in Machine Translation Research", "abstract": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu\u2019s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.", "citation_count": "14", "reference_count": "754", "date": "2006", "authors": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn"], "related_topics": ["Evaluation of machine translation", "Machine translation software usability", "Machine translation", "BLEU", "Quality (business)", "Natural language processing", "Metric (unit)", "Computer science", "Artificial intelligence"]}
{"id": "2087735403", "references": ["2143539737", "222053410", "2153635508", "2169279899", "2116492146", "2159107349", "2147192413", "2115081467", "2895810819", "2164777277"], "title": "Findings of the 2012 Workshop on Statistical Machine Translation", "abstract": "This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.", "citation_count": "63", "reference_count": "779", "date": "2012", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia"], "related_topics": ["Evaluation of machine translation", "Machine translation", "Task (project management)", "Ranking", "Natural language processing", "Quality (business)", "Computer science", "Data mining", "Artificial intelligence", "Translation (geometry)", "Measure (data warehouse)"]}
{"id": "2115081467", "references": ["1819903106", "2018869373", "2061910127", "2101105183", "2108325777", "2169279899", "2149327368", "2152311128", "2147192413", "2164777277"], "title": "Findings of the 2009 Workshop on Statistical Machine Translation", "abstract": "This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness.", "citation_count": "48", "reference_count": "776", "date": "2009", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "related_topics": ["Evaluation of machine translation", "Machine translation", "Task (project management)", "Correctness", "Ranking (information retrieval)", "Natural language processing", "Computer science", "Translation (geometry)", "Quality (business)", "Measure (data warehouse)", "Artificial intelligence"]}
{"id": "2895810819", "references": ["2270190199", "2760656271", "2087735403", "2903193068", "2941196361", "2133459682", "2512924740", "2184135559", "2963903950"], "title": "Findings of the 2011 Workshop on Statistical Machine Translation", "abstract": "This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.", "citation_count": "0", "reference_count": "1,073", "date": "2011", "authors": ["Chris Callison-Burch", "Philipp Koehn", "Christof Monz", "Omar Zaidan"], "related_topics": ["Machine translation", "Task (project management)", "Ranking", "Natural language processing", "Short Message Service", "Quality (business)", "Haitian Creole", "Computer science", "Test (assessment)", "Service (systems architecture)", "Artificial intelligence"]}
{"id": "2144161366", "references": ["2493581502", "2166049352", "2151103935", "2124776405", "1782590233", "2152195021", "1554663460", "2613176274", "2162915993", "1595498733"], "title": "A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation", "abstract": "While many models of biological object recognition share a common set of \u2018\u2018broad-stroke\u2019\u2019 properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model\u2014e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct \u2018\u2018parts\u2019\u2019 have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3\u2019s IBM Cell Processor). In analogy to highthroughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.", "citation_count": "38", "reference_count": "335", "date": "2009", "authors": ["Nicolas Pinto", "David Doukhan", "James J. DiCarlo", "David Daniel Cox"], "related_topics": ["Cognitive neuroscience of visual object recognition", "CUDA", "Set (abstract data type)", "Stream processing", "Pooling", "Network architecture", "Representation (mathematics)", "Machine learning", "Facial recognition system", "Computer science", "Artificial intelligence"]}
{"id": "2132424367", "references": ["2156163116", "1652505363", "2110798204", "2310919327", "2100495367", "2139427956", "2029949252", "2064675550", "2172174689", "2122410182"], "title": "Deep, big, simple neural nets for handwritten digit recognition", "abstract": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.", "citation_count": "24", "reference_count": "1,219", "date": "2010", "authors": ["Dan Claudiu Cire\u015fan", "Ueli Meier", "Luca Maria Gambardella", "J\u00fcrgen Schmidhuber"], "related_topics": ["MNIST database", "Artificial neural network", "Overfitting", "Perceptron", "Backpropagation", "Word error rate", "Pattern recognition (psychology)", "Graphics", "Speech recognition", "Computer science"]}
{"id": "1998042868", "references": ["2100588357", "2161969291", "2030536784", "2031489346", "1488125194", "2164598857", "2168356304", "2142159465", "2148596671", "1663973292"], "title": "End-to-end scene text recognition", "abstract": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition.", "citation_count": "21", "reference_count": "970", "date": "2011", "authors": ["Kai Wang", "Boris Babenko", "Serge Belongie"], "related_topics": ["3D single-object recognition", "Intelligent character recognition", "Optical character recognition", "Sketch recognition", "Word recognition", "Cognitive neuroscience of visual object recognition", "Word (computer architecture)", "Reading (process)", "Speech recognition", "Computer science"]}
{"id": "60686164", "references": ["1480376833", "3016663334", "1510052597", "1499307573", "2141363466", "2911964244", "2147148915", "2005059149", "1533856049", "2435432491"], "title": "Sequential model-based optimization for general algorithm configuration", "abstract": "State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.", "citation_count": "25", "reference_count": "1,839", "date": "2011", "authors": ["Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "related_topics": ["Local search (optimization)", "Solver", "Integer programming", "Computational problem", "Boolean satisfiability problem", "Tree (data structure)", "Categorical variable", "Algorithm", "Mathematical optimization", "Sequential model", "Mathematics"]}
{"id": "2106411961", "references": ["1746819321", "2145094598", "2136922672", "2097998348", "1994197834", "2118858186", "2310919327", "2123649031", "1554663460", "2144161366"], "title": "Algorithms for Hyper-Parameter Optimization", "abstract": "Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.", "citation_count": "20", "reference_count": "2,613", "date": "2011", "authors": ["James S. Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "related_topics": ["Bayesian optimization", "Random search", "Feature learning", "Artificial neural network", "Deep belief network", "Machine learning", "Contextual image classification", "Artificial intelligence", "Algorithm", "Hyperparameter", "Computer science"]}
{"id": "2099201756", "references": ["2121863487", "2041946752", "2131824593", "1510052597", "2951665052", "2109910161", "2100495367", "2018044188", "1576452626", "2903158431"], "title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "abstract": "We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.", "citation_count": "75", "reference_count": "1,945", "date": "2010", "authors": ["Eric Brochu", "Vlad M. Cora", "Nando de Freitas"], "related_topics": ["Bayesian optimization", "Reinforcement learning", "Bayesian probability", "User modeling", "Machine learning", "Sampling (statistics)", "Computer science", "Function (engineering)", "Selection (genetic algorithm)", "Artificial intelligence"]}
{"id": "2951665052", "references": ["1746819321", "2108114251", "1625390266", "2099111195", "2146766088", "1510052597", "2099201756", "50486269", "2168405694", "2168464387"], "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "abstract": "Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.", "citation_count": "32", "reference_count": "1,398", "date": "2010", "authors": ["Niranjan Srinivas", "Andreas Krause", "Matthias Seeger", "Sham M. Kakade"], "related_topics": ["Regret", "Multi-armed bandit", "Bayesian optimization", "Stochastic optimization", "Gaussian process", "Reproducing kernel Hilbert space", "Norm (mathematics)", "Curse of dimensionality", "Covariance", "Mathematical optimization", "Computer science"]}
{"id": "1973333099", "references": ["2143022286", "1567512734", "1977046327", "1585773866", "2033900415", "2038669746", "2026645785", "2018044188", "2027792629", "999207820"], "title": "Bayesian Calibration of computer models", "abstract": "We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.", "citation_count": "63", "reference_count": "3,588", "date": "2001", "authors": ["Marc C. Kennedy", "Anthony O'Hagan"], "related_topics": ["Calibration (statistics)", "Uncertainty analysis", "Gaussian process emulator", "Mathematical model", "Context (language use)", "Computer experiment", "Gaussian process", "Stochastic process", "Algorithm", "Statistics", "Mathematics"]}
{"id": "2165599843", "references": ["2112447569", "2113651538", "2567948266", "2172085063", "1536929369", "2049633694", "1516111018", "2159426623", "1880262756", "2001082470"], "title": "Online Learning for Latent Dirichlet Allocation", "abstract": "We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.", "citation_count": "26", "reference_count": "1,728", "date": "2010", "authors": ["Matthew Hoffman", "Francis R. Bach", "David M. Blei"], "related_topics": ["Latent Dirichlet allocation", "Topic model", "Stochastic optimization", "Bayes' theorem", "Machine learning", "Data mining", "Fraction (mathematics)", "Computer science", "Artificial intelligence", "Online learning"]}
{"id": "1983364832", "references": ["2136922672", "2117130368", "2130325614", "2151103935", "2310919327", "2100495367", "2546302380", "2162915993", "2912934387", "2072128103"], "title": "3D Convolutional Neural Networks for Human Action Recognition", "abstract": "We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.", "citation_count": "59", "reference_count": "9,188", "date": "2013", "authors": ["Shuiwang Ji", "Wei Xu", "Ming Yang", "Kai Yu"], "related_topics": ["Deep learning", "Convolutional neural network", "Artificial neural network", "Feature extraction", "Gesture recognition", "Contextual image classification", "Pattern recognition", "Machine learning", "Kernel (image processing)", "Computer science", "Artificial intelligence"]}
{"id": "2063978378", "references": ["2158940042", "1678356000", "2084812512", "2135046866", "1554944419", "2798909945", "2110065044", "1594031697", "2912934387", "3124955340"], "title": "Least angle regression", "abstract": "The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.", "citation_count": "39", "reference_count": "10,437", "date": "2004", "authors": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani", "Hemant Ishwaran", "Keith Knight", "Jean Michel Loubes", "Pascal Massart", "David Madigan", "Greg Ridgeway", "Saharon Rosset", "J. I. Zhu", "Robert A. Stine", "Berwin A. Turlach", "Sanford Weisberg"], "related_topics": ["Elastic net regularization", "Model selection", "Linear model", "Feature selection", "Least-angle regression", "Ordinary least squares", "Linear regression", "Regression analysis", "Algorithm", "Statistics", "Mathematics"]}
{"id": "1980862600", "references": ["2038056950", "2140661818", "2029948425", "2135547275", "2039686608", "1979285432", "2015982521"], "title": "Statistical Learning by 8-Month-Old Infants", "abstract": "Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input.", "citation_count": "7", "reference_count": "5,991", "date": "1996", "authors": ["Jenny R. Saffran", "Richard N. Aslin", "Elissa L. Newport"], "related_topics": ["Language acquisition", "Speech segmentation", "Artificial grammar learning", "Speech perception", "Language development", "Verbal learning", "Text segmentation", "Discrimination learning", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2294072136", "references": ["1520943203", "2154361524", "2006969979", "47415966", "1575431606", "2038698865"], "title": "Improving Statistical Natural Language Translation with Categories and Rules", "abstract": "This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based techniques enter an overall statistical approach leading to about 50 percent correctly translated sentences applied to the very difficult English-German VERBMOBIL spontaneous speech corpus.", "citation_count": "6", "reference_count": "55", "date": "1998", "authors": ["Franz Josef Och", "Hans Weber"], "related_topics": ["Example-based machine translation", "Machine translation", "Lexicon", "Beam search", "Word (computer architecture)", "Natural language processing", "Speech recognition", "Translation (geometry)", "Computer science", "Process (engineering)", "Artificial intelligence", "Natural language translation"]}
{"id": "2107551411", "references": ["2138584836", "1562694524", "2012511220", "1603508585", "2065627366", "2134627713", "2006969979", "2038698865", "2422872931", "2146418175"], "title": "A DP based Search Algorithm for Statistical Machine Translation", "abstract": "We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP). During the search process two statistical knowledge sources are combined: a translation model and a bigram language model. This search algorithm expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string. We present experimental results on the Verbmobil task.", "citation_count": "12", "reference_count": "84", "date": "1998", "authors": ["S. NieBen", "S. Vogel", "H. Ney", "C. Tillmann"], "related_topics": ["Example-based machine translation", "Beam search", "Rule-based machine translation", "Search algorithm", "Best-first search", "Bigram", "Transfer-based machine translation", "Machine translation", "Language model", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2113106066", "references": ["2139647714", "2012511220", "1603508585", "3088213079", "2006969979", "2096312440", "2158164089", "2166810516", "2422872931", "2146418175"], "title": "Speech translation: coupling of recognition and translation", "abstract": "In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.", "citation_count": "11", "reference_count": "212", "date": "1999", "authors": ["H. Ney"], "related_topics": ["Speech translation", "Rule-based machine translation", "Example-based machine translation", "Word error rate", "Noisy channel model", "Decision rule", "Translation (geometry)", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2158164089", "references": ["2115755647", "2139647714", "2065627366", "2144789800", "2134627713", "2006969979", "2160721289", "2038698865", "2422872931", "2146418175"], "title": "A DP-based Search Using Monotone Alignments in Statistical Translation", "abstract": "In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results. The statistical translation uses two sources of information: a translation model and a language model. The language model used is a standard bigram model. For the translation model, the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions. Thus, the approach amounts to a first-order Hidden Markov model (HMM) as they are used successfully in speech recognition for the time alignment problem. Under the assumption that the alignment is monotone with respect to the word order in both languages, an efficient search strategy for translation can be formulated. The details of the search algorithm are described. Experiments on the EuTrans corpus produced a word error rate of 5.1%.", "citation_count": "10", "reference_count": "133", "date": "1997", "authors": ["Christoph Tillmann", "Stephan Vogel", "Hermann Ney", "Alex Zubiaga"], "related_topics": ["Word error rate", "Cache language model", "Language model", "Bigram", "Hidden Markov model", "Search algorithm", "Translation (geometry)", "Monotone polygon", "Word order", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2038698865", "references": ["1562694524", "2065627366", "2134627713", "2006969979", "1575431606", "2160721289"], "title": "HMM-based word alignment in statistical translation", "abstract": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora.", "citation_count": "6", "reference_count": "1,104", "date": "1996", "authors": ["Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "related_topics": ["Word error rate", "Hidden Markov model", "Word (computer architecture)", "Constraint (information theory)", "Speech recognition", "Natural language processing", "Translation (geometry)", "Computer science", "Artificial intelligence", "Inversion transduction grammars"]}
{"id": "2196555355", "references": ["2139895384", "2952654140", "1942952754", "2097333193", "2154384676", "2006969979", "2016747603", "2158388102", "2162346592", "2110882317"], "title": "Automatic Acquisition of Hierarchical Transduction Models for Machine Translation", "abstract": "We describe a method for the fully automatic learning of hierarchical finite state translation models. The input to the method is transcribed speech utterances and their corresponding human translations, and the output is a set of head transducers, i.e. statistical lexical head-outward transducers. A word-alignment function and a head-ranking function are first obtained, and then counts are generated for hypothesized state transitions of head transducers whose lexical translations and word order changes are consistent with the alignment. The method has been applied to create an English-Spanish translation model for a Speech translation application, with word accuracy of over 75% as measured by a string-distance comparison to three reference translations.", "citation_count": "10", "reference_count": "65", "date": "1998", "authors": ["Hiyan Alshawi", "Srinivas Bangalore", "Shona Douglas"], "related_topics": ["Speech translation", "Example-based machine translation", "Rule-based machine translation", "Machine translation", "Synchronous context-free grammar", "Word error rate", "Transfer-based machine translation", "Word order", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1916559533", "references": ["2097333193", "2101105183", "2117400858", "1574901103", "2122922578", "2006969979", "1973923101", "2049633694", "2135843243", "2101210369"], "title": "Statistical Machine Translation", "abstract": "This introductory text to statistical machine translation (SMT) provides all of the theories and methods needed to build a statistical machine translator, such as Google Language Tools and Babelfish. In general, statistical techniques allow automatic translation systems to be built quickly for any language-pair using only translated texts and generic software. With increasing globalization, statistical machine translation will be central to communication and commerce. Based on courses and tutorials, and classroom-tested globally, it is ideal for instruction or self-study, for advanced undergraduates and graduate students in computer science and/or computational linguistics, and researchers in natural language processing. The companion website provides open-source corpora and tool-kits.", "citation_count": "68", "reference_count": "2,012", "date": "2010", "authors": ["Philipp Koehn"], "related_topics": ["Hybrid machine translation", "Machine translation", "Example-based machine translation", "Machine translation software usability", "Computer-assisted translation", "Computational linguistics", "Software", "Natural language processing", "Computer science", "Artificial intelligence", "Automatic translation"]}
{"id": "2134800885", "references": ["2155607551", "2172097231", "1549285799", "2061910127", "2101105183", "16967297", "22168010", "2124807415", "2128808215", "1631260214"], "title": "KenLM: Faster and Smaller Language Model Queries", "abstract": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The Probing data structure uses linear probing hash tables and is designed for speed. Compared with the widely-used SRILM, our Probing model is 2.4 times as fast while using 57% of the memory. The Trie data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. Trie simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.", "citation_count": "16", "reference_count": "1,119", "date": "2011", "authors": ["Kenneth Heafield"], "related_topics": ["X-fast trie", "Hash array mapped trie", "Trie", "Data structure", "Linear probing", "Hash table", "Interpolation search", "Language model", "Parallel computing", "Computer science"]}
{"id": "2159981908", "references": ["2140173168", "2050619059", "3021916629", "2097726431", "40549020", "2127492100", "2166706824", "1532325895", "1988520084", "2114524997"], "title": "Rumor has it: Identifying Misinformation in Microblogs", "abstract": "A rumor is commonly defined as a statement whose true value is unverifiable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-specific memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Finally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations.", "citation_count": "26", "reference_count": "770", "date": "2011", "authors": ["Vahed Qazvinian", "Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei"], "related_topics": ["Rumor", "Disinformation", "Misinformation", "Microblogging", "Social media", "Information retrieval", "Internet privacy", "Statement (logic)", "Computer science"]}
{"id": "2135843243", "references": ["1632114991", "2125838338", "2166394306", "1551773846", "2130636661", "1965364888", "1522263329", "2046224275", "1554031433", "1773803948"], "title": "TnT -- A Statistical Part-of-Speech Tagger", "abstract": "Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora.", "citation_count": "12", "reference_count": "2,042", "date": "2000", "authors": ["Thorsten Brants"], "related_topics": ["Trigram tagger", "Markov model", "Principle of maximum entropy", "Smoothing", "Part of speech", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence", "Part-of-speech tagging"]}
{"id": "2732923061", "references": ["331093680", "2165687056", "2101105183", "1560911282", "1940432948", "1258814950", "1767147833", "1515929066", "1807210243", "9490244"], "title": "Proficiency and Performance in Language Testing.", "abstract": "", "citation_count": "0", "reference_count": "29", "date": "1993", "authors": ["James Child"], "related_topics": ["Language assessment", "Language proficiency", "Linguistics", "Psychology", "Evaluation methods", "Test selection"]}
{"id": "2001810881", "references": ["638025976", "2913739034", "2046319108", "2101105183"], "title": "Corpus-based comprehensive and diagnostic MT evaluation: initial Arabic, Chinese, French, and Spanish results", "abstract": "We describe two metrics for automatic evaluation of machine translation quality. These metrics, BLEU and NEE, are compared to human judgment of quality of translation of Arabic, Chinese, French, and Spanish documents into English.", "citation_count": "4", "reference_count": "58", "date": "2002", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "John Henderson", "Florence Reeder"], "related_topics": ["Evaluation of machine translation", "BLEU", "Natural language processing", "Quality (business)", "Linguistics", "Computer science", "Arabic", "Artificial intelligence", "Corpus based", "Human judgment"]}
{"id": "3037252522", "references": ["2016336271", "1969655989", "2006969979", "2155802047", "2103330339"], "title": "The ARPA MT Evaluation Methodologies: Evolution, Lessons, and Future Approaches", "abstract": "", "citation_count": "5", "reference_count": "202", "date": "1994", "authors": ["John S. White", "Theresa A. O\u2019Connell", "Francis E. O\u2019Mara"], "related_topics": ["Machine translation", "Computer science", "Software engineering"]}
{"id": "2129765547", "references": ["1489181569", "2134031652", "2117652747", "2006969979", "3017143921"], "title": "Empirical Methods for Exploiting Parallel Texts", "abstract": "The translation of a text can be viewed as a detailed annotation of the text's meaning. From this point of view, texts that exist in two languages (bitexts) are the richest accessible source of linguistic knowledge. Such knowledge can be exploited in many ways, if it can be automatically acquired. The acquisition process is invariably based on automatic methods for inducing translational equivalence relations between the two halves of a bitext. At the word token level, these relations are called bitext maps; at the word type level, they are called translation models. This dissertation advances the state of the art in methods for determining both kinds of translational equivalence. It also shows how to integrate these methods to exploit a much wider variety of bitexts than was previously possible.\r\nThe dissertation begins by showing that the language-specific aspects of the bitext mapping problem can be encapsulated and modularized away, leaving only a problem of geometric pattern recognition. The best solution is then the one that maximizes the signal-to-noise ratio in the search space and employs the fastest and most accurate search algorithm. The dissertation presents new methods for maximizing the signal strength, for filtering noise, and for searching the resulting scatterplot in linear expected space and time. The unprecedented accuracy of this solution enables a new application of bitext maps--automatic detection of omissions in translations.\r\nThe second half of the dissertation makes a number of advances in statistical translation modeling. First, it proves the feasibility of modeling translational equivalence independently of word order. Second, the dissertation shows why and how translation models can benefit from an explicit noise model. Third, it shows how the noise model can be conditioned on almost any kind of pre-existing language-specific knowledge, and that even simple linguistic clues can significantly improve translation model accuracy. Fourth, the dissertation shows how to automatically determine the sense inventories of words in bitext and how to automatically discover word sequences that are translated as a unit. This information enables translation models that account for polysemy and for phrasal translations.", "citation_count": "5", "reference_count": "227", "date": "2001", "authors": ["Ilya Dan Melamed", "Mitchell Marcus"], "related_topics": ["Word order", "Equivalence (formal languages)", "Polysemy", "Search algorithm", "Natural language processing", "Equivalence relation", "Computer science", "Empirical research", "Exploit", "Annotation", "Artificial intelligence"]}
{"id": "133045130", "references": ["2396590899", "2038698865", "2116316001", "2101105183", "1991635536", "2114434620", "2161792612", "1973923101", "2006969979", "2158388102"], "title": "The CMU Statistical Machine Translation System", "abstract": "In this paper we describe the components of our statistical machine translation system. This system combines phrase-tophrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-toEnglish and Arabic-to-English translation tasks are presented.", "citation_count": "13", "reference_count": "184", "date": "2003", "authors": ["Stephan Vogel", "Ying Zhang", "Fei Huang", "Alicia Tribble", "Ashish Venugopal", "Bing Zhao", "Alex Waibel"], "related_topics": ["Lexicon", "Natural language processing", "Translation (geometry)", "Speech recognition", "Computer science", "Artificial intelligence", "Bilingual corpus", "Machine translation system"]}
{"id": "2139403546", "references": ["1555286493", "2012511220", "1650993530", "1559942044", "2006969979", "2158164089", "1667614912", "2011039300", "2035227369", "2146418175"], "title": "Fast Decoding and Optimal Decoding for Machine Translation", "abstract": "A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.", "citation_count": "11", "reference_count": "325", "date": "2001", "authors": ["Ulrich Germann", "Michael Jahr", "Kevin Knight", "Daniel Marcu", "Kenji Yamada"], "related_topics": ["Soft-decision decoder", "Sequential decoding", "List decoding", "Decoding methods", "Machine translation", "Optimization problem", "Set (abstract data type)", "Algorithm", "Translation (geometry)", "Computer science"]}
{"id": "2030750105", "references": ["2049633694", "1575431606", "2006969979"], "title": "But dictionaries are data too", "abstract": "Although empiricist approaches to machine translation depend vitally on data in the form of large bilingual corpora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum likelihood estimates of the parameters of a probabilistic model from this combined data and we show how these parameters are affected by inclusion of the dictionary for some sample words.", "citation_count": "3", "reference_count": "91", "date": "1993", "authors": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Meredith J. Goldsmith", "Jan Hajic", "Robert L. Mercer", "Surya Mohanty"], "related_topics": ["Bilingual dictionary", "Example-based machine translation", "Machine translation", "Probabilistic logic", "Statistical model", "Natural language processing", "Computer science", "Sample (statistics)", "Artificial intelligence", "Bilingual corpus", "Machine translation system"]}
{"id": "1811404221", "references": ["1632114991", "2159343191", "2016336271", "2138787466", "1540222736", "91190427", "2006969979", "3037393508", "2343954916", "1672171594"], "title": "Manual Annotation of Translational Equivalence: The Blinker Project", "abstract": "Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the research community from this http URL . The annotations can be used for several purposes. First, they can be used as a standard data set for developing and testing translation lexicons and statistical translation models. Second, researchers in lexical semantics will be able to mine the annotations for insights about cross-linguistic lexicalization patterns. Third, the annotations can be used in research into certain recently proposed methods for monolingual word-sense disambiguation. This paper describes the annotated texts, the specially-designed annotation tool, and the strategies employed to increase the consistency of the annotations. The annotation process was repeated five times by different annotators. Inter-annotator agreement rates indicate that the annotations are reasonably reliable and that the method is easy to replicate.", "citation_count": "11", "reference_count": "121", "date": "1998", "authors": ["I. Dan Melamed"], "related_topics": ["Annotation", "Lexicalization", "Natural language processing", "Lexical semantics", "Information retrieval", "Equivalence (formal languages)", "Modern English", "Computer science", "Artificial intelligence", "Manual annotation"]}
{"id": "1525706028", "references": ["1489181569", "2097333193", "2117652747", "2963010813", "1973923101", "2006969979", "1811404221", "1979102019", "1672171594", "3140453591"], "title": "Evaluation of word alignment systems", "abstract": "This project evaluates two different systems that generate wordalignments on English-Swedish data. The systems to be used are the Giza++ system, that may generate a variety of statistical translati ...", "citation_count": "17", "reference_count": "71", "date": "2000", "authors": ["Lars Ahrenberg", "Magnus Merkel", "Anna S\u00e5gvall Hein", "J\u00f6rg Tiedemann"], "related_topics": ["Language technology", "Quantitative linguistics", "Variety (linguistics)", "Word (computer architecture)", "Natural language processing", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "1979102019", "references": ["1517947178", "1916559533", "1562694524", "2006969979", "1529616844", "1811404221", "136130055", "1575431606", "2038698865"], "title": "A comparison of alignment models for statistical machine translation", "abstract": "In this paper, we present and compare various alignment models for statistical machine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments. We also compare the impact of different alignment models on the translation quality of a statistical machine translation system.", "citation_count": "9", "reference_count": "277", "date": "2000", "authors": ["Franz Josef Och", "Hermann Ney"], "related_topics": ["Machine translation", "Viterbi algorithm", "Translation (geometry)", "Pattern recognition", "Measure (data warehouse)", "Computer science", "Scheme (programming language)", "Quality (business)", "Artificial intelligence"]}
{"id": "136130055", "references": ["1966812932", "2020749563", "3017143921", "2055528812", "2127836646", "2153295945"], "title": "Forming Word Classes by Statistical Clustering for Statistical Language Modelling", "abstract": "In statistical language modelling there is always a problem of sparse data. A way to reduce this problem is to form groups of words in order to get equivalence classes. In this paper we present a clustering algorithm that builds abstract word equivalence classes. The algorithm finds a local optimum according to a maximum-likelihood criterion. Experiments were made on an English 1.1-million word corpus and a German 100,000-word corpus. Compared to a word bigram model, the use of clustered equivalence classes in a bigram class model leads to a significant improvement, as measured by the perplexity. Depending on the size of the training material, the automatically clustered word classes are even better than manually determined categories.", "citation_count": "6", "reference_count": "57", "date": "1993", "authors": ["Reinhard Kneser", "Hermann Ney"], "related_topics": ["Bigram", "Cluster analysis", "Perplexity", "Word (computer architecture)", "Part of speech", "Local optimum", "Natural language processing", "Sparse matrix", "Computer science", "German", "Artificial intelligence"]}
{"id": "1972573551", "references": ["2000196122", "2067551521", "2092654472", "2164418233", "1551104980", "1986543644", "1572948005", "2159398820", "2038248725"], "title": "Generalized Phrase Structure Grammar", "abstract": "\"Generalized Phrase Structure Grammar\" provides the definitive exposition of the theory of grammar originally proposed by Gerald Gazdar and developed during half a dozen years' work with his colleagues Ewan Klein, Geoffrey Pullum, and Ivan Sag. This long-awaited book contains both detailed specifications of the theory and extensive illustrations of its power to describe large parts of English grammar. Experts who wish to evaluate the theory and students learning GPSP for the first time will find this book an invaluable guide.The initial chapters lay out the theoretical machinery of GPSP in a readily intelligible way. Combining informal discussion with precise formalization, the authors describe all major aspects of their grammatical system, including a complete theory of syntactic features, phrase structure rules, meta rules, and feature instantiation principles. The book then shows just what a GPSP analysis of English syntax can accomplish. Topics include the internal structure of phrases, unbounded dependency constructions of many varieties, and coordinate conjunction a construction long considered the sticking point for phrase structure approaches to syntax.The book concludes with a well developed proposal for a model theoretic semantic system to go along with GPSP syntax. Throughout, the authors maintain the highest standards of explicitness and rigor in developing and assessing their grammatical system. Their aim is to provide the best possible test of the hypothesis that syntactic description can be accomplished in a single-level system. And more generally, it is their intention to formulate a grammatical framework in which linguistic universals follow directly from the form of the system and therefore require no explicit statement. Their book sets new methodological standards for work in generative grammar while presenting a grammatical system of extraordinary scope.\"", "citation_count": "0", "reference_count": "4,203", "date": "1985", "authors": ["Gerald Gazdar", "Ewan Klein", "Geoffrey Pullum", "Ivan Sag"], "related_topics": ["Generalized phrase structure grammar", "Generative grammar", "Phrase structure rules", "Head-driven phrase structure grammar", "Relational grammar", "Emergent grammar", "Word grammar", "Government and binding theory", "Linguistics", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2052449326", "references": ["2153439141", "3089319657", "2142708806", "2100796029", "1976241232", "2167434254", "1541301615", "2099247782", "1982944197", "2110882317"], "title": "Three new probabilistic models for dependency parsing: an exploration", "abstract": "After presenting a novel O(n3) parsing algorithm for dependency grammar, we develop three contrasting ways to stochasticize it. We propose (a) a lexical affinity model where words struggle to modify each other, (b) a sense tagging model where words fluctuate randomly in their selectional preferences, and (c) a generative model where the speaker fleshes out each word's syntactic and conceptual structure without regard to the implications for the hearer. We also give preliminary empirical results from evaluating the three models' parsing performance on annotated Wall Street Journal training text (derived from the Penn Treebank). In these results, the generative model performs significantly better than the others, and does about equally well at assigning part-of-speech tags.", "citation_count": "14", "reference_count": "918", "date": "1996", "authors": ["Jason M. Eisner"], "related_topics": ["Data-oriented parsing", "S-attributed grammar", "Top-down parsing", "Parsing", "Dependency grammar", "Generative model", "Treebank", "Syntax", "Natural language processing", "Probabilistic logic", "Computer science", "Artificial intelligence"]}
{"id": "2069912724", "references": ["2128430409", "2167434254", "2029825515", "2008506796", "1924403233", "2121227244", "1594031697", "1542847127", "2021758792"], "title": "Decision tree parsing using a hidden derivation model", "abstract": "Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years.", "citation_count": "12", "reference_count": "123", "date": "1994", "authors": ["F. Jelinek", "J. Lafferty", "D. Magerman", "R. Mercer", "A. Ratnaparkhi", "S. Roukos"], "related_topics": ["Parsing", "Attribute grammar", "Top-down parsing", "Parser combinator", "Operator-precedence grammar", "Generative grammar", "Emergent grammar", "Recursive descent parser", "Natural language processing", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2087165009", "references": ["2153439141", "2963532001", "2027979924", "2092654472", "1986543644", "73274768", "2094061585", "2110882317"], "title": "Procedure for quantitatively comparing the syntactic coverage of English grammars", "abstract": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&amp;T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes).", "citation_count": "0", "reference_count": "640", "date": "1991", "authors": ["S. Abney", "S. Flickenger", "C. Gdaniec", "C. Grishman", "P. Harrison", "D. Hindle", "R. Ingria", "F. Jelinek", "J. Klavans", "M. Liberman", "M. Marcus", "S. Roukos", "B. Santorini", "T. Strzalkowski", "E. Black"], "related_topics": ["Parsing", "Brown Corpus", "Grammarian", "Grammar", "Prima facie", "Rule-based machine translation", "Statistical parsing", "Logos Bible Software", "Linguistics", "Philosophy"]}
{"id": "2162455891", "references": ["2153439141", "2138425667", "2091671846", "2134495021", "1529084404", "1955233831", "2096435848", "2099247782", "1936920915", "1975040318"], "title": "A Fully Statistical Approach to Natural Language Interfaces", "abstract": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.", "citation_count": "15", "reference_count": "191", "date": "1996", "authors": ["Scott Miller", "David Stallard", "Robert Bobrow", "Richard Schwartz"], "related_topics": ["Language identification", "Natural language user interface", "Semantic interpretation", "Semantic computing", "Parsing", "Natural language", "Statistical model", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2017691720", "references": ["2154683974", "2067191022", "2151103935", "2119300483", "2083305840", "2911964244", "1528789833", "2121947440", "2124351162", "1999478155"], "title": "Constrained parametric min-cuts for automatic object segmentation", "abstract": "We present a novel framework for generating and ranking plausible objects hypotheses in an image using bottom-up processes and mid-level cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge about properties of individual object classes, by solving a sequence of constrained parametric min-cut problems (CPMC) on a regular image grid. We then learn to rank the object hypotheses by training a continuous model to predict how plausible the segments are, given their mid-level region properties. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset. It achieves the same average best segmentation covering as the best performing technique to date [2], 0.61 when using just the top 7 ranked segments, instead of the full hierarchy in [2]. Our method achieves 0.78 average best covering using 154 segments. In a companion paper [18], we also show that the algorithm achieves state-of-the art results when used in a segmentation-based recognition pipeline.", "citation_count": "34", "reference_count": "584", "date": "2010", "authors": ["Joao Carreira", "Cristian Sminchisescu"], "related_topics": ["Scale-space segmentation", "Segmentation-based object categorization", "Image segmentation", "Segmentation", "Feature extraction", "Ranking", "Parametric statistics", "Object (computer science)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2129305389", "references": ["3097096317", "2161969291", "2151103935", "2031489346", "1625255723", "2110158442", "2168356304", "2162915993", "1999478155", "2131846894"], "title": "Segmentation as selective search for object recognition", "abstract": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.", "citation_count": "30", "reference_count": "794", "date": "2011", "authors": ["Koen E. A. van de Sande", "Jasper R. R. Uijlings", "Theo Gevers", "Arnold W. M. Smeulders"], "related_topics": ["Segmentation-based object categorization", "Scale-space segmentation", "Image segmentation", "Cognitive neuroscience of visual object recognition", "Segmentation", "Pascal (programming language)", "Pattern recognition", "Support vector machine", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2130306094", "references": ["2618530766", "2022508996", "2108598243", "2161969291", "2146502635", "2031489346", "2100495367", "2168356304", "2167510172", "2072128103"], "title": "Deep Neural Networks for Object Detection", "abstract": "Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.", "citation_count": "20", "reference_count": "1,323", "date": "2013", "authors": ["Christian Szegedy", "Alexander Toshev", "Dumitru Erhan"], "related_topics": ["Object detection", "Viola\u2013Jones object detection framework", "Minimum bounding box", "Contextual image classification", "Pascal (programming language)", "Pattern recognition", "Machine learning", "Inference", "Computer science", "Artificial intelligence", "Deep neural networks"]}
{"id": "2122146326", "references": ["2618530766", "3097096317", "2129305389", "2161969291", "2147717514", "2168356304", "2037227137", "2120419212", "1736726159", "2094728533"], "title": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine", "abstract": "Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.", "citation_count": "22", "reference_count": "395", "date": "2013", "authors": ["Thomas Dean", "Mark A. Ruzon", "Mark Segal", "Jonathon Shlens", "Sudheendra Vijayanarasimhan", "Jay Yagnik"], "related_topics": ["Viola\u2013Jones object detection framework", "Object detection", "Object-class detection", "Filter bank", "Filter (signal processing)", "Locality-sensitive hashing", "Convolution", "Object (computer science)", "Ground truth", "Contextual image classification", "Algorithm", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2128715914", "references": ["2166049352", "2161969291", "2154422044", "2172188317", "2128272608", "2146103513", "2168356304", "2161406034", "1999478155", "2124386111"], "title": "What is an object", "abstract": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.", "citation_count": "32", "reference_count": "1,215", "date": "2010", "authors": ["Bogdan Alexe", "Thomas Deselaers", "Vittorio Ferrari"], "related_topics": ["Object detection", "Image segmentation", "Pascal (programming language)", "Pixel", "Computer vision", "Pattern recognition", "Prior probability", "Bayesian probability", "Detector", "Mathematics", "Artificial intelligence", "Bayesian framework"]}
{"id": "201288405", "references": ["1558333962", "2053306448", "2101711363", "2117621558", "2162161511", "2158388102", "1513168562", "2963965928"], "title": "Two-Level Morphology: A General Computational Model for Word-Form Recognition and Production", "abstract": "", "citation_count": "0", "reference_count": "1,120", "date": "1983", "authors": ["Kimmo Koskenniemi"], "related_topics": ["Computational linguistics", "Computational model", "Word (computer architecture)", "Natural language processing", "Artificial intelligence", "Computer science", "Production (computer science)", "Morphology (biology)", "Computational morphology"]}
{"id": "1513168562", "references": ["1535681052", "1983606164", "13674071", "2569714427", "2065700902", "1823507963", "2002089154", "1598851216", "2167954650", "201288405"], "title": "Regular models of phonological rule systems", "abstract": "This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter.", "citation_count": "14", "reference_count": "973", "date": "1994", "authors": ["Ronald M. Kaplan", "Martin Kay"], "related_topics": ["Regular language", "Phonological rule", "Rule-based machine translation", "Rewriting", "Phonology", "Formalism (philosophy)", "Representation (mathematics)", "Set (abstract data type)", "Programming language", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2302255633", "references": ["1904365287", "2097117768", "1861492603", "2194775991", "2183341477", "1677182931", "3118608800", "2962835968", "2117539524", "1836465849"], "title": "Identity Mappings in Deep Residual Networks", "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.", "citation_count": "22", "reference_count": "5,665", "date": "2016", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "related_topics": ["Residual", "Block (data storage)", "Identity (object-oriented programming)", "Generalization", "Code (cryptography)", "Convergence (routing)", "Algorithm", "Series (mathematics)", "Mathematics", "Residual neural network"]}
{"id": "2963037989", "references": ["2102605133", "1904365287", "639708223", "2097117768", "2161969291", "1536680647", "2109255472", "2168356304", "2117539524", "2963542991"], "title": "You Only Look Once: Unified, Real-Time Object Detection", "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.", "citation_count": "35", "reference_count": "16,216", "date": "2016", "authors": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "related_topics": ["Object detection", "Frame rate", "Frame (networking)", "Artificial neural network", "Pattern recognition", "Pipeline (computing)", "Artificial intelligence", "False positive paradox", "Computer science", "Base (topology)"]}
{"id": "2062118960", "references": ["2102605133", "2618530766", "2161381512", "2155541015", "2145287260", "2128017662", "2141362318", "1849277567", "2113325037", "2963542991"], "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition", "abstract": "Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.", "citation_count": "49", "reference_count": "3,823", "date": "2014", "authors": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "related_topics": ["Feature (machine learning)", "Visual Word", "Convolutional neural network", "Feature extraction", "Deep learning", "Feature (computer vision)", "Contextual image classification", "Image retrieval", "Feature detection (computer vision)", "Pattern recognition", "Classifier (UML)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2145287260", "references": ["2618530766", "2047508432", "2536626143", "1782590233", "2310919327", "1976948919", "2163808566", "1498436455", "2168231600", "2072128103"], "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect =&gt; align =&gt; represent =&gt; classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.", "citation_count": "34", "reference_count": "5,650", "date": "2014", "authors": ["Yaniv Taigman", "Ming Yang", "Marc'Aurelio Ranzato", "Lior Wolf"], "related_topics": ["Three-dimensional face recognition", "Face hallucination", "Facial recognition system", "DeepFace", "Artificial neural network", "Pattern recognition", "Classifier (UML)", "Computer vision", "Computer science", "Artificial intelligence", "Face verification"]}
{"id": "2106706098", "references": ["1988684120", "1997063559", "1582801283", "1989457171", "2136796925", "2138309709", "2116044718", "2111051773", "2171166366", "1555683961"], "title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination", "abstract": "Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.", "citation_count": "22", "reference_count": "7,219", "date": "1995", "authors": ["Peter J. Green"], "related_topics": ["Reversible-jump Markov chain Monte Carlo", "Markov chain Monte Carlo", "Variable-order Bayesian network", "Gibbs sampling", "Markov chain", "Markov model", "Markov chain mixing time", "Bayesian statistics", "Applied mathematics", "Calculus", "Mathematics"]}
{"id": "2954064014", "references": ["2030723843", "2141870784", "2124313187", "1521785144", "1970101292", "2110575115", "2090267299", "2135046866", "2035379092", "1755563775"], "title": "Practical Optimization", "abstract": "", "citation_count": "0", "reference_count": "12,723", "date": "1981", "authors": ["Philip E. Gill"], "related_topics": ["Computer science"]}
{"id": "2102201073", "references": ["2162870748", "133977063", "2146766088", "2017977879", "2040615655", "3000332379", "2082102453", "2091886411", "1594031697", "2798909945"], "title": "Multivariate Adaptive Regression Splines", "abstract": "A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.", "citation_count": "31", "reference_count": "9,131", "date": "1991", "authors": ["Jerome H. Friedman"], "related_topics": ["Multivariate adaptive regression splines", "Nonparametric regression", "Recursive partitioning", "Projection pursuit regression", "Regression analysis", "Basis function", "Linear regression", "Bayesian multivariate linear regression", "Applied mathematics", "Statistics", "Mathematics"]}
{"id": "2797583072", "references": ["1746819321", "2164278908", "2155965977", "648151759", "1479807131", "1678356000", "2166604768", "2135046866", "1964357740", "2116206245"], "title": "Generalized Additive Models.", "abstract": "", "citation_count": "0", "reference_count": "11,911", "date": "1991", "authors": ["R. A. Brown", "T. J. Hastie", "R. J. Tibshirani"], "related_topics": ["Generalized additive model for location, scale and shape", "Generalized additive model", "Backfitting algorithm", "Projection pursuit regression", "Additive model", "Applied mathematics", "Computer science", "Generalised additive model", "P splines"]}
{"id": "2117897510", "references": ["2063698478", "2061905897", "2079100340", "2033260623", "1528761061", "1981492171", "2324309783", "2069645522", "1989584148", "1971382239"], "title": "Bootstrap Methods: Another Look at the Jackknife", "abstract": "We discuss the following problem given a random sample X = (X 1, X 2,\u2026, X n) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \\(\\theta \\left( {\\hat F} \\right) - \\theta \\left( F \\right)\\), \u03b8 some parameter of interest.) A general method, called the \u201cbootstrap\u201d, is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.", "citation_count": "14", "reference_count": "24,579", "date": "1979", "authors": ["Bradley Efron"], "related_topics": ["Jackknife resampling", "Random variable", "Sampling distribution", "Probability distribution", "Studentization", "Bootstrap aggregating", "Resampling", "Nonlinear regression", "Combinatorics", "Mathematics", "Statistics"]}
{"id": "2007069447", "references": ["2022480405", "2319794630", "2083875149", "2034562813", "1969423031", "2046811824", "2152977846", "2163738067", "2111051773", "2797502950"], "title": "Variable selection via Gibbs sampling", "abstract": "Abstract A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability\u2014the promising ones\u2014can then be identified by their more frequent appearance in the Gibbs sample.", "citation_count": "22", "reference_count": "3,084", "date": "1993", "authors": ["Edward I. George", "Robert E. McCulloch"], "related_topics": ["Gibbs sampling", "Mixture model", "Posterior probability", "g-prior", "Feature selection", "Multinomial distribution", "Probabilistic logic", "Latent variable", "Data mining", "Statistics", "Mathematics"]}
{"id": "191129667", "references": ["2158940042", "2132984323", "2092543127", "2151693816", "654435104", "2013987111", "2102201073", "2079724595", "2033484654", "2146842127"], "title": "Wavelet Shrinkage: Asymptopia?", "abstract": "Much recent effort has sought asymptotically minimax methods for recovering infinite dimensional objects-curves, densities, spectral densities, images-from noisy data. A now rich and complex body of work develops nearly or exactly minimax estimators for an array of interesting problems. Unfortunately, the results have rarely moved into practice, for a variety of reasons-among them being similarity to known methods, computational intractability and lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data: translate the empirical wavelet coefficients towards the origin by an amount \u221a(2 log n) /\u221an. The proposal differs from those in current use, is computationally practical and is spatially adaptive; it thus avoids several of the previous objections. Further, the method is nearly minimax both for a wide variety of loss functions-pointwise error, global error measured in L p -norms, pointwise and global error in estimation of derivatives-and for a wide range of smoothness classes, including standard Holder and Sobolev classes, and bounded variation. This is a much broader near optimality than anything previously proposed: we draw loose parallels with near optimality in robustness and also with the broad near eigenfunction properties of wavelets themselves. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity", "citation_count": "41", "reference_count": "2,738", "date": "1995", "authors": ["David L. Donoho", "Iain M. Johnstone", "G\u00e9rard Kerkyacharian", "Dominique Picard"], "related_topics": ["Minimax", "Pointwise", "Wavelet", "Minimax estimator", "Density estimation", "Smoothing", "Bounded variation", "Robustness (computer science)", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "1594031697", "references": ["2583466288", "2164240509", "2127218421", "2022554507", "2123838014", "1976123439", "2150418026", "3017143921", "1970074386", "2011039300"], "title": "Classification and regression trees", "abstract": "Background. Introduction to Tree Classification. Right Sized Trees and Honest Estimates. Splitting Rules. Strengthening and Interpreting. Medical Diagnosis and Prognosis. Mass Spectra Classification. Regression Trees. Bayes Rules and Partitions. Optimal Pruning. Construction of Trees from a Learning Sample. Consistency. Bibliography. Notation Index. Subject Index.", "citation_count": "24", "reference_count": "51,813", "date": "1983", "authors": ["Leo Breiman", "Jerome H Friedman", "Richard A Olshen", "Charles J Stone"], "related_topics": ["Classification Tree Method", "Pruning (decision trees)", "Decision tree learning", "Information Fuzzy Networks", "Logistic model tree", "Alternating decision tree", "Optimal discriminant analysis", "Recursive partitioning", "Statistics", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1496357020", "references": ["255556494", "1951289974", "2552194003", "1521626219", "2143117649", "2530395818", "2000042664", "1663973292"], "title": "All of Statistics: A Concise Course in Statistical Inference", "abstract": "WINNER OF THE 2005 DEGROOT PRIZE! This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level.", "citation_count": "0", "reference_count": "1,782", "date": "2014", "authors": ["Larry Wasserman"], "related_topics": ["Statistics education", "Probability and statistics", "Mathematical statistics", "Computational statistics", "Nonparametric statistics", "Statistical inference", "Bootstrapping (linguistics)", "Range (statistics)", "Data science", "Computer science", "Statistics"]}
{"id": "2963969878", "references": ["2963207607", "2963223306", "2123442489", "2964153729", "2099471712", "2250539671", "2038721957", "2551396370", "2543927648", "2963748441"], "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely.", "citation_count": "32", "reference_count": "898", "date": "2017", "authors": ["Robin Jia", "Percy Liang"], "related_topics": ["Question answering", "Reading comprehension", "Adversarial system", "Scheme (programming language)", "F1 score", "Natural language processing", "Computer science", "Reward system", "Artificial intelligence"]}
{"id": "1933349210", "references": ["2618530766", "1947481528", "2481240925", "1861492603", "1895577753", "2153579005", "2962835968", "2155893237", "1486649854"], "title": "VQA: Visual Question Answering", "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.", "citation_count": "61", "reference_count": "2,718", "date": "2015", "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "related_topics": ["Question answering", "Visual reasoning", "Context (language use)", "Natural language", "Visualization", "Information retrieval", "Natural language processing", "Task (project management)", "Knowledge extraction", "Computer science", "Artificial intelligence"]}
{"id": "2963626623", "references": ["1550206324", "1614298861", "2963012544", "2250966211", "2147152072", "2118585731", "1832693441", "2149684865", "1615991656", "2097726431"], "title": "Bag of Tricks for Efficient Text Classification", "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.", "citation_count": "22", "reference_count": "2,841", "date": "2017", "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "related_topics": ["Classifier (linguistics)", "Deep learning", "Orders of magnitude (bit rate)", "Pattern recognition", "Computer science", "Artificial intelligence", "Multicore cpu"]}
{"id": "2413794162", "references": ["2964308564", "1916559533", "2146502635", "2095705004", "2170738476", "2250539671", "2156387975", "2064675550", "2267186426", "1840435438"], "title": "A Decomposable Attention Model for Natural Language Inference", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "citation_count": "29", "reference_count": "924", "date": "2016", "authors": ["Ankur P. Parikh", "Oscar Tackstrom", "Dipanjan Das", "Jakob Uszkoreit"], "related_topics": ["Parallelizable manifold", "Artificial intelligence", "Computer science", "Simple (abstract algebra)", "Attention model", "Natural language inference"]}
{"id": "1970381522", "references": ["1632114991", "2151401338", "2125943921", "1659833910", "2141282920", "2525127255", "2158847908", "2130158090", "2115792525", "2149489787"], "title": "Cheap and Fast -- But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks", "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.", "citation_count": "31", "reference_count": "2,306", "date": "2008", "authors": ["Rion Snow", "Brendan O'Connor", "Daniel Jurafsky", "Andrew Ng"], "related_topics": ["Textual entailment", "Natural language", "Task (computing)", "Annotation", "Natural language processing", "Event (computing)", "Word (computer architecture)", "Computer science", "Quality (business)", "Similarity (psychology)", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "2185175083", "references": ["2251861449", "2120779048", "2117805756", "1647729745", "2248026759", "2525127255", "2109586012", "2066134726", "1897761818", "1984052055"], "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.", "citation_count": "35", "reference_count": "1,143", "date": "2014", "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "related_topics": ["Inference", "Graph (abstract data type)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2996428491", "references": ["2251939518", "2963403868", "2131744502", "2153579005", "2970597249", "2964350391", "2250539671", "2963341956", "2963748441", "2965373594"], "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.", "citation_count": "49", "reference_count": "1,297", "date": "2020", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut"], "related_topics": ["Deep learning", "Natural language", "Feature learning", "Machine learning", "Computer science", "Empirical evidence", "Transformer (machine learning model)", "Artificial intelligence", "Improved performance", "Self supervised learning"]}
{"id": "2396767181", "references": ["2182572585", "2102065370", "2525127255", "2130158090", "1971714853"], "title": "The Seventh PASCAL Recognizing Textual Entailment Challenge.", "abstract": "", "citation_count": "5", "reference_count": "505", "date": "2008", "authors": ["Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo"], "related_topics": ["Textual entailment", "Pascal (programming language)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2182572585", "references": ["2140424253", "2134061542", "2154652894"], "title": "Overview of the TAC 2008 Update Summarization Task.", "abstract": "The summarization track at the Text Analysis Conference (TAC) is a direct continuation of the Document Understanding Conference (DUC) series of workshops, focused on providing common data and evaluation framework for research in automatic summarization. In the TAC 2008 summarization track, the main task was to produce two 100-word summaries from two related sets of 10 documents, where the second summary was an update summary. While all of the 71 submitted runs were automatically scored with the ROUGE and BE metrics, NIST assessors manually evaluated only 57 of the submitted runs for readability, content, and overall responsiveness.", "citation_count": "3", "reference_count": "220", "date": "2008", "authors": ["Hoa Trang Dang", "Karolina Owczarzak"], "related_topics": ["Automatic summarization", "Multi-document summarization", "Readability", "Task (project management)", "Information retrieval", "NIST", "Computer science"]}
{"id": "2134061542", "references": ["2128530885", "3021916629", "1501617060", "2102065370", "2036002592", "1985463960", "2907147533", "1977747299", "1940278502", "2002664886"], "title": "Applying the Pyramid Method in DUC 2005", "abstract": "In DUC 2005, the pyramid method for content evaluation was used for the first time in a crosssite evaluation. We discuss the method used in creating pyramid models and performing peer annotation. Analysis of score averages for the peers indicates that the best systems score half as well as humans, and that systems can be grouped into better and worse performers. There were few significant differences among systems. High score correlations between sets from different annotators, and good interannotator agreement, indicate that participants can perform annotation reliably. We found that a modified pyramid score gave good results and would simplify peer annotation in the future.", "citation_count": "10", "reference_count": "80", "date": "2005", "authors": ["Kathleen McKeown", "Rebecca Passonneau", "Ani Nenkova", "Sergey Sigelman"], "related_topics": ["Pyramid", "Annotation", "Information retrieval", "Data mining", "Computer science", "Information technology"]}
{"id": "1990524510", "references": ["2150824314", "2158997610", "2101105183", "2117805756", "1978394996", "1569415500", "2100935296", "2136480620", "1647729745", "2525127255"], "title": "Measuring the Semantic Similarity of Texts", "abstract": "This paper presents a knowledge-based method for measuring the semantic-similarity of texts. While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.", "citation_count": "20", "reference_count": "438", "date": "2005", "authors": ["Courtney Corley", "Rada Mihalcea"], "related_topics": ["Semantic similarity", "Similarity heuristic", "Similarity (network science)", "Normalized compression distance", "Metric (mathematics)", "Natural language processing", "Information retrieval", "Matching (statistics)", "Computer science", "Artificial intelligence"]}
{"id": "2102065370", "references": ["2150824314", "2160204597", "2101105183", "2137591918", "3021916629", "2138180048", "1985463960", "2149593800", "1977747299", "2169401582"], "title": "Evaluating Content Selection in Summarization: The Pyramid Method", "abstract": "", "citation_count": "10", "reference_count": "615", "date": "2004", "authors": ["Ani Nenkova", "Rebecca J. Passonneau"], "related_topics": ["Automatic summarization", "Pyramid", "Multi-document summarization", "Selection (genetic algorithm)", "Information retrieval", "Computer science", "Information technology", "Content (measure theory)"]}
{"id": "137514618", "references": ["2167435923", "1520917717", "2126034021", "2048679005", "2107130271", "2785349534", "16933249", "2118021410", "2129468719", "2103931177"], "title": "PROBABILISTIC TEXTUAL ENTAILMENT: GENERIC APPLIED MODELING OF LANGUAGE VARIABILITY", "abstract": "", "citation_count": "11", "reference_count": "285", "date": "2004", "authors": ["Ido Dagan", "Oren Glickman"], "related_topics": ["Textual entailment", "Text graph", "Universal Networking Language", "Question answering", "Probabilistic logic", "Natural language processing", "Information retrieval", "Computer science", "Artificial intelligence"]}
{"id": "2002664886", "references": ["2151170651", "2099201756", "2624431344", "2090650059", "2061504941", "2112422413", "2148540129", "2158847908", "2155243985", "1531237901"], "title": "Nonparametric statistics for the behavioral sciences", "abstract": "This is the revision of the classic text in the field, adding two new chapters and thoroughly updating all others. The original structure is retained, and the book continues to serve as a combined text/reference.", "citation_count": "0", "reference_count": "54,083", "date": "1956", "authors": ["Sidney Siegel"], "related_topics": ["Nonparametric statistics", "Structure (mathematical logic)", "Field (computer science)", "Behavioural sciences", "Statistics", "Applied psychology", "Psychology"]}
{"id": "2123442489", "references": ["1508977358", "2096765155", "2251939518", "1521626219", "2167072947", "1996430422", "2250789336", "2096797897", "2251758222", "2147218300"], "title": "The Stanford CoreNLP Natural Language Processing Toolkit", "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.", "citation_count": "14", "reference_count": "6,528", "date": "2014", "authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "related_topics": ["Pipeline (software)", "Computer science", "Natural language processing", "Quality (business)", "Government (linguistics)", "SIMPLE (military communications protocol)", "Artificial intelligence"]}
{"id": "2113459411", "references": ["2147152072", "2163455955", "2117130368", "2118585731", "2158139315", "2166706824", "1662133657", "1880262756", "2132339004", "2114524997"], "title": "Learning Word Vectors for Sentiment Analysis", "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.", "citation_count": "28", "reference_count": "2,822", "date": "2011", "authors": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "related_topics": ["Sentiment analysis", "Natural language processing", "Information retrieval", "Leverage (statistics)", "Computer science", "Artificial intelligence"]}
{"id": "3037029945", "references": ["2144578941", "2557764419", "2923014074", "2970597249", "2978017171", "2963341956", "2963748441", "2889787757", "2996428491", "2965373594"], "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing.", "abstract": "In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.", "citation_count": "27", "reference_count": "8", "date": "2020", "authors": ["Ziqing Yang", "Yiming Cui", "Zhipeng Chen", "Wanxiang Che", "Ting Liu", "Shijin Wang", "Guoping Hu"], "related_topics": ["Sequence labeling", "Supervised learning", "Distillation", "Set (abstract data type)", "Artificial neural network", "Computer science", "Natural language processing", "Code (cryptography)", "Reading comprehension", "Artificial intelligence"]}
{"id": "3092111011", "references": ["3034457371", "3105966348", "2938830017", "3038012435", "2970390493", "2978017171", "2970597249", "2990704537", "1599016936", "2963341956"], "title": "Light Pre-Trained Chinese Language Model for NLP Tasks", "abstract": "We present the results of shared-task 1 held in the 2020 Conference on Natural Language Processing and Chinese Computing (NLPCC): Light Pre-Trained Chinese Language Model for NLP tasks. This shared-task examines the performance of light language models on four common NLP tasks: Text Classification, Named Entity Recognition, Anaphora Resolution and Machine Reading Comprehension. To make sure that the models are light-weight, we put restrictions and requirements on the number of parameters and inference speed of the participating models. In total, 30 teams registered our tasks. Each submission was evaluated through our online benchmark system (https://www.cluebenchmarks.com/nlpcc2020.html), with the average score over the four tasks as the final score. Various ideas and frameworks were explored by the participants, including data enhancement, knowledge distillation and quantization. The best model achieved an average score of 75.949, which was very close to BERT-base (76.460). We believe this shared-task highlights the potential of light-weight models and calls for further research on the development and exploration of light-weight models.", "citation_count": "12", "reference_count": "2", "date": "2020", "authors": ["Junyi Li", "Hai Hu", "Xuanwei Zhang", "Minglei Li", "Lu Li", "Liang Xu"], "related_topics": ["Language model", "Named-entity recognition", "Natural language processing", "Inference", "Comprehension", "Computer science", "Benchmark (computing)", "Resolution (logic)", "Quantization (music)", "Anaphora (linguistics)", "Artificial intelligence"]}
{"id": "3146365155", "references": ["2963112338", "2951534261", "3102286003", "3105966348", "2909544278", "1821462560", "2978017171", "2970454332", "2963341956", "3099446234"], "title": "Simplified TinyBERT: Knowledge Distillation for Document Retrieval.", "abstract": "Despite the effectiveness of utilizing BERT for document ranking, the computational cost of such approaches is non-negligible when compared to other retrieval methods. To this end, this paper first empirically investigates the applications of knowledge distillation models on document ranking task. In addition, on top of the recent TinyBERT, two simplifications are proposed. Evaluation on MS MARCO document re-ranking task confirms the effectiveness of the proposed simplifications.", "citation_count": "19", "reference_count": "3", "date": "2020", "authors": ["Xuanang Chen", "Ben He", "Kai Hui", "Le Sun", "Yingfei Sun"], "related_topics": ["Document retrieval", "Ranking (information retrieval)", "Task (project management)", "Information retrieval", "Computer science", "Distillation"]}
{"id": "3098576111", "references": ["2923014074", "2996428491", "2963403868", "2319920447", "1821462560", "2964121744", "2300242332", "2963341956", "2963748441", "2964118293"], "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT", "abstract": "Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.", "citation_count": "37", "reference_count": "9", "date": "2020", "authors": ["Wei Zhang", "Lu Hou", "Yichun Yin", "Lifeng Shang", "Xiao Chen", "Xin Jiang", "Qun Liu"], "related_topics": ["Transformer (machine learning model)", "Computer engineering", "Computer science", "Low bit", "Distillation"]}
{"id": "3064953855", "references": ["1482214997", "2964110616", "2963403868", "2536015822", "1821462560", "2134797427", "3082274269", "1523493493", "2136189984", "2963341956"], "title": "PARADE: Passage Representation Aggregation for Document Reranking", "abstract": "We present PARADE, an end-to-end Transformer-based model that considers document-level context for document reranking. PARADE leverages passage-level relevance representations to predict a document relevance score, overcoming the limitations of previous approaches that perform inference on passages independently. Experiments on two ad-hoc retrieval benchmarks demonstrate PARADE's effectiveness over such methods. We conduct extensive analyses on PARADE's efficiency, highlighting several strategies for improving it. When combined with knowledge distillation, a PARADE model with 72\\% fewer parameters achieves effectiveness competitive with previous approaches using BERT-Base. Our code is available at \\url{this https URL}.", "citation_count": "60", "reference_count": "28", "date": "2020", "authors": ["Canjia Li", "Andrew Yates", "Sean MacAvaney", "Ben He", "Yingfei Sun"], "related_topics": ["Parade", "Information retrieval", "Inference", "Transformer (machine learning model)", "Computer science"]}
{"id": "3101045333", "references": ["3130954105", "3137596946", "3146365155", "3128741255", "3139732141"], "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers", "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.", "citation_count": "0", "reference_count": "78", "date": "2020", "authors": ["Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou"], "related_topics": ["Transformer (machine learning model)", "Language model", "Computer engineering", "Computer science", "Distillation", "Self attention"]}
{"id": "3138154797", "references": ["2618530766", "2963446712", "2108598243", "2194775991", "3118608800", "2257979135", "2612445135", "2099471712", "2963341956", "1836465849"], "title": "Knowledge Distillation: A Survey", "abstract": "In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.", "citation_count": "330", "reference_count": "32", "date": "2021", "authors": ["Jianping Gou", "Baosheng Yu", "Stephen John Maybank", "Dacheng Tao"], "related_topics": ["Scalability", "Deep learning", "Data science", "Computer science", "Variety (cybernetics)", "Distillation", "Pattern recognition (psychology)", "Computational complexity theory", "Architecture", "ENCODE", "Artificial intelligence"]}
{"id": "3109684201", "references": ["2618530766", "639708223", "2194775991", "2096733369", "2963173190", "1903029394", "2099471712", "2970971581", "2117539524", "1836465849"], "title": "A Metric Learning Reality Check", "abstract": "Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.", "citation_count": "71", "reference_count": "82", "date": "2020", "authors": ["Kevin Musgrave", "Serge J. Belongie", "Ser-Nam Lim"], "related_topics": ["Metric (mathematics)", "Field (computer science)", "Computer science", "Theoretical computer science", "Reality check"]}
{"id": "3128741952", "references": ["2955058313", "2963420686", "2130660124", "2108598243", "2955425717", "2963495494", "2194775991", "2592929672", "1514535095", "3105081694"], "title": "Deep metric learning-based image retrieval system for chest radiograph and its clinical applications in COVID-19.", "abstract": "In recent years, deep learning-based image analysis methods have been widely applied in computer-aided detection, diagnosis and prognosis, and has shown its value during the public health crisis of the novel coronavirus disease 2019 (COVID-19) pandemic. Chest radiograph (CXR) has been playing a crucial role in COVID-19 patient triaging, diagnosing and monitoring, particularly in the United States. Considering the mixed and unspecific signals in CXR, an image retrieval model of CXR that provides both similar images and associated clinical information can be more clinically meaningful than a direct image diagnostic model. In this work we develop a novel CXR image retrieval model based on deep metric learning. Unlike traditional diagnostic models which aim at learning the direct mapping from images to labels, the proposed model aims at learning the optimized embedding space of images, where images with the same labels and similar contents are pulled together. The proposed model utilizes multi-similarity loss with hard-mining sampling strategy and attention mechanism to learn the optimized embedding space, and provides similar images, the visualizations of disease-related attention maps and useful clinical information to assist clinical decisions. The model is trained and validated on an international multi-site COVID-19 dataset collected from 3 different sources. Experimental results of COVID-19 image retrieval and diagnosis tasks show that the proposed model can serve as a robust solution for CXR analysis and patient management for COVID-19. The model is also tested on its transferability on a different clinical decision support task for COVID-19, where the pre-trained model is applied to extract image features from a new dataset without any further training. The extracted features are then combined with COVID-19 patient's vitals, lab tests and medical histories to predict the possibility of airway intubation in 72 hours, which is strongly associated with patient prognosis, and is crucial for patient care and hospital resource planning. These results demonstrate our deep metric learning based image retrieval model is highly efficient in the CXR retrieval, diagnosis and prognosis, and thus has great clinical value for the treatment and management of COVID-19 patients.", "citation_count": "57", "reference_count": "0", "date": "2021", "authors": ["Aoxiao Zhong", "Xiang Li", "Dufan Wu", "Hui Ren", "Kyung Sang Kim", "Young-Gon Kim", "Varun Buch", "Nir Neumark", "Bernardo Bizzo", "Won Young Tak", "Soo Young Park", "Yu Rim Lee", "Min Kyu Kang", "Jung Gil Park", "Byung Seok Kim", "Woo Jin Chung", "Ning Guo", "Ittai Dayan", "Mannudeep K. Kalra", "Quanzheng Li"], "related_topics": ["Image retrieval", "Deep learning", "Clinical decision support system", "Metric (unit)", "Machine learning", "Chest radiograph", "Sampling (medicine)", "Task (project management)", "Computer science", "Embedding", "Artificial intelligence"]}
{"id": "3095121901", "references": ["3097055006", "3022061250", "3128513979", "3144102935", "3145385912", "3134652006", "3122640483", "3122750337", "3163602117"], "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments", "abstract": "Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or \"views\") of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a \"swapped\" prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.", "citation_count": "0", "reference_count": "196", "date": "2020", "authors": ["Mathilde Caron", "Ishan Misra", "Julien Mairal", "Priya Goyal", "Piotr Bojanowski", "Armand Joulin"], "related_topics": ["Unsupervised learning", "Feature (machine learning)", "Online algorithm", "Consistency (database systems)", "Machine learning", "Representation (mathematics)", "Computer science", "Image (mathematics)", "Artificial intelligence"]}
{"id": "3103215803", "references": ["3111932027", "3111996865"], "title": "Self-supervised Co-Training for Video Representation Learning", "abstract": "The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.", "citation_count": "0", "reference_count": "32", "date": "2020", "authors": ["Tengda Han", "Weidi Xie", "Andrew Zisserman"], "related_topics": ["Feature learning", "Co-training", "Noise (video)", "Optical flow", "Representation (mathematics)", "Machine learning", "RGB color model", "Class (biology)", "Computer science", "Scheme (programming language)", "Artificial intelligence"]}
{"id": "3124635886", "references": ["2962879692", "2099111195", "2963403868", "2194775991", "3118608800", "2970971581", "2963341956", "2117539524", "2064675550", "1836465849"], "title": "Self-supervised Representation Learning with Relative Predictive Coding", "abstract": "This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance.", "citation_count": "58", "reference_count": "1", "date": "2021", "authors": ["Yao-Hung Hubert Tsai", "Martin Q. Ma", "Muqiao Yang", "Han Zhao", "Louis-Philippe Morency", "Ruslan Salakhutdinov"], "related_topics": ["Stability (learning theory)", "Feature learning", "Mutual information", "Variance (accounting)", "Logarithm", "Benchmark (computing)", "Machine learning", "Sensitivity (control systems)", "Computer science", "Key (cryptography)", "Artificial intelligence"]}
{"id": "3106428938", "references": ["3123939835", "3144539868", "3104110041", "3145385912", "3101658985", "3151935374", "3114632476", "3163602117"], "title": "What Makes for Good Views for Contrastive Learning", "abstract": "Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:this http URL", "citation_count": "0", "reference_count": "145", "date": "2020", "authors": ["Yonglong Tian", "Chen Sun", "Ben Poole", "Dilip Krishnan", "Cordelia Schmid", "Phillip Isola"], "related_topics": ["Feature learning", "Mutual information", "Field (computer science)", "Machine learning", "Pascal (programming language)", "Object detection", "Segmentation", "Computer science", "Selection (linguistics)", "Code (cryptography)", "Artificial intelligence"]}
{"id": "3007332492", "references": ["2102605133", "2618530766", "2964308564", "2108598243", "2963403868", "2194775991", "2964121744", "1903029394", "2117539524", "1836465849"], "title": "Benchmarking Graph Neural Networks", "abstract": "Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.", "citation_count": "100", "reference_count": "175", "date": "2020", "authors": ["Vijay Prakash Dwivedi", "Chaitanya K. Joshi", "Thomas Laurent", "Yoshua Bengio", "Xavier Bresson"], "related_topics": ["Message passing", "Machine learning", "Benchmarking", "Computer science", "Artificial intelligence", "Graph", "Graph neural networks"]}
{"id": "3098824823", "references": ["2962739339", "2923014074", "2963403868", "2123442489", "2963026768", "2970597249", "2963756346", "2963341956", "2996428491", "2965373594"], "title": "Transformers: State-of-the-Art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.", "citation_count": "34", "reference_count": "435", "date": "2020", "authors": ["Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R\u00e9mi Louf", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer", "Patrick von Platen", "Clara Ma", "Yacine Jernite", "Julien Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush"], "related_topics": ["Transformer (machine learning model)", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "2970986510", "references": ["1614298861", "2962739339", "2117130368", "2963403868", "2525778437", "2962784628", "2153579005", "2081580037", "2250539671", "2963341956"], "title": "Knowledge Enhanced Contextual Word Representations", "abstract": "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert\u2019s runtime is comparable to BERT\u2019s and it scales to large KBs.", "citation_count": "61", "reference_count": "194", "date": "2019", "authors": ["Matthew E. Peters", "Mark Neumann", "Robert L. Logan", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A. Smith"], "related_topics": ["Entity linking", "WordNet", "Relationship extraction", "Perplexity", "Language model", "Word (computer architecture)", "Natural language processing", "Computer science", "Task (project management)", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "2980282514", "references": ["2962739339", "2923014074", "2963403868", "2123442489", "2963026768", "2970597249", "2963756346", "2963341956", "2996428491", "2965373594"], "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing.", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.", "citation_count": "35", "reference_count": "1,225", "date": "2019", "authors": ["Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R\u00e9mi Louf", "Morgan Funtowicz", "Jamie Brew"], "related_topics": ["Transformer (machine learning model)", "Computer science", "Natural language processing", "Artificial intelligence"]}
{"id": "3098903812", "references": ["3139488800", "3129166376", "3152836006", "3165814564", "3155742828", "3131944163", "3037013468", "3144596436"], "title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "citation_count": "0", "reference_count": "1,357", "date": "2020", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "related_topics": ["Language model", "Constructed language", "Task (computing)", "Sentence", "Natural language processing", "Word (computer architecture)", "Computer science", "Contrast (statistics)", "Test (assessment)", "Autoregressive model", "Artificial intelligence"]}
{"id": "1975879668", "references": ["2053154970", "2026715452", "1979773093", "2051470741", "2037789405", "2020954436", "2015793039"], "title": "Measuring nominal scale agreement among many raters.", "abstract": "", "citation_count": "7", "reference_count": "8,231", "date": "1971", "authors": ["Joseph L. Fleiss"], "related_topics": ["Inter-rater reliability", "Fleiss' kappa", "Level of measurement", "Statistics", "Agreement", "Computer science", "Chance agreement", "Statistical analysis"]}
{"id": "2125980212", "references": ["2150916025", "2151930506", "2097184821", "2161936973", "2127675794", "2105423800", "2167847032", "2121516976", "2096598900", "2097776316"], "title": "Fast-join: An efficient method for fuzzy token matching based string similarity join", "abstract": "String similarity join that finds similar string pairs between two string sets is an essential operation in many applications, and has attracted significant attention recently in the database community. A significant challenge in similarity join is to implement an effective fuzzy match operation to find all similar string pairs which may not match exactly. In this paper, we propose a new similarity metrics, called \u201cfuzzy token matching based similarity\u201d, which extends token-based similarity functions (e.g., Jaccard similarity and Cosine similarity) by allowing fuzzy match between two tokens. We study the problem of similarity join using this new similarity metrics and present a signature-based method to address this problem. We propose new signature schemes and develop effective pruning techniques to improve the performance. Experimental results show that our approach achieves high efficiency and result quality, and significantly outperforms state-of-the-art methods.", "citation_count": "22", "reference_count": "157", "date": "2011", "authors": ["Jiannan Wang", "Guoliang Li", "Jianhua Fe"], "related_topics": ["Similarity (network science)", "Cosine similarity", "String metric", "Normalized compression distance", "String searching algorithm", "Approximate string matching", "String (computer science)", "Jaccard index", "Theoretical computer science", "Data mining", "Computer science"]}
{"id": "3024622987", "references": ["2922551710", "2970771982", "2251939518", "2163455955", "2963403868", "2752201871", "2911489562", "2963341956", "2996428491", "2965373594"], "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter", "abstract": "In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10-30% marginal improvement compared to its base model, BERT-Large, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular social media posts from Twitter.", "citation_count": "12", "reference_count": "110", "date": "2020", "authors": ["Martin M\u00fcller", "Marcel Salath\u00e9", "Per Egil Kummervold"], "related_topics": ["Transformer (machine learning model)", "Natural language processing", "Computer science", "Artificial intelligence", "Coronavirus disease 2019 (COVID-19)"]}
{"id": "2164777277", "references": ["1975879668", "2798510847", "2053154970", "2088041869", "2063803293", "1989774929", "2021142183", "2037789405", "2018385240", "2133012565"], "title": "The measurement of observer agreement for categorical data", "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.", "citation_count": "33", "reference_count": "65,668", "date": "1977", "authors": ["J. R. Landis", "Gary G Koch"], "related_topics": ["Categorical variable", "Fleiss' kappa", "Intra-rater reliability", "Cohen's kappa", "Multivariate statistics", "Inter-rater reliability", "Statistical hypothesis testing", "Observer (special relativity)", "Statistics", "Mathematics"]}
{"id": "3104186312", "references": ["1632114991", "2933138175", "2963403868", "2962784628", "2964121744", "2153848201", "2963626623", "2911489562", "2963341956", "2965373594"], "title": "BERTweet: A pre-trained language model for English Tweets", "abstract": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet", "citation_count": "36", "reference_count": "57", "date": "2020", "authors": ["Dat Quoc Nguyen", "Thanh Vu", "Anh Tuan Nguyen"], "related_topics": ["Language model", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2251743902", "references": ["2964308564", "2158899491", "2101105183", "2130942839", "6908809", "1606347560", "2251682575", "1753482797", "2964199361", "2130903752"], "title": "Multi-Task Learning for Multiple Language Translation", "abstract": "In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.", "citation_count": "21", "reference_count": "407", "date": "2015", "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang"], "related_topics": ["Language translation", "Example-based machine translation", "Computer-assisted translation", "Rule-based machine translation", "Machine translation", "Machine translation software usability", "Universal Networking Language", "Multi-task learning", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1579027943", "references": ["3022251767", "1994309501", "2188233853", "2963851840", "2080233464", "2962727772", "2046253206", "2899771611"], "title": "Compiling fast partial derivatives of functions given by algorithms", "abstract": "If the gradient of the function y = f(x/sub 1/,..., x/sub n/) is desired, where f is given by an algoritym Af(x, n, y), most numerical analysts will use numerical differencing. This is a sampling scheme that approximates derivatives by the slope of secants in closely spaced points. Symbolic methods that make full use of the program text of Af should be able to come up with a better way to evaluate the gradient of F. The system Jake described produces gradients significantly faster than numerical differencing. Jake can handle algorithms Af with arbitrary flow of control. Measurements performed on one particular machine suggest that Jake is faster than numerical differencing for n &gt; 8. Somewhat weaker results were obtained for the problem of computing Jacobians of arbitrary shape.", "citation_count": "0", "reference_count": "177", "date": "1980", "authors": ["Bert Speelpenning"], "related_topics": ["Function (mathematics)", "Partial derivative", "Algorithm", "Control flow", "Mathematics", "Calculation methods", "Sampling scheme"]}
{"id": "2154359981", "references": ["1550206324", "2163455955", "2117130368", "2118585731", "71795751", "2014902591", "2132166724", "2113459411", "2160660844", "2114524997"], "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "abstract": "Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.", "citation_count": "21", "reference_count": "1,247", "date": "2012", "authors": ["Sida Wang", "Christopher Manning"], "related_topics": ["Sentiment analysis", "Bigram", "Naive Bayes classifier", "Feature (machine learning)", "Support vector machine", "Snippet", "Pattern recognition", "Word (computer architecture)", "Task (computing)", "Computer science", "Artificial intelligence"]}
{"id": "2584341106", "references": ["2561715562", "2962809918", "1793121960", "2963907629", "2551396370", "2507756961", "1840435438"], "title": "Memory Networks", "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.", "citation_count": "0", "reference_count": "1,407", "date": "2015", "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "related_topics": ["Memory map", "Flat memory model", "Distributed shared memory", "Chaining", "Question answering", "Context (language use)", "Inference", "Task (project management)", "Natural language processing", "Artificial intelligence", "Computer science"]}
{"id": "1565746575", "references": ["2009543464", "2167277498", "1585743408", "2030360178", "1966280301", "2115012618", "2084812512", "1524761913", "2121044470", "2024081693"], "title": "Statistical Comparisons of Classifiers over Multiple Data Sets", "abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.", "citation_count": "39", "reference_count": "9,938", "date": "2006", "authors": ["Janez Dem\u0161ar"], "related_topics": ["Ensembles of classifiers", "Nemenyi test", "Friedman test", "Wilcoxon signed-rank test", "Statistical hypothesis testing", "Data set", "Set (abstract data type)", "Classifier chains", "Machine learning", "Data mining", "Computer science", "Artificial intelligence"]}
{"id": "2141362318", "references": ["1634005169", "2427881153", "2151103935", "2073965851", "2172188317", "2128017662", "2085261163", "1660390307", "2033819227", "2131846894"], "title": "Object retrieval with large vocabularies and fast spatial matching", "abstract": "In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, \"web-scale \" image corpora.", "citation_count": "17", "reference_count": "3,309", "date": "2007", "authors": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "related_topics": ["Image retrieval", "Vocabulary", "Quantization (image processing)", "Object (computer science)", "Margin (machine learning)", "Ranking", "Matching (statistics)", "Information retrieval", "Computer science"]}
{"id": "2066134726", "references": ["2134270519", "2536626143", "2108598243", "2101105183", "2145607950", "2137471889", "2109586012", "2054279472", "1897761818", "2098411764"], "title": "Baby talk: Understanding and generating simple image descriptions", "abstract": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.", "citation_count": "50", "reference_count": "626", "date": "2011", "authors": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "related_topics": ["Natural language", "Parsing", "Baby talk", "Natural language processing", "Computer science", "Image (mathematics)", "Simple (philosophy)", "Text mining", "Artificial intelligence"]}
{"id": "21006490", "references": ["2154956324", "28412257", "2108598243", "2102765684", "2145607950", "1576445103", "1604938182", "2038721957", "2135046866", "2160218441"], "title": "WSABIE: scaling up to large vocabulary image annotation", "abstract": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory.", "citation_count": "26", "reference_count": "780", "date": "2011", "authors": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "related_topics": ["Automatic image annotation", "Vocabulary", "Ranking", "Data mining", "Embedding", "Computer science", "Image (mathematics)", "Joint (audio engineering)", "Scaling"]}
{"id": "1897761818", "references": ["1532257412", "2046589395", "1647729745", "2147625498", "2120419212", "1666447063", "2119775030", "2106624428", "2502277634", "2142194269"], "title": "Every picture tells a story: generating sentences from images", "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us-ingdata. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.", "citation_count": "27", "reference_count": "1,091", "date": "2010", "authors": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "related_topics": ["Sentence", "Meaning (existential)", "Discriminative model", "Natural language processing", "Machine translation", "Image (mathematics)", "Synecdoche", "Computer science", "Artificial intelligence"]}
{"id": "2094728533", "references": ["23685451"], "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "abstract": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.", "citation_count": "1", "reference_count": "3,937", "date": "2008", "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "related_topics": ["Query language", "Graph database", "Tuple", "Infobox", "Semantic network", "Interface (Java)", "World Wide Web", "Scalability", "Structure (mathematical logic)", "Computer science"]}
{"id": "2101234009", "references": ["2035776949", "2118585731", "1571024744", "2097360283", "2153635508", "2146292423", "2040387238", "2063978378", "2097850441", "2047804403"], "title": "Scikit-learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "citation_count": "13", "reference_count": "41,201", "date": "2011", "authors": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg", "Jake Vanderplas", "Alexandre Passos", "David Cournapeau", "Matthieu Brucher", "Matthieu Perrot", "\u00c9douard Duchesnay"], "related_topics": ["Unsupervised learning", "Python (programming language)", "Instance-based learning", "Source code", "Supervised learning", "Documentation", "Usability", "Gradient boosting", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2075456404", "references": ["2139043937", "2171468534", "2022204871", "38739846", "2108598243", "2031489346", "2122369144", "1566135517", "1590495275", "2097726431"], "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "abstract": "We address the challenge of sentiment analysis from visual content. In contrast to existing methods which infer sentiment or emotion directly from visual low-level features, we propose a novel approach based on understanding of the visual concepts that are strongly related to sentiments. Our key contribution is two-fold: first, we present a method built upon psychological theories and web mining to automatically construct a large-scale Visual Sentiment Ontology (VSO) consisting of more than 3,000 Adjective Noun Pairs (ANP). Second, we propose SentiBank, a novel visual concept detector library that can be used to detect the presence of 1,200 ANPs in an image. The VSO and SentiBank are distinct from existing work and will open a gate towards various applications enabled by automatic sentiment analysis. Experiments on detecting sentiment of image tweets demonstrate significant improvement in detection accuracy when comparing the proposed SentiBank based predictors with the text-based approaches. The effort also leads to a large publicly available resource consisting of a visual sentiment ontology, a large detector library, and the training/testing benchmark for visual sentiment analysis.", "citation_count": "47", "reference_count": "604", "date": "2013", "authors": ["Damian Borth", "Rongrong Ji", "Tao Chen", "Thomas Breuel", "Shih-Fu Chang"], "related_topics": ["Sentiment analysis", "Ontology (information science)", "Web mining", "Noun", "Natural language processing", "Information retrieval", "Adjective", "Construct (python library)", "Computer science", "Benchmark (computing)", "Contrast (statistics)", "Artificial intelligence"]}
{"id": "2078807908", "references": ["2170658603", "2166049352", "1606858007", "2147238549", "2108598243", "2151103935", "2031489346", "2104915826", "1576445103", "1511924373"], "title": "AVA: A large-scale database for aesthetic visual analysis", "abstract": "With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.", "citation_count": "19", "reference_count": "587", "date": "2012", "authors": ["Naila Murray", "Luca Marchesotti", "Florent Perronnin"], "related_topics": ["Preference", "Visualization", "Data science", "Metadata", "Scale (chemistry)", "Variety (cybernetics)", "Computer science", "Database", "Key (cryptography)", "Aesthetic preference"]}
{"id": "1511924373", "references": ["740415", "2130660124", "2156909104", "2143668817", "2125148312", "2137471889", "2109868644", "2135705692", "1594031697", "2062024414"], "title": "Studying aesthetics in photographic images using a computational approach", "abstract": "Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities of photographs is a highly subjective task. Hence, there is no unanimously agreed standard for measuring aesthetic value. In spite of the lack of firm rules, certain features in photographic images are believed, by many, to please humans more than certain others. In this paper, we treat the challenge of automatically inferring aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated online photo sharing Website as data source. We extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images. Automated classifiers are built using support vector machines and classification trees. Linear regression on polynomial terms of the features is also applied to infer numerical aesthetics ratings. The work attempts to explore the relationship between emotions which pictures arouse in people, and their low-level content. Potential applications include content-based image retrieval and digital photography.", "citation_count": "25", "reference_count": "1,226", "date": "2006", "authors": ["Ritendra Datta", "Dhiraj Joshi", "Jia Li", "James Z. Wang"], "related_topics": ["Photography", "Beauty", "Rule of thirds", "Image retrieval", "Digital photography", "Image processing", "Digital image", "Computer science", "Aesthetics"]}
{"id": "2135957164", "references": ["2115738369", "2119228922", "1969258103", "2128272608", "1533198222", "2156595177", "2014558261", "2117301471", "2054802006", "2139047169"], "title": "Graph-Based Visual Saliency", "abstract": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: first forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti &amp; Koch ([2], [3], [4]) achieve only 84%.", "citation_count": "13", "reference_count": "3,905", "date": "2006", "authors": ["Jonathan Harel", "Christof Koch", "Pietro Perona"], "related_topics": ["Kadir\u2013Brady saliency detector", "Graph (abstract data type)", "Computer vision", "Computer science", "Artificial intelligence", "Graph based", "Saliency map", "Salient object detection", "Salient objects", "Visual saliency"]}
{"id": "2157462866", "references": ["2131975293", "2164278908", "2109722477", "2166706236", "2173213060", "2146502635", "1603765807", "1564947197", "2148087609", "2168231600"], "title": "A reliable effective terascale linear learning system", "abstract": "We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.", "citation_count": "34", "reference_count": "376", "date": "2014", "authors": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u00edk", "John Langford"], "related_topics": ["Online machine learning", "Active learning (machine learning)", "Instance-based learning", "Stability (learning theory)", "Semi-supervised learning", "Computational learning theory", "Algorithmic learning theory", "Component (UML)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2159686933", "references": ["2135463994", "2098947662", "2113341759", "1554705102", "1571461735", "1532977286", "2125848778", "2104671481", "3017143921", "2138451337"], "title": "Example-based learning for view-based human face detection", "abstract": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system.", "citation_count": "17", "reference_count": "2,887", "date": "1998", "authors": ["K.-K. Sung", "T. Poggio"], "related_topics": ["Feature vector", "Face detection", "Object detection", "Mixture model", "Contextual image classification", "Facial recognition system", "Metric (mathematics)", "Pattern matching", "Pattern recognition", "Classifier (UML)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2128272608", "references": ["1497599070", "2142768220", "2152752164", "2089597841", "2149095485", "2115441154", "1502980253", "1486735428", "2160903697", "2093353037"], "title": "A model of saliency-based visual attention for rapid scene analysis", "abstract": "A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.", "citation_count": "16", "reference_count": "12,701", "date": "1998", "authors": ["L. Itti", "C. Koch", "E. Niebur"], "related_topics": ["Kadir\u2013Brady saliency detector", "Salience (neuroscience)", "Visual search", "Feature extraction", "Feature (computer vision)", "Artificial neural network", "Object detection", "Seam carving", "Computer vision", "Pattern recognition", "Computer science", "Artificial intelligence", "Saliency map", "Scene analysis", "Visual attention", "Visual saliency"]}
{"id": "2032579724", "references": ["2014293907", "2160938187", "2058254034", "1944127002", "2122163057", "2043481572", "2062579188", "2032118018", "1986006829", "1975993749"], "title": "Green fluorescent protein as a marker for gene expression", "abstract": "A complementary DNA for the Aequorea victoria green fluorescent protein (GFP) produces a fluorescent product when expressed in prokaryotic (Escherichia coli) or eukaryotic (Caenorhabditis elegans) cells. Because exogenous substrates and cofactors are not required for this fluorescence, GFP expression can be used to monitor gene expression and protein localization in living organisms.", "citation_count": "24", "reference_count": "9,779", "date": "1994", "authors": ["Martin Chalfie", "Yuan Tu", "Ghia Euskirchen", "William W. Ward", "Douglas C. Prasher"], "related_topics": ["Green fluorescent protein", "Aequorea victoria", "Reporter gene", "Aequorea", "Gene product", "Kaede", "Complementary DNA", "Gene expression", "Cell biology", "Molecular biology", "Biology"]}
{"id": "1594381468", "references": ["2468808683", "2046576215", "2299313072"], "title": "How and Why Do Plants Inactivate Homologous (Trans)genes", "abstract": "Gene silencing in transgenic plants has emerged in the last 5 years as a topic of intense interest for both applied and basic plant scientists. From the applied side, gene silencing has come as an unwelcome surprise. Early reviews of the prospects for plant genetic engineering did not pinpoint this as a potential obstacle. Rather, the anticipated challenge was to identify tissueand stage-specific promoters that could be used to obtain regulated transgene expression. Yet, silencing of transgenes is turning out to be a substantial problem, as described recently in an aptly titled review, \"Transgene Inactivation: Plants Fight Back!\" (Finnegan and McElroy, 1994). According to this article, of 30 companies polled, nearly all reported some problems with unwanted silencing of transgenes. While companies are struggling to find ways to avoid silencing, a small cadre of basic scientists has become fascinated by the phenomenon and is analyzing a variety of silencing systems. To this latter group, the phenomenon of silencing represents more than just an unwanted response to foreign genes; rather, it has opened a door that might lead to a deeper understanding of previously unsuspected ways that plants naturally use homologous or complementary nucleic acid sequences to modify gene expression, both in the nucleus at the DNA level and in the nucleus or cytoplasm as a means to control excess production of mRNA or replication of RNA pathogens. Initial studies of transgenic plants concentrated on those showing tissue-specific and/or high levels of expression. Plants that did not exhibit the desired expression characteristics were usually discarded. It was only a matter of time, however, until such plants would begin to be treated as objects of scientific investigation in their own right. In the late 1980s, papers started to appear that were devoted solely to describing cases of silencing that involved transgene/transgene or transgene/endogenous gene interactions. A central feature of all of these studies was that silencing was associated with multiple copies of homologous DNA sequences, which could comprise protein-coding regions, promoters, or both. Although the initial impulse was to lump these first cases together as a single phenomenon, it has since become clear that several different mechanisms are probably involved. These different mechanisms are united, however, in the sense that they all involve variations on nucleic acid interactions: DNA-DNA, RNA-RNA, and DNA-RNA. A general term that encompasses all of these phenomena is \"homology-dependent gene silencing.\" Numerous detailed reviews of this topic are available (Jorgensen, 1992; Matzke and Matzke, 1993; Flavell, 1994; Matzke et al., 1994b; Mol et al., 1994).", "citation_count": "3", "reference_count": "749", "date": "1995", "authors": ["M. A. Matzke", "A. J. M. Matzke"], "related_topics": ["Gene silencing", "Transgene", "Gene", "Gene expression", "Transcription (biology)", "RNA", "DNA", "Promoter", "Genetics", "Biology"]}
{"id": "1992194142", "references": ["2028622989", "2160938187", "2149919973", "2051013729", "1555021163", "1944127002", "1821435713", "1969757078", "2015292449", "2055043387"], "title": "The C. elegans genome sequencing project: a beginning", "abstract": "The long-term goal of this project is the elucidation of the complete sequence of the Caenorhabditis elegans genome. During the first year methods have been developed and a strategy implemented that is amenable to large-scale sequencing. The three cosmids sequenced in this initial phase are surprisingly rich in genes, many of which have mammalian homologues.", "citation_count": "65", "reference_count": "585", "date": "1992", "authors": ["J. Sulston", "Z. Du", "K. Thomas", "R. Wilson", "L. Hillier", "R. Staden", "N. Halloran", "P. Green", "J. Thierry-Mieg", "L. Qiu", "S. Dear", "A. Coulson", "M. Craxton", "R. Durbin", "M. Berks", "M. Metzstein", "T. Hawkins", "R. Ainscough", "R. Waterston"], "related_topics": ["Genome project", "Cancer genome sequencing", "Genome", "DNA sequencing", "Caenorhabditis elegans", "Genomic organization", "Complete sequence", "Gene", "Genetics", "Biology"]}
{"id": "1944127002", "references": ["314047257", "2115708583", "2018868264"], "title": "THE GENETICS OF CAENORHABDITIS ELEGANS", "abstract": "Methods are described for the isolation, complementation and mapping of mutants of Caenorhabditis elegans, a small free-living nematode worm. About 300 EMS-induced mutants affecting behavior and morphology have been characterized and about one hundred genes have been defined. Mutations in 77 of these alter the movement of the animal. Estimates of the induced mutation frequency of both the visible mutants and X chromosome lethals suggests that, just as in Drosophila, the genetic units in C. elegans are large.", "citation_count": "3", "reference_count": "15,420", "date": "1974", "authors": ["S Brenner"], "related_topics": ["WormBook", "Caenorhabditis briggsae", "Caenorhabditis", "Caenorhabditis elegans", "Daf-16", "Defecation rhythm", "Caenorhabditis remanei", "Pristionchus pacificus", "Genetics", "Biology"]}
{"id": "2022846624", "references": ["2092017548", "2036853011", "2089033636", "2183562487", "1967881544", "1549105044", "2088545432", "1587044218", "2015686762", "2130485280"], "title": "PKR: a new name and new roles", "abstract": "The double-stranded RNA (dsRNA)-activated protein kinase, now called PKR, was first discovered by virtue of its ability to phosphorylate translation initiation factor eIF-2 and inhibit its activity. Recent studies have shown that expression of inactive mutants of PKR in cultured cells causes them to acquire characteristics typical of transformed cells. These and other findings indicate that PKR plays a role in the normal control of cell growth and differentiation. It seems likely that, in addition to eIF-2, PKR has other substrates including the protein I-kappa B, which regulates the transcription of certain genes. Indeed, it now seems likely that PKR mediates the regulation of selected genes by dsRNA.", "citation_count": "35", "reference_count": "336", "date": "1995", "authors": ["Christopher G. Proud"], "related_topics": ["Protein kinase R", "EIF-2 kinase", "Protein kinase A", "Transcription (biology)", "RNA silencing", "RNA", "Gene", "Cell growth", "Genetics", "Biology"]}
{"id": "1989952069", "references": ["1969042688", "2053753856", "2008290466", "2021409703", "1944127002", "2038029561", "1984944970", "2006130293", "1986006829", "2011389479"], "title": "The embryonic cell lineage of the nematode Caenorhabditis elegans.", "abstract": "The embryonic cell lineage of Caenorhabditis elegans has been traced from zygote to newly hatched larva, with the result that the entire cell lineage of this organism is now known. During embryogenesis 671 cells are generated; in the hermaphrodite 113 of these (in the male 111) undergo programmed death and the remainder either differentiate terminally or become postembryonic blast cells. The embryonic lineage is highly invariant, as are the fates of the cells to which it gives rise. In spite of the fixed relationship between cell ancestry and cell fate, the correlation between them lacks much obvious pattern. Thus, although most neurons arise from the embryonic ectoderm, some are produced by the mesoderm and a few are sisters to muscles; again, lineal boundaries do not necessarily coincide with functional boundaries. Nevertheless, cell ablation experiments (as well as previous cell isolation experiments) demonstrate substantial cell autonomy in at least some sections of embryogenesis. We conclude that the cell lineage itself, complex as it is, plays an important role in determining cell fate. We discuss the origin of the repeat units (partial segments) in the body wall, the generation of the various orders of symmetry, the analysis of the lineage in terms of sublineages, and evolutionary implications.", "citation_count": "41", "reference_count": "4,250", "date": "1983", "authors": ["J.E. Sulston", "E. Schierenberg", "J.G. White", "J.N. Thomson"], "related_topics": ["Cell fate determination", "Lineage (genetic)", "Cellular differentiation", "Embryonic stem cell", "Cell division", "Caenorhabditis elegans", "Embryogenesis", "Mesoderm", "Cell biology", "Genetics", "Biology"]}
{"id": "2151068726", "references": ["1594700207", "2066398640", "1944127002", "2036338927", "1999730046", "2039075455", "1989952069", "2125471204", "2149871164", "2061424654"], "title": "MEX-3 Is a KH Domain Protein That Regulates Blastomere Identity in Early C. elegans Embryos", "abstract": "After the first division of the C. elegans embryo, the posterior blastomere can produce numerous muscles while the anterior blastomere cannot. We show here that maternal-effect lethal mutations in the gene mex-3 cause descendants of the anterior blastomere to produce muscles by a pattern of development similar to that of a descendant of the wild-type posterior blastomere. mex-3 encodes a probable RNA-binding protein that is distributed unequally in early embryos and that is a component of germline-specific granules called P granules. We propose that MEX-3 contributes to anterior-posterior asymmetry by regulating one or more mRNAs involved in specifying the fate of the posterior blastomere.", "citation_count": "46", "reference_count": "312", "date": "1996", "authors": ["Bruce W Draper", "Craig C Mello", "Bruce Bowerman", "Jeff Hardin", "James R Priess"], "related_topics": ["Blastomere", "KH domain", "Embryo", "Morphogenesis", "Regulation of gene expression", "Gene", "In situ hybridization", "Sequence alignment", "Genetics", "Biology"]}
{"id": "1581412826", "references": ["2032579724", "2144634347", "1952115186", "2160467221", "2064417988", "1985910275", "2122163057", "2138007390", "2063466760", "2043481572"], "title": "Chapter 19 DNA Transformation", "abstract": "Publisher Summary   This chapter discusses DNA transformation. DNA transformation assays in a whole organism provide experimental links between molecular structure and phenotype. Experiments with transgenic Caenorhabditis elegans start in general with the injection of DNA into the adult gonad. Effects on phenotype or gene expression patterns can be analyzed either in F1 progeny derived from the injected animals or in derived transgenic lines. Germ-line transformation has been achieved by microinjection of DNA directly into oocyte nuclei or by microinjection of DNA into the cytoplasm of the hermaphrodite syncytial gonad. Three forms of heritable DNA transformation have been observed in C. elegans are: (1) extra chromosomal transformation; (2) non-homologous integration; and (3) homologous integration. Setting up microinjection in a laboratory already equipped for C. elegans genetics and molecular biology requires a modest investment in space and money. A separate easily scoreable marker gene to identify transformed animals can be extremely useful in a variety of injection experiments. The propensity for injected DNA molecules to recombine with each other generally allows one to coinject the selectable marker with a DNA segment to be tested for activity.", "citation_count": "45", "reference_count": "1,443", "date": "1995", "authors": ["Craig Mello", "Andrew Fire"], "related_topics": ["In vitro recombination", "Molecular cloning", "Marker gene", "Selectable marker", "DNA", "Transformation (genetics)", "Transgene", "Caenorhabditis elegans", "Genetics", "Biology"]}
{"id": "2034831897", "references": ["2031372024", "2050509627", "2141019817", "2139796895", "2150824061", "2047439748", "2070016382", "2009310436", "2160686164", "2062230922"], "title": "A similarity between viral defense and gene silencing in plants.", "abstract": "Gene silencing in plants, in which an endogenous gene is suppressed by introduction of a related transgene, has been used for crop improvement. Observations that viruses are potentially both initiators and targets of gene silencing suggested that this phenomenon may be related to natural defense against viruses. Supporting this idea, it was found that nepovirus infection of nontransgenic plants induces a resistance mechanism that is similar to transgene-induced gene silencing.", "citation_count": "12", "reference_count": "1,060", "date": "1997", "authors": ["Frank Ratcliff", "Bryan D. Harrison", "David C. Baulcombe"], "related_topics": ["Gene silencing", "Transgene", "Gene", "Nepovirus", "Gene expression", "Nicotiana", "Virus", "Endogeny", "Genetics", "Biology"]}
{"id": "2149871164", "references": ["2020257412", "2144634347", "1944127002", "2015065370", "2138270253", "2122163057", "2106258670", "2011756230", "2015292449", "2055043387"], "title": "par-1, a Gene Required for Establishing Polarity in C. Elegans Embryos, Encodes a Putative Ser/Thr Kinase That Is Asymmetrically Distributed", "abstract": "Abstract  The first cleavage of C. elegans is asymmetric, generating daughter cells with different sizes, cytoplasmic components, and fates. Mutations in the  par -1 gene disrupt this asymmetry. We report here that  par -1 encodes a putative Ser/Thr kinase with similarity to kinases from yeasts and mammals. Two strong alleles have mutations in the kinase domain, suggesting that kinase activity is essential for  par -1 function. PAR-1 protein is localized to the posterior periphery of the zygote and is distributed in a polar fashion preceding the asymmetric divisions of the germline lineage. Because PAR-1 distribution in the germline correlates with the distribution of germline-specific P granules, it is possible that PAR-1 functions in germ line development as well as in establishing embryonic polarity.", "citation_count": "48", "reference_count": "1,965", "date": "1995", "authors": ["Su Guo", "Kenneth J. Kemphues"], "related_topics": ["Cyclin-dependent kinase 2", "Kinase activity", "MAP2K7", "Germ line development", "Germline", "Cell polarity", "Protein kinase domain", "Cell division", "Molecular biology", "Biology"]}
{"id": "2145023731", "references": ["2007057443", "2016396776", "2003370853", "1968245656", "1995756857", "128364430", "2130355536", "1533832053", "2002882922", "1555351000"], "title": "A Computational Approach to Edge Detection", "abstract": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.", "citation_count": "13", "reference_count": "38,241", "date": "1986", "authors": ["John Canny"], "related_topics": ["Canny edge detector", "Edge detection", "Deriche edge detector", "Image gradient", "Sobel operator", "Marr\u2013Hildreth algorithm", "Interest point detection", "Roberts cross", "Artificial intelligence", "Mathematics"]}
{"id": "2032210760", "references": ["2112076978", "2067885219", "1676820704", "2152761983", "1966280301", "1975846642", "2084812512", "2107890099", "1605688901", "3124955340"], "title": "Improved boosting algorithms using confidence-rated predictions", "abstract": "We describe several improvements to Freund and Schapire\u2018s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.", "citation_count": "24", "reference_count": "3,990", "date": "1998", "authors": ["Robert E. Schapire", "Yoram Singer"], "related_topics": ["BrownBoost", "Boosting (machine learning)", "AdaBoost", "Alternating decision tree", "LPBoost", "LogitBoost", "Multiclass classification", "Decision tree", "Machine learning", "Algorithm", "Mathematics", "Artificial intelligence"]}
{"id": "2093191240", "references": ["2118269922", "2074429597", "2061749544", "2151135734", "2149173084", "3017143921", "1576247523", "2147956424", "2094078373", "2110499305"], "title": "QBIC project: querying images by content, using color, texture, and shape", "abstract": "In the query by image content (QBIC) project we are studying methods to query large on-line image databases using the images' content as the basis of the queries. Examples of the content we use include color, texture, and shape of image objects and regions. Potential applications include medical (`Give me other images that contain a tumor with a texture like this one'), photo-journalism (`Give me images that have blue at the top and red at the bottom'), and many others in art, fashion, cataloging, retailing, and industry. Key issues include derivation and computation of attributes of images and objects that provide useful query functionality, retrieval methods based on similarity as opposed to exact match, query by image example or user drawn image, the user interfaces, query refinement and navigation, high dimensional database indexing, and automatic and semi-automatic database population. We currently have a prototype system written in X/Motif and C running on an RS/6000 that allows a variety of queries, and a test database of over 1000 images and 1000 objects populated from commercially available photo clip art images. In this paper we present the main algorithms for color texture, shape and sketch query that we use, show example query results, and discuss future directions.\u00a9 (1993) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.", "citation_count": "14", "reference_count": "3,024", "date": "1993", "authors": ["Carlton Wayne Niblack", "Ron Barber", "Will Equitz", "Myron D. Flickner", "Eduardo H. Glasman", "Dragutin Petkovic", "Peter Yanker", "Christos Faloutsos", "Gabriel Taubin"], "related_topics": ["Query optimization", "Query by Example", "Query expansion", "Query language", "Web query classification", "Spatial query", "Web search query", "Image texture", "Information retrieval", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2135705692", "references": ["2008297189", "2168175751", "1917380066", "2049633694", "2103414828", "2160066518", "2162630772", "2049694710", "2914885528", "1991605728"], "title": "Blobworld: image segmentation using expectation-maximization and its application to image querying", "abstract": "Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions that roughly correspond to objects, we allow querying at the level of objects rather than global image properties. We present results indicating that querying for images using Blobworld produces higher precision than does querying using color and texture histograms of the entire image in cases where the image contains distinctive objects.", "citation_count": "49", "reference_count": "2,016", "date": "2002", "authors": ["C. Carson", "S. Belongie", "H. Greenspan", "J. Malik"], "related_topics": ["Image texture", "Image segmentation", "Feature detection (computer vision)", "Automatic image annotation", "Image processing", "Binary image", "Image retrieval", "Color image", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1490632837", "references": ["2162413715", "2041752335", "2563709626", "2113834765", "2042755403", "2065301447", "2162703509", "2107790757", "1490632837", "2522324076"], "title": "Pyramid-based texture analysis/synthesis", "abstract": "This paper describes a method for synthesizing images that match the texture appearance of a given digitized sample. This synthesis is completely automatic and requires only the \"target\" texture as input. It allows generation of as much texture as desired so that any object can be covered. The approach is based on a model of human texture perception, and has potential to be a practically useful tool for image processing and graphics applications.", "citation_count": "17", "reference_count": "2,020", "date": "1995", "authors": ["D.J. Heeger", "J.R. Bergen"], "related_topics": ["Texture atlas", "Texture filtering", "Texture compression", "Image texture", "Texture synthesis", "Bidirectional texture function", "Projective texture mapping", "Texture mapping unit", "UVW mapping", "Texel", "Image processing", "Pyramid", "Histogram", "Computer vision", "Continuous wavelet transform", "Computer science", "Artificial intelligence"]}
{"id": "2025653905", "references": ["2149706766", "1528905581", "2102201073", "2797583072", "2049633694", "2137983211", "2044758663", "3017143921", "1492221128"], "title": "Hierarchical mixtures of experts and the EM algorithm", "abstract": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain.", "citation_count": "34", "reference_count": "3,907", "date": "1994", "authors": ["Michael I. Jordan", "Robert A. Jacobs"], "related_topics": ["Mixture model", "Supervised learning", "Expectation\u2013maximization algorithm", "Product of experts", "Artificial neural network", "Systems architecture", "Statistical model", "Maximization", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2186428165", "references": ["3029645440", "2156909104", "2124776405", "1995945562", "1639032689", "2148603752", "1880262756", "1554944419", "2912565176", "2008620264"], "title": "Data Mining: Concepts and Techniques (2nd edition)", "abstract": "The book Knowledge Discovery in Databases, edited by Piatetsky-Shapiro and Frawley [PSF91], is an early collection of research papers on knowledge discovery from data. The book Advances in Knowledge Discovery and Data Mining, edited by Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy [FPSSe96], is a collection of later research results on knowledge discovery and data mining. There have been many data mining books published in recent years, including Predictive Data Mining by Weiss and Indurkhya [WI98], Data Mining Solutions: Methods and Tools for Solving Real-World Problems by Westphal and Blaxton [WB98], Mastering Data Mining: The Art and Science of Customer Relationship Management by Berry and Linofi [BL99], Building Data Mining Applications for CRM by Berson, Smith, and Thearling [BST99], Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank [WF05], Principles of Data Mining (Adaptive Computation and Machine Learning) by Hand, Mannila, and Smyth [HMS01], The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman [HTF01], Data Mining: Introductory and Advanced Topics by Dunham, and Data Mining: Multimedia, Soft Computing, and Bioinformatics by Mitra and Acharya [MA03]. There are also books containing collections of papers on particular aspects of knowledge discovery, such as Machine Learning and Data Mining: Methods and Applications edited by Michalski, Brakto, and Kubat [MBK98], and Relational Data Mining edited by Dzeroski and Lavrac [De01], as well as many tutorial notes on data mining in major database, data mining and machine learning conferences.", "citation_count": "500", "reference_count": "2,505", "date": "2006", "authors": ["Jiawei Han", "Micheline Kamber"], "related_topics": ["Knowledge extraction", "Relational data mining", "Soft computing", "Data science", "Customer relationship management", "Computer science", "Data mining", "Statistical learning"]}
{"id": "2121601095", "references": ["2217896605", "2033419168", "2145023731", "2099111195", "1560013842", "2121647436", "1554663460", "2104095591", "1594031697", "2138451337"], "title": "Detecting faces in images: a survey", "abstract": "Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research.", "citation_count": "174", "reference_count": "5,804", "date": "2002", "authors": ["Ming-Hsuan Yang", "D.J. Kriegman", "N. Ahuja"], "related_topics": ["Face detection", "Object-class detection", "Three-dimensional face recognition", "Object detection", "Orientation (computer vision)", "Facial recognition system", "Facial motion capture", "Pose", "Feature extraction", "Cognitive neuroscience of visual object recognition", "Computer vision", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2115689562", "references": ["1997063559", "2145023731", "2028310195", "3003662786", "1991848143", "2098947662", "2098693229", "2113341759", "2100115174", "2620619910"], "title": "Human and machine recognition of faces: a survey", "abstract": "The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists between studies in psychophysics and the engineering literature. Most importantly, there exists no evaluation or benchmarking studies using large databases with the image quality that arises in commercial and law enforcement applications In this paper, we first present different applications of face recognition in commercial and law enforcement sectors. This is followed by a brief overview of the literature on face recognition in the psychophysics community. We then present a detailed overview of move than 20 years of research done in the engineering community. Techniques for segmentation/location of the face, feature extraction and recognition are reviewed. Global transform and feature based methods using statistical, structural and neural classifiers are summarized. &gt;", "citation_count": "189", "reference_count": "4,269", "date": "1995", "authors": ["R. Chellappa", "C.L. Wilson", "S. Sirohey"], "related_topics": ["Face Recognition Grand Challenge", "Feature extraction", "Eigenface", "Facial recognition system", "Credit card", "Pattern recognition (psychology)", "Information extraction", "Image processing", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2153791616", "references": ["2136922672", "2085529605", "1652505363", "2145889472", "2049633694", "2100495367", "2029949252", "2116064496", "1983578042", "2072128103"], "title": "Whatever next? Predictive brains, situated agents, and the future of cognitive science", "abstract": "Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this \"hierarchical prediction machine\" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.", "citation_count": "350", "reference_count": "3,932", "date": "2013", "authors": ["Andy Clark"], "related_topics": ["Action (philosophy)", "Unified Science", "Perception", "Top-down and bottom-up design", "Generative model", "Situated", "Agency (philosophy)", "Variety (cybernetics)", "Cognitive science", "Cognitive psychology", "Psychology"]}
{"id": "2161160262", "references": ["2999729612", "1634005169", "2482402870", "1991848143", "1575476631", "1971784203", "3017143921", "2132549764", "2011039300", "1992419399"], "title": "An efficient k-means clustering algorithm: analysis and implementation", "abstract": "In k-means clustering, we are given a set of n data points in d-dimensional space R/sup d/ and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.", "citation_count": "51", "reference_count": "5,900", "date": "2002", "authors": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "related_topics": ["CURE data clustering algorithm", "Canopy clustering algorithm", "Data stream clustering", "Lloyd's algorithm", "Cluster analysis", "Determining the number of clusters in a data set", "Correlation clustering", "Linde\u2013Buzo\u2013Gray algorithm", "Ramer\u2013Douglas\u2013Peucker algorithm", "FSA-Red Algorithm", "Nearest-neighbor chain algorithm", "k-means clustering", "Fuzzy clustering", "k-medians clustering", "Affinity propagation", "DBSCAN", "Clustering high-dimensional data", "k-medoids", "Constrained clustering", "Difference-map algorithm", "Image segmentation", "k-d tree", "Data set", "Heuristic", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2124592837", "references": ["2296297488", "2096990042", "2145023731", "2129119440", "2141403362", "2016396776", "2164507445", "2078085827", "1560188253", "2740373864"], "title": "A robust visual method for assessing the relative performance of edge-detection algorithms", "abstract": "A new method for evaluating edge detection algorithms is presented and applied to measure the relative performance of algorithms by Canny, Nalwa-Binford, Iverson-Zucker, Bergholm, and Rothwell. The basic measure of performance is a visual rating score which indicates the perceived quality of the edges for identifying an object. The process of evaluating edge detection algorithms with this performance measure requires the collection of a set of gray-scale images, optimizing the input parameters for each algorithm, conducting visual evaluation experiments and applying statistical analysis methods. The novel aspect of this work is the use of a visual task and real images of complex scenes in evaluating edge detectors. The method is appealing because, by definition, the results agree with visual evaluations of the edge images.", "citation_count": "43", "reference_count": "681", "date": "1997", "authors": ["M.D. Heath", "S. Sarkar", "T. Sanocki", "K.W. Bowyer"], "related_topics": ["Edge detection", "Real image", "Enhanced Data Rates for GSM Evolution", "Pattern recognition", "Computer vision", "Measure (mathematics)", "Set (abstract data type)", "Algorithm", "Process (computing)", "Computer science", "Detector", "Object (computer science)", "Artificial intelligence"]}
{"id": "2120838001", "references": ["1997063559", "2103384342", "1667165204", "2100115174", "2131329059", "2144520790", "2058834119", "2167034998", "2098301339", "2180838288"], "title": "The statistics of natural images", "abstract": "Recently there has been a resurgence of interest in the properties of natural images. Their statistics are important not only in image compression but also for the study of sensory processing in biology, which can be viewed as satisfying certain \u2018design criteria\u2019. This review summarizes previous work on image statistics and presents our own data. Perhaps the most notable property of natural images is an invariance to scale. We present data to support this claim as well as evidence for a hierarchical invariance in natural scenes. These symmetries provide a powerful description of natural images as they greatly restrict the class of allowed distributions.", "citation_count": "82", "reference_count": "1,064", "date": "1994", "authors": ["Daniel L Ruderman"], "related_topics": ["Image compression", "Property (programming)", "Natural (music)", "Scale (descriptive set theory)", "Class (philosophy)", "Image (mathematics)", "restrict", "Statistics", "Mathematics"]}
{"id": "2139643804", "references": ["2077887717", "2124731682", "2150920547", "2145012779", "1753871439", "2167034998", "2101933716", "2094211891"], "title": "Statistics of natural images and models", "abstract": "Large calibrated datasets of 'random' natural images have recently become available. These make possible precise and intensive statistical studies of the local nature of images. We report results ranging from the simplest single pixel intensity to joint distribution of 3 Haar wavelet responses. Some of these statistics shed light on old issues such as the near scale-invariance of image statistics and some are entirely new. We fit mathematical models to some of the statistics and explain others in terms of local image features.", "citation_count": "8", "reference_count": "741", "date": "1999", "authors": ["Jinggang Huang", "D. Mumford"], "related_topics": ["Image processing", "Haar wavelet", "Mathematical model", "Joint probability distribution", "Probability distribution", "Random variable", "Histogram", "Entropy (information theory)", "Pattern recognition", "Computer vision", "Statistics", "Mathematics", "Artificial intelligence"]}
{"id": "2101933716", "references": ["2145889472", "2022735534", "2105464873", "2019502123", "2120838001", "2170319235", "2042755403", "2167034998", "2137234026", "2180838288"], "title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "abstract": "Properties of the receptive fields of simple cells in macaque cortex were compared with properties of independent component filters generated by independent component analysis (ICA) on a large set of natural images. Histograms of spatial frequency bandwidth, orientation tuning bandwidth, aspect ratio and length of the receptive fields match well. This indicates that simple cells are well tuned to the expected statistics of natural stimuli. There is no match, however, in calculated and measured distributions for the peak of the spatial frequency response: the filters produced by ICA do not vary their spatial scale as much as simple cells do, but are fixed to scales close to the finest ones allowed by the sampling lattice. Possible ways to resolve this discrepancy are discussed.", "citation_count": "40", "reference_count": "1,425", "date": "1998", "authors": ["J H van Hateren", "A van der Schaaf"], "related_topics": ["Spatial frequency", "Receptive field", "Efficient coding hypothesis", "Independent component analysis", "Visual cortex", "Bandwidth (signal processing)", "Optical resolution", "Histogram", "Pattern recognition", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1537519384", "references": ["1593038947", "1985517056", "2103559027", "2115755118", "2150920547", "2251490743", "2139643804", "2167034998", "2137234026", "1490632837"], "title": "Scales in Natural Images and a Consequence on their Bounded Variation Norm", "abstract": "This paper introduces a new method for analyzing scaling phenomena in natural images, and draws some consequences as to whether natural images belong to the space of functions with bounded variation.", "citation_count": "20", "reference_count": "26", "date": "1999", "authors": ["Luis Alvarez", "Yann Gousseau", "Jean-Michel Morel"], "related_topics": ["Bounded variation", "Norm (mathematics)", "Scaling", "Pure mathematics", "Calculus", "Mathematics"]}
{"id": "2150117517", "references": ["2128439793", "1562206072", "2145023731", "2099111195", "2126174118", "2156741519", "2149396346", "1569587969", "2117812871", "2164814911"], "title": "Fundamental bounds on edge detection: an information theoretic evaluation of different edge cues", "abstract": "We treat the problem of edge detection as one of statistical inference. Local edge cues, implemented by filters, provide information about the likely positions of edges which can be used as input to higher-level models. Different edge cues can be evaluated by the statistical effectiveness of their corresponding filters evaluated on a dataset of 100 presegmented images. We use information theoretic measures to determine the effectiveness of a variety of different edge detectors working at multiple scales on black and white and color images. Our results give quantitative measures for the advantages of multi-level processing, for the use of chromaticity in addition to greyscale, and for the relative effectiveness of different detectors.", "citation_count": "10", "reference_count": "128", "date": "1999", "authors": ["S. Konishi", "A.L. Yuille", "J. Coughlan", "Song Chun Zhu"], "related_topics": ["Edge detection", "Object detection", "Grayscale", "Information theory", "Statistical inference", "Entropy (information theory)", "Pattern recognition", "Computer vision", "Probability distribution", "Detector", "Mathematics", "Artificial intelligence"]}
{"id": "2126326837", "references": ["2156909104", "2055522016", "2119821739", "2124351082", "1761010383", "2032210760", "2087347434", "2148603752", "2093191240", "2914885528"], "title": "Support vector machines for histogram-based image classification", "abstract": "Traditional classification approaches generalize poorly on image classification tasks, because of the high dimensionality of the feature space. This paper shows that support vector machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms. Heavy-tailed RBF kernels of the form K(x, y)=e/sup -/spl rho///spl Sigma//sub i//sup |xia-yia|b/ with a /spl les/1 and b/spl les/2 are evaluated on the classification of images extracted from the Corel stock photo collection and shown to far outperform traditional polynomial or Gaussian radial basis function (RBF) kernels. Moreover, we observed that a simple remapping of the input x/sub i//spl rarr/x/sub i//sup a/ improves the performance of linear SVM to such an extend that it makes them, for this problem, a valid alternative to RBF kernels.", "citation_count": "16", "reference_count": "2,057", "date": "1999", "authors": ["O. Chapelle", "P. Haffner", "V.N. Vapnik"], "related_topics": ["Contextual image classification", "Support vector machine", "Feature vector", "Radial basis function", "Kernel (image processing)", "Histogram", "Polynomial", "Artificial neural network", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2107198582", "references": ["1989871863", "1513966746", "2023751161", "2043647324", "2582614493", "2081519360", "2135323728", "2139762693", "2341059552"], "title": "Symmetry-seeking models and 3D object reconstruction", "abstract": "We propose models of 3D shape which may be viewed as deformable bodies composed of simulated elastic material. In contrast to traditional, purely geometric models of shape, deformable models are active\u2014their shapes change in response to externally applied forces. We develop a deformable model for 3D shape which has a preference for axial symmetry. Symmetry is represented even though the model does not belong to a parametric shape family such as (generalized) cylinders. Rather, a symmetry-seeking property is designed into internal forces that constrain the deformations of the model. We develop a framework for 3D object reconstruction based on symmetry-seeking models. Instances of these models are formed from monocular image data through the action of external forces derived from the data. The forces proposed in this paper deform the model in space so that the shape of its projection into the image plane is consistent with the 2D silhouette of an object of interest. The effectiveness of our approach is demonstrated using natural images.", "citation_count": "28", "reference_count": "496", "date": "1988", "authors": ["Demetri Terzopoulos", "Andrew P. Witkin", "Michael Kass"], "related_topics": ["Active shape model", "Shape analysis (digital geometry)", "Image plane", "Axial symmetry", "Silhouette", "Image processing", "Parametric statistics", "Mathematical analysis", "Geometry", "Mathematics", "Monocular image"]}
{"id": "1977699267", "references": ["2075357105", "1974939198", "2048330959", "2155195832", "1965509304", "2094108350", "2043234782", "2074897560", "391302643", "2136113379"], "title": "Elastic Matching of Line Drawings", "abstract": "Recently a dynamic elastic model [1] was proposed for automatic image matching. Examples were shown applying the model to dot patterns and gray scale pictures. This paper extends the model to line drawings. Examples are shown on handprint and animation, suggesting the use of dynamic matching for shape recognition and for motion correspondence.", "citation_count": "12", "reference_count": "282", "date": "1981", "authors": ["D. J. Burr"], "related_topics": ["Elastic matching", "Pattern matching", "Matching (statistics)", "Image registration", "Animation", "Pattern recognition (psychology)", "Grayscale", "Handwriting recognition", "Image segmentation", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2003370853", "references": ["2074798463", "2019222286", "1622620102", "1999908130", "1995756857", "2105672294", "2117731089", "2116360511", "2130355536", "1964415410"], "title": "Theory of Edge Detection", "abstract": "A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr &amp; Ullman 1979).", "citation_count": "52", "reference_count": "9,566", "date": "1980", "authors": ["D. Marr", "E. Hildreth"], "related_topics": ["Edge detection", "Marr\u2013Hildreth algorithm", "Difference of Gaussians", "Blob detection", "Scale-space axioms", "Gaussian", "Filter (signal processing)", "Scale (descriptive set theory)", "Mathematical analysis", "Mathematics"]}
{"id": "1995756857", "references": ["1997494543", "2003370853", "1494482350", "1999908130", "2048330959", "2081519360", "2130355536", "1597474747", "1533832053", "1964415410"], "title": "A Computational Theory of Human Stereo Vision", "abstract": "An algorithm is proposed for solving the stereoscopic matching problem. The algorithm consists of five steps: (1) Each image is filtered at different orientations with bar masks of four sizes that increase with eccentricity; the equivalent filters are one or two octaves wide. (2) Zero-crossings in the filtered images, which roughly correspond to edges, are localized. Positions of the ends of lines and edges are also found. (3) For each mask orientation and size, matching takes place between pairs of zero-crossings or terminations of the same sign in the two images, for a range of disparities up to about the width of the mask\u2019s central region. (4) Wide masks can control vergence movements, thus causing small masks to come into correspondence. (5) When a correspondence is achieved, it is stored in a dynamic buffer, called the 2\u00bd-D sketch.", "citation_count": "65", "reference_count": "2,949", "date": "1979", "authors": ["D. Marr", "T. Poggio"], "related_topics": ["Orientation (computer vision)", "Stereopsis", "Matching (graph theory)", "Stereoscopy", "Random dot stereogram", "Bar (music)", "Vergence (optics)", "Eccentricity (behavior)", "Computer vision", "Mathematics", "Artificial intelligence"]}
{"id": "1531060698", "references": ["65138405", "2165810248", "2000523160", "2105096388", "2129855380"], "title": "Computational vision and regularization theory", "abstract": "Descriptions of physical properties of visible surfaces, such as their distance and the presence of edges, must be recovered from the primary image data. Computational vision aims to understand how such descriptions can be obtained from inherently ambiguous and noisy data. A recent development in this field sees early vision as a set of ill-posed problems, which can be solved by the use of regularization methods. These lead to algorithms and parallel analog circuits that can solve \u2018ill-posed problems\u2019 and which are suggestive of neural equivalents in the brain.", "citation_count": "0", "reference_count": "2,167", "date": "1991", "authors": ["Tomaso Poggio", "Vincent Torre", "Christof Koch"], "related_topics": ["Regularization (mathematics)", "Field (computer science)", "Set (psychology)", "Computer vision", "Image (mathematics)", "Analogue electronics", "Development (topology)", "Mathematics", "Artificial intelligence", "Computational vision", "Regularization theory"]}
{"id": "2582614493", "references": ["1533832053", "1501400124", "1995756857", "2109863423", "1531060698", "2069501481", "2911709767", "2139762693", "2341059552"], "title": "Signal matching through scale space", "abstract": "Given a collection of similar signals that have been deformed with respect to each other, the general signal-matching problem is to recover the deformation. We formulate the problem as the minimization of an energy measure that combines a smoothness term and a similarity term. The minimization reduces to a dynamic system governed by a set of coupled, first-order differential equations. The dynamic system finds an optimal solution at a coarse scale and then tracks it continuously to a fine scale. Among the major themes in recent work on visual signal matching have been the notions of matching as constrained optimization, of variational surface reconstruction, and of coarse-to-fine matching. Our solution captures these in a precise, succinct, and unified form. Results are presented for one-dimensional signals, a motion sequence, and a stereo pair.", "citation_count": "22", "reference_count": "377", "date": "1987", "authors": ["Andrew P. Witkin", "Demetri Terzopoulos", "Michael Kass"], "related_topics": ["Scale space", "Matching (statistics)", "Scale (ratio)", "Constrained optimization", "Similarity (geometry)", "Term (time)", "Sequence", "Minification", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "1631253743", "references": ["1597286183", "2150535417", "2161958784", "2115689562", "2131910503", "3022071797", "2154841693", "2148107745", "2019635781", "2104095591"], "title": "Ill-Posed Problems and Regularization Analysis in Early Vision", "abstract": "While in classical optics the problem is to determine the images of physical objects, vision is confronted with the inverse problem of recovering three-dimensional shape from the light distribution in the image. Most processes of early vision such as stereomatching, computation of motion and all the ``structure from\" processes can be regarded as solutions to inverse problems. This common characteristic of early vision can be formalized---{\\it most early vision problems are ``ill-posed problems\" in the sense of Hadamard}. We will show that a mathematical theory developed for regularizing ill-posed problems leads in a natural way to the solution of early vision problems in terms of variational principles of a certain class. This is a new theoretical framework for some of the variational solutions already obtained in the analysis of early vision processes. It also shows how several other problems in early vision can be approached and solved.", "citation_count": "0", "reference_count": "190", "date": "1984", "authors": ["Tomaso Poggio", "Vincent Torre"], "related_topics": ["Graph cuts in computer vision", "Inverse problem", "Well-posed problem", "Mathematical theory", "Regularization (mathematics)", "Hadamard transform", "Calculus", "Computation", "Inversion (meteorology)", "Mathematics"]}
{"id": "2139762693", "references": ["1997063559", "3141390961", "2022140147", "2133155955", "3021591933", "2003945700", "2109863423", "2177721432", "2581275558", "2074797641"], "title": "Regularization of Inverse Visual Problems Involving Discontinuities", "abstract": "Inverse problems, such as the reconstruction problems that arise in early vision, tend to be mathematically ill-posed. Through regularization, they may be reformulated as well-posed variational principles whose solutions are computable. Standard regularization theory employs quadratic stabilizing functionals that impose global smoothness constraints on possible solutions. Discontinuities present serious difficulties to standard regularization, however, since their reconstruction requires a precise spatial control over the smoothing properties of stabilizers. This paper proposes a general class of controlled-continuity stabilizers which provide the necessary control over smoothness. These nonquadratic stabilizing functionals comprise multiple generalized spline kernels combined with (noncontinuous) continuity control functions. In the context of computational vision, they may be thought of as controlled-continuity constraints. These generic constraints are applicable to visual reconstruction problems that involve both continuous regions and discontinuities, for which global smoothness constraints fail.", "citation_count": "34", "reference_count": "1,258", "date": "1986", "authors": ["Demetri Terzopoulos"], "related_topics": ["Backus\u2013Gilbert method", "Regularization (mathematics)", "Inverse problem", "Spline (mathematics)", "Smoothing", "Discontinuity (linguistics)", "Classification of discontinuities", "Quadratic equation", "Mathematical optimization", "Mathematics"]}
{"id": "2045798786", "references": ["2166627017", "2072632478", "2133246412", "2127787769", "29412004", "2129733001", "197595011", "2133138342", "2062533874", "1999927042"], "title": "The Representation and Matching of Pictorial Structures", "abstract": "The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection.", "citation_count": "11", "reference_count": "1,828", "date": "1973", "authors": ["M.A. Fischler", "R.A. Elschlager"], "related_topics": ["Matching (statistics)", "Object (computer science)", "Representation (systemics)", "Metric (mathematics)", "Theoretical computer science", "Scheme (programming language)", "Base (topology)", "Dynamic programming", "Mathematics", "Primary problem"]}
{"id": "2053631808", "references": ["2799004609", "2151436648", "1552744309", "2905110430", "2901284226", "1995875735", "2088164510", "2983896310", "2797646349"], "title": "Quasi-random set systems", "abstract": "There are many properties of mathematical objects that satisfy what is sometimes called a 0-1 law, in the following sense. Under some natural probability measure on the set of objects, the measure of the subset of objects having the given property is either 0 or 1. In the latter case we can say that almost all the objects have the property. Familiar examples of this phenomenon are the following: almost all real numbers are transcendental (or normal to every base), almost all integers are composite, almost all continuous real functions are nondifferentiable, etc. It is often the case that the objects under consideration can be partitioned into a countable number of finite classes Cn, with the probability assigned to an object in Cn being just 1/ICn I. In this case, we say that a property Pn satisfies a 0-1 law if the fraction of the number of objects in Cn that satisfy Pn either tends to 0 or tends to 1 as n -x 0o. For example, almost all graphs on n vertices have maximum cliques and maximum independent sets of size at most 2 log n, almost all Boolean functions with n variables have circuit complexity (1 + o( 1 ))2n and almost all binary codes of length n with at most 2nR codewords (with R less than the binary symmetric channel capacity C) have arbitrarily small error probability (a special case of Shannon's coding theorem; see [S48]). One of the first general results of this type was the theorem of Fagin [F76] and Glebskii et al. [GKLT69], which asserts that every property of graphs that can be expressed in first-order logic satisfies a 0-1 law (see [SS88] for recent striking developments in this topic). One obvious method for finding explicit objects having some property Pn shared by almost all objects in Cn is simply to select one at random. With overwhelming probability (tending to 1 as n -x oc), the selected object will have property Pn. Unfortunately, it may be (and often is) extremely difficult to prove that any particular object does indeed satisfy Pn . It is our purpose in this paper to describe a method that can to a certain extent circumvent this difficulty. We will show that, for a variety of families, it is possible to identify a natural hierarchy of equivalence classes of properties, all of which are shared by almost all objects in the family. Any object satisfying", "citation_count": "34", "reference_count": "126", "date": "1991", "authors": ["F. R. K. Chung", "R. L. Graham"], "related_topics": ["Probability measure", "Random element", "Countable set", "Property (philosophy)", "Real number", "Measure (mathematics)", "Base (topology)", "Set function", "Discrete mathematics", "Mathematics"]}
{"id": "2901284226", "references": ["1998400388", "164275096", "1552744309", "2905110430", "2064197162", "23397926", "1984080039", "2079810243", "2088164510", "184868352"], "title": "Quasi-random graphs", "abstract": "We introduce a large equivalence class of graph properties, all of which are shared by so-called random graphs. Unlike random graphs, however, it is often relatively easy to verify that a particular family of graphs possesses some property in this class.", "citation_count": "17", "reference_count": "539", "date": "1989", "authors": ["Fan R. K. Chung", "Ronald L. Graham", "Richard M. Wilson"], "related_topics": ["Random graph", "Equivalence class (music)", "Graph property", "Class (set theory)", "Property (philosophy)", "Combinatorics", "Mathematics", "Quasi random"]}
{"id": "2027808858", "references": ["2120062331", "2083120484", "2751862591", "1556192255", "1983403768", "2023642785", "2018344665", "2138412601", "2074599161", "2001759372"], "title": "Asymptotic analysis of a random walk on a hypercube with many dimensions", "abstract": "In nearest neighbor random walk on an n-dimensional cube a particle moves to one of its nearest neighbors (or stays fixed) with equal probability. the particle starts at 0. How long does it take to reach its stationary distribution? in fact, this occurs surprisingly rapidly. Previous analysis has shown that the total variation distance to stationarity is large if the number of steps N is   1/4n log n. This paper derives an explicit expression for the variation distance as n \u2192 \u221e in the transition region N \u02dc 1/4n log n. This permits the first careful evaluation of a cutoff phenomenon observed in a wide variety of Markov chains. the argument involves Fourier analysis to express the probability as a contour integral and saddle point approximation. the asymptotic results are in good agreement with numerical results for n as small as 100.", "citation_count": "10", "reference_count": "215", "date": "1990", "authors": ["Persi Diaconis", "Ronald L. Graham", "John A. Morrison"], "related_topics": ["Random walk", "Asymptotic analysis", "Stationary distribution", "k-nearest neighbors algorithm", "Total variation", "Saddle point", "Markov chain", "Hypercube", "Mathematical analysis", "Combinatorics", "Mathematics"]}
{"id": "2119605622", "references": ["1980911747", "2151103935", "2154422044", "2012778485", "2128017662", "2177274842", "1625255723", "2164598857", "1677409904", "2124386111"], "title": "Speeded-Up Robust Features (SURF)", "abstract": "This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.", "citation_count": "39", "reference_count": "12,286", "date": "2008", "authors": ["Herbert Bay", "Andreas Ess", "Tinne Tuytelaars", "Luc Van Gool"], "related_topics": ["GLOH", "Principal curvature-based region detector", "Hessian affine region detector", "Interest point detection", "Robustness (computer science)", "Image processing", "Image registration", "Camera resectioning", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2142194269", "references": ["2118020653", "2068576236", "2154683974", "2096691069", "2112020727", "2533739470", "2034328688", "2162915993", "2134135198", "2020163092"], "title": "Learning realistic human actions from movies", "abstract": "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.", "citation_count": "21", "reference_count": "4,181", "date": "2008", "authors": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "related_topics": ["Image retrieval", "Visual learning", "Support vector machine", "Contextual image classification", "Classifier (UML)", "Machine learning", "Visualization", "Scripting language", "Cognitive neuroscience of visual object recognition", "Computer science", "Histogram", "Artificial intelligence"]}
{"id": "2963574257", "references": ["2335728318", "2036054070", "1904365287", "2156163116", "2141200610", "2310919327", "2962820688", "3118608800", "2546302380", "1665214252"], "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks", "abstract": "Abstract: We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "citation_count": "10", "reference_count": "687", "date": "2013", "authors": ["Matthew D. Zeiler", "Rob Fergus"], "related_topics": ["Pooling", "Convolutional neural network", "Multinomial distribution", "Regularization (mathematics)", "Pattern recognition", "Computer science", "Effective method", "Artificial intelligence"]}
{"id": "2152839228", "references": ["2169875292", "1904365287", "2119821739", "2170382128", "1994197834", "2310919327", "2962820688", "2144982973", "2136189984", "2120480077"], "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning", "abstract": "Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.", "citation_count": "44", "reference_count": "1,231", "date": "2014", "authors": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "related_topics": ["Throughput (business)", "Computer engineering", "Parallel computing", "Key (cryptography)", "Set (abstract data type)", "Emphasis (telecommunications)", "Energy (signal processing)", "Computer science", "Range (mathematics)", "Simd processor"]}
{"id": "2162741153", "references": ["2618530766", "2130325614", "2161969291", "2110798204", "2310919327", "2031454541", "2100495367", "2546302380", "2168356304", "2100556411"], "title": "Pedestrian Detection with Unsupervised Multi-stage Feature Learning", "abstract": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.", "citation_count": "37", "reference_count": "929", "date": "2013", "authors": ["Pierre Sermanet", "Koray Kavukcuoglu", "Soumith Chintala", "Yann Lecun"], "related_topics": ["Deep learning", "Feature learning", "Pedestrian detection", "Object detection", "Neural coding", "Network model", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2607333215", "references": ["2102605133", "2022508996", "2145023731", "1903029394", "2110158442", "2037227137", "125693051", "2962835968", "2117539524", "1999478155"], "title": "Holistically-Nested Edge Detection", "abstract": "We develop a new edge detection algorithm that addresses two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSDS500 dataset (ODS F-score of 0.790) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 s per image) that is orders of magnitude faster than some CNN-based edge detection algorithms developed before HED. We also observe encouraging results on other boundary detection benchmark datasets such as Multicue and PASCAL-Context.", "citation_count": "57", "reference_count": "1,975", "date": "2017", "authors": ["Saining Xie", "Zhuowen Tu"], "related_topics": ["Edge detection", "Deep learning", "Feature learning", "Convolutional neural network", "Enhanced Data Rates for GSM Evolution", "Pattern recognition (psychology)", "Benchmark (computing)", "Computer vision", "Computer science", "Image (mathematics)", "Artificial intelligence"]}
{"id": "2308045930", "references": ["2102605133", "2618530766", "2022508996", "2131846894", "2108598243", "2161969291", "2310919327", "2062118960", "2168231600", "2963542991"], "title": "Large-scale Video Classi\ufb01cation with Convolutional Neural Networks", "abstract": "", "citation_count": "31", "reference_count": "1,947", "date": "2014", "authors": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "related_topics": ["Scale (ratio)", "Convolutional neural network", "Computer science", "Pattern recognition", "Artificial intelligence"]}
{"id": "2122825543", "references": ["2138550913", "2109363337", "2143426320", "1975900269", "2157795344", "2074682976", "2135046866", "1554944419", "2063978378", "2798909945"], "title": "Regularization and variable selection via the elastic net", "abstract": "Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the", "citation_count": "32", "reference_count": "14,066", "date": "2005", "authors": ["Hui Zou", "Trevor Hastie"], "related_topics": ["Elastic net regularization", "Lasso (statistics)", "Feature selection", "Regularization (mathematics)", "Least-angle regression", "Contrast (statistics)", "Representation (mathematics)", "Applied mathematics", "Statistics", "Mathematics", "Real world data"]}
{"id": "2087681821", "references": ["2047508432", "3097096317", "2102512156", "1796263212", "1963599662", "1554944419", "1990937109", "2076017598", "2038952578", "2136000821"], "title": "One Millisecond Face Alignment with an Ensemble of Regression Trees", "abstract": "This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.", "citation_count": "19", "reference_count": "2,309", "date": "2014", "authors": ["Vahid Kazemi", "Josephine Sullivan"], "related_topics": ["Gradient boosting", "Overfitting", "Boosting (machine learning)", "Feature selection", "Facial recognition system", "Regularization (mathematics)", "Regression analysis", "Prior probability", "Pattern recognition", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2117756735", "references": ["2432517183", "2145096794", "2610857016", "2033419168", "2296616510", "2202343345", "2147717514", "1554944419", "2124608575", "2798909945"], "title": "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions", "abstract": "Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed\u2014either explicitly or implicitly\u2014to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the $k$ dominant components of the singular value decomposition of an $m \\times n$ matrix. (i) For a dense input matrix, randomized algorithms require $\\bigO(mn \\log(k))$ floating-point operations (flops) in contrast to $ \\bigO(mnk)$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to $\\bigO(k)$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.", "citation_count": "125", "reference_count": "3,208", "date": "2011", "authors": ["N. Halko", "P. G. Martinsson", "J. A. Tropp"], "related_topics": ["Matrix decomposition", "Sparse matrix", "Block matrix", "Logical matrix", "LU decomposition", "Singular value decomposition", "QR decomposition", "State-transition matrix", "Algorithm", "Mathematics"]}
{"id": "2179438025", "references": ["2114104545", "2081098333", "2097360283", "2138207763", "2152239989", "2160697532", "1554944419", "2134526812", "2110065044", "2140729960"], "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2", "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at                   http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html\n                                  .", "citation_count": "61", "reference_count": "26,599", "date": "2014", "authors": ["Michael I Love", "Wolfgang Huber", "Simon Anders"], "related_topics": ["MRNA Sequencing", "Integrator complex", "Count data", "Fold change", "Replicate", "Outlier", "Fold (higher-order function)", "Interpretability", "Statistics", "Evolutionary biology", "Biology"]}
{"id": "1548802052", "references": ["2154053567", "2163922914", "2115706991", "2076063813", "2138204974", "2123649031", "2098535678", "2132549764", "2072128103"], "title": "Independent Component Analysis", "abstract": "In this chapter, we discuss a statistical generative model called independent component analysis. It is basically a proper probabilistic formulation of the ideas underpinning sparse coding. It shows how sparse coding can be interpreted as providing a Bayesian prior, and answers some questions which were not properly answered in the sparse coding framework.", "citation_count": "0", "reference_count": "9,213", "date": "2001", "authors": ["Aapo Hyvarinen", "Juha Karhunen", "Erkki Oja"], "related_topics": ["Neural coding", "Generative model", "Prior probability", "Probabilistic logic", "Independent component analysis", "Negentropy", "Theoretical computer science", "Infomax", "FastICA", "Pattern recognition", "Mathematics", "Artificial intelligence"]}
{"id": "2097308346", "references": ["2077776048", "1578099820", "2165874743", "2001141328", "2156718197", "2124776405", "2053186076", "2285257517", "2121947440", "2140095548"], "title": "Laplacian Eigenmaps for dimensionality reduction and data representation", "abstract": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.", "citation_count": "31", "reference_count": "8,200", "date": "2003", "authors": ["Mikhail Belkin", "Partha Niyogi"], "related_topics": ["Manifold alignment", "Diffusion map", "Spectral clustering", "Laplacian matrix", "Nonlinear dimensionality reduction", "Isomap", "Dimensionality reduction", "Laplace\u2013Beltrami operator", "Theoretical computer science", "Artificial intelligence", "Mathematics"]}
{"id": "2071949631", "references": ["2999729612", "2913066018", "2496585406", "1988837127", "1975152892", "2085487226", "1535743328", "1989076816", "2047555270", "1594031697"], "title": "Estimating the number of clusters in a data set via the gap statistic", "abstract": "We propose a method (the \u2018gap statistic\u2019) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.", "citation_count": "16", "reference_count": "5,161", "date": "2001", "authors": ["Robert Tibshirani", "Guenther Walther", "Trevor Hastie"], "related_topics": ["Ancillary statistic", "Statistic", "PRESS statistic", "Completeness (statistics)", "Null distribution", "Cluster analysis", "Mean squared error", "Linear discriminant analysis", "Algorithm", "Statistics", "Mathematics"]}
{"id": "2011832962", "references": ["2117853077", "2109970232", "2109363337", "2135187880", "2150926065", "2049633694", "2147246240", "2117812871", "2059334100", "2087684630"], "title": "Model-Based Clustering, Discriminant Analysis, and Density Estimation", "abstract": "Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent development...", "citation_count": "127", "reference_count": "4,480", "date": "2002", "authors": ["Chris Fraley", "Adrian E Raftery"], "related_topics": ["Cluster analysis", "Fuzzy clustering", "Clustering high-dimensional data", "Correlation clustering", "CURE data clustering algorithm", "Consensus clustering", "Linear discriminant analysis", "Density estimation", "Data mining", "Computer science"]}
{"id": "2964253222", "references": ["2963060032", "2963359529", "2963744840", "2964301649", "2963143631", "2970115835", "2962700793", "3106412272", "2962729158"], "title": "Towards Deep Learning Models Resistant to Adversarial Attacks.", "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples\u2014inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at\u00a0this https URL\u00a0and\u00a0this https URL.", "citation_count": "0", "reference_count": "3,172", "date": "2018", "authors": ["Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu"], "related_topics": ["Adversarial machine learning", "Concrete security", "Deep learning", "Adversarial system", "Adversary", "Artificial neural network", "Robustness (computer science)", "Computer security", "Robust optimization", "Computer science", "Artificial intelligence"]}
{"id": "3117548433", "references": ["2962879692", "2963399829", "2964137095", "2963446712", "2108598243", "2809090039", "3137695714", "2194775991", "3118608800", "2962835968"], "title": "Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization", "abstract": "The early phase of training has been shown to be important in two ways for deep neural networks. First, the degree of regularization in this phase significantly impacts the final generalization. Second, it is accompanied by a rapid change in the local loss curvature influenced by regularization choices. Connecting these two findings, we show that stochastic gradient descent (SGD) implicitly penalizes the trace of the Fisher Information Matrix (FIM) from the beginning of training. We argue it is an implicit regularizer in SGD by showing that explicitly penalizing the trace of the FIM can significantly improve generalization. We further show that the early value of the trace of the FIM correlates strongly with the final generalization. We highlight that in the absence of implicit or explicit regularization, the trace of the FIM can increase to a large value early in training, to which we refer as catastrophic Fisher explosion. Finally, to gain insight into the regularization effect of penalizing the trace of the FIM, we show that it limits memorization by reducing the learning speed of examples with noisy labels more than that of the clean examples, and 2) trajectories with a low initial trace of the FIM end in flat minima, which are commonly associated with good generalization.", "citation_count": "53", "reference_count": "1", "date": "2021", "authors": ["Stanislaw Kamil Jastrzebski", "Devansh Arpit", "Oliver \u00c5strand", "Giancarlo Kerg", "Huan Wang", "Caiming Xiong", "richard socher", "Kyunghyun Cho", "Krzysztof J. Geras"], "related_topics": ["Fisher information", "Stochastic gradient descent", "Regularization (mathematics)", "Hessian matrix", "Maxima and minima", "Matrix (mathematics)", "Applied mathematics", "Curvature", "Computer science", "Early phase"]}
{"id": "3061525482", "references": ["2618530766", "2217896605", "2302255633", "2108598243", "1815076433", "2194775991", "2095705004", "104184427", "2962835968", "1836465849"], "title": "Training Deep Neural Networks Without Batch Normalization.", "abstract": "Training neural networks is an optimization problem, and finding a decent set of parameters through gradient descent can be a difficult task. A host of techniques has been developed to aid this process before and during the training phase. One of the most important and widely used class of method is normalization. It is generally favorable for neurons to receive inputs that are distributed with zero mean and unit variance, so we use statistics about dataset to normalize them before the first layer. However, this property cannot be guaranteed for the intermediate activations inside the network. A widely used method to enforce this property inside the network is batch normalization. It was developed to combat covariate shift inside networks. Empirically it is known to work, but there is a lack of theoretical understanding about its effectiveness and potential drawbacks it might have when used in practice. This work studies batch normalization in detail, while comparing it with other methods such as weight normalization, gradient clipping and dropout. The main purpose of this work is to determine if it is possible to train networks effectively when batch normalization is removed through adaption of the training process.", "citation_count": "27", "reference_count": "1", "date": "2020", "authors": ["Divya Gaur", "Joachim Folz", "Andreas Dengel"], "related_topics": ["Normalization (statistics)", "Artificial neural network", "Gradient descent", "Optimization problem", "Data mining", "Computer science", "Deep neural networks"]}
{"id": "3019371514", "references": ["1533861849", "1614298861", "1529731474", "3003257820", "2310919327", "1677182931", "2919115771", "2153579005", "2217402295", "2964153729"], "title": "Revisiting Initialization of Neural Networks", "abstract": "The proper initialization of weights is crucial for the effective training and fast convergence of deep neural networks (DNNs). Prior work in this area has mostly focused on balancing the variance among weights per layer to maintain stability of (i) the input data propagated forwards through the network and (ii) the loss gradients propagated backwards, respectively. This prevalent heuristic is however agnostic of dependencies among gradients across the various layers and captures only firstorder effects. In this paper, we propose and discuss an initialization principle that is based on a rigorous estimation of the global curvature of weights across layers by approximating and controlling the norm of their Hessian matrix. The proposed approach is more systematic and recovers previous results for DNN activations such as smooth functions, dropouts, and ReLU. Our experiments on Word2Vec and the MNIST/CIFAR image classification tasks confirm that tracking the Hessian norm is a useful diagnostic tool which helps to more rigorously initialize weights", "citation_count": "21", "reference_count": "0", "date": "2020", "authors": ["Maciej Skorski"], "related_topics": ["Initialization", "Hessian matrix", "MNIST database", "Artificial neural network", "Stability (learning theory)", "Contextual image classification", "Algorithm", "Curvature", "Computer science"]}
{"id": "3100971012", "references": ["3152804149", "3094151208", "3119368353", "3130602232", "3128633047", "3165214204", "3135058862", "3132672614", "3120901154"], "title": "Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks", "abstract": "Batch normalization dramatically increases the largest trainable depth of residual networks, and this benefit has been crucial to the empirical success of deep residual networks on a wide range of benchmarks. We show that this key benefit arises because, at initialization, batch normalization downscales the residual branch relative to the skip connection, by a normalizing factor on the order of the square root of the network depth. This ensures that, early in training, the function computed by normalized residual blocks in deep networks is close to the identity function (on average). We use this insight to develop a simple initialization scheme that can train deep residual networks without normalization. We also provide a detailed empirical study of residual networks, which clarifies that, although batch normalized networks can be trained with larger learning rates, this effect is only beneficial in specific compute regimes, and has minimal benefits when the batch size is small.", "citation_count": "0", "reference_count": "23", "date": "2020", "authors": ["Soham De", "Samuel L. Smith"], "related_topics": ["Residual", "Normalization (statistics)", "Initialization", "Identity function", "Square root", "Algorithm", "Computer science", "Empirical research", "Residual Blocks"]}
{"id": "2604763608", "references": ["2271840356", "2963207607", "2155541015", "2963504252", "2963341924", "2601450892", "2964121744", "3091905774", "2753160622", "1836465849"], "title": "Model-agnostic meta-learning for fast adaptation of deep networks", "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.", "citation_count": "32", "reference_count": "3,402", "date": "2017", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "related_topics": ["Multi-task learning", "Meta learning (computer science)", "Semi-supervised learning", "Learning classifier system", "Online machine learning", "Stability (learning theory)", "Reinforcement learning", "Artificial neural network", "Gradient descent", "Contextual image classification", "Machine learning", "Artificial intelligence", "Computer science", "Generalization error", "Training set"]}
{"id": "3137695714", "references": ["2963096987", "2963399829", "2892035503", "2891612330", "2964013315", "2809090039", "2962949934", "2964212410", "2963735582"], "title": "Understanding deep learning requires rethinking generalization.", "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \r\nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \r\nWe interpret our experimental findings by comparison with traditional models.", "citation_count": "0", "reference_count": "2,609", "date": "2017", "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "related_topics": ["Deep learning", "Artificial neural network", "Generalization", "Data point", "Regularization (mathematics)", "Contextual image classification", "Sample (statistics)", "Machine learning", "Computer science", "Simple (abstract algebra)", "Artificial intelligence"]}
{"id": "2732026016", "references": ["2618530766", "2097117768", "2108598243", "1861492603", "2031489346", "2194775991", "2310919327", "1677182931", "2962835968", "2117539524"], "title": "Places: A 10 Million Image Database for Scene Recognition", "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.", "citation_count": "36", "reference_count": "1,538", "date": "2018", "authors": ["Bolei Zhou", "Agata Lapedriza", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"], "related_topics": ["Convolutional neural network", "Deep learning", "Context (language use)", "Object (computer science)", "Visualization", "Semantics", "Computer vision", "Computer science", "Resource (project management)", "Text mining", "Artificial intelligence", "Image database"]}
{"id": "2963466845", "references": ["2913279579", "3045460727", "2890139949", "2970854840", "3129973872", "3027914507", "3138288340"], "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison", "abstract": "Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.", "citation_count": "0", "reference_count": "502", "date": "2019", "authors": ["Jeremy Irvin", "Pranav Rajpurkar", "Michael Ko", "Yifan Yu", "Silviana Ciurea-Ilcus", "Chris Chute", "Henrik Marklund", "Behzad Haghgoo", "Robyn L. Ball", "Katie S. Shpanskaya", "Jayne Seekins", "David A. Mong", "Safwan S. Halabi", "Jesse K. Sandberg", "Ricky Jones", "David B. Larson", "Curtis P. Langlotz", "Bhavik N. Patel", "Matthew P. Lungren", "Andrew Y. Ng"], "related_topics": ["Chest radiograph", "Medical imaging", "Radiography", "Pattern recognition", "Set (abstract data type)", "Pleural effusion", "Computer science", "Edema", "Artificial intelligence"]}
{"id": "2963749936", "references": ["2618530766", "2163922914", "2097117768", "2194775991", "2095705004", "2964153729", "1849277567", "2962835968", "2117539524", "1836465849"], "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations", "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "citation_count": "42", "reference_count": "722", "date": "2017", "authors": ["David Bau", "Bolei Zhou", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"], "related_topics": ["Interpretability", "Discriminative model", "Artificial neural network", "Property (programming)", "Semantics", "Contextual image classification", "Dropout (neural networks)", "Representation (mathematics)", "Pattern recognition", "Artificial intelligence", "Computer science"]}
{"id": "3149085914", "references": ["2147139740", "1499666380", "2168559056", "3123169106", "3129973872", "40146600", "2110428344", "2115669451", "2130210496"], "title": "Learning to Teach", "abstract": "", "citation_count": "0", "reference_count": "1,488", "date": "2006", "authors": ["Bergeron Bette", "Wolfe Michael", "Enz Billie J"], "related_topics": ["Mathematics education", "Psychology", "Learning to teach"]}
{"id": "2560023338", "references": ["2155893237", "2618530766", "2097117768", "1861492603", "2109255472", "2031489346", "2194775991", "1903029394", "2962835968", "1836465849"], "title": "Pyramid Scene Parsing Network", "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.", "citation_count": "43", "reference_count": "4,614", "date": "2017", "authors": ["Hengshuang Zhao", "Jianping Shi", "Xiaojuan Qi", "Xiaogang Wang", "Jiaya Jia"], "related_topics": ["Parsing", "Pascal (programming language)", "Contextual image classification", "Feature extraction", "Machine learning", "Image segmentation", "Vocabulary", "Object detection", "Computer science", "Semantics", "Artificial intelligence"]}
{"id": "2976016473", "references": ["2962767366", "2963399829", "2964311892", "2187089797", "2963858333", "2116341502", "2048679005", "2606780347", "2963782635", "3136604105"], "title": "GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning", "abstract": "", "citation_count": "19", "reference_count": "29", "date": "2019", "authors": ["Vikas Verma", "Meng Qu", "Alex Lamb", "Yoshua Bengio", "Juho Kannala", "Jian Tang"], "related_topics": ["Semi-supervised learning", "Artificial intelligence", "Computer science", "Graph neural networks"]}
{"id": "2987875759", "references": ["2963374479", "2963399829", "2964137095", "2963542245", "2978426779", "2194775991", "2919115771", "2964253222", "2964153729", "2962729158"], "title": "Interpolated Adversarial Training: Achieving Robust Neural Networks Without Sacrificing Too Much Accuracy", "abstract": "Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%.", "citation_count": "23", "reference_count": "32", "date": "2019", "authors": ["Alex Lamb", "Vikas Verma", "Juho Kannala", "Yoshua Bengio"], "related_topics": ["Robustness (computer science)", "Adversarial system", "Deep learning", "Artificial neural network", "Machine learning", "Computer science", "Interpolation", "Adversary", "Standard error", "Artificial intelligence", "Training methods"]}
{"id": "3035687950", "references": ["2962879692", "2963684088", "2963373786", "2963073614", "1861492603", "1901129140", "2963881378", "1834627138", "2964121744", "2099471712"], "title": "A U-Net Based Discriminator for Generative Adversarial Networks", "abstract": "Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the proposed COCO-Animals dataset.", "citation_count": "48", "reference_count": "29", "date": "2020", "authors": ["Edgar Schonfeld", "Bernt Schiele", "Anna Khoreva"], "related_topics": ["Discriminator", "Image segmentation", "Real image", "Image quality", "Pattern recognition", "Focus (optics)", "Consistency (database systems)", "Artificial intelligence", "Decoding methods", "Computer science"]}
{"id": "2996501936", "references": ["2963399829", "2964137095", "1479807131", "2964159205", "2978426779", "2530816535", "2963956526", "2964121744", "2107008379", "3136604105"], "title": "ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring", "abstract": "We improve the recently-proposed ``MixMatch semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. - Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. - Augmentation anchoring} feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between 5 times and 16 times less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach 93.73% accuracy (compared to MixMatch's accuracy of 93.58% with 4000 examples) and a median accuracy of 84.92% with just four labels per class.", "citation_count": "26", "reference_count": "155", "date": "2020", "authors": ["David Berthelot", "Nicholas Carlini", "Ekin D. Cubuk", "Alex Kurakin", "Kihyuk Sohn", "Han Zhang", "Colin Raffel"], "related_topics": ["Semi-supervised learning", "Marginal distribution", "Matching (graph theory)", "Anchoring", "Pattern recognition", "Computer science", "Class (biology)", "Distribution (number theory)", "Artificial intelligence"]}
{"id": "2962369866", "references": ["2963373786", "2964015378", "2962739339", "2531409750", "2117130368", "2194775991", "2153579005", "3118608800", "2250539671", "2963341956"], "title": "Unsupervised Data Augmentation for Consistency Training.", "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at this https URL.", "citation_count": "62", "reference_count": "495", "date": "2019", "authors": ["Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V. Le"], "related_topics": ["Deep learning", "Transfer of learning", "Word error rate", "Machine learning", "Computer science", "Invariant (mathematics)", "Artificial intelligence", "Labeled data"]}
{"id": "3100859887", "references": ["3092787476", "3130198422", "3145385912", "3135367836", "3132004818", "3101821705", "3124635886"], "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "abstract": "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.", "citation_count": "0", "reference_count": "207", "date": "2020", "authors": ["Ting Chen", "Simon Kornblith", "Kevin Swersky", "Mohammad Norouzi", "Geoffrey E. Hinton"], "related_topics": ["Machine learning", "Class (biology)", "Contrast (statistics)", "Computer science", "Key (cryptography)", "Artificial intelligence", "Residual neural network", "Supervised training"]}
{"id": "3035160371", "references": ["2618530766", "2963207607", "2963373786", "2963446712", "2097117768", "2194775991", "2183341477", "2095705004", "2962835968", "2964153729"], "title": "Self-Training With Noisy Student Improves ImageNet Classification", "abstract": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.", "citation_count": "91", "reference_count": "426", "date": "2020", "authors": ["Qizhe Xie", "Minh-Thang Luong", "Eduard Hovy", "Quoc V. Le"], "related_topics": ["Pattern recognition", "Artificial intelligence", "Image resolution", "Entropy (information theory)", "Computer science", "Self training"]}
{"id": "3162734203", "references": ["2963012544", "2963399829", "2963751529", "2954996726", "2963216553", "2081580037", "2746314669", "1975244201", "2963341956", "2739046565"], "title": "A Survey of Data Augmentation Approaches for NLP.", "abstract": "Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP", "citation_count": "129", "reference_count": "0", "date": "2021", "authors": ["Steven Y. Feng", "Varun Gangal", "Jason Wei", "Sarath Chandar", "Soroush Vosoughi", "Teruko Mitamura", "Eduard H. Hovy"], "related_topics": ["Natural language", "Natural language processing", "Computer science", "Popularity", "Work (electrical)", "Artificial neural network", "Artificial intelligence", "Training set"]}
{"id": "3001197829", "references": ["2335728318", "2963399829", "2964137095", "2108598243", "1479807131", "2118858186", "2095705004", "3118608800", "2964121744", "104184427"], "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "abstract": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at this https URL.", "citation_count": "60", "reference_count": "255", "date": "2020", "authors": ["Kihyuk Sohn", "David Berthelot", "Chun-Liang Li", "Zizhao Zhang", "Nicholas Carlini", "Ekin D. Cubuk", "Alex Kurakin", "Han Zhang", "Colin Raffel"], "related_topics": ["Semi-supervised learning", "Machine learning", "Computer science", "Regularization (mathematics)", "Artificial intelligence"]}
{"id": "3125645205", "references": ["2335728318", "2963207607", "2964137095", "2156909104", "2302255633", "2194775991", "3118608800", "2970971581", "2963341956", "2964153729"], "title": "How Does Mixup Help With Robustness and Generalization", "abstract": "Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.", "citation_count": "45", "reference_count": "13", "date": "2021", "authors": ["Linjun Zhang", "Zhun Deng", "Kenji Kawaguchi", "Amirata Ghorbani", "James Zou"], "related_topics": ["Robustness (computer science)", "Overfitting", "Generalization", "Regularization (mathematics)", "Upper and lower bounds", "Algorithm", "Computer science", "Sign (mathematics)", "Regular polygon", "Simple (abstract algebra)"]}
{"id": "3108796939", "references": ["2962879692", "2963684088", "2962760235", "2310919327", "2183341477", "1959608418", "3118608800", "2100495367", "3003301247", "2099471712"], "title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation", "abstract": "We aim to build image generation models that generalize to new domains from few examples. To this end, we first investigate the generalization properties of classic image generators, and discover that autoencoders generalize extremely well to new domains, even when trained on highly constrained data. We leverage this insight to produce a robust, unsupervised few-shot image generation algorithm, and introduce a novel training procedure based on recovering an image from data augmentations. Our Augmentation-Interpolative AutoEncoders synthesize realistic images of novel objects from only a few reference images, and outperform both prior interpolative models and supervised few-shot image generators. Our procedure is simple and lightweight, generalizes broadly, and requires no category labels or other supervision during training.", "citation_count": "47", "reference_count": "0", "date": "2021", "authors": ["Davis Wertheimer", "Omid Poursaeed", "Bharath Hariharan"], "related_topics": ["Autoencoder", "Pattern recognition", "Leverage (statistics)", "Computer science", "Artificial intelligence", "Image generation"]}
{"id": "3118146262", "references": ["3127397172", "3102554603", "3094371524", "3121422529", "3119760211", "3109512660", "3157476704"], "title": "InstaHide: Instance-hiding Schemes for Private Distributed Learning", "abstract": "How can multiple distributed entities collaboratively train a shared deep net on their private data while preserving privacy? This paper introduces InstaHide, a simple encryption of training images, which can be plugged into existing distributed deep learning pipelines. The encryption is efficient and applying it during training has minor effect on test accuracy. InstaHide encrypts each training image with a \"one-time secret key\" which consists of mixing a number of randomly chosen images and applying a random pixel-wise mask. Other contributions of this paper include: (a) Using a large public dataset (e.g. ImageNet) for mixing during its encryption, which improves security. (b) Experimental results to show effectiveness in preserving privacy against known attacks with only minor effects on accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstrating that use of the pixel-wise mask is important for security, since Mixup alone is shown to be insecure to some some efficient attacks. (e) Release of a challenge dataset this https URL Our code is available at this https URL", "citation_count": "0", "reference_count": "20", "date": "2020", "authors": ["Yangsibo Huang", "Zhao Song", "Kai Li", "Sanjeev Arora"], "related_topics": ["Encryption", "Key (cryptography)", "Code (cryptography)", "Computational problem", "Deep learning", "Computer science", "Theoretical computer science", "Image (mathematics)", "SIMPLE (military communications protocol)", "Mixing (physics)", "Artificial intelligence"]}
{"id": "3130223764", "references": ["639708223", "2097117768", "2108598243", "2031489346", "2194775991", "2153579005", "2183341477", "3118608800", "2963341956", "1836465849"], "title": "i-Mix: A Strategy for Regularizing Contrastive Representation Learning", "abstract": "Contrastive representation learning has shown to be an effective way of learning representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective regularization strategy for improving contrastive representation learning in both vision and non-vision domains. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of self-supervised representations across domains, resulting in significant performance gains on downstream tasks. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code will be released.", "citation_count": "80", "reference_count": "3", "date": "2021", "authors": ["Kibok Lee", "Yian Zhu", "Kihyuk Sohn", "Chun-Liang Li", "Jinwoo Shin", "Honglak Lee"], "related_topics": ["Feature learning", "Domain knowledge", "Virtual class", "Classifier (linguistics)", "Machine learning", "Regularization (mathematics)", "Computer science", "Code (cryptography)", "Quality (business)", "Simple (abstract algebra)", "Artificial intelligence"]}
{"id": "3107669106", "references": ["2962879692", "2963373786", "2963226019", "2963981733", "2962897886", "2108598243", "2310919327", "1959608418", "1834627138", "2099471712"], "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models", "abstract": "Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at https://github.com/ningyu1991/InclusiveGAN.git.", "citation_count": "51", "reference_count": "10", "date": "2020", "authors": ["Ning Yu", "Ke Li", "Peng Zhou", "Jitendra Malik", "Larry S. Davis", "Mario Fritz"], "related_topics": ["Underrepresented Minority", "Machine learning", "Computer science", "Code (cryptography)", "Generative grammar", "Control (management)", "Artificial intelligence"]}
{"id": "3137863028", "references": ["3022061250", "2778733008", "2147093486", "3034978746", "2603705233", "3123215519", "3130223764", "3162983435", "3137863028", "3149173402"], "title": "$i$-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning", "abstract": "Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at this https URL.", "citation_count": "0", "reference_count": "1", "date": "2020", "authors": ["Kibok Lee", "Yian Zhu", "Kihyuk Sohn", "Chun-Liang Li", "Jinwoo Shin", "Honglak Lee"], "related_topics": ["Feature learning", "Domain knowledge", "Virtual class", "Classifier (UML)", "Natural language processing", "Computer science", "Regularization (linguistics)", "Image (mathematics)", "Code (cryptography)", "Quality (business)", "Artificial intelligence"]}
{"id": "3093382707", "references": ["1536680647", "2963403868", "1901129140", "2194775991", "2964121744", "1903029394", "2099471712", "2963341956", "2117539524", "1836465849"], "title": "Uniform Priors for Data-Efficient Transfer", "abstract": "Deep Neural Networks have shown great promise on a variety of downstream applications; but their ability to adapt and generalize to new data and tasks remains a challenging problem. However, the ability to perform few or zero-shot adaptation to novel tasks is important for the scalability and deployment of machine learning models. It is therefore crucial to understand what makes for good, transferable features in deep networks that best allow for such adaptation. In this paper, we shed light on this by showing that features that are most transferable have high uniformity in the embedding space and propose a uniformity regularization scheme that encourages better transfer and feature reuse. We evaluate the regularization on its ability to facilitate adaptation to unseen tasks and data, for which we conduct a thorough experimental study covering four relevant, and distinct domains: few-shot Meta-Learning, Deep Metric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution classification. Across all experiments, we show that uniformity regularization consistently offers benefits over baseline methods and is able to achieve state-of-the-art performance in Deep Metric Learning and Meta-Learning.", "citation_count": "73", "reference_count": "0", "date": "2021", "authors": ["Samarth Sinha", "Karsten Roth", "Anirudh Goyal", "Marzyeh Ghassemi", "Hugo Larochelle", "Animesh Garg"], "related_topics": ["Meta learning (computer science)", "Transfer of learning", "Metric (mathematics)", "Feature (machine learning)", "Adaptation (computer science)", "Machine learning", "Regularization (mathematics)", "Embedding", "Computer science", "Artificial intelligence"]}
{"id": "3019676419", "references": ["2753798143", "2028279608", "2108068107", "1975875968", "2119717200", "2188365844", "2087312216", "2043398720", "1988037271", "2529996553"], "title": "PaccMann RL : Designing Anticancer Drugs From Transcriptomic Data via Reinforcement Learning.", "abstract": "The pharmaceutical industry has experienced a significant productivity decline: Less than 0.01% of drug candidates obtain market approval, with an estimated 10\u201315 years until market release and costs that range between one [2] to three billion dollars per drug [3].", "citation_count": "34", "reference_count": "8", "date": "2020", "authors": ["Jannis Born", "Matteo Manica", "Ali Oskooei", "Joris Cadow", "Mar\u00eda Rodr\u00edguez Mart\u00ednez"], "related_topics": ["Productivity", "Pharmaceutical industry", "Reinforcement learning", "Biotechnology", "Business"]}
{"id": "2531409750", "references": ["2618530766", "2097117768", "2194775991", "2183341477", "2963911037", "2964350391", "1849277567", "2962835968", "2117539524", "1836465849"], "title": "Xception: Deep Learning with Depthwise Separable Convolutions", "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.", "citation_count": "18", "reference_count": "5,179", "date": "2017", "authors": ["Francois Chollet"], "related_topics": ["Convolution", "Convolutional neural network", "Deep learning", "Pointwise", "Convolutional code", "Algorithm", "Contextual image classification", "Separable space", "Artificial intelligence", "Interpretation (model theory)", "Mathematics"]}
{"id": "2963096987", "references": ["2963207607", "2552194003", "2103496339", "2161388792", "3137695714", "2963504252", "2597603852", "2137983211", "2963233958", "2964153729"], "title": "A closer look at memorization in deep networks", "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.", "citation_count": "24", "reference_count": "523", "date": "2017", "authors": ["Devansh Arpit", "Stanis\u0142aw Jastrz\u0119bski", "Nicolas Ballas", "David Krueger", "Emmanuel Bengio", "Maxinder S. Kanwal", "Tegan Maharaj", "Asja Fischer", "Aaron Courville", "Yoshua Bengio", "Simon Lacoste-Julien"], "related_topics": ["Deep learning", "Memorization", "Robustness (computer science)", "Machine learning", "Computer science", "Regularization (mathematics)", "Artificial intelligence", "Training set"]}
{"id": "2964292098", "references": ["2618530766", "2302255633", "2146502635", "2194775991", "2310919327", "1677182931", "2095705004", "3118608800", "2064675550", "1836465849"], "title": "Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach", "abstract": "We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures &amp;#x2014; stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers &amp;#x2014; demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.", "citation_count": "41", "reference_count": "494", "date": "2017", "authors": ["Giorgio Patrini", "Alessandro Rozza", "Aditya Krishna Menon", "Richard Nock", "Lizhen Qu"], "related_topics": ["Noise measurement", "MNIST database", "Artificial neural network", "Robustness (computer science)", "Word embedding", "Normalization (statistics)", "Pooling", "Network architecture", "Pattern recognition", "Residual", "Artificial intelligence", "Computer science"]}
{"id": "2073459066", "references": ["2110105238", "2150593711", "1998739300", "2029698681", "2033387072", "2045964207", "2004791924", "1501500081", "2123297508", "2199495299"], "title": "k-means++: the advantages of careful seeding", "abstract": "The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is \u0398(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.", "citation_count": "21", "reference_count": "7,843", "date": "2007", "authors": ["David Arthur", "Sergei Vassilvitskii"], "related_topics": ["Cluster analysis", "k-means clustering", "Seeding", "Algorithm", "Simplicity", "Mathematics"]}
{"id": "1597379537", "references": ["2156909104", "2069317438", "2117400858", "1530699444", "2158195707", "2019363670", "1994851566", "3017143921", "2134237567", "2101210369"], "title": "Learning to resolve natural language ambiguities: a unified approach", "abstract": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.", "citation_count": "35", "reference_count": "287", "date": "1998", "authors": ["Dan Roth"], "related_topics": ["Multi-task learning", "Winnow", "Semi-supervised learning", "Feature vector", "Natural language", "Ambiguity resolution", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "1528056001", "references": ["1593793857", "1997063559", "1652505363", "2099111195", "1528905581", "2159080219", "2049633694", "2084812512", "2076118331", "2025653905"], "title": "Factorial Hidden Markov Models", "abstract": "Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable\u2014the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward\u2013backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach\u2018s chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.", "citation_count": "38", "reference_count": "1,534", "date": "1995", "authors": ["Zoubin Ghahramani", "Michael I. Jordan"], "related_topics": ["Hidden semi-Markov model", "Forward\u2013backward algorithm", "Hidden Markov model", "Gibbs sampling", "Approximate inference", "Graphical model", "Exact algorithm", "Bayesian network", "Algorithm", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1592796124", "references": ["1934019294", "2156515921", "2156577800", "165283731", "2120708938", "2111305191", "2139193890", "202303397"], "title": "A Gaussian Prior for Smoothing Maximum Entropy Models", "abstract": "Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.", "citation_count": "0", "reference_count": "436", "date": "1999", "authors": ["Stanley F. Chen", "Ronald Rosenfeld"], "related_topics": ["Smoothing", "Maximum entropy probability distribution", "Additive smoothing", "Maximum entropy spectral estimation", "Expectation\u2013maximization algorithm", "Principle of maximum entropy", "Maximum-entropy Markov model", "Maximum likelihood sequence estimation", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "1557074680", "references": ["1979459060", "2045656233", "1828401780", "2160842254", "1996903695", "2797583072", "2767905780", "2096175520", "2134237567", "1594031697"], "title": "Statistical Models for Text Segmentation", "abstract": "This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.", "citation_count": "25", "reference_count": "882", "date": "1999", "authors": ["Doug Beeferman", "Adam Berger", "John Lafferty"], "related_topics": ["Text segmentation", "Feature (machine learning)", "Metric (mathematics)", "Language model", "Precision and recall", "Statistical model", "Decision tree", "Principle of maximum entropy", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "2158873310", "references": ["2139416101", "1493490255", "1934019294", "3099668342", "2102667697", "3088592174", "202303397", "2146089916"], "title": "Maximum entropy models for natural language ambiguity resolution", "abstract": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy.\r\nWe discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages:\r\nState-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources.\r\nKnowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or \"knowledge-poor\", but yet succeed in approximating complex linguistic relationships.\r\nReusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis.\r\nThe experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models.", "citation_count": "0", "reference_count": "747", "date": "1998", "authors": ["Adwait Ratnaparkhi", "Mitchell P. Marcus"], "related_topics": ["Principle of maximum entropy", "Maximum-entropy Markov model", "Natural language", "Statistical model", "Sentence", "Ambiguity resolution", "Range (mathematics)", "Natural language processing", "Computer science", "Task (computing)", "Artificial intelligence"]}
{"id": "2081687495", "references": ["2076526090", "1571096757", "2334801970", "2017580301", "1525757074", "2099247782", "2055528812", "1483126227", "2046224275", "2121407024"], "title": "A Simple Rule-Based Part of Speech Tagger", "abstract": "Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.", "citation_count": "12", "reference_count": "2,223", "date": "1992", "authors": ["Eric Brill"], "related_topics": ["Brill tagger", "Trigram tagger", "Rule-based system", "Natural language", "Part of speech", "Text processing", "Natural language processing", "Set (abstract data type)", "Software portability", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2149706766", "references": ["1527883571", "2894813436", "2026319679", "3021257214", "1488252886", "18968369", "1483128843", "2067642555", "2159047538", "1693339475"], "title": "Induction of Decision Trees", "abstract": "The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.", "citation_count": "24", "reference_count": "25,324", "date": "1986", "authors": ["J. R. Quinlan"], "related_topics": ["Intelligent decision support system", "Influence diagram", "Decision engineering", "Decision tree", "ID3 algorithm", "Incremental decision tree", "Decision tree learning", "Decision stump", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2170381724", "references": ["1632114991", "2081687495", "1718065290", "2100796029", "1571096757", "2102924265", "2166394306", "2099247782", "2046224275", "1554031433"], "title": "Some advances in transformation-based part of speech tagging", "abstract": "Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger-can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty.", "citation_count": "14", "reference_count": "505", "date": "1994", "authors": ["Eric Brill"], "related_topics": ["Rule-based machine translation", "Part of speech", "Natural language processing", "Speech recognition", "Computer science", "Transformation (function)", "Simple (abstract algebra)", "Artificial intelligence", "Part-of-speech tagging"]}
{"id": "2046224275", "references": ["1991133427", "1491178396", "2752853835", "2105594594", "2100796029", "1571096757", "2099247782", "2113547509", "2326587081", "2171850596"], "title": "A Practical Part-of-Speech Tagger", "abstract": "We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.", "citation_count": "22", "reference_count": "919", "date": "1992", "authors": ["Doug Cutting", "Julian Kupiec", "Jan Pedersen", "Penelope Sibun"], "related_topics": ["Trigram tagger", "Hidden Markov model", "Phrase", "Part of speech", "Lexicon", "Natural language processing", "Speech recognition", "Computer science", "Resource (project management)", "Function (engineering)", "Artificial intelligence", "Word-sense disambiguation"]}
{"id": "2167837909", "references": ["2432517183", "1997063559", "2796837256", "2797638056", "2056760934", "2581275558", "2981264952", "2114220616", "2019635781", "2059998776"], "title": "Noncausal Gauss Markov random fields: parameter structure and estimation", "abstract": "The parameter structure of noncausal homogeneous Gauss Markov random fields (GMRF) defined on finite lattices is studied. For first-order (nearest neighbor) and a special class of second-order fields, a complete characterization of the parameter space and a fast implementation of the maximum likelihood estimator of the field parameters are provided. For general higher order fields, tight bounds for the parameter space are presented and an efficient procedure for ML estimation is described. Experimental results illustrate the application of the approach presented and the viability of the present method in fitting noncausal models to 2-D data. &gt;", "citation_count": "27", "reference_count": "116", "date": "1993", "authors": ["N. Balram", "J.M.F. Moura"], "related_topics": ["Random field", "Estimation theory", "Markov model", "Parameter space", "Markov process", "Stochastic process", "Field (mathematics)", "k-nearest neighbors algorithm", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "107938046", "references": ["1997063559", "2083875149", "1528905581", "1565709818", "2004014822", "2615953416", "2138309709", "2037139490", "2169714917", "1666636243"], "title": "Constrained Monte Carlo Maximum Likelihood for Dependent Data", "abstract": "Maximum likelihood estimates (MLEs) in autologistic models and other exponential family models for dependent data can be calculated with Markov chain Monte Carlo methods (the Metropolis algorithm or the Gibbs sampler), which simulate ergodic Markov chains having equilibrium distributions in the model. From one realization of such a Markov chain, a Monte Carlo approximant to the whole likelihood function can be constructed. The parameter value (if any) maximizing this function approximates the MLE", "citation_count": "67", "reference_count": "1,118", "date": "1992", "authors": ["Charles J. Geyer", "Elizabeth A. Thompson"], "related_topics": ["Markov chain Monte Carlo", "Hybrid Monte Carlo", "Markov chain", "Monte Carlo method", "Monte Carlo molecular modeling", "Dynamic Monte Carlo method", "Monte Carlo method in statistical physics", "Monte Carlo integration", "Applied mathematics", "Mathematics"]}
{"id": "2035301451", "references": ["2129139611", "1553969202", "2097333193", "2169528353"], "title": "A statistical approach to sense disambiguation in machine translation", "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the word's translations.", "citation_count": "4", "reference_count": "110", "date": "1991", "authors": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "related_topics": ["SemEval", "Machine translation", "Context (language use)", "Word (computer architecture)", "Natural language processing", "Mutual information", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2089969354", "references": ["1997063559", "1970820012", "2125838338", "2049633694", "2157629899", "1535702289", "2053887662", "1507849272", "2169714917", "1989222601"], "title": "A Variational Method for Estimating the Parameters of MRF from Complete or Incomplete Data", "abstract": "We introduce a new method (to be referred to as the variational method, VM) for estimating the parameters of Gibbs distributions with random variables (\"spins\") taking values in a Euclidean space Rn, n &gt; 1, from complete or degraded data. The method applies also to the case of iid random variables governed by exponential families, and appears to be new even in this case. For complete data, the VM is computationally more efficient than, and as reliable as, the maximum pseudo-likelihood method. For incomplete data, the VM leads to an estimation procedure reminiscent of, but simpler than, the EM algorithm. In the former case, we show that under natural assumptions a certain form of the variational estimators is strongly consistent and asymptotically normal. We also present numerical experiments that demonstrate the computational efficiency and accuracy of the variational estimators.", "citation_count": "28", "reference_count": "28", "date": "1993", "authors": ["Murilo P. Almeida", "Basilis Gidas"], "related_topics": ["Variational method", "Estimator", "Expectation\u2013maximization algorithm", "Exponential family", "Asymptotic distribution", "Random variable", "Euclidean space", "Applied mathematics", "Consistency (statistics)", "Mathematical optimization", "Mathematics"]}
{"id": "2168029744", "references": ["1746819321", "2147880316", "2156909104", "2119821739", "2139212933", "2148603752", "2135046866", "1548802052", "2078204800", "3023786531"], "title": "Kernel methods in machine learning", "abstract": "We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.\n\n        \nWe cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.", "citation_count": "152", "reference_count": "1,817", "date": "2008", "authors": ["Thomas Hofmann", "Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "related_topics": ["Reproducing kernel Hilbert space", "Kernel embedding of distributions", "Kernel (statistics)", "Kernel method", "Radial basis function kernel", "Tree kernel", "String kernel", "Kernel principal component analysis", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2168020168", "references": ["2024046085", "2112076978", "2156909104", "2125055259", "2119821739", "1678356000", "1975846642", "2084812512", "2087347434", "3124955340"], "title": "The Boosting Approach to Machine Learning An Overview", "abstract": "Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost\u2019s training error and generalization error; boosting\u2019s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting.", "citation_count": "79", "reference_count": "2,376", "date": "2003", "authors": ["Robert E. Schapire"], "related_topics": ["BrownBoost", "Boosting (machine learning)", "Gradient boosting", "LPBoost", "LogitBoost", "AdaBoost", "Multiclass classification", "Machine learning", "Linear programming", "Computer science", "Artificial intelligence"]}
{"id": "2107890099", "references": ["3121531027", "2030144199", "2110325612", "1564947197", "1956559956", "1975846642", "2032210760", "1833785989", "2124591829", "3124955340"], "title": "An efficient boosting algorithm for combining preferences", "abstract": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations.", "citation_count": "25", "reference_count": "2,830", "date": "2003", "authors": ["Yoav Freund", "Raj Iyer", "Robert E. Schapire", "Yoram Singer"], "related_topics": ["Ranking SVM", "Learning to rank", "Query expansion", "Boosting (machine learning)", "Machine learning", "Test data", "Computer science", "Search engine", "Artificial intelligence", "Efficient algorithm", "Training set"]}
{"id": "2105842272", "references": ["2147880316", "2008652694", "1574901103", "2047221353", "2105644991", "2157791002", "2890040444", "2148603752", "2053463056", "2429914308"], "title": "Large Margin Methods for Structured and Interdependent Output Variables", "abstract": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.", "citation_count": "30", "reference_count": "2,595", "date": "2005", "authors": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "related_topics": ["Structured prediction", "Optimization problem", "Margin (machine learning)", "Time complexity", "Computational intelligence", "Quadratic programming", "Statistical classification", "Optical character recognition", "Theoretical computer science", "Mathematical optimization", "Computer science"]}
{"id": "1718065290", "references": ["2122214306", "1859173823", "2140482227", "1571096757", "2798425628", "2070611481", "2099247782", "2134237567", "2127836646", "2077574412"], "title": "Coping with ambiguity and unknown words through probabilistic models", "abstract": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning caseframe informationfor verbsfrom example uses.From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical informationfrom a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text.", "citation_count": "18", "reference_count": "384", "date": "1993", "authors": ["Ralph Weischedel", "Richard Schwartz", "Jeff Palmucci", "Marie Meteer", "Lance Ramshaw"], "related_topics": ["Ambiguity", "Probabilistic logic", "Natural language", "Part of speech", "Natural language processing", "Utterance", "Computer science", "Coping (psychology)", "Artificial intelligence"]}
{"id": "2015042937", "references": ["1859173823", "2049901611", "2167434254", "2032558547", "1924403233", "2121227244", "1594031697", "1542847127", "2001792610"], "title": "A maximum entropy model for prepositional phrase attachment", "abstract": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence.", "citation_count": "12", "reference_count": "293", "date": "1994", "authors": ["Adwait Ratnaparkhi", "Jeff Reynar", "Salim Roukos"], "related_topics": ["Balanced sentence", "Inverted sentence", "Sentence", "Atomic sentence", "Parsing", "Context (language use)", "Natural language", "Principle of maximum entropy", "Natural language processing", "Linguistics", "Computer science", "Artificial intelligence"]}
{"id": "2112861996", "references": ["2341171179", "1966812932", "1571096757", "2012837062", "1976241232", "2099247782", "1597533204", "2134237567", "2046224275", "1990005915"], "title": "Tagging English text with a probabilistic model", "abstract": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available.", "citation_count": "26", "reference_count": "791", "date": "1994", "authors": ["Bernard Merialdo"], "related_topics": ["Markov model", "Hidden Markov model", "Statistical model", "Context (language use)", "Part of speech", "Sentence", "Word (computer architecture)", "Natural language processing", "Computer science", "Frequency", "Artificial intelligence"]}
{"id": "2162315106", "references": ["2142735254", "2145166062", "2067297267", "2003485756", "2107974372", "2009570821", "2151141101", "2020302957", "2113601822", "2132598047"], "title": "Introducing DOTUR, a Computer Program for Defining Operational Taxonomic Units and Estimating Species Richness", "abstract": "Although copious qualitative information describes the members of the diverse microbial communities on Earth, statistical approaches for quantifying and comparing the numbers and compositions of lineages in communities are lacking. We present a method that addresses the challenge of assigning sequences to operational taxonomic units (OTUs) based on the genetic distances between sequences. We developed a computer program, DOTUR, which assigns sequences to OTUs by using either the furthest, average, or nearest neighbor algorithm for each distance level. DOTUR uses the frequency at which each OTU is observed to construct rarefaction and collector's curves for various measures of richness and diversity. We analyzed 16S rRNA gene libraries derived from Scottish and Amazonian soils and the Sargasso Sea with DOTUR, which assigned sequences to OTUs rapidly and reliably based on the genetic distances between sequences and identified previous inconsistencies and errors in assigning sequences to OTUs. An analysis of the two 16S rRNA gene libraries from soil demonstrated that they do not contain enough sequences to support a claim that they contain different numbers of bacterial lineages with statistical confidence (P &gt; 0.05), nor do they contain enough sequences to provide a robust estimate of species richness when an OTU is defined as containing sequences that are no more than 3% different from each other. In contrast, the richness of OTUs at the 3% level in the Sargasso Sea collection began to plateau after the sampling of 690 sequences. We anticipate that an equivalent extent of sampling for soil would require sampling more than 10,000 sequences, almost 100 times the size of typical sequence collections obtained from soil.", "citation_count": "31", "reference_count": "2,767", "date": "2005", "authors": ["Patrick D. Schloss", "Jo Handelsman"], "related_topics": ["Species richness", "Genetic distance", "Ribosomal DNA", "Taxonomy (biology)", "Evolutionary biology", "Ecology", "Biology", "Computer program", "RNA RIBOSOMAL 16S", "Sargasso sea", "Statistical Confidence"]}
{"id": "2168133698", "references": ["2124985265", "2103441770", "2097341408", "2171777347", "2119180969", "2108234281", "2885727518", "2112113834", "2012016911", "1663973292"], "title": "A framework for variation discovery and genotyping using next-generation DNA sequencing data", "abstract": "Recent advances in sequencing technology make it possible to comprehensively catalogue genetic variation in population samples, creating a foundation for understanding human disease, ancestry and evolution. The amounts of raw data produced are prodigious and many computational steps are required to translate this output into high-quality variant calls. We present a unified analytic framework to discover and genotype variation among multiple samples simultaneously that achieves sensitive and specific results across five sequencing technologies and three distinct, canonical experimental designs. Our process includes (1) initial read mapping; (2) local realignment around indels; (3) base quality score recalibration; (4) SNP discovery and genotyping to find all potential variants; and (5) machine learning to separate true segregating variation from machine artifacts common to next-generation sequencing technologies. We discuss the application of these tools, instantiated in the Genome Analysis Toolkit (GATK), to deep whole-genome, whole-exome capture, and multi-sample low-pass (~4\u00d7) 1000 Genomes Project datasets.", "citation_count": "42", "reference_count": "8,786", "date": "2011", "authors": ["Mark A DePristo", "Eric Banks", "Ryan Poplin", "Kiran V Garimella", "Jared R Maguire", "Christopher Hartl", "Anthony A Philippakis", "Guillermo del Angel", "Manuel A Rivas", "Matt Hanna", "Aaron McKenna", "Tim J Fennell", "Andrew M Kernytsky", "Andrey Y Sivachenko", "Kristian Cibulskis", "Stacey B Gabriel", "David Altshuler", "Mark J Daly"], "related_topics": ["DNA sequencing theory", "Variant Call Format", "1000 Genomes Project", "Population", "DNA sequencing", "Genotyping", "Human genome", "Genome", "Computational biology", "Genetics", "Biology"]}
{"id": "2153233077", "references": ["2168909179", "2139212933", "2124776405", "2158714788", "2053186076", "1554663460", "2148603752", "1497256448", "2912565176", "2055043387"], "title": "Survey of clustering algorithms", "abstract": "Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.", "citation_count": "287", "reference_count": "6,785", "date": "2005", "authors": ["Rui Xu", "D. Wunsch"], "related_topics": ["Cluster analysis", "Field (computer science)", "Data science", "Machine learning", "Travelling salesman problem", "Variety (cybernetics)", "Artificial neural network", "Computer science", "Adaptive resonance theory", "Data set", "Artificial intelligence"]}
{"id": "2138122982", "references": ["2141885858", "2125838338", "2087064593", "2158714788", "2116423958", "2102122585", "2009570821", "2612240612", "2055043387", "2142678478"], "title": "Accelerated Profile HMM Searches", "abstract": "Profile hidden Markov models (profile HMMs) and probabilistic inference methods have made important contributions to the theory of sequence database homology search. However, practical use of profile HMM methods has been hindered by the computational expense of existing software implementations. Here I describe an acceleration heuristic for profile HMMs, the \u201cmultiple segment Viterbi\u201d (MSV) algorithm. The MSV algorithm computes an optimal sum of multiple ungapped local alignment segments using a striped vector-parallel approach previously described for fast Smith/Waterman alignment. MSV scores follow the same statistical distribution as gapped optimal local alignment scores, allowing rapid evaluation of significance of an MSV score and thus facilitating its use as a heuristic filter. I also describe a 20-fold acceleration of the standard profile HMM Forward/Backward algorithms using a method I call \u201csparse rescaling\u201d. These methods are assembled in a pipeline in which high-scoring MSV hits are passed on for reanalysis with the full HMM Forward/Backward algorithm. This accelerated pipeline is implemented in the freely available HMMER3 software package. Performance benchmarks show that the use of the heuristic MSV filter sacrifices negligible sensitivity compared to unaccelerated profile HMM searches. HMMER3 is substantially more sensitive and 100- to 1000-fold faster than HMMER2. HMMER3 is now about as fast as BLAST for protein searches.", "citation_count": "43", "reference_count": "3,324", "date": "2015", "authors": ["Sean R. Eddy"], "related_topics": ["Hidden Markov model", "Viterbi algorithm", "Smith\u2013Waterman algorithm", "Markov chain", "Algorithm", "Sequence database", "Software", "Data mining", "Computer science", "Probabilistic inference", "Software implementation"]}
{"id": "2145191876", "references": ["2040191060", "2050869737", "2120456037", "2151464048", "2136145671", "2083381199", "2009570821", "2103017472", "2085442947", "2168783124"], "title": "Evolutionarily conserved elements in vertebrate, insect, worm, and yeast genomes", "abstract": "We have conducted a comprehensive search for conserved elements in vertebrate genomes, using genome-wide multiple alignments of five vertebrate species (human, mouse, rat, chicken, and Fugu rubripes). Parallel searches have been performed with multiple alignments of four insect species (three species of Drosophila and Anopheles gambiae), two species of Caenorhabditis, and seven species of Saccharomyces. Conserved elements were identified with a computer program called phastCons, which is based on a two-state phylogenetic hidden Markov model (phylo-HMM). PhastCons works by fitting a phylo-HMM to the data by maximum likelihood, subject to constraints designed to calibrate the model across species groups, and then predicting conserved elements based on this model. The predicted elements cover roughly 3%-8% of the human genome (depending on the details of the calibration procedure) and substantially higher fractions of the more compact Drosophila melanogaster (37%-53%), Caenorhabditis elegans (18%-37%), and Saccharaomyces cerevisiae (47%-68%) genomes. From yeasts to vertebrates, in order of increasing genome size and general biological complexity, increasing fractions of conserved bases are found to lie outside of the exons of known protein-coding genes. In all groups, the most highly conserved elements (HCEs), by log-odds score, are hundreds or thousands of bases long. These elements share certain properties with ultraconserved elements, but they tend to be longer and less perfectly conserved, and they overlap genes of somewhat different functional categories. In vertebrates, HCEs are associated with the 3' UTRs of regulatory genes, stable gene deserts, and megabase-sized regions rich in moderately conserved noncoding sequences. Noncoding HCEs also show strong statistical evidence of an enrichment for RNA secondary structure.", "citation_count": "94", "reference_count": "3,783", "date": "2005", "authors": ["Adam Siepel", "Gill Bejerano", "Jakob Skou Pedersen", "Angie S Hinrichs", "Minmei Hou", "Kate Rosenbloom", "Hiram Clawson", "John Spieth", "Ladeana W Hillier", "Stephen Richards", "George M Weinstock", "Richard K Wilson", "Richard A Gibbs", "W James Kent", "Webb Miller", "David Haussler"], "related_topics": ["Conserved non-coding sequence", "Conserved sequence", "Genome", "Human genome", "Caenorhabditis", "Human accelerated regions", "Phylogenetic tree", "Genome size", "Evolutionary biology", "Genetics", "Biology"]}
{"id": "2045843097", "references": ["2055043387", "2119966574", "2149429041", "2158381555", "2009570821", "2042881722", "2097654784", "2151831732", "2015292449", "2113601822"], "title": "RNAmmer: consistent and rapid annotation of ribosomal RNA genes", "abstract": "The publication of a complete genome sequence is usually accompanied by annotations of its genes. In contrast to protein coding genes, genes for ribosomal RNA (rRNA) are often poorly or inconsistently annotated. This makes comparative studies based on rRNA genes difficult. We have therefore created computational predictors for the major rRNA species from all kingdoms of life and compiled them into a program called RNAmmer. The program uses hidden Markov models trained on data from the 5S ribosomal RNA database and the European ribosomal RNA database project. A pre-screening step makes the method fast with little loss of sensitivity, enabling the analysis of a complete bacterial genome in less than a minute. Results from running RNAmmer on a large set of genomes indicate that the location of rRNAs can be predicted with a very high level of accuracy. Novel, unannotated rRNAs are also predicted in many genomes. The software as well as the genome analysis results are available at the CBS web server.", "citation_count": "27", "reference_count": "4,452", "date": "2007", "authors": ["Karin Lagesen", "Peter Fischer Hallin", "Einar Andreas R\u00f8dland", "Hans Henrik St\u00e6rfeldt", "Torbjorn Rognes", "David Ussery"], "related_topics": ["Ribosomal RNA", "5.8S ribosomal RNA", "5S ribosomal RNA", "18S ribosomal RNA", "28S ribosomal RNA", "Minimal genome", "Gene prediction", "Intron", "Computational biology", "Genetics", "Biology"]}
{"id": "2132511032", "references": ["3097096317", "2116013899", "2151103935", "2165949425", "2085261163", "2121947440", "2121927366", "2798766386", "2130103520", "3124955340"], "title": "SD-VBS: The San Diego Vision Benchmark Suite", "abstract": "In the era of multi-core, computer vision has emerged as an exciting application area which promises to continue to drive the demand for both more powerful and more energy efficient processors. Although there is still a long way to go, vision has matured significantly over the last few decades, and the list of applications that are useful to end users continues to grow. The parallelism inherent in vision applications makes them a promising workload for multi-core and many-core processors.", "citation_count": "27", "reference_count": "220", "date": "2009", "authors": ["Sravanthi Kota Venkata", "Ikkjin Ahn", "Donghwan Jeon", "Anshuman Gupta", "Christopher Louie", "Saturnino Garcia", "Serge Belongie", "Michael Bedford Taylor"], "related_topics": ["End user", "Suite", "Benchmark (computing)", "Multi-core processor", "Efficient energy use", "Workload", "Computer architecture", "Computer science"]}
{"id": "2115781554", "references": ["2151447307", "1580948147", "2105280352", "100327610", "2070493638", "2911964244", "1601081659", "2119652329", "2912934387", "3124955340"], "title": "Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design", "abstract": "Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods (Hannah and Dunson, 2011; Magnani and Boyd, 2009) are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. We empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design.", "citation_count": "20", "reference_count": "23", "date": "2012", "authors": ["Lauren Hannah", "David B. Dunson"], "related_topics": ["Convex optimization", "Linear matrix inequality", "Nonlinear programming", "Conic optimization", "Proper convex function", "Geometric programming", "Ensemble learning", "Piecewise linear function", "Mathematical optimization", "Mathematics"]}
{"id": "2109826612", "references": ["1480376833", "2130979840", "2112076978", "2119821739", "2911964244", "2148143831", "1554944419", "2118978333", "2115709314", "3124955340"], "title": "Class prediction for high-dimensional class-imbalanced data", "abstract": "The goal of class prediction studies is to develop rules to accurately predict the class membership of new samples. The rules are derived using the values of the variables available for each subject: the main characteristic of high-dimensional data is that the number of variables greatly exceeds the number of samples. Frequently the classifiers are developed using class-imbalanced data, i.e., data sets where the number of samples in each class is not equal. Standard classification methods used on class-imbalanced data often produce classifiers that do not accurately predict the minority class; the prediction is biased towards the majority class. In this paper we investigate if the high-dimensionality poses additional challenges when dealing with class-imbalanced prediction. We evaluate the performance of six types of classifiers on class-imbalanced data, using simulated data and a publicly available data set from a breast cancer gene-expression microarray study. We also investigate the effectiveness of some strategies that are available to overcome the effect of class imbalance. Our results show that the evaluated classifiers are highly sensitive to class imbalance and that variable selection introduces an additional bias towards classification into the majority class. Most new samples are assigned to the majority class from the training set, unless the difference between the classes is very large. As a consequence, the class-specific predictive accuracies differ considerably. When the class imbalance is not too severe, down-sizing and asymmetric bagging embedding variable selection work well, while over-sampling does not. Variable normalization can further worsen the performance of the classifiers. Our results show that matching the prevalence of the classes in training and test set does not guarantee good performance of classifiers and that the problems related to classification with class-imbalanced data are exacerbated when dealing with high-dimensional data. Researchers using class-imbalanced data should be careful in assessing the predictive accuracy of the classifiers and, unless the class imbalance is mild, they should always use an appropriate method for dealing with the class imbalance problem.", "citation_count": "45", "reference_count": "208", "date": "2010", "authors": ["Rok Blagus", "Lara Lusa"], "related_topics": ["Test set", "Feature selection", "Data set", "Normalization (statistics)", "Machine learning", "Data mining", "Computer science", "Embedding", "Artificial intelligence", "Class prediction", "High dimensional", "Imbalanced data", "Training set"]}
{"id": "2122005484", "references": ["2127347346", "2170102584", "1506806321", "2022710553", "2100045669", "2041454412", "3124955340", "1996573126", "2131147042", "2168463792"], "title": "Piggyback CrowdSensing (PCS): energy efficient crowdsourcing of mobile sensor data by exploiting smartphone app opportunities", "abstract": "Fueled by the widespread adoption of sensor-enabled smartphones, mobile crowdsourcing is an area of rapid innovation. Many crowd-powered sensor systems are now part of our daily life -- for example, providing highway congestion information. However, participation in these systems can easily expose users to a significant drain on already limited mobile battery resources. For instance, the energy burden of sampling certain sensors (such as WiFi or GPS) can quickly accumulate to levels users are unwilling to bear. Crowd system designers must minimize the negative energy side-effects of participation if they are to acquire and maintain large-scale user populations.To address this challenge, we propose Piggyback CrowdSensing (PCS), a system for collecting mobile sensor data from smartphones that lowers the energy overhead of user participation. Our approach is to collect sensor data by exploiting Smartphone App Opportunities -- that is, those times when smartphone users place phone calls or use applications. In these situations, the energy needed to sense is lowered because the phone need no longer be woken from an idle sleep state just to collect data. Similar savings are also possible when the phone either performs local sensor computation or uploads the data to the cloud. To efficiently use these sporadic opportunities, PCS builds a lightweight, user-specific prediction model of smartphone app usage. PCS uses this model to drive a decision engine that lets the smartphone locally decide which app opportunities to exploit based on expected energy/quality trade-offs.We evaluate PCS by analyzing a large-scale dataset (containing 1,320 smartphone users) and building an end-to-end crowdsourcing application that constructs an indoor WiFi localization database. Our findings show that PCS can effectively collect large-scale mobile sensor datasets (e.g., accelerometer, GPS, audio, image) from users while using less energy (up to 90% depending on the scenario) compared to a representative collection of existing approaches.", "citation_count": "33", "reference_count": "197", "date": "2013", "authors": ["Nicholas D. Lane", "Yohan Chon", "Lin Zhou", "Yongzhe Zhang", "Fan Li", "Dongwon Kim", "Guanzhong Ding", "Feng Zhao", "Hojung Cha"], "related_topics": ["Crowdsourcing", "Cloud computing", "Efficient energy use", "Phone", "Upload", "Decision support system", "Exploit", "Overhead (computing)", "Computer network", "Embedded system", "Computer science"]}
{"id": "2031910487", "references": ["2618530766", "1872489089", "2148461049", "2144354855", "1983364832", "2310919327", "2963542991", "2962820688", "2144982973", "3124955340"], "title": "A Convolutional Neural Network for Automatic Analysis of Aerial Imagery", "abstract": "This paper introduces a new method to automate the detection of marine species in aerial imagery using a Machine Learning approach. Our proposed system has at its core, a convolutional neural network. We compare this trainable classifier to a handcrafted classifier based on color features, entropy and shape analysis. Experiments demonstrate that the convolutional neural network outperforms the handcrafted solution. We also introduce a negative training example-selection method for situations where the original training set consists of a collection of labeled images in which the objects of interest (positive examples) have been marked by a bounding box. We show that picking random rectangles from the background is not necessarily the best way to generate useful negative examples with respect to learning.", "citation_count": "27", "reference_count": "14", "date": "2014", "authors": ["Frederic Maire", "Luis Mejias", "Amanda Hodgson"], "related_topics": ["Deep learning", "Convolutional neural network", "Time delay neural network", "Artificial neural network", "Minimum bounding box", "Pattern recognition", "Image processing", "Computer vision", "Entropy (information theory)", "Computer science", "Aerial imagery", "Artificial intelligence"]}
{"id": "2120520366", "references": ["2160598920", "1630964756", "2146257637", "2037768235", "2019363670", "1670263352", "1574164692", "2130802299", "1520252399", "3124955340"], "title": "Ensembling Rule Based Classifiers for Detecting Network Intrusions", "abstract": "An intrusion is defined as a violation of the security policy of the system, and hence, intrusion detection mainly refers to the mechanisms that are developed to detect violations of system security policy. Recently, data mining techniques have gained importance in providing the valuable information which in turn can help to enhance the decision on identifying the intrusions (attacks). In this paper; we evaluate the performance of various rule based classifiers like: JRip, RIDOR, NNge and Decision Table using ensemble approach in order to build an efficient network intrusion detection system. We use KDDCup\u201999, intrusion detection benchmark dataset (which is a part of DARPA evaluation program) for our experimentation. It can be observed from the results that the proposed approach is accurate in detecting network intrusions, provides low false positive rate, simple, reliable and faster in building an efficient network intrusion system.", "citation_count": "16", "reference_count": "24", "date": "2009", "authors": ["Mrutyunjaya Panda", "Manas Ranjan Patra"], "related_topics": ["Anomaly-based intrusion detection system", "Intrusion detection system", "Rule-based system", "Decision table", "Security policy", "Statistical classification", "Benchmark (computing)", "Machine learning", "Data mining", "Artificial neural network", "Computer science", "Artificial intelligence"]}
{"id": "2132827378", "references": ["1992825118", "2062903088", "1956559956", "2153635508", "2032210760", "2162915993", "2020163092", "2162762921", "2912934387", "3124955340"], "title": "Informedia @ TRECVID 2011", "abstract": "The Informedia group participated in three tasks this year, including: Multimedia Event Detection (MED), Semantic Indexing (SIN) and Surveillance Event Detection. Generally, all of these tasks consist of three main steps: extracting feature, training detector and fusing. In the feature extraction part, we extracted a lot of low-level features, high-level features and text features. Especially, we used the Spatial-Pyramid Matching technique to represent the low-level visual local features, such as SIFT and MoSIFT, which describe the location information of feature points. In the detector training part, besides the traditional SVM, we proposed a Sequential Boosting SVM classifier to deal with the large-scale unbalance classification problem. In the fusion part, to take the advantages from different features, we tried three different fusion methods: early fusion, late fusion and double fusion. Double fusion is a combination of early fusion and late fusion. The experimental results demonstrated that double fusion is consistently better, or at least comparable than early fusion and late fusion.", "citation_count": "18", "reference_count": "16", "date": "2011", "authors": ["Lei Bao", "Shoou-I Yu", "Zhen-Zhong Lan", "Arnold Overwijk", "Qin Jin", "Brian Langner", "Michael Garbus", "Susanne Burger", "Florian Metze", "Alexander Hauptmann"], "related_topics": ["Feature (computer vision)", "Feature extraction", "Boosting (machine learning)", "Support vector machine", "Scale-invariant feature transform", "Fusion", "Search engine indexing", "Pattern recognition", "Speech recognition", "Matching (graph theory)", "Computer science", "Artificial intelligence"]}
{"id": "1984194948", "references": ["2101849132", "2097512404", "2003709967", "1990054309", "2008393432", "1536214225", "2135737723", "3124955340", "2127070222", "2102625004"], "title": "Quantitative performance analysis of object detection algorithms on underwater video footage", "abstract": "Object detection in underwater unconstrained environments is useful in domains like marine biology and geology, where the scientists need to study fish populations, underwater geological events etc. However, in literature, very little can be found regarding fish detection in unconstrained underwater videos. Nevertheless, the unconstrained underwater video domain constitutes a perfect soil for bringing state-of-the-art object detection algorithms to their limits because of the nature of the scenes, which often present with a number of intrinsic difficulties (e.g. multi-modal backgrounds, complex textures and color patterns, ever-changing illumination etc..).In this paper, we evaluated the performance of six state-of-the-art object detection algorithms in the task of fish detection in unconstrained, underwater video footage, discussing the properties of each of them and giving a detailed report of the achieved performance.", "citation_count": "15", "reference_count": "13", "date": "2012", "authors": ["Isaak Kavasidis", "Simone Palazzo"], "related_topics": ["Object detection", "Object-class detection", "Underwater", "Computer vision", "Computer science", "Algorithm", "Domain (software engineering)", "Artificial intelligence", "Fish"]}
{"id": "2114839088", "references": ["2153746365", "2111993661", "2046589280", "1981673219", "2063965450", "2078088780", "2127744755", "2797148637", "3124955340", "1973948212"], "title": "SafeVchat: detecting obscene content and misbehaving users in online video chat services", "abstract": "Online video chat services such as Chatroulette, Omegle, and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular, with over a million users per month in the case of Chatroulette. A key problem encountered in such systems is the presence of flashers and obscene content. This problem is especially acute given the presence of underage minors in such systems. This paper presents SafeVchat, a novel solution to the problem of flasher detection that employs an array of image detection algorithms. A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not, based on Dempster-Shafer Theory. The paper introduces a novel, motion-based skin detection method that achieves significantly higher recall and better precision. The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com.", "citation_count": "23", "reference_count": "19", "date": "2011", "authors": ["Xinyu Xing", "Yu-Li Liang", "Hanqiang Cheng", "Jianxun Dang", "Sui Huang", "Richard Han", "Xue Liu", "Qin Lv", "Shivakant Mishra"], "related_topics": ["Key (cryptography)", "Multimedia", "Computer science", "Motion (physics)", "Content (measure theory)", "Online video"]}
{"id": "2070277296", "references": ["2151103935", "2157284958", "2137560623", "2153635508", "2098642880", "2148645123", "2121927366", "2113137767", "1566135517", "3124955340"], "title": "Tree detection from aerial imagery", "abstract": "We propose an automatic approach to tree detection from aerial imagery. First a pixel-level classifier is trained to assign a {tree, non-tree} label to each pixel in an aerial image. The pixel-level classification is then refined by a partitioning algorithm to a clean image segmentation of tree and non-tree regions. Based on the refined segmentation results, we adopt template matching followed by greedy selection to locate individual tree crowns.As training a pixel-level classifier requires manual generation of ground-truth tree masks, we propose methods for automatic model and training data selection to minimize the manual work and scale the algorithm to the entire globe. We test the algorithm on thousands of production aerial images across different countries. We demonstrate high-quality tree detection results as well as good scalability of the proposed approach.", "citation_count": "18", "reference_count": "18", "date": "2009", "authors": ["Lin Yang", "Xiaqing Wu", "Emil Praun", "Xiaoxu Ma"], "related_topics": ["Image segmentation", "Aerial image", "Template matching", "Segmentation", "Pixel", "Classifier (UML)", "Pattern recognition", "Scalability", "Computer vision", "Geography", "Aerial imagery", "Artificial intelligence"]}
{"id": "2130851608", "references": ["1632114991", "2081687495", "2117400858", "1718065290", "2100796029", "1571096757", "2112861996", "2099247782", "1483126227", "2046224275"], "title": "Unsupervised Learning of Disambiguation Rules for Part-of-Speech Tagging", "abstract": "In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text1.", "citation_count": "25", "reference_count": "271", "date": "1999", "authors": ["E. Brill", "M. Pop"], "related_topics": ["Unsupervised learning", "Speech recognition", "Natural language processing", "Computer science", "Artificial intelligence", "Part-of-speech tagging"]}
{"id": "2119966617", "references": ["1632767318", "2016871293", "1571096757", "1648417313", "2156202195", "2007780422", "2099247782", "1977182536", "2010595692", "1579136630"], "title": "Combining Trigram-Based and Feature-Based Methods for Context-Sensitive Spelling Correction", "abstract": "This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as peace and piece, or quiet and quite) and also the problem of correcting particular word usage errors (such as amount and number, or among and between). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance.", "citation_count": "11", "reference_count": "158", "date": "1996", "authors": ["Andrew Golding", "Yves Schabes"], "related_topics": ["Trigram", "Word processing", "Word usage", "Spelling", "Context (language use)", "Word (computer architecture)", "Part of speech", "Feature (machine learning)", "Natural language processing", "Speech recognition", "Grammar", "Computer science", "Spell", "Artificial intelligence"]}
{"id": "1519443010", "references": ["66690650", "2068017609", "2118996379", "2118456496", "1597379537", "2148362501", "2085606725", "2157963512"], "title": "Automatic Rule Acquisition for Spelling Correction", "abstract": "", "citation_count": "0", "reference_count": "183", "date": "1997", "authors": ["Lidia Mangu", "Eric Brill"], "related_topics": ["Spelling", "Computer science", "Speech recognition"]}
{"id": "2118996379", "references": ["1632114991", "2167277498", "2119821739", "2140785063", "2158195707", "2313581450", "3017143921", "3124873412", "2912934387", "3124955340"], "title": "A Winnow-Based Approach to Context-Sensitive Spelling Correction", "abstract": "A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts depend on only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. In the work reported here, we present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting to for too, casual for causal, and so on. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned conditions (2) When compared with other systems in the literature, WinSpell exhibits the highest performances (3) While several aspects of WinSpell\u2018s architecture contribute to its superiority over BaySpell, the primary factor is that it is able to learn a better linear separator than BaySpell learnss (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.", "citation_count": "39", "reference_count": "445", "date": "1999", "authors": ["Andrew R. Golding", "Dan Roth"], "related_topics": ["Winnow", "Test set", "Supervised learning", "Unsupervised learning", "Feature vector", "Natural language", "Spelling", "Machine learning", "Computer science", "Voting", "Artificial intelligence"]}
{"id": "1648417313", "references": ["1632114991", "2313581450", "2007780422"], "title": "A Bayesian Hybrid Method for Context-sensitive Spelling Correction.", "abstract": "Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.", "citation_count": "3", "reference_count": "114", "date": "1996", "authors": ["Andrew R. Golding"], "related_topics": ["Decision list", "Context (language use)", "Syntax (programming languages)", "Word (computer architecture)", "Bayesian probability", "Natural language processing", "Spelling", "Component (UML)", "Complementarity (molecular biology)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2156202195", "references": ["1516391399", "2129139611", "1999114220", "2047620598", "1976241232", "2040004971", "2099247782", "1977182536", "2428981601", "1554031433"], "title": "DECISION LISTS FOR LEXICAL AMBIGUITY RESOLUTION: Application to Accent Restoration in Spanish and French", "abstract": "This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.", "citation_count": "16", "reference_count": "571", "date": "1994", "authors": ["David Yarowsky"], "related_topics": ["Ambiguity", "Decision list", "Context (language use)", "Resolution (logic)", "Syntax", "Natural language processing", "Accent (music)", "Task (project management)", "Class (biology)", "Speech recognition", "Computer science", "Artificial intelligence", "Lexical ambiguity", "Lexicography", "Spanish language"]}
{"id": "1953828586", "references": ["1632114991", "1491178396", "2160842254", "1567570606", "1623072288", "2096175520", "1986543644", "2099247782", "2110882317", "1773803948"], "title": "Learning to Parse Natural Language with Maximum Entropy Models", "abstract": "This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In practice, the running time of the parser on a test sentence is linear with respect to the sentence length. We also demonstrate that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn. Furthermore, this paper shows that research into rescoring the top 20 parses returned by the parser might yield accuracies dramatically higher than the state-of-the-art.", "citation_count": "28", "reference_count": "370", "date": "1999", "authors": ["Adwait Ratnaparkhi"], "related_topics": ["Top-down parsing", "Parser combinator", "Parsing", "Maximum-entropy Markov model", "Natural language", "Sentence", "Principle of maximum entropy", "Natural language processing", "Machine learning", "Computer science", "Artificial intelligence", "Sentence length"]}
{"id": "1977182536", "references": ["2110529160", "2137638032", "1550769831", "2129139611", "1980491396", "2117652747", "1548013757", "2007780422", "2099247782", "2019911971"], "title": "A method for disambiguating word senses in a large corpus", "abstract": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics.", "citation_count": "30", "reference_count": "878", "date": "1992", "authors": ["William A. Gale", "Kenneth Ward Church", "David Yarowsky"], "related_topics": ["Noun", "Polysemy", "Document retrieval", "Natural language", "Computational linguistics", "Ambiguity", "Lexical analysis", "Natural language processing", "Computer science", "Bayesian probability", "Artificial intelligence"]}
{"id": "1988842251", "references": ["2091273188", "2147152072", "1648417313", "2156202195", "2007780422", "2098162425", "2150272696", "1977182536", "2010595692", "1983578042"], "title": "Contextual Spelling Correction Using Latent Semantic Analysis", "abstract": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier.", "citation_count": "16", "reference_count": "88", "date": "1997", "authors": ["Michael P. Jones", "James H . Martin"], "related_topics": ["Spelling", "Sentence", "Probabilistic latent semantic analysis", "Latent semantic analysis", "Context (language use)", "Naive Bayes classifier", "Natural language processing", "Speech recognition", "Word (computer architecture)", "Computer science", "Artificial intelligence"]}
{"id": "2111494971", "references": ["169539560", "2029641002", "1592735339"], "title": "Tangent Prop - A formalism for specifying selected invariances in an adaptive network", "abstract": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera).\r\n\r\nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform.", "citation_count": "3", "reference_count": "336", "date": "1991", "authors": ["Patrice Simard", "Bernard Victorri", "Yann LeCun", "John Denker"], "related_topics": ["A priori and a posteriori", "Invariant (physics)", "Theoretical computer science", "Tangent", "Mathematics", "Formalism (philosophy of mathematics)", "Training set"]}
{"id": "2137291015", "references": ["2111494971", "23758216", "1991848143", "60275550", "2154579312"], "title": "Efficient Pattern Recognition Using a New Transformation Distance", "abstract": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases.", "citation_count": "5", "reference_count": "747", "date": "1992", "authors": ["Patrice Simard", "Yann LeCun", "John S. Denker"], "related_topics": ["Elastic matching", "Distance measures", "Statistical classification", "Invariant (mathematics)", "NIST", "Radial basis function", "Scaling", "Pattern recognition", "Algorithm", "Euclidean geometry", "Mathematics", "Artificial intelligence"]}
{"id": "2607313294", "references": ["2134586063", "2166312020", "2183513372", "2159901481", "1986324151", "2166469100", "2045249386", "2127574124", "1016921196", "1498655523"], "title": "Writer independent and writer adaptive neural network for on-line character recognition", "abstract": "", "citation_count": "0", "reference_count": "23", "date": "1992", "authors": ["I. Guyon", "D. Henderson", "P. Albrecht", "Yann Lecun", "J. S. Denker"], "related_topics": ["Line (text file)", "Artificial neural network", "Computer science", "Speech recognition", "Character recognition"]}
{"id": "1980501707", "references": ["2004075725", "2141278204", "2135346934", "2140539590", "2140766383", "1529808766", "3017143921", "2020246210", "2161012272", "2176028050"], "title": "Neural Network Classifiers Estimate Bayesian a posteriori Probabilities.", "abstract": "Many neural network classifiers provide outputs which estimate Bayesian a posteriori probabilities. When the estimation is accurate, network outputs can be treated as probabilities and sum to one. Simple proofs show that Bayesian probabilities are estimated when desired network outputs are 1 of M (one output unity, all others zero) and a squared-error or cross-entropy cost function is used. Results of Monte Carlo simulations performed using multilayer perceptron (MLP) networks trained with backpropagation, radial basis function (RBF) networks, and high-order polynomial networks graphically demonstrate that network outputs provide good estimates of Bayesian probabilities. Estimation accuracy depends on network complexity, the amount of training data, and the degree to which training data reflect true likelihood distributions and a priori class probabilities. Interpretation of network outputs as Bayesian probabilities allows outputs from multiple networks to be combined for higher level decision making, sim...", "citation_count": "20", "reference_count": "1,537", "date": "1991", "authors": ["Michael D. Richard", "Richard P. Lippmann"], "related_topics": ["Bayesian statistics", "Variable-order Bayesian network", "Probabilistic neural network", "Multilayer perceptron", "Network complexity", "Artificial neural network", "Bayesian probability", "Backpropagation", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2140539590", "references": ["2042264548", "2154642048", "2110485445", "1770825568", "3036751298", "2135346934", "1966812932", "2173629880", "2007431958", "2176028050"], "title": "Links between Markov models and multilayer perceptrons", "abstract": "The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained. &gt;", "citation_count": "35", "reference_count": "1,203", "date": "1990", "authors": ["H. Bourlard", "C.J. Wellekens"], "related_topics": ["Multilayer perceptron", "Markov model", "Hidden Markov model", "Viterbi algorithm", "Markov process", "Perceptron", "Artificial neural network", "Maximum a posteriori estimation", "Probability distribution", "Pattern recognition", "Embedding", "Computer science", "Artificial intelligence", "Maximum likelihood"]}
{"id": "2099070536", "references": ["169539560", "2148099973", "2055075080", "2147800946", "2115240329", "2057619148", "2100921332", "2132793646", "2504871398", "1498436455"], "title": "LeRec: a NN/HMM hybrid for on-line handwriting recognition", "abstract": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.", "citation_count": "13", "reference_count": "209", "date": "1995", "authors": ["Yoshua Bengio", "Yann LeCun", "Craig Nohl", "Chris Burges"], "related_topics": ["Handwriting recognition", "Hidden Markov model", "Markov model", "Normalization (image processing)", "Preprocessor", "Artificial neural network", "Expectation\u2013maximization algorithm", "Pixel", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2150884987", "references": ["1979500821", "2097863906", "2171277043", "1505136099", "2060542468", "2092845679", "1498431753", "2137242837", "1992402718", "2056763477"], "title": "Adaptive mixtures of local experts", "abstract": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.", "citation_count": "13", "reference_count": "4,718", "date": "1991", "authors": ["Robert A. Jacobs", "Michael I. Jordan", "Steven J. Nowlan", "Geoffrey E. Hinton"], "related_topics": ["Semi-supervised learning", "Supervised learning", "Competitive learning", "Unsupervised learning", "Multilayer perceptron", "Preference learning", "Modular neural network", "Product of experts", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2055075080", "references": ["1501400124", "2010392031", "2062219129", "2172505344", "2038658268", "1975470983", "2034967522", "1977699267", "2062361515", "1735553891"], "title": "The state of the art in online handwriting recognition", "abstract": "This survey describes the state of the art of online handwriting recognition during a period of renewed activity in the field. It is based on an extensive review of the literature, including journal articles, conference proceedings, and patents. Online versus offline recognition, digitizer technology, and handwriting properties and recognition problems are discussed. Shape recognition algorithms, preprocessing and postprocessing techniques, experimental systems, and commercial products are examined. &gt;", "citation_count": "165", "reference_count": "1,332", "date": "1990", "authors": ["C.C. Tappert", "C.Y. Suen", "T. Wakahara"], "related_topics": ["Intelligent character recognition", "Handwriting recognition", "Handwriting", "Pattern recognition (psychology)", "Field (computer science)", "Speech recognition", "User interface", "Natural language", "Computer science", "Artificial intelligence"]}
{"id": "2170541567", "references": ["1883377074", "1994616650", "1501273589", "2054584253", "1567923870", "2044437287", "2124229187"], "title": "Connectionist probability estimation in the DECIPHER speech recognition system", "abstract": "The authors have previously demonstrated that feedforward networks can be used to estimate local output probabilities in hidden Markov model (HMM) speech recognition systems (Renals et al., 1991). These connectionist techniques are integrated into the DECIPHER system, with experiments being performed using the speaker-independent DARPA RM database. The results indicate that: connectionist probability estimation can improve performance of a context-independent maximum-likelihood-trained HMM system; performance of the connectionist system is close to what can be achieved using (context-dependent) HMM systems of much higher complexity; and mixing connectionist and maximum-likelihood estimates can improve the performance of the state-of-the-art context-independent HMM system. &gt;", "citation_count": "7", "reference_count": "85", "date": "1992", "authors": ["S. Renals", "N. Morgan", "M. Cohen", "H. Franco"], "related_topics": ["Hidden Markov model", "Artificial neural network", "Speech recognition", "Connectionism", "Pattern recognition", "Entropy (information theory)", "Feed forward", "Parametric statistics", "DECIPHER", "Entropy (energy dispersal)", "Transfer function", "Computer science", "Entropy (statistical thermodynamics)", "Entropy (classical thermodynamics)", "Entropy (arrow of time)", "Entropy (order and disorder)", "Artificial intelligence", "Maximum likelihood"]}
{"id": "2124229187", "references": ["2054658115", "2140539590", "1877570817", "2140766383"], "title": "A probabilistic approach to the understanding and training of neural network classifiers", "abstract": "It is shown that training a neural network using a mean-square-error criterion gives network outputs that approximate posterior class probabilities. Based on this probabilistic interpretation of the network operation, information-theoretic training criteria such as maximum mutual information and the Kullback-Liebler measure are investigated. It is shown that both of these criteria are equivalent to the maximum-likelihood estimation (MLE) of the network parameters. MLE of a network allows for the comparison of network models using the Akaike information criterion and the minimum-description length criterion. &gt;", "citation_count": "4", "reference_count": "234", "date": "1990", "authors": ["H. Gish"], "related_topics": ["Probabilistic neural network", "Bayesian information criterion", "Akaike information criterion", "Probabilistic logic", "Artificial neural network", "Network model", "Mutual information", "Estimation theory", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2416739038", "references": ["2071628512", "3125537303", "2102013737", "2343875716", "2166608639", "2027197837", "2137983211", "2136492299", "1988115241", "2020246210"], "title": "Theory of the Back Propagation Neural Network", "abstract": "", "citation_count": "0", "reference_count": "505", "date": "1989", "authors": ["R. Hecht-Nielsen"], "related_topics": ["Computer science", "Artificial intelligence", "Back propagation neural network"]}
{"id": "2090270852", "references": ["2137983211", "1654142532"], "title": "Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions", "abstract": "K.M. Hornik, M. Stinchcombe, and H. White (Univ. of California at San Diego, Dept. of Economics Discussion Paper, June 1988; to appear in Neural Networks) showed that multilayer feedforward networks with as few as one hidden layer, no squashing at the output layer, and arbitrary sigmoid activation function at the hidden layer are universal approximators: they are capable of arbitrarily accurate approximation to arbitrary mappings, provided sufficiently many hidden units are available. The present authors obtain identical conclusions but do not require the hidden-unit activation to be sigmoid. Instead, it can be a rather general nonlinear function. Thus, multilayer feedforward networks possess universal approximation capabilities by virtue of the presence of intermediate layers with sufficiently many parallel processors; the properties of the intermediate-layer activation function are not so crucial. In particular, sigmoid activation functions are not necessary for universal approximation. &gt;", "citation_count": "2", "reference_count": "317", "date": "1989", "authors": ["Stinchcombe", "White"], "related_topics": ["Universal approximation theorem", "Activation function", "Rectifier (neural networks)", "Sigmoid function", "Feedforward neural network", "Function approximation", "Artificial neural network", "Nonlinear system", "Topology", "Theoretical computer science", "Mathematics"]}
{"id": "1613359937", "references": ["2042264548", "2040870580", "1987980016", "1652505363", "2152088994", "1505652865", "1971735090"], "title": "Capabilities of three-layered perceptrons", "abstract": "A theorem is proved to the effect that three-layered perceptrons with an infinite number of computing units can represent arbitrary mapping if the desired mapping and the input-output characteristics of the computing units satisfy some constraints. The proof is constructive, and each coefficient is explicitly presented. The theorem theoretically guarantees a kind of universality for three-layered perceptrons. Although two-layered perceptrons (simple perceptrons) cannot represent arbitrary functions, three-layers prove necessary and sufficient. The relationship between the model used in the proof and the distributed storage and processing of information is also discussed. &gt;", "citation_count": "7", "reference_count": "504", "date": "1988", "authors": ["Irie", "Miyake"], "related_topics": ["Perceptron", "Artificial neural network", "Constructive", "Algorithm", "Distributed data store", "Universality (dynamical systems)", "Computer science", "Infinite number"]}
{"id": "2056099894", "references": ["2160837607", "2108207895", "2137344397", "3122732203", "1541527977", "2137983211", "2963423916", "1974708997", "1999814123", "2124758339"], "title": "Probability and Measure", "abstract": "Probability. Measure. Integration. Random Variables and Expected Values. Convergence of Distributions. Derivatives and Conditional Probability. Stochastic Processes. Appendix. Notes on the Problems. Bibliography. List of Symbols. Index.", "citation_count": "0", "reference_count": "11,776", "date": "1979", "authors": ["Patrick Billingsley"], "related_topics": ["Probability measure", "Regular conditional probability", "Conditional probability distribution", "Conditional probability", "Joint probability distribution", "Probability distribution", "Convolution of probability distributions", "Marginal distribution", "Statistics", "Mathematics"]}
{"id": "1490039160", "references": ["2153135464", "1903060567", "1974439116", "2081229365", "568489446", "2073078790", "2027197837", "2042243448", "1870973791"], "title": "The\u0301orie des distributions", "abstract": "Ce traite a marque une date dans le progres des mathematiques et de la physique en levant l\u2019ambiguite. que constituait le succes des methodes de calcul symbolique aupres des physiciens et l\u2019inacceptabilite de leurs formules au regard de la rigueur mathematiques. Le merite revient a Laurent Schwartz d\u2019avoir englobe dans une theorie qui est a la fois une synthese et une simplifications, des procedes heterogenes et souvent incorrects utilises dans des domaines tres divers.\nUne definition correcte et une etude systematique de ces etres nouveaux, les distributions, leur ont donne droit de cite. dans l\u2019usage courant. Leur utilisation extensive dans de nombreuses branches des mathematiques pures et appliquees, de la physique et des sciences de l\u2019ingenieur fait de ce livre un classique des mathematiques modernes.", "citation_count": "0", "reference_count": "6,092", "date": "1966", "authors": ["Laurent Schwartz"], "related_topics": ["Humanities", "Philosophy"]}
{"id": "1537887709", "references": ["2039638574", "1970847130", "2082494109", "3124302457", "2141278389", "2027197837", "2004492771", "2105340951", "2133010631", "3105657163"], "title": "Weighted Sobolev Spaces", "abstract": "Introduction Motivation Weight Domain Hardy Inequality Part One: POWER-TYPE WEIGHTS: Some Elementary Assertions Density of Smooth Functions Imbedding Theorems Miscellaneous Part Two: GENERAL WEIGHTS: Several Elementary Results Density of Smooth Functions Imbedding Theorems Part Three: APPLICATIONS: Formulation of the Problem Power-Type Weights General Weights References List of Imbeddings Index.", "citation_count": "0", "reference_count": "841", "date": "1985", "authors": ["Alois Kufner"], "related_topics": ["Sobolev inequality", "Domain (mathematical analysis)", "Sobolev space", "Pure mathematics", "Discrete mathematics", "Mathematics"]}
{"id": "2107600630", "references": ["2137027750", "2170599822", "2082183672", "2142384583", "2072181047", "2149043292", "1988925211", "2100921332", "2086826280", "2154579312"], "title": "An automatic reading system for handwritten numeral amounts on French checks", "abstract": "We present an automatic recognition system applied to handwritten numeral check amounts. This system is based on a segmentation-by-recognition probabilistic model. The application is described from the field amount localization to the hypothesis generation of amounts. An explicit segmentation algorithm determines cut regions on digit links and provides a multiple spatial representation. The best path for the segmentation is determined by the combination of the recognition scores, segmentation weights and the outputs of a probabilistic parser. Training is done by a bootstrapping technique, which significantly improves the performances of the different algorithms. It also allows the use of a reject class at the recognition step. The system was evaluated on 10000 database images to show its robustness.", "citation_count": "16", "reference_count": "81", "date": "1995", "authors": ["E. Lethelier", "M. Leroux", "M. Gilloux"], "related_topics": ["Image segmentation", "Handwriting recognition", "Numeral system", "Optical character recognition", "Segmentation", "Probabilistic logic", "Robustness (computer science)", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2227933188", "references": ["1564135739", "2126297944", "2127321876", "2017787659", "2128458196", "2120111750", "252878706", "2120240539", "2121839820", "1521064364"], "title": "Historical review of OCR research and development", "abstract": "Research and development of OCR systems are considered from a historical point of view. The historical development of commercial systems is included. Both template matching and structure analysis approaches to R&amp;D are considered. It is noted that the two approaches are coming closer and tending to merge. Commercial products are divided into three generations, for each of which some representative OCR systems are chosen and described in some detail. Some comments are made on recent techniques applied to OCR, such as expert systems and neural networks, and some open problems are indicated. The authors' views and hopes regarding future trends are presented. &gt;", "citation_count": "0", "reference_count": "1,360", "date": "1995", "authors": ["Shunji Mori", "Ching Y. Suen", "Kazuhiko Yamamoto"], "related_topics": ["Optical character recognition", "Intelligent word recognition", "Expert system", "Document processing", "Point (typography)", "Intelligent character recognition", "Information retrieval", "Merge (linguistics)", "Natural language processing", "Template matching", "Computer science", "Artificial intelligence"]}
{"id": "49742075", "references": ["2138957397", "2133003941", "1996941612", "2064782300", "2474134051", "2058668884", "2524625303", "2010398978", "139350513", "51975515"], "title": "Moment-preserving thresholding: a new approach", "abstract": "", "citation_count": "0", "reference_count": "229", "date": "1995", "authors": ["Wen-Hsiang Tsai"], "related_topics": ["Balanced histogram thresholding", "Thresholding", "Moment (mathematics)", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2151513848", "references": ["2032375421", "2185122414", "2181242105", "2095891604"], "title": "Name and Address Block Reader system for tax form processing", "abstract": "The reading of names and addresses is one of the most complex tasks in automated forms processing. The paper describes an integrated real time system to read names and addresses on tax forms of the Internal Revenue Service of the United States. The Name and Address Block Reader (NABR) system accepts both machine printed and hand printed address block images as input. The application software has two major steps: document analysis (connected component analysis, address block extraction, label detection, hand print/machine print discrimination); and document recognition. Document recognition has two non identical streams for machine print and hand print; key steps are: address parsing, character recognition, word recognition and postal database lookup (ZIP+4 and City-State-ZIP files). Real time throughput (8,500 forms per hour) is achieved by employing a loosely coupled multiprocessing architecture. The functional architecture, software design, system architecture and hardware implementation are described. Performance evaluation on machine printed and handwritten addresses are presented.", "citation_count": "4", "reference_count": "53", "date": "1995", "authors": ["S.N. Srihari", "Yong-Chul Shin", "V. Ramanaprasad", "Dar-Shyang Lee"], "related_topics": ["Systems architecture", "Handwriting recognition", "Block (data storage)", "Feature extraction", "Software design", "Application software", "Computer science", "Information retrieval", "Embedded system"]}
{"id": "2126727781", "references": ["2102545463", "1690636059", "2471214383", "2172263315", "2124054731", "2023626880", "2081410745", "2105364438", "2052277674", "2118311394"], "title": "Document Image Analysis", "abstract": "", "citation_count": "30", "reference_count": "482", "date": "1996", "authors": ["Lawrence O'Gorman", "Rangachar Kasturi"], "related_topics": ["Automatic image annotation", "Document layout analysis", "Computer vision", "Computer science", "Image (mathematics)", "Artificial intelligence"]}
{"id": "2124776405", "references": ["2026131661", "2153233077", "2118978333", "3145506661", "2097308346", "2071707134", "2132549764", "2111072639", "1964357740", "2108384452"], "title": "Neural Networks: A Comprehensive Foundation", "abstract": "From the Publisher:\r\nThis book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts.", "citation_count": "0", "reference_count": "40,861", "date": "1998", "authors": ["Simon Haykin"], "related_topics": ["Modular neural network", "Function (engineering)", "Process (engineering)", "Artificial neural network", "Field (computer science)", "Emerging technologies", "Modular design", "Computer science", "Committee machine", "Artificial intelligence", "Machine learning"]}
{"id": "2072181047", "references": ["2159498975", "1963937384", "1979819178", "2008313333", "1536628706", "1966591781", "2061315523", "1623080549", "2062361515", "2122827492"], "title": "Recognition of isolated and simply connected hand-written numerals", "abstract": "Abstract   In this paper the authors describe the results of their investigation into the development of a recognition algorithm for identifying numerals that may be isolated or connected, broken or continuous. Using a structural classification scheme, the recognition algorithm is derived as a tree classifier. In an extensive test experiment, an accuracy of 99% was realized with isolated numerals. When connected numerals were also included a recognition accuracy of 93% was obtained.", "citation_count": "15", "reference_count": "201", "date": "1986", "authors": ["M. Shridhar", "A. Badreldin"], "related_topics": ["Classifier (linguistics)", "Numeral system", "Classifier (UML)", "Speech recognition", "Simply connected space", "Computer science"]}
{"id": "1519534430", "references": ["2058929792", "2027298168", "2061681083", "2104474593", "2147947791", "2097486270", "1989584148", "2079145130", "2163166770", "2118377301"], "title": "29 Optical character recognition\u2014Theory and practice", "abstract": "Publisher Summary   This chapter presents an overview of Optical Character Recognition (OCR) for statisticians interested in extending their endeavors from the traditional realm of pattern classification to the many other alluring aspects of OCR. The most important dimensions of data entry from the point of view of a project manager considering the acquisition of an OCR system are described in the chapter. The major applications are categorized according to the type of data to be converted to computer-readable form and optical scanners are described. The preprocessing necessary before the actual character classification can take place is discussed in the chapter. It outlines the classical decision-theoretic formulation of the character classification problem. The various statistical approximations to the optimal classifier, including dimensionality reduction, feature extraction, and feature selection is discussed with references to the appropriate statistical techniques. The importance of accurate estimation of the error and reject rates are discussed in the chapter and a fundamental relation between the error rate and the reject rate in optimal systems are described in the chapter.", "citation_count": "32", "reference_count": "65", "date": "1982", "authors": ["George Nagy"], "related_topics": ["Feature extraction", "Optical character recognition", "Dimensionality reduction", "Word error rate", "Feature selection", "Classifier (UML)", "Preprocessor", "Data mining", "Computer science", "Project manager"]}
{"id": "2122827492", "references": ["1528287150", "2118017998", "1982485672", "40044188", "2133246412", "2808650095", "1972969203", "2122741244", "2116360511", "50900203"], "title": "Fourier Descriptors for Plane Closed Curves", "abstract": "A method for the analysis and synthesis of closed curves in the plane is developed using the Fourier descriptors FD's of Cosgriff [1]. A curve is represented parametrically as a function of arc length by the accumulated change in direction of the curve since the starting point. This function is expanded in a Fourier series and the coefficients are arranged in the amplitude/phase-angle form. It is shown that the amplitudes are pure form invariants as well as are certain simple functions of phase angles. Rotational and axial symmetry are related directly to simple properties of the Fourier descriptors. An analysis of shape similarity or symmetry can be based on these relationships; also closed symmetric curves can be synthesized from almost arbitrary Fourier descriptors. It is established that the Fourier series expansion is optimal and unique with respect to obtaining coefficients insensitive to starting point. Several examples are provided to indicate the usefulness of Fourier descriptors as features for shape discrimination and a number of interesting symmetric curves are generated by computer and plotted out.", "citation_count": "11", "reference_count": "2,691", "date": "1972", "authors": ["Charles T. Zahn", "Ralph Z. Roskies"], "related_topics": ["Discrete Fourier series", "Fourier series", "Fourier analysis", "Fourier transform", "Fourier shell correlation", "Discrete-time Fourier transform", "Harmonic analysis", "Half range Fourier series", "Mathematical analysis", "Geometry", "Mathematics"]}
{"id": "2256679588", "references": ["2023795513", "2149659980", "2558231299", "2071365591", "2019258010", "2101785709", "2142075376", "2153158097", "2785684543", "2026116201"], "title": "Neocognitron: a neural network model for a mechanism of visual pattern recognition", "abstract": "A recognition with a large-scale network is simulated on a PDP-11/34 minicomputer and is shown to have a great capability for visual pattern recognition. The model consists of nine layers of cells. The authors demonstrate that the model can be trained to recognize handwritten Arabic numerals even with considerable deformations in shape. A learning-with-a-teacher process is used for the reinforcement of the modifiable synapses in the new large-scale model, instead of the learning-without-a-teacher process applied to a previous model. The authors focus on the mechanism for pattern recognition rather than that for self-organization.", "citation_count": "0", "reference_count": "913", "date": "1988", "authors": ["Kunihiko Fukushima", "Sei Miyake", "Takayuki Ito"], "related_topics": ["Neocognitron", "Pattern recognition (psychology)", "Time delay neural network", "Artificial neural network", "Arabic numerals", "Pattern recognition", "Minicomputer", "Process (computing)", "Focus (optics)", "Computer science", "Artificial intelligence"]}
{"id": "2064838583", "references": ["2086699924", "2120163503", "2105594594", "2135346934", "2099070536", "2131877510", "1988925211", "1553004968", "2171850596", "2077574412"], "title": "Hidden Markov Model Based Word Recognition and Its Application to Legal Amount Reading on French Checks", "abstract": "A hidden Markov model (HMM) based word recognition algorithm for the recognition of legal amounts from French bank checks is presented. This algorithm is part of the A2iA INTERCHEQUE recognition system. The algorithm starts from images of handwritten words which have been automatically segmented from binary check images. After finding the lower-case zone on the complete amount, words are slant corrected and then segmented into graphemes. Then, features are extracted from the graphemes, and the feature vectors are vector quantized resulting in a sequence of symbols for each word. Likelihoods of all word classes are computed by a set of HMMs, which have been previously trained using either the Viterbi algorithm or the Baum?Welch algorithm. The various parameters of the system have been identified and their importance evaluated. Results have been obtained on large real-life data bases of French handwritten checks. The HMM-based system has been shown to outperform a holistic word recognizer and another HMM-type word recognizer from the A2iA INTERCHEQUE recognition system. Word recognition rates of about 89% for the 26-word vocabulary relevant for legal amount recognition on French bank checks have been obtained. More recently, a Neural Network?HMM hybrid has been designed, which produces even better recognition rates.", "citation_count": "14", "reference_count": "88", "date": "1998", "authors": ["S. Knerr", "E. Augustin", "O. Baret", "D. Price"], "related_topics": ["Intelligent word recognition", "Word recognition", "Hidden Markov model", "Viterbi algorithm", "Feature vector", "Word (computer architecture)", "Artificial neural network", "Vocabulary", "Speech recognition", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2149597185", "references": ["2154642048", "2145023731", "2105594594", "2172505344", "1541960437", "2110871230", "30902120", "1553004968", "2116360511", "2122028591"], "title": "An off-line cursive handwriting recognition system", "abstract": "Describes a complete system for the recognition of off-line handwriting. Preprocessing techniques are described, including segmentation and normalization of word images to give invariance to scale, slant, slope and stroke thickness. Representation of the image is discussed and the skeleton and stroke features used are described. A recurrent neural network is used to estimate probabilities for the characters represented in the skeleton. The operation of the hidden Markov model that calculates the best word in the lexicon is also described. Issues of vocabulary choice, rejection, and out-of-vocabulary word recognition are discussed.", "citation_count": "21", "reference_count": "368", "date": "1998", "authors": ["A.W. Senior", "A.J. Robinson"], "related_topics": ["Intelligent character recognition", "Handwriting recognition", "Optical character recognition", "Handwriting", "Word recognition", "Recurrent neural network", "Hidden Markov model", "Image segmentation", "Feature extraction", "Normalization (statistics)", "Vocabulary", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2113292028", "references": ["2111488376", "2148099973", "2055075080", "2049633694", "2147800946", "413857758", "2162363099"], "title": "Word normalization for online handwritten word recognition", "abstract": "We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the expectation-maximisation algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models.", "citation_count": "7", "reference_count": "50", "date": "1994", "authors": ["Y. Bengio", "Y. Le Cun"], "related_topics": ["Word recognition", "Hidden Markov model", "Cursive", "Handwriting", "Artificial neural network", "Normalization (statistics)", "Stylus", "Speech recognition", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "183625566", "references": ["2056133372", "2989448192", "2152088994", "2151058089", "1877570817", "2030377865", "1505652865", "1529808766", "1572157493", "2176028050"], "title": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition", "abstract": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct.", "citation_count": "10", "reference_count": "1,190", "date": "1990", "authors": ["John S. Bridle"], "related_topics": ["Softmax function", "Probabilistic neural network", "Posterior probability", "Mean squared error", "Perceptron", "Probabilistic logic", "Class discrimination", "Boltzmann machine", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2148295954", "references": ["1652505363", "2111488376", "2148099973", "2055075080", "2049633694", "2162363099"], "title": "Word normalization for on-line handwritten word recognition", "abstract": "We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the EM algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models.", "citation_count": "6", "reference_count": "31", "date": "1994", "authors": ["Yoshua Bengio", "Yann Lecun"], "related_topics": ["Intelligent word recognition", "Word recognition", "Hidden Markov model", "Cursive", "Handwriting", "Expectation\u2013maximization algorithm", "Artificial neural network", "Normalization (statistics)", "Pattern recognition", "Speech recognition", "Computer science", "Artificial intelligence"]}
{"id": "2077863651", "references": ["2046485094", "9865929", "1612759903", "2477425312", "1592735339", "2100659887", "2121145809"], "title": "The IRESTE On/Off (IRONOFF) dual handwriting database", "abstract": "Databases for character recognition algorithms are of fundamental interest for the training of statistics based recognition methods (neural networks, hidden Markov models) as well as for benchmarking existing recognition systems. Such databases currently exist, but none of them gives access to the online data (pen trajectory) and offline data (digital images) for the same writing signal. We have developed such a dual on/off database, named IRONOFF. Currently, it contains a large number of isolated characters, digits, and cursive words written by French writers. We have designed this database so that, given an online point, it can be mapped at the correct location in the corresponding scanned image, and conversely, each offline pixel can be temporally indexed. Since we think this database is of interest for a large part of the research community, it is publicly available.", "citation_count": "7", "reference_count": "255", "date": "1999", "authors": ["C. Viard-Gaudin", "P.M. Lallican", "S. Knerr", "P. Binter"], "related_topics": ["Optical character recognition", "Handwriting recognition", "Cursive", "Handwriting", "Hidden Markov model", "Point (typography)", "Artificial neural network", "Digital image", "Computer science", "Database"]}
{"id": "101240229", "references": ["2107645346", "2111920731", "2158078830", "2162395775", "2007714563", "2161369007", "2152928267", "2171498274", "2147345686", "2202508626"], "title": "Towards General Cursive Script Recognition", "abstract": "", "citation_count": "0", "reference_count": "11", "date": "1999", "authors": ["Urs-Viktor Marti", "Horst Bunke"], "related_topics": ["Cursive", "Natural language processing", "Computer science", "Artificial intelligence", "Script recognition"]}
{"id": "2143331230", "references": ["2019575783", "2171541062", "2108263314", "1985554184", "2127589108", "2988119488", "26816478", "2171590421", "2107890099", "3023786531"], "title": "Learning to rank using gradient descent", "abstract": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.", "citation_count": "17", "reference_count": "2,684", "date": "2005", "authors": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender"], "related_topics": ["Stochastic gradient descent", "Ranking SVM", "Gradient descent", "Learning to rank", "Backpropagation", "Neighbourhood components analysis", "Artificial neural network", "Ranking (information retrieval)", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "1602492977", "references": ["2154642048", "1652505363", "2125055259", "2119821739", "2077658674", "1512098439", "1604938182", "2032210760", "2148603752", "1594031697"], "title": "On the Learnability and Design of Output Codes for Multiclass Problems", "abstract": "Output coding is a general framework for solving multiclass categorization problems. Previous research on output codes has focused on building multiclass machines given predefined output codes. In this paper we discuss for the first time the problem of designing output codes for multiclass problems. For the design problem of discrete codes, which have been used extensively in previous works, we present mostly negative results. We then introduce the notion of continuous codes and cast the design problem of continuous codes as a constrained optimization problem. We describe three optimization problems corresponding to three different norms of the code matrix. Interestingly, for the l2 norm our formalism results in a quadratic program whose dual does not depend on the length of the code. A special case of our formalism provides a multiclass scheme for building support vector machines which can be solved efficiently. We give a time and space efficient algorithm for solving the quadratic program. We describe preliminary experiments with synthetic data show that our algorithm is often two orders of magnitude faster than standard quadratic programming packages. We conclude with the generalization properties of the algorithm.", "citation_count": "24", "reference_count": "969", "date": "2002", "authors": ["Koby Crammer", "Yoram Singer"], "related_topics": ["Quadratic programming", "Optimization problem", "Support vector machine", "Special case", "Mathematical optimization", "Learnability", "Synthetic data", "Coding (social sciences)", "Spacetime", "Mathematics"]}
{"id": "2104269704", "references": ["1601740268", "2156909104", "2119821739", "1512098439", "2124776405", "1596717185", "2148603752", "2087347434", "2489822048", "2108995755"], "title": "Kernel-based methods for hyperspectral image classification", "abstract": "This paper presents the framework of kernel-based methods in the context of hyperspectral image classification, illustrating from a general viewpoint the main characteristics of different kernel-based approaches and analyzing their properties in the hyperspectral domain. In particular, we assess performance of regularized radial basis function neural networks (Reg-RBFNN), standard support vector machines (SVMs), kernel Fisher discriminant (KFD) analysis, and regularized AdaBoost (Reg-AB). The novelty of this work consists in: 1) introducing Reg-RBFNN and Reg-AB for hyperspectral image classification; 2) comparing kernel-based methods by taking into account the peculiarities of hyperspectral images; and 3) clarifying their theoretical relationships. To these purposes, we focus on the accuracy of methods when working in noisy environments, high input dimension, and limited training sets. In addition, some other important issues are discussed, such as the sparsity of the solutions, the computational burden, and the capability of the methods to provide outputs that can be directly interpreted as probabilities.", "citation_count": "46", "reference_count": "1,465", "date": "2005", "authors": ["G. Camps-Valls", "L. Bruzzone"], "related_topics": ["Radial basis function kernel", "Kernel Fisher discriminant analysis", "Tree kernel", "Kernel (statistics)", "Kernel embedding of distributions", "Support vector machine", "Hyperspectral imaging", "Linear discriminant analysis", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence"]}
{"id": "2159680539", "references": ["2024046085", "3097096317", "2134557905", "2110764733", "1678356000", "2154422044", "2030536784", "2310919327", "2017337590", "2124386111"], "title": "Sharing Visual Features for Multiclass and Multiview Object Detection", "abstract": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the runtime cost of the classifier, is observed to scale approximately logarithmically with the number of classes. The features selected by joint training are generic edge-like features, whereas the features chosen by training each class separately tend to be more object-specific. The generic features generalize better and considerably reduce the computational cost of multiclass object detection", "citation_count": "45", "reference_count": "953", "date": "2007", "authors": ["A. Torralba", "K.P. Murphy", "W.T. Freeman"], "related_topics": ["Object detection", "Classifier (UML)", "Feature (computer vision)", "Supervised learning", "Multi-task learning", "Boosting (machine learning)", "Time complexity", "Feature selection", "Contextual image classification", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2133864802", "references": ["1570448133", "2172000360", "1576520375", "2124351082", "1975846642", "1596717185", "2148603752", "2087347434", "2966207845", "3124955340"], "title": "In Defense of One-Vs-All Classification", "abstract": "We consider the problem of multiclass classification. Our main thesis is that a simple \"one-vs-all\" scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.", "citation_count": "47", "reference_count": "2,232", "date": "2004", "authors": ["Ryan Rifkin", "Aldebaro Klautau"], "related_topics": ["Multiclass classification", "Structured support vector machine", "Support vector machine", "Machine learning", "Simple (philosophy)", "Position (vector)", "Binary number", "Scheme (programming language)", "Mathematics", "Artificial intelligence", "Experimental work"]}
{"id": "2026131661", "references": ["2154053567", "2172000360", "2047028564", "2119821739", "2109363337", "2124776405", "2084812512", "1596717185", "2111072639", "1498436455"], "title": "Extreme Learning Machine for Regression and Multiclass Classification", "abstract": "Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the \u201cgeneralized\u201d single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.", "citation_count": "52", "reference_count": "4,840", "date": "2012", "authors": ["Guang-Bin Huang", "Hongming Zhou", "Xiaojian Ding", "Rui Zhang"], "related_topics": ["Multiclass classification", "Extreme learning machine", "Support vector machine", "Binary classification", "Feature (machine learning)", "Feedforward neural network", "Kernel (statistics)", "Kernel (linear algebra)", "Pattern recognition", "Machine learning", "Regularization (mathematics)", "Least squares", "Regression analysis", "Computer science", "Artificial intelligence"]}
{"id": "2011430131", "references": ["2140190241", "2151103935", "1565377632", "2084812512", "2061901927", "1880262756", "2121947440", "2095293504", "2244663144", "1663973292"], "title": "Data clustering: 50 years beyond K-means", "abstract": "Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.", "citation_count": "144", "reference_count": "8,804", "date": "2010", "authors": ["Anil K. Jain"], "related_topics": ["Cluster analysis", "Correlation clustering", "Fuzzy clustering", "Conceptual clustering", "Canopy clustering algorithm", "CURE data clustering algorithm", "Consensus clustering", "Brown clustering", "Hierarchical clustering", "Clustering high-dimensional data", "FLAME clustering", "Biclustering", "DBSCAN", "Affinity propagation", "k-means clustering", "Hierarchical clustering of networks", "Unsupervised learning", "Supervised learning", "Feature selection", "Ranking", "Feature extraction", "Linear discriminant analysis", "Similitude", "Machine learning", "Data mining", "Artificial intelligence", "Computer science"]}
{"id": "2148143831", "references": ["2170654002", "1588282782", "2125055259", "2155653793", "1990748933", "2096942889", "1670263352", "2799061466", "2084812512", "2058732827"], "title": "SMOTE: synthetic minority over-sampling technique", "abstract": "An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.", "citation_count": "29", "reference_count": "14,620", "date": "2002", "authors": ["Nitesh V. Chawla", "Kevin W. Bowyer", "Lawrence O. Hall", "W. Philip Kegelmeyer"], "related_topics": ["Naive Bayes classifier", "Receiver operating characteristic", "Classifier (UML)", "Oversampling and undersampling in data analysis", "Prior probability", "Convex hull", "Oversampling", "Pattern recognition", "Machine learning", "Mathematics", "Artificial intelligence", "Minority class"]}
{"id": "2111072639", "references": ["2112076978", "2172000360", "2139055047", "1604591429", "2124776405", "2099579348", "2084812512", "1988115241", "3150838238"], "title": "Extreme learning machine: Theory and applications", "abstract": "Abstract   It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called  e xtreme  l earning  m achine (ELM) for  s ingle-hidden  l ayer  f eedforward neural  n etworks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.  1", "citation_count": "24", "reference_count": "10,442", "date": "2006", "authors": ["Guang-Bin Huang", "Qin-Yu Zhu", "Chee Kheong Siew"], "related_topics": ["Extreme learning machine", "Wake-sleep algorithm", "Competitive learning", "Artificial neural network", "Computational learning theory", "Stability (learning theory)", "Types of artificial neural networks", "Instance-based learning", "Online machine learning", "Feedforward neural network", "Active learning (machine learning)", "Unsupervised learning", "Learning classifier system", "Recurrent neural network", "Rprop", "Feature learning", "Intelligent control", "Time delay neural network", "Support vector machine", "Artificial intelligence", "Machine learning", "Computer science", "Generalization error"]}
{"id": "2212660284", "references": ["1510073064", "2132870739", "2170120409", "2124101779", "2143668817", "1991567646", "2084812512", "3023786531", "2337490104", "2099741732"], "title": "A kernel two-sample test", "abstract": "We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.", "citation_count": "99", "reference_count": "2,478", "date": "2012", "authors": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "related_topics": ["Test statistic", "Reproducing kernel Hilbert space", "Statistic", "Energy distance", "One- and two-tailed tests", "Kernel (statistics)", "Asymptotic distribution", "Kernel method", "Applied mathematics", "Statistics", "Mathematics"]}
{"id": "2086472796", "references": ["2171277043", "3022351671", "65738273", "1531060698", "2054658115", "2019635781", "2074797641", "2090344516", "2066366061", "1498436455"], "title": "Regularization algorithms for learning that are equivalent to multilayer networks.", "abstract": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage.", "citation_count": "19", "reference_count": "1,410", "date": "1990", "authors": ["T. Poggio", "F. Girosi"], "related_topics": ["Regularization perspectives on support vector machines", "Artificial neural network", "Basis function", "Regularization (mathematics)", "Radial basis function", "Algorithm", "Hypersurface", "Equivalence (formal languages)", "Computer science", "Regularization theory"]}
{"id": "2171277043", "references": ["2034099719", "2042264548", "2143956139", "23758216", "2150593711", "2127218421", "2072773743", "1524100745", "2084544490", "2007700211"], "title": "Fast learning in networks of locally-tuned processing units", "abstract": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use.", "citation_count": "20", "reference_count": "6,337", "date": "1989", "authors": ["John Moody", "Christian J. Darken"], "related_topics": ["Semi-supervised learning", "Unsupervised learning", "Competitive learning", "Feature learning", "Online machine learning", "Supervised learning", "Stability (learning theory)", "Instance-based learning", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2266946488", "references": ["2003949305", "2167485994", "1883186006", "2141870784", "2099968818", "2159687282", "2162409952", "2087347434", "2049981393", "3144673228"], "title": "Linear and nonlinear programming", "abstract": "This new edition covers the central concepts of practical optimization techniques, with an emphasis on methods that are both state-of-the-art and popular. One major insight is the connection between the purely analytical character of an optimization problem and the behavior of algorithms used to solve a problem. This was a major theme of the first edition of this book and the fourth edition expands and further illustrates this relationship. As in the earlier editions, the material in this fourth edition is organized into three separate parts. Part I is a self-contained introduction to linear programming. The presentation in this part is fairly conventional, covering the main elements of the underlying theory of linear programming, many of the most effective numerical algorithms, and many of its important special applications. Part II, which is independent of Part I, covers the theory of unconstrained optimization, including both derivations of the appropriate optimality conditions and an introduction to basic algorithms. This part of the book explores the general properties of algorithms and defines various notions of convergence. Part III extends the concepts developed in the second part to constrained optimization problems. Except for a few isolated sections, this part is also independent of Part I. It is possible to go directly into Parts II and III omitting Part I, and, in fact, the book has been used in this way in many universities.New to this edition is a chapter devoted to Conic Linear Programming, a powerful generalization of Linear Programming. Indeed, many conic structures are possible and useful in a variety of applications. It must be recognized, however, that conic linear programming is an advanced topic, requiring special study. Another important topic is an accelerated steepest descent method that exhibits superior convergence properties, and for this reason, has become quite popular. The proof of the convergence property for both standard and accelerated steepest descent methods are presented in Chapter 8. As in previous editions, end-of-chapter exercises appear for all chapters.From the reviews of the Third Edition: this very well-written book is a classic textbook in Optimization. It should be present in the bookcase of each student, researcher, and specialist from the host of disciplines from which practical optimization applications are drawn. (Jean-Jacques Strodiot, Zentralblatt MATH, Vol. 1207, 2011)", "citation_count": "0", "reference_count": "8,333", "date": "1984", "authors": ["David G. Luenberger", "Yinyu Ye"], "related_topics": ["Nonlinear programming", "Linear programming", "Optimization problem", "Conic section", "Generalization", "Gradient descent", "Method of steepest descent", "Calculus", "Property (philosophy)", "Computer science", "Statistics"]}
{"id": "2076118331", "references": ["2042264548", "2154642048", "2103496339", "2102201073", "2049633694", "2137983211", "2147800946", "1594031697", "1498436455"], "title": "Neural networks and the bias/variance dilemma", "abstract": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.", "citation_count": "90", "reference_count": "4,348", "date": "1992", "authors": ["Stuart Geman", "Elie Bienenstock", "Ren\u00e9 Doursat"], "related_topics": ["Feedforward neural network", "Time delay neural network", "Types of artificial neural networks", "Artificial neural network", "Recurrent neural network", "Catastrophic interference", "Neural modeling fields", "Probabilistic neural network", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "9657784", "references": ["2144906988", "1974314988", "2293768274", "2112507308", "2296452361", "2151298633", "2082190528", "1618905105", "2114296159", "1667072054"], "title": "Evasion attacks against machine learning at test time", "abstract": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "citation_count": "24", "reference_count": "963", "date": "2013", "authors": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "related_topics": ["Pre-play attack", "Adversarial machine learning", "Malware", "Support vector machine", "Classifier (UML)", "Statistical classification", "Artificial neural network", "Adversary", "Machine learning", "Computer science", "Computer security", "Artificial intelligence"]}
{"id": "2115252128", "references": ["2142623206", "1494068061", "2153635508", "1618905105", "1486089539", "2154462399", "2153290280", "1633751774"], "title": "Dlib-ml: A Machine Learning Toolkit", "abstract": "There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.", "citation_count": "9", "reference_count": "2,205", "date": "2009", "authors": ["Davis E. King"], "related_topics": ["Python (programming language)", "Design by contract", "Debugging", "Software", "Cluster analysis", "MATLAB", "Bayesian network", "Documentation", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2964212410", "references": ["2948069880", "2970121940", "2912070915", "2912237282", "2904981516", "2962816513", "2964309657", "2995085126", "2970859221"], "title": "On calibration of modern neural networks", "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.", "citation_count": "0", "reference_count": "1,309", "date": "2017", "authors": ["Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger"], "related_topics": ["Platt scaling", "Artificial neural network", "Document classification", "Normalization (statistics)", "Correctness", "Machine learning", "Calibration", "Computer science", "Image (mathematics)", "Artificial intelligence", "Temperature scaling"]}
{"id": "1648445109", "references": ["2156909104", "2102201073", "1604938182", "1554663460", "2148603752", "2078204800", "2117812871", "2087347434", "1618905105", "1988520084"], "title": "Sparse bayesian learning and the relevance vector machine", "abstract": "This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.", "citation_count": "38", "reference_count": "6,820", "date": "2001", "authors": ["Michael E. Tipping"], "related_topics": ["Relevance vector machine", "Variable-order Bayesian network", "Wake-sleep algorithm", "Support vector machine", "Bayesian inference", "Bayesian probability", "Probabilistic logic", "Basis function", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2115150266", "references": ["3097096317", "2129305389", "2161969291", "2535410496", "2144794286", "2110158442", "1618905105", "2168356304", "2162915993", "2162762921"], "title": "Semantic segmentation using regions and parts", "abstract": "We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.", "citation_count": "36", "reference_count": "334", "date": "2012", "authors": ["Pablo Arbelaez", "Bharath Hariharan", "Chunhui Gu", "Saurabh Gupta", "Lubomir Bourdev", "Jitendra Malik"], "related_topics": ["Image segmentation", "Contextual image classification", "Segmentation", "Object detection", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Pascal (programming language)", "Computer vision", "Computer science", "Image resolution", "Artificial intelligence"]}
{"id": "1999954155", "references": ["1570448133", "2149706766", "2052684427", "2032210760", "2146241755", "1618905105", "2053463056", "1565746575", "2133990480", "2912934387"], "title": "Classifier chains for multi-label classification", "abstract": "The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.", "citation_count": "42", "reference_count": "2,317", "date": "2011", "authors": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "related_topics": ["Classifier chains", "Multi-label classification", "Time complexity", "Ensemble learning", "Computational complexity theory", "Chaining", "Scalability", "Relevance (information retrieval)", "Machine learning", "Data mining", "Computer science", "Artificial intelligence"]}
{"id": "2125055259", "references": ["2112076978", "2153010521", "607505555", "2142827986", "2149684865", "2017337590", "1484413656", "2148143831", "2132549764"], "title": "C4.5: Programs for Machine Learning", "abstract": "From the Publisher:\r\nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation.\r\n\r\nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.\r\n\r\nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.", "citation_count": "0", "reference_count": "27,075", "date": "1992", "authors": ["J. Ross Quinlan"], "related_topics": ["ID3 algorithm", "Intelligent decision support system", "C4.5 algorithm", "Rule induction", "Source code", "Incremental decision tree", "Expert system", "Decision stump", "Computer science", "Machine learning", "Artificial intelligence"]}
{"id": "2161920802", "references": ["2610857016", "2156909104", "2132870739", "2119821739", "2139212933", "1604938182", "2087347434", "2140095548", "2798766386", "2108995755"], "title": "New Support Vector Algorithms", "abstract": "We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter \u03bd lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter epsilon in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of \u03bd, and report experimental results.", "citation_count": "51", "reference_count": "3,452", "date": "2000", "authors": ["Bernhard Sch\u00f6lkopf", "Alex J. Smola", "Robert C. Williamson", "Peter L. Bartlett"], "related_topics": ["Support vector machine", "Free parameter", "Regularization (mathematics)", "Algorithm", "Regression analysis", "Lagrange multiplier", "Lipschitz continuity", "Regression", "Hilbert space", "Mathematics"]}
{"id": "2119479037", "references": ["2167277498", "2109363337", "2143426320", "2157795344", "2017337590", "2148603752", "2087347434", "2103333826", "1594031697", "3023786531"], "title": "An introduction to variable and feature selection", "abstract": "Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.", "citation_count": "47", "reference_count": "17,391", "date": "2003", "authors": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "related_topics": ["Feature selection", "Feature (computer vision)", "Minimum redundancy feature selection", "Word processing", "Model selection", "Variables", "Variable (computer science)", "Text processing", "Machine learning", "Data mining", "Computer science", "Artificial intelligence"]}
{"id": "2096729078", "references": ["1575210522", "2059975159", "1985953075", "1605923270", "2016381774", "2063060349", "2039846678", "2114779636", "2005685204", "2050456292"], "title": "Chemical Similarity Searching", "abstract": "This paper reviews the use of similarity searching in chemical databases. It begins by introducing the concept of similarity searching, differentiating it from the more common substructure searching, and then discusses the current generation of fragment-based measures that are used for searching chemical structure databases. The next sections focus upon two of the principal characteristics of a similarity measure:\u2009 the coefficient that is used to quantify the degree of structural resemblance between pairs of molecules and the structural representations that are used to characterize molecules that are being compared in a similarity calculation. New types of similarity measure are then compared with current approaches, and examples are given of several applications that are related to similarity searching.", "citation_count": "91", "reference_count": "2,015", "date": "1998", "authors": ["Peter Willett", "John M. Barnard and", "Geoffrey M. Downs"], "related_topics": ["Chemical similarity", "Similarity measure", "Similarity (network science)", "Semantic similarity", "Chemical database", "Data mining", "Fragment (logic)", "Focus (optics)", "Current (mathematics)", "Mathematics"]}
{"id": "1840338487", "references": ["2477400917", "2046079134", "2125055259", "2132166479", "2129476886", "2170112109", "1625504505", "2076118331", "1594031697", "2912934387"], "title": "On Bias, Variance, 0/1\u2014Loss, and the Curse-of-Dimensionality", "abstract": "The classification problem is considered in which an output variable y assumes discrete values with respective probabilities that depend upon the simultaneous values of a set of input variables x = {x_1,....,x_n}. At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule. These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification. This can dramatically mitigate the effect of the bias associated with some simple estimators like \u201cnaive\u201d Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures. This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification, and why \u201cbagging/aggregating\u201d classifiers can often improve accuracy. These results also suggest simple modifications to these procedures that can (sometimes dramatically) further improve their classification performance.", "citation_count": "20", "reference_count": "1,313", "date": "1997", "authors": ["Jerome H. Friedman"], "related_topics": ["Bayes error rate", "Classification rule", "Mean squared error", "Estimator", "Naive Bayes classifier", "Curse of dimensionality", "Variance (accounting)", "Bayes' theorem", "Statistics", "Mathematics"]}
{"id": "2005685204", "references": ["2046311259", "1963790256", "2069026105", "2007570703", "2056838301", "2000376564", "1986139296", "2069530087", "1978184203", "2046344452"], "title": "Use of Structure\u2212Activity Data To Compare Structure-Based Clustering Methods and Descriptors for Use in Compound Selection", "abstract": "An evaluation of a variety of structure-based clustering methods for use in compound selection is presented. The use of MACCS, Unity and Daylight 2D descriptors; Unity 3D rigid and flexible descriptors and two in-house 3D descriptors based on potential pharmacophore points, are considered. The use of Ward's and group-average hierarchical agglomerative, Guenoche hierarchical divisive, and Jarvis\u2212Patrick nonhierarchical clustering methods are compared. The results suggest that 2D descriptors and hierarchical clustering methods are best at separating biologically active molecules from inactives, a prerequisite for a good compound selection method. In particular, the combination of MACCS descriptors and Ward's clustering was optimal.", "citation_count": "17", "reference_count": "772", "date": "1996", "authors": ["and Robert D. Brown", "Yvonne C. Martin"], "related_topics": ["Hierarchical clustering", "Cluster analysis", "Selection (genetic algorithm)", "Pattern recognition", "Data mining", "Pharmacophore", "Structure (mathematical logic)", "Mathematics", "3d descriptors", "Artificial intelligence", "Selection method", "Structure based"]}
{"id": "2133199783", "references": ["2156909104", "1528905581", "2109363337", "2147246240", "191383808", "1989076816", "2087684630", "2148694408", "1973948212"], "title": "Tumor classification by partial least squares using microarray gene expression data", "abstract": "Motivation: One important application of gene expression microarray data is classification of samples into categories, such as the type of tumor. The use of microarrays allows simultaneous monitoring of thousands of genes expressions per sample. This ability to measure gene expression en masse has resulted in data with the number of variables p (genes) far exceeding the number of samples N . Standard statistical methodologies in classification and prediction do not work well or even at all when N &lt; p. Modification of existing statistical methodologies or development of new methodologies is needed for the analysis of microarray data. Results: We propose a novel analysis procedure for classifying (predicting) human tumor samples based on microarray gene expressions. This procedure involves dimension reduction using Partial Least Squares (PLS) and classification using Logistic Discrimination (LD) and Quadratic Discriminant Analysis (QDA). We compare PLS to the well known dimension reduction method of Principal Components Analysis (PCA). Under many circumstances PLS proves superior; we illustrate a condition when PCA particularly fails to predict well relative to PLS. The proposed methods were applied to five different microarray data sets involving various human tumor samples: (1) normal versus ovarian tumor; (2) Acute Myeloid Leukemia (AML) versus Acute Lymphoblastic Leukemia (ALL); (3) Diffuse Large B-cell Lymphoma (DLBCLL) versus B-cell Chronic Lymphocytic Leukemia (BCLL); (4) normal versus colon tumor; and (5) Non-SmallCell-Lung-Carcinoma (NSCLC) versus renal samples. Stability of classification results and methods were further assessed by re-randomization studies. Availability: The methodology can be implemented using a combination of standard statistical methods, available, for example, in SAS. Illustrative SAS code is available from the first author.", "citation_count": "28", "reference_count": "1,055", "date": "2002", "authors": ["Danh V. Nguyen", "David M. Rocke"], "related_topics": ["Partial least squares regression", "Quadratic classifier", "Linear discriminant analysis", "Microarray analysis techniques", "Principal component analysis", "DNA microarray", "Dimensionality reduction", "Microarray", "Computational biology", "Statistics", "Biology"]}
{"id": "2134262590", "references": ["2135463994", "2033419168", "2994340921", "1770825568", "2130259898", "2121647436", "2123977795", "2128716185", "2012352340", "2138451337"], "title": "PCA versus LDA", "abstract": "In the context of the appearance-based paradigm for object recognition, it is generally believed that algorithms based on LDA (linear discriminant analysis) are superior to those based on PCA (principal components analysis). In this communication, we show that this is not always the case. We present our case first by using intuitively plausible arguments and, then, by showing actual results on a face database. Our overall conclusion is that when the training data set is small, PCA can outperform LDA and, also, that PCA is less sensitive to different training data sets.", "citation_count": "18", "reference_count": "3,884", "date": "2001", "authors": ["A.M. Martinez", "A.C. Kak"], "related_topics": ["Linear discriminant analysis", "Principal component analysis", "Context (language use)", "Facial recognition system", "Pattern recognition (psychology)", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Face (geometry)", "Set (abstract data type)", "Machine learning", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "1963932623", "references": ["2112447569", "2113606819", "2129131372", "2162915993", "2160547390", "2097018403", "2153663612", "2127271355", "2129812935", "2027922120"], "title": "Label Consistent K-SVD: Learning a Discriminative Dictionary for Recognition", "abstract": "A label consistent K-SVD (LC-KSVD) algorithm to learn a discriminative dictionary for sparse coding is presented. In addition to using class labels of training data, we also associate label information with each dictionary item (columns of the dictionary matrix) to enforce discriminability in sparse codes during the dictionary learning process. More specifically, we introduce a new label consistency constraint called \"discriminative sparse-code error\" and combine it with the reconstruction error and the classification error to form a unified objective function. The optimal solution is efficiently obtained using the K-SVD algorithm. Our algorithm learns a single overcomplete dictionary and an optimal linear classifier jointly. The incremental dictionary learning algorithm is presented for the situation of limited memory resources. It yields dictionaries so that feature points with the same class labels have similar sparse codes. Experimental results demonstrate that our algorithm outperforms many recently proposed sparse-coding techniques for face, action, scene, and object category recognition under the same learning conditions.", "citation_count": "60", "reference_count": "1,118", "date": "2013", "authors": ["Zhuolin Jiang", "Zhe Lin", "L. S. Davis"], "related_topics": ["K-SVD", "Supervised learning", "Linear classifier", "Statistical classification", "Discriminative model", "Feature (machine learning)", "k-means clustering", "Contextual image classification", "Facial recognition system", "Singular value decomposition", "Pattern recognition", "Speech recognition", "Computer science", "Matrix (mathematics)", "Artificial intelligence", "Training set"]}
{"id": "3102431071", "references": ["2618530766", "2136922672", "2161969291", "2151103935", "2109255472", "2031489346", "2310919327", "3118608800", "2117539524", "2963911037"], "title": "PCANet: A Simple Deep Learning Baseline for Image Classification?", "abstract": "In this paper, we propose a very simple deep learning network for image classification that is based on very basic data processing components: 1) cascaded principal component analysis (PCA); 2) binary hashing; and 3) blockwise histograms. In the proposed architecture, the PCA is employed to learn multistage filter banks. This is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus called the PCA network (PCANet) and can be extremely easily and efficiently designed and learned. For comparison and to provide a better understanding, we also introduce and study two simple variations of PCANet: 1) RandNet and 2) LDANet. They share the same topology as PCANet, but their cascaded filters are either randomly selected or learned from linear discriminant analysis. We have extensively tested these basic networks on many benchmark visual data sets for different tasks, including Labeled Faces in the Wild (LFW) for face verification; the MultiPIE, Extended Yale B, AR, Facial Recognition Technology (FERET) data sets for face recognition; and MNIST for hand-written digit recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state-of-the-art features either prefixed, highly hand-crafted, or carefully learned [by deep neural networks (DNNs)]. Even more surprisingly, the model sets new records for many classification tasks on the Extended Yale B, AR, and FERET data sets and on MNIST variations. Additional experiments on other public data sets also demonstrate the potential of PCANet to serve as a simple but highly competitive baseline for texture classification and object recognition.", "citation_count": "63", "reference_count": "1,193", "date": "2015", "authors": ["Tsung-Han Chan", "Kui Jia", "Shenghua Gao", "Jiwen Lu", "Zinan Zeng", "Yi Ma"], "related_topics": ["MNIST database", "Convolutional neural network", "Facial recognition system", "Deep learning", "Contextual image classification", "Feature extraction", "Linear discriminant analysis", "Cognitive neuroscience of visual object recognition", "Pattern recognition", "Principal component analysis", "Histogram", "Hash function", "Computer science", "Artificial intelligence"]}
{"id": "2102544846", "references": ["2135463994", "2130972944", "2994340921", "2130259898", "2121647436", "2120954940", "2146474141", "2021012145", "2138451337"], "title": "Two-dimensional PCA: a new approach to appearance-based face representation and recognition", "abstract": "In this paper, a new technique coined two-dimensional principal component analysis (2DPCA) is developed for image representation. As opposed to PCA, 2DPCA is based on 2D image matrices rather than 1D vectors so the image matrix does not need to be transformed into a vector prior to feature extraction. Instead, an image covariance matrix is constructed directly using the original image matrices, and its eigenvectors are derived for image feature extraction. To test 2DPCA and evaluate its performance, a series of experiments were performed on three face image databases: ORL, AR, and Yale face databases. The recognition rate across all trials was higher using 2DPCA than PCA. The experimental results also indicated that the extraction of image features is computationally more efficient using 2DPCA than PCA.", "citation_count": "19", "reference_count": "4,559", "date": "2004", "authors": ["Jian Yang", "D. Zhang", "A.F. Frangi", "Jing-yu Yang"], "related_topics": ["Feature detection (computer vision)", "Feature extraction", "Feature (computer vision)", "Eigenface", "Facial recognition system", "Principal component analysis", "Multilinear subspace learning", "Multilinear principal component analysis", "Covariance matrix", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2912990735", "references": ["2134262590", "2152826865", "2125127226", "2994340921", "2082308025", "2121647436", "2123921160", "1578352865", "1997011019", "2138451337"], "title": "Multi-PIE", "abstract": "A close relationship exists between the advancement of face recognition algorithms and the availability of face databases varying factors that affect facial appearance in a controlled manner. The CMU PIE database has been very influential in advancing research in face recognition across pose and illumination. Despite its success the PIE database has several shortcomings: a limited number of subjects, a single recording session and only few expressions captured. To address these issues we collected the CMU Multi-PIE database. It contains 337 subjects, imaged under 15 view points and 19 illumination conditions in up to four recording sessions. In this paper we introduce the database and describe the recording procedure. We furthermore present results from baseline experiments using PCA and LDA classifiers to highlight similarities and differences between PIE and Multi-PIE.", "citation_count": "15", "reference_count": "2,399", "date": "2010", "authors": ["Ralph Gross", "Iain Matthews", "Jeffrey Cohn", "Takeo Kanade", "Simon Baker"], "related_topics": ["Facial recognition system", "Face (geometry)", "Session (computer science)", "Speech recognition", "Affect (psychology)", "Baseline (configuration management)", "Computer science", "Close relationship", "Facial appearance"]}
{"id": "2132467081", "references": ["2050834445", "2097018403", "2160547390", "2135046866", "2126607811", "2123921160", "2100556411", "2129812935", "2798909945", "2536599074"], "title": "Sparse representation or collaborative representation: Which helps face recognition?", "abstract": "As a recently proposed technique, sparse representation based classification (SRC) has been widely used for face recognition (FR). SRC first codes a testing sample as a sparse linear combination of all the training samples, and then classifies the testing sample by evaluating which class leads to the minimum representation error. While the importance of sparsity is much emphasized in SRC and many related works, the use of collaborative representation (CR) in SRC is ignored by most literature. However, is it really the l 1 -norm sparsity that improves the FR accuracy? This paper devotes to analyze the working mechanism of SRC, and indicates that it is the CR but not the l1-norm sparsity that makes SRC powerful for face classification. Consequently, we propose a very simple yet much more efficient face classification scheme, namely CR based classification with regularized least square (CRC_RLS). The extensive experiments clearly show that CRC_RLS has very competitive classification results, while it has significantly less complexity than SRC.", "citation_count": "29", "reference_count": "2,101", "date": "2011", "authors": ["Lei Zhang", "Meng Yang", "Xiangchu Feng"], "related_topics": ["Sparse approximation", "Contextual image classification", "Facial recognition system", "Pattern recognition", "Norm (mathematics)", "Machine learning", "Computer science", "Least squares", "Artificial intelligence"]}
{"id": "2046079134", "references": ["2606886058", "1545214528", "2125798790"], "title": "The Self-Organizing Map", "abstract": "Abstract   An overview of the self-organizing map algorithm, on which the papers in this issue are based, is presented in this article.", "citation_count": "3", "reference_count": "12,785", "date": "1998", "authors": ["Teuvo Kohonen"], "related_topics": ["Self-organizing map", "Growing self-organizing map", "Unsupervised learning", "Learning vector quantization", "Artificial neural network", "Cluster analysis", "Artificial intelligence", "Difference-map algorithm", "Machine learning", "Computer science", "Self"]}
{"id": "2113341759", "references": ["2090196588", "2135463994", "2143956139", "1548502347", "2086479969", "2032361618", "1770825568", "2135346934", "2055712799", "2138451337"], "title": "Face recognition: features versus templates", "abstract": "Two new algorithms for computer recognition of human faces, one based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second based on almost-gray-level template matching, are presented. The results obtained for the testing sets show about 90% correct recognition using geometrical features and perfect recognition using template matching. &gt;", "citation_count": "28", "reference_count": "4,190", "date": "1993", "authors": ["R. Brunelli", "T. Poggio"], "related_topics": ["Three-dimensional face recognition", "3D single-object recognition", "Template matching", "Facial recognition system", "Pattern recognition (psychology)", "Template", "Pattern recognition", "Set (abstract data type)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "1770825568", "references": ["1989702938", "2121601095", "2067191022", "2136040699", "2132103241", "2132549764", "2108995755", "2161160262", "2136251662", "1992419399"], "title": "Introduction to statistical pattern recognition (2nd ed.)", "abstract": "", "citation_count": "0", "reference_count": "7,085", "date": "1990", "authors": ["Keinosuke Fukunaga"], "related_topics": ["Computer science", "Pattern recognition", "Artificial intelligence", "Statistical pattern"]}
{"id": "1679913846", "references": ["2169064301", "2122646361", "1998025025", "2135187880", "2150926065", "2157795344", "2117812871", "2132549764", "1501500081", "2097645701"], "title": "Self-Organizing Maps", "abstract": "The Self-Organising Map (SOM) algorithm was introduced by the author in 1981. Its theory and many applications form one of the major approaches to the contemporary artificial neural networks field, and new technologies have already been based on it. The most important practical applications are in exploratory data analysis, pattern recognition, speech analysis, robotics, industrial and medical diagnostics, instrumentation, and control, and literally hundreds of other tasks. In this monograph the mathematical preliminaries, background, basic ideas, and implications are expounded in a manner which is accessible without prior expert knowledge.", "citation_count": "0", "reference_count": "23,381", "date": "1995", "authors": ["Teuvo Kohonen"], "related_topics": ["Neural gas", "Growing self-organizing map", "Self-organizing map", "Instrumentation (computer programming)", "Nervous system network models", "Pattern recognition (psychology)", "Field (computer science)", "Generative topographic map", "Data science", "Artificial intelligence", "Computer science"]}
{"id": "2159173611", "references": ["2798461040", "1981367467", "2098947662", "2159686933", "2049633694", "2138313032", "2138451337", "1973436000", "2148694408", "2157418942"], "title": "Probabilistic visual learning for object detection", "abstract": "We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distributions) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands. &gt;", "citation_count": "12", "reference_count": "558", "date": "1995", "authors": ["B. Moghaddam", "A. Pentland"], "related_topics": ["Unsupervised learning", "Object detection", "Density estimation", "Visual learning", "Cognitive neuroscience of visual object recognition", "Visual search", "Probabilistic logic", "Multivariate normal distribution", "Multivariate statistics", "Pattern recognition", "Computer vision", "Computer science", "Artificial intelligence", "Training set"]}
{"id": "2051719061", "references": ["2042986967", "1991848143", "2055569927", "2023810448", "3142300713", "2581275558", "2022306346", "2070097792", "307896644", "2017811812"], "title": "An analogue approach to the travelling salesman problem using an elastic net method", "abstract": "The travelling salesman problem is a classical problem in the field of combinatorial optimization, concerned with efficient methods for maximizing or minimizing a function of many independent variables. Given the positions of N cities, which in the simplest case lie in the plane, what is the shortest closed tour in which each city can be visited once? We describe how a parallel analogue algorithm, derived from a formal model for the establishment of topographically ordered projections in the brain, can be applied to the travelling salesman problem. Using an iterative procedure, a circular closed path is gradually elongated non-uniformly until it eventually passes sufficiently near to all the cities to define a tour. This produces shorter tour lengths than another recent parallel analogue algorithm, scales well with the size of the problem, and is naturally extendable to a large class of optimization problems involving topographic mappings between geometrical structures.", "citation_count": "19", "reference_count": "1,128", "date": "1987", "authors": ["Richard Durbin", "David Willshaw"], "related_topics": ["Lin\u2013Kernighan heuristic", "Travelling salesman problem", "Bottleneck traveling salesman problem", "2-opt", "Quadratic assignment problem", "Combinatorial optimization", "Extremal optimization", "Optimization problem", "Applied mathematics", "Computer science"]}
{"id": "2135463994", "references": ["2565264375", "2798461040", "1633869374", "147723833", "1770825568", "2130259898", "2051366222", "1509349128", "1535031115", "1507699566"], "title": "Application of the Karhunen-Loeve procedure for the characterization of human faces", "abstract": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. &gt;", "citation_count": "19", "reference_count": "3,932", "date": "1990", "authors": ["M. Kirby", "L. Sirovich"], "related_topics": ["Eigenface", "Covariance matrix", "Basis (linear algebra)", "Mirror image", "Face (geometry)", "Karhunen\u2013Lo\u00e8ve theorem", "Symmetry (geometry)", "Data set", "Algorithm", "Combinatorics", "Artificial intelligence", "Mathematics"]}
{"id": "2079948225", "references": ["2024390075", "2326003143", "1491926324", "2046384002", "2017558200", "2056934252", "2048330959", "2032533296", "1981025738", "1864836097"], "title": "Oscillatory responses in cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties", "abstract": "A FUNDAMENTAL step in visual pattern recognition is the establishment of relations between spatially separate features. Recently, we have shown that neurons in the cat visual cortex have oscillatory responses in the range 40\u201360 Hz (refs 1,2) which occur in synchrony for cells in a functional column and are tightly correlated with a local oscillatory field potential. This led us to hypothesize that the synchronization of oscillatory responses of spatially distributed, feature selective cells might be a way to establish relations between features in different parts of the visual field2,3. In support of this hypothesis, we demonstrate here that neurons in spatially separate columns can synchronize their oscillatory responses. The synchronization has, on average, no phase difference, depends on the spatial separation and the orientation preference of the cells and is influenced by global stimulus properties.", "citation_count": "22", "reference_count": "5,089", "date": "1989", "authors": ["Charles M. Gray", "Peter K\u00f6nig", "Andreas K. Engel", "Wolf Singer"], "related_topics": ["Visual cortex", "Receptive field", "Stimulus (physiology)", "Binding problem", "Visual perception", "ReFS", "Electrophysiology", "Synchronization", "Neuroscience", "Physics", "Anatomy"]}
{"id": "3144368627", "references": ["2162366870", "2101508170", "2100970777", "1850405760", "2099529102", "1499934958", "2095757522", "2166029537", "2598646247"], "title": "Communicating Sequential Processes", "abstract": "", "citation_count": "0", "reference_count": "15,259", "date": "1985", "authors": ["Tony Hoare"], "related_topics": ["Communicating sequential processes", "JCSP", "Algebra of Communicating Processes", "Calculus of communicating systems", "occam", "Process calculus", "Actor model and process calculi", "Computer science", "Concurrent ML", "Distributed computing"]}
{"id": "2011039300", "references": ["3098915991", "1660562555", "2070722739", "2038651258", "2130531694", "2095293504", "2137813581", "1594031697"], "title": "Computers and Intractability: A Guide to the Theory of NP-Completeness", "abstract": "", "citation_count": "0", "reference_count": "66,091", "date": "1979", "authors": ["Michael R. Garey", "David S. Johnson"], "related_topics": ["Cook\u2013Levin theorem", "Completeness (order theory)", "NP", "Karp's 21 NP-complete problems", "MAX-3SAT", "Boolean hierarchy", "Complete", "co-NP-complete", "Computer science", "Theoretical computer science", "Algorithm"]}
{"id": "1914401667", "references": ["2103083704", "2138100172", "2171074980", "2075165262", "2069696140", "2003370853", "2141572998", "2319041464", "2116360511", "2017600612"], "title": "An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex", "abstract": "1. Using the two-dimensional (2D) spatial and spectral response profiles described in the previous two reports, we test Daugman's generalization of Marcelja's hypothesis that simple receptive fields belong to a class of linear spatial filters analogous to those described by Gabor and referred to here as 2D Gabor filters. 2. In the space domain, we found 2D Gabor filters that fit the 2D spatial response profile of each simple cell in the least-squared error sense (with a simplex algorithm), and we show that the residual error is devoid of spatial structure and statistically indistinguishable from random error. 3. Although a rigorous statistical approach was not possible with our spectral data, we also found a Gabor function that fit the 2D spectral response profile of each simple cell and observed that the residual errors are everywhere small and unstructured. 4. As an assay of spatial linearity in two dimensions, on which the applicability of Gabor theory is dependent, we compare the filter parameters estimated from the independent 2D spatial and spectral measurements described above. Estimates of most parameters from the two domains are highly correlated, indicating that assumptions about spatial linearity are valid. 5. Finally, we show that the functional form of the 2D Gabor filter provides a concise mathematical expression, which incorporates the important spatial characteristics of simple receptive fields demonstrated in the previous two reports. Prominent here are 1) Cartesian separable spatial response profiles, 2) spatial receptive fields with staggered subregion placement, 3) Cartesian separable spectral response profiles, 4) spectral response profiles with axes of symmetry not including the origin, and 5) the uniform distribution of spatial phase angles. 6. We conclude that the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields. Thus it seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains.", "citation_count": "22", "reference_count": "2,232", "date": "1987", "authors": ["J. P. Jones", "L. A. Palmer"], "related_topics": ["Gabor filter", "Spatial frequency", "Simple cell", "Filter (signal processing)", "Receptive field", "Function (mathematics)", "Uniform distribution (continuous)", "Residual", "Algorithm", "Mathematics", "Communication"]}
{"id": "2105644991", "references": ["2147880316", "740415", "2156909104", "2962735828", "2139686264", "2159080219", "2157791002", "2098678088", "2798766386", "1513861746"], "title": "Max-Margin Markov Networks", "abstract": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.", "citation_count": "17", "reference_count": "1,780", "date": "2003", "authors": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller"], "related_topics": ["Structured support vector machine", "Structured prediction", "Graphical model", "Support vector machine", "Markov chain", "Kernel (linear algebra)", "Quadratic programming", "Classifier (UML)", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "2175582831", "references": ["2015861736", "2161381512", "2145287260", "1905829557", "1970456555", "2828022637", "1974744233", "2047875689", "2305141285", "1990315422"], "title": "Synergistic face detection and pose estimation with energy-based models", "abstract": "We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single configuration, on three standard data sets - one for frontal pose, one for rotated faces, and one for profiles - and find that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system's accuracy on both face detection and pose estimation is improved by training for the two tasks together.", "citation_count": "0", "reference_count": "79", "date": "2006", "authors": ["Margarita Osadchy", "Yann LeCun", "Matthew L. Miller"], "related_topics": ["3D pose estimation", "Pose", "Face detection", "Facial recognition system", "Face (geometry)", "Image processing", "Computer vision", "Set (abstract data type)", "Image (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "2137813581", "references": ["1593793857", "2150218618", "2135764410", "2125838338", "2171265988", "7241855", "2129031807", "2121606987", "1559536185", "2011039300"], "title": "Factor graphs and the sum-product algorithm", "abstract": "Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of \"local\" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative \"turbo\" decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms.", "citation_count": "29", "reference_count": "7,543", "date": "2001", "authors": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "related_topics": ["Factor graph", "Forward algorithm", "Belief propagation", "Prime-factor FFT algorithm", "Ramer\u2013Douglas\u2013Peucker algorithm", "Search algorithm", "Weighted Majority Algorithm", "Reverse-delete algorithm", "Suurballe's algorithm", "In-place algorithm", "FSA-Red Algorithm", "Push\u2013relabel maximum flow algorithm", "Output-sensitive algorithm", "Difference-map algorithm", "Competitive analysis", "Floyd\u2013Warshall algorithm", "Dinic's algorithm", "Viterbi algorithm", "Graph theory", "Bipartite graph", "Tanner graph", "Bayesian network", "Turbo code", "Hidden Markov model", "Viterbi decoder", "Expectation propagation", "Factorization", "Decoding methods", "Theoretical computer science", "Computer science"]}
{"id": "2032361618", "references": ["2575521957", "1985246210", "2020839919", "2027406879", "143884444", "2050908357", "34639563", "32941422"], "title": "Automatic extraction of face-features", "abstract": "Abstract   We describe software whose ultimate aim is to make measurements from a grey-scale image of a face. Cognitive psychologists have investigated human face recognition: we combine these insights with pattern recognition methods for computer face recognition. An application for our programs is in mug-shot retrieval; a subject sees a face, and is subsequently asked to guide a search for it from a large face database by providing a number of judgemental ratings. As each face is added to the database, 37 measurements of size and shape have to be made. Our software is designed to automate much of this. We describe algorithms used for extracting facial features such as head outline, location of eyes, eyebrows and mouth from a grey-scale image of the face. The methods involve first obtaining curves from edge detectors, and then combining and inducing them as necessary. Our approach is in part a multiresolution one; knowledge of appropriate positions of features at a given resolution is used to guide searches at finer resolutions. The results so far are encouraging; some measurements can be made accurately with a high probability of success, while failure, when it occurs, is recognised as such.", "citation_count": "8", "reference_count": "387", "date": "1987", "authors": ["I. Craw", "H. Elis", "J. R. Lishman"], "related_topics": ["Three-dimensional face recognition", "Face detection", "Facial recognition system", "Pattern recognition (psychology)", "Face (geometry)", "Computer vision", "Pattern recognition", "Computer science", "Enhanced Data Rates for GSM Evolution", "Resolution (logic)", "Artificial intelligence"]}
{"id": "1509703770", "references": ["3036751298", "2004417889", "2161070585", "2062104878", "2047057213", "1507849272", "2070862086", "2176028050", "2138451337", "2072128103"], "title": "Parallel Models of Associative Memory", "abstract": "Contents: G.E. Hinton, J.A. Anderson, Introduction to the Updated Edition. D.E. Rumelhart, D.A. Norman, Introduction. J.A. Anderson, G.E. Hinton, Models of Information Processing in the Brain. J.A. Feldman, A Connectionist Model of Visual Memory. D. Willshaw, Holography, Associative Memory, and Inductive Generalization. T. Kohonen, E. Oja, P. Lehtio, Storage and Processing of Information in Distributed Associative Memory Systems. S.E. Fahlman, Representing Implicit Knowledge. G.E. Hinton, Implementing Semantic Networks in Parallel Hardware. T.J. Sejnowski, Skeleton Filters in the Brain. J.A. Anderson, M.C. Mozer, Categorization and Selective Neurons. S. Geman, Notes on a Self-Organizing Machine. R. Ratcliff, Parallel-Processing Mechanisms and Processing of Organized Information in Human Memory.", "citation_count": "0", "reference_count": "1,263", "date": "1989", "authors": ["Geoffrey E. Hinton", "James A. Anderson"], "related_topics": ["Content-addressable memory", "Visual memory", "Connectionism", "Information processing", "Semantic network", "Generalization", "Self-organizing map", "Artificial intelligence", "Categorization", "Computer science"]}
{"id": "2125848778", "references": ["2051719061", "2107198582", "2082206048", "2164741953", "1574225613", "1481387016", "1977699267", "2104095591", "2740373864", "2045798786"], "title": "Feature extraction from faces using deformable templates", "abstract": "A method for detecting and describing the features of faces using deformable templates is described. The feature of interest, an eye for example, is described by a parameterized template. An energy function is defined which links edges, peaks, and valleys in the image intensity to corresponding properties of the template. The template then interacts dynamically with the image, by altering its parameter values to minimize the energy function, thereby deforming itself to find the best fit. The final parameter values can be used as descriptors for the features. This method is demonstrated by showing deformable templates detecting eyes and mouths in real images. &gt;", "citation_count": "20", "reference_count": "2,707", "date": "1989", "authors": ["A.L. Yuille", "D.S. Cohen", "P.W. Hallinan"], "related_topics": ["Feature (computer vision)", "Feature extraction", "Real image", "Edge detection", "Template", "Face detection", "Facial recognition system", "Pattern recognition (psychology)", "Computer vision", "Computer science", "Artificial intelligence"]}
{"id": "2125999363", "references": ["2006852055", "2169718527", "2045817341", "2565808444", "3121926921", "1498436455"], "title": "Categorization of faces using unsupervised feature extraction", "abstract": "The proposal of G. Cottrell et al. (1987) that their image compression network might be used to extract image features for pattern recognition automatically, is tested by training a neural network to compress 64 face images, spanning 11 subjects, and 13 nonface images. Features extracted in this manner (the output of the hidden units) are given as input to a one-layer network trained to distinguish faces from nonfaces and to attach a name and sex to the face images. The network successfully recognizes new images of familiar faces, categorizes novel images as to their `faceness' and, to a great extent, gender, and exhibits continued accuracy over a considerable range of partial or shifted input", "citation_count": "6", "reference_count": "237", "date": "1990", "authors": ["M.K. Fleming", "G.W. Cottrell"], "related_topics": ["Feature extraction", "Feature (computer vision)", "Pattern recognition (psychology)", "Image compression", "Face (geometry)", "Data compression", "Categorization", "Pattern recognition", "Artificial neural network", "Computer vision", "Range (mathematics)", "Computer science", "Artificial intelligence"]}
{"id": "1526492552", "references": ["1989477529", "2040036684", "2076063813", "2118615399", "2162950292", "2166206801", "2098580305", "2120480077", "2138451337"], "title": "Stimulus-selective properties of inferior temporal neurons in the macaque", "abstract": "Previous studies have reported that some neurons in the inferior temporal (IT) cortex respond selectively to highly specific complex objects. In the present study, we conducted the first systematic survey of the responses of IT neurons to both simple stimuli, such as edges and bars, and highly complex stimuli, such as models of flowers, snakes, hands, and faces. If a neuron responded to any of these stimuli, we attempted to isolate the critical stimulus features underlying the response. We found that many of the responsive neurons responded well to virtually every stimulus tested. The remaining, stimulus-selective cells were often selective along the dimensions of shape, color, or texture of a stimulus, and this selectivity was maintained throughout a large receptive field. Although most IT neurons do not appear to be \u201cdetectors\u201d for complex objects, we did find a separate population of cells that responded selectively to faces. The responses of these cells were dependent on the configuration of specific face features, and their selectivity was maintained over changes in stimulus size and position. A particularly high incidence of such cells was found deep in the superior temporal sulcus. These results indicate that there may be specialized mechanisms for the analysis of faces in IT cortex.", "citation_count": "0", "reference_count": "1,823", "date": "1984", "authors": ["R Desimone", "TD Albright", "CG Gross", "C Bruce"], "related_topics": ["Receptive field", "Temporal cortex", "Stimulus (physiology)", "Superior temporal sulcus", "Population", "Neuron", "Electrophysiology", "Macaque", "Neuroscience", "Biology"]}
{"id": "1507699566", "references": ["2135463994", "2098693229", "2282794079", "2095999100", "2078118580", "2154164341", "2168239064", "1983240749", "2025588980", "2138451337"], "title": "Practical Face Recognition and Verification with Wisard", "abstract": "WISARD (Wilkie, Aleksander, and Stonham\u2019s Recognition Device) is a general purpose pattern recognition machine with a special semi-parallel structure unlike that of conventional single instruction single data computers. The machine is self-adapting. It does not require programming where an explict set of rules, defining the operations to be performed on the data, have to be supplied. The behaviour of the system is established by a learning process whereby a representative set of patterns from the class of data to be recognised, is input to the machine. A wide range of pattern recognition problems can be solved with this approach, they include industrial inspection, speech recognition, medical pattern recognition and artificial vision.", "citation_count": "0", "reference_count": "122", "date": "1986", "authors": ["T. J. Stonham"], "related_topics": ["Three-dimensional face recognition", "Pattern recognition (psychology)", "Face Recognition Grand Challenge", "Facial recognition system", "Face detection", "Set (abstract data type)", "Class (computer programming)", "Structure (mathematical logic)", "Artificial intelligence", "Computer science"]}
{"id": "2055712799", "references": ["2074163268", "2067398582", "1834283795", "2075554361", "2003370853", "2153709524", "1964415410", "2033266778", "2978983090", "2103504761"], "title": "Smart sensing within a pyramid vision machine", "abstract": "A machine is designed, based on a pyramid architecture, that supports smart sensing and related highly efficient processing. Key elements of the design are (a) hierarchical data structures for image representation, (b) fine-to-coarse algorithms for the fast generation of image measures, (c) coarse-to-fine search strategies that rapidly locate objects or events within a scene, and (d) high-level control mechanisms that guide data gathering even as visual information is being interpreted. This system, known as the Pyramid Vision Machine, achieves high performance at modest cost. Design considerations and several applications are described. &gt;", "citation_count": "18", "reference_count": "432", "date": "1988", "authors": ["P.J. Burt"], "related_topics": ["Pyramid (image processing)", "Systems architecture", "Data structure", "Information extraction", "Hierarchical database model", "Algorithm design", "Key (cryptography)", "Computer vision", "Computer science", "Architecture", "Artificial intelligence"]}
{"id": "1986450498", "references": ["3142790270", "2005151061", "1763512962", "1990309597", "1539686131", "2050271480", "2117731089", "2084338988", "1597739853", "2139314678"], "title": "Visual neurones responsive to faces", "abstract": "Abstract  Populations of visual neurones have been discovered in one area of the temporal association cortex that respond to different aspects of facial information. The responses of these cells have many of the properties hypothesized for \u2018gnostic units' and provide insight into the final stages of visual processing leading to recognition of an object as a face and more specifically the identity of the face.", "citation_count": "29", "reference_count": "597", "date": "1987", "authors": ["David I. Perrett", "Amanda J. Mistlin", "Andrew J. Chitty"], "related_topics": ["Visual processing", "Face perception", "Temporal lobe", "Cortex (anatomy)", "Neuroanatomy", "Association (psychology)", "Identity (object-oriented programming)", "Neurophysiology", "Communication", "Psychology"]}
{"id": "2148461049", "references": ["2134557905", "2156163116", "2105464873", "2118858186", "1624854622", "2310919327", "3118608800", "2546302380", "2144161366", "2132424367"], "title": "Flexible, high performance convolutional neural networks for image classification", "abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.", "citation_count": "29", "reference_count": "1,481", "date": "2011", "authors": ["Dan C. Cire\u015fan", "Ueli Meier", "Jonathan Masci", "Luca M. Gambardella", "J\u00fcrgen Schmidhuber"], "related_topics": ["Convolutional neural network", "MNIST database", "Artificial neural network", "Feature (machine learning)", "Contextual image classification", "Pattern recognition", "Image processing", "Machine learning", "Computer science", "Object (computer science)", "Artificial intelligence"]}
{"id": "2144982973", "references": ["2166049352", "2134557905", "2161969291", "2154422044", "2057175746", "2310919327", "1624854622", "2177274842", "2152473410", "2124386111"], "title": "Robust Object Recognition with Cortex-Like Mechanisms", "abstract": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex", "citation_count": "71", "reference_count": "1,995", "date": "2007", "authors": ["T. Serre", "L. Wolf", "S. Bileschi", "M. Riesenhuber", "T. Poggio"], "related_topics": ["3D single-object recognition", "Cognitive neuroscience of visual object recognition", "Feature (machine learning)", "Face detection", "Artificial neural network", "Template matching", "Pattern matching", "Categorization", "Hierarchical control system", "Visual cortex", "Artificial intelligence", "Computer science", "Image processing"]}
{"id": "2138857742", "references": ["2025768430", "2136922672", "2117130368", "2001141328", "2187089797", "2110798204", "2310919327", "2100495367", "2116064496", "2072128103"], "title": "Why Does Unsupervised Pre-training Help Deep Learning?", "abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.", "citation_count": "55", "reference_count": "1,807", "date": "2010", "authors": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "related_topics": ["Unsupervised learning", "Semi-supervised learning", "Competitive learning", "Deep belief network", "Wake-sleep algorithm", "Instance-based learning", "Multi-task learning", "Supervised learning", "Machine learning", "Artificial intelligence", "Computer science"]}
{"id": "2138621811", "references": ["2006119904", "1578099820", "2147152072", "1568713441", "2079672501", "3013264884", "2798909945", "2089192108", "1996764654", "2148694408"], "title": "Authoritative sources in a hyperlinked environment", "abstract": "The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of \u201cauthorative\u201d information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of \u201chub pages\u201d that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.", "citation_count": "52", "reference_count": "16,207", "date": "1999", "authors": ["Jon M. Kleinberg"], "related_topics": ["HITS algorithm", "Link farm", "Link analysis", "Anchor text", "Webgraph", "Citation graph", "PageRank", "Graph (abstract data type)", "Information retrieval", "Computer science"]}
{"id": "2032164462", "references": ["1996741810", "2030668727", "2304531936", "2108789285", "2156960699", "2132653096", "2493364698", "2147627917", "1591751678", "2154890045"], "title": "Complete gradient optimization of a recurrent network applied to /b/,/d/,/g/ discrimination", "abstract": "A complete gradient optimization technique for connectionist networks with recurrent links is presented. A truncated gradient technique is available in the literature. A network with recurrent links for discrimination of /b/, /d/, and /g/ in the context of following /i/, /a/, or /u/ is designed. The performance of this network as optimized with either of the two techniques is then compared. It will be shown that the complete gradient is exact and simpler, and it will be demonstrated that it leads to superior performance.", "citation_count": "0", "reference_count": "63", "date": "1988", "authors": ["Raymond L. Watrous", "Bruce Ladendorf", "Gary Kuhn"], "related_topics": ["Connectionism", "Context (language use)", "Artificial neural network", "Artificial intelligence", "Computer science"]}
{"id": "2047515372", "references": ["1597286183", "2100677568", "2177721432", "2016589492"], "title": "A learning algorithm for analog, fully recurrent neural networks", "abstract": "A learning algorithm for recurrent neural networks is derived. This algorithm allows a network to learn specified trajectories in state space in response to various input sequences. The network dynamics are described by a system of coupled differential equations that specify the continuous change of the unit activities and weights over time. The algorithm is nonlocal, in that a change in the connection weight between two units may depend on the values for some of the weights between different units. However, the operation of a learned network (fixed weights) is local. If the network units are specified to behave like electronic amplifiers, then an analog implementation of the learned network is straightforward. An example demonstrates the use of the algorithm in a completely connected network of four units. The network creates a limit cycle attractor in order to perform the specified task. &gt;", "citation_count": "4", "reference_count": "98", "date": "1989", "authors": ["Gherrity"], "related_topics": ["Echo state network", "Time delay neural network", "Recurrent neural network", "Network simulation", "Random neural network", "Feedforward neural network", "Network dynamics", "Wake-sleep algorithm", "Probabilistic neural network", "Cellular neural network", "Stochastic neural network", "Deep learning", "Artificial neural network", "Nervous system network models", "Leabra", "Types of artificial neural networks", "Competitive learning", "Physical neural network", "State space", "Attractor", "Limit cycle", "Algorithm", "Computer science", "Artificial intelligence"]}
{"id": "2133656308", "references": ["1996741810", "2532851845", "2148099973", "99063960", "2068200243"], "title": "The problem of learning long-term dependencies in recurrent networks", "abstract": "The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested. &gt;", "citation_count": "5", "reference_count": "233", "date": "1993", "authors": ["Y. Bengio", "P. Frasconi", "P. Simard"], "related_topics": ["Recurrent neural network", "Gradient descent", "Artificial neural network", "Robustness (computer science)", "Algorithm", "Attractor", "Pattern recognition", "Intelligent Network", "Computer science", "Background noise", "Artificial intelligence", "Optimization methods"]}
{"id": "2121553911", "references": ["2110485445", "1800750688", "2081965739", "1505652865", "2119796132", "2057653135", "2049093093", "1498436455", "2016589492"], "title": "Finite state automata and simple recurrent networks", "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t1, together with element t, to predict element t 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.", "citation_count": "9", "reference_count": "688", "date": "1989", "authors": ["Axel Cleeremans", "David Servan-Schreiber", "James L. McClelland"], "related_topics": ["Network simulation", "Network architecture", "Sequence", "Finite-state machine", "Grammar", "Natural language", "Set (abstract data type)", "Element (category theory)", "Algorithm", "Mathematics"]}
{"id": "2094096029", "references": ["2042264548", "2154642048", "2046432185", "2110485445", "2021774695", "2173629880", "2165758113", "2078626246", "2143503258", "2016589492"], "title": "Learning the dynamic nature of speech with back-propagation for sequences", "abstract": "Abstract   A novel learning algorithm is proposed, called Back-Propagation for Sequences (BPS), for a particular class of dynamic neural networks in which some units have local feedback. These networks can be trained to respond to sequences of input patterns and seem particularly suited for phoneme recognition. They exhibit a forgetting behavior and consequently only recently past information is taken into account for classification purposes. BPS permits online weight updating and it has the same time complexity and space requirements as back-propagation (BP) applied to feedforward networks. We present experimental results for problems connected with Automatic Speech Recognition.", "citation_count": "20", "reference_count": "53", "date": "1992", "authors": ["Yoshua Bengio", "Renato De Mori", "Marco Gori"], "related_topics": ["Artificial neural network", "Backpropagation", "Time complexity", "Forgetting", "Feed forward", "Speech recognition", "Artificial intelligence", "Class (biology)", "Computer science"]}
{"id": "2171074980", "references": ["2070473094", "2026258334", "2022772618"], "title": "A SIMPLEX METHOD FOR FUNCTION MINIMIZATION", "abstract": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.", "citation_count": "3", "reference_count": "38,508", "date": "1965", "authors": ["John A. Nelder", "R. Mead"], "related_topics": ["Nelder\u2013Mead method", "Simplex algorithm", "Hessian matrix", "Neighbourhood (graph theory)", "Simplex", "Function (mathematics)", "Vertex (geometry)", "NEWUOA", "Applied mathematics", "Mathematics"]}
{"id": "2029673686", "references": ["2098964861", "2065606540", "3140710696", "1988062549", "2123859926", "1540889165", "2067120253", "2105377328", "2024174582", "2026258334"], "title": "A general-purpose global optimizer: implementation and applications", "abstract": "Abstract   This paper, written from a user stand-point, advocates the Adaptive Random Search strategy as an efficient tool for global optimization. First is presented a brief overview of the various types of methods available in the literature for global optimization, and practical advantages of the random search approach are advanced. Some modifications, which were found to improve the efficiency and versatility of the method, and a detailed description of the practical implementation of the resulting algorithm are presented. The routine is used first to treat seven test-cases from the literature for comparison purposes. Then two examples are treated related to automatic control theory. The first one is a parameter estimation problem. the second one a control problem. Finally a practical application of the method to automated registration in medical nuclear imagery is presented.", "citation_count": "18", "reference_count": "159", "date": "1984", "authors": ["Luc Pronzato", "Eric Walter", "Alain Venot", "Jean-Francois Lebruchec"], "related_topics": ["Random search", "Global optimization", "Automatic control", "Estimation theory", "Data mining", "Mathematical optimization", "Computer science", "Control (management)", "Adaptive method", "General purpose"]}
{"id": "2065606540", "references": ["2124548165", "2088978850", "2080089060", "2963520355", "2162737890", "2010861271", "2147171518", "2119401655", "3103275550"], "title": "A controlled random search procedure for global optimisation", "abstract": "", "citation_count": "0", "reference_count": "361", "date": "1977", "authors": ["Wyn L. Price"], "related_topics": ["Random search", "Mathematical optimization", "Computer science"]}
{"id": "1652464192", "references": ["2134148744", "1231707848", "2088978850", "1992790175", "2103815986", "2161917943", "2072974808", "2154030608", "1977371710", "2128193809"], "title": "Concepts of scale in simulated annealing", "abstract": "Simulated annealing is a powerful technique for finding near\u2010optimal solutions to NP\u2010complete combinatorial optimization problems. In this technique, the states of a physical system are generalized to states of a system being optimized, the physical energy is generalized to the function being minimized, and the temperature is generalized to a control parameter for the optimization process. Wire length minimization in circuit placement is used as an example to show how ideas from statistical physics can elucidate the annealing process. The mean of the distribution of states in energy is a maximum energy scale of the system, its standard deviation defines the maximum temperature scale, and the minimum change in energy defines the minimum temperature scale. These temperature scales tell us where to begin and end an annealing schedule. The \u2018\u2018size\u2019\u2019 of a class of moves within the state space of the system is defined as the average change in the energy induced by moves of that class. These move scales are related to the characteristic temperature scales of a system, and show that a move class should be used when it gives an average change in energy on the order of the temperature. This, in turn, helps improve the performance of the algorithm.", "citation_count": "0", "reference_count": "316", "date": "2008", "authors": ["Steve R. White"], "related_topics": ["Adaptive simulated annealing", "Simulated annealing", "Physical system", "Standard deviation", "Scale of temperature", "Minification", "Statistical physics", "Very-large-scale integration", "Mathematical optimization", "Annealing (metallurgy)", "Mathematics"]}
{"id": "2022772618", "references": ["2098841537", "2076063813", "1986254541", "2057502998", "2162737890", "2010334716", "2119401655", "2127336290", "295690833"], "title": "A Rapidly Convergent Descent Method for Minimization", "abstract": "", "citation_count": "0", "reference_count": "6,347", "date": "1963", "authors": ["R. Fletcher", "M. J. D. Powell"], "related_topics": ["Descent (aeronautics)", "Mathematics", "Minification", "Mathematical optimization"]}
{"id": "2056760934", "references": ["2052864234"], "title": "Equation of state calculations by fast computing machines", "abstract": "A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two\u2010dimensional rigid\u2010sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four\u2010term virial coefficient expansion.", "citation_count": "1", "reference_count": "40,856", "date": "1953", "authors": ["Nicholas Metropolis", "Arianna W. Rosenbluth", "Marshall N. Rosenbluth", "Augusta H. Teller", "Edward Teller"], "related_topics": ["Equation of State Calculations by Fast Computing Machines", "Monte Carlo method", "Monte Carlo molecular modeling", "Hybrid Monte Carlo", "Direct simulation Monte Carlo", "Quantum Monte Carlo", "Variational Monte Carlo", "Monte Carlo integration", "Physics", "Computational chemistry"]}
{"id": "2152710595", "references": ["2059676477"], "title": "`` Direct Search'' Solution of Numerical and Statistical Problems", "abstract": "", "citation_count": "1", "reference_count": "6,234", "date": "1961", "authors": ["Robert Hooke", "T. A. Jeeves"], "related_topics": ["Pattern search", "Computer science", "Mathematical optimization", "Direct search", "Pattern search algorithm"]}
{"id": "2012231377", "references": ["2098841537", "3017323153", "2162166182", "1804110266", "2115167851", "2090351291", "2148138104", "2018215034", "2850653342", "2963263347"], "title": "Function minimization by conjugate gradients", "abstract": "", "citation_count": "0", "reference_count": "6,143", "date": "1964", "authors": ["R. Fletcher", "C. M. Reeves"], "related_topics": ["Nonlinear conjugate gradient method", "Conjugate residual method", "Derivation of the conjugate gradient method", "Gradient method", "Biconjugate gradient method", "Conjugate", "Wolfe conditions", "Mathematical analysis", "Computer science", "Function minimization"]}
{"id": "2026258334", "references": ["1982478920", "1985334587", "1581808154", "1520710862", "2342863993", "2162737890", "2310612427", "2919979744", "2099544223", "2119401655"], "title": "An Automatic Method for Finding the Greatest or Least Value of a Function", "abstract": "", "citation_count": "0", "reference_count": "3,878", "date": "1960", "authors": ["H. H. Rosenbrock"], "related_topics": ["Value (mathematics)", "Function (mathematics)", "Computer science", "Rosenbrock function", "Statistics"]}
{"id": "2086699924", "references": ["3152187716", "2007321142", "2077574412"], "title": "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains", "abstract": "", "citation_count": "3", "reference_count": "6,127", "date": "1970", "authors": ["Leonard E. Baum", "Ted Petrie", "George Soules", "Norman Weiss"], "related_topics": ["Examples of Markov chains", "Markov chain", "Markov model", "Variable-order Markov model", "Markov property", "Markov renewal process", "Markov process", "Markov kernel", "Algorithm", "Mathematics"]}
{"id": "2077804127", "references": ["1991133427", "1994396704", "2020127877", "2131513205", "1628850721", "1538748823", "1535702289", "1597533204", "1920769845", "1990005915"], "title": "Speaker-independent phone recognition using hidden Markov models", "abstract": "Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems. &gt;", "citation_count": "20", "reference_count": "1,330", "date": "1989", "authors": ["K.-F. Lee", "H.-W. Hon"], "related_topics": ["Hidden Markov model", "Markov process", "TIMIT", "Language model", "Spectrogram", "Smoothing", "Speech recognition", "Linear predictive coding", "Pattern recognition", "Natural language", "Computer science", "Artificial intelligence"]}
{"id": "2169415433", "references": ["2042264548", "2154642048", "2110485445", "2105594594", "1991848143", "94523489", "2173629880", "2581275558", "3017143921", "2293063825"], "title": "Review of neural networks for speech recognition", "abstract": "The performance of current speech recognition systems is far below that of humans. Neural nets offer the potential of providing massive parallelism, adaptation, and new algorithmic approaches to problems in speech recognition. Initial studies have demonstrated that multilayer networks with time delays can provide excellent discrimination between small sets of pre-segmented difficult-to-discriminate words, consonants, and vowels. Performance for these small vocabularies has often exceeded that of more conventional approaches. Physiological front ends have provided improved recognition accuracy in noise and a cochlea filter-bank that could be used in these front ends has been implemented using micro-power analog VLSI techniques. Techniques have been developed to scale networks up in size to handle larger vocabularies, to reduce training time, and to train nets with recurrent connections. Multilayer perceptron classifiers are being integrated into conventional continuous-speech recognizers. Neural net archit...", "citation_count": "94", "reference_count": "805", "date": "1989", "authors": ["Richard P. Lippmann"], "related_topics": ["Time delay neural network", "Multilayer perceptron", "Artificial neural network", "Noise (video)", "Speech recognition", "Very-large-scale integration", "Machine learning", "Computer science", "Massively parallel", "Adaptation (computer science)", "Scale (map)", "Artificial intelligence"]}
{"id": "2140766383", "references": ["2141899961", "2151058089", "1877570817", "2007857129", "2169415433", "2030377865", "183625566", "1529808766", "1507849272", "2176028050"], "title": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters", "abstract": "One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'.", "citation_count": "13", "reference_count": "410", "date": "1989", "authors": ["John S. Bridle"], "related_topics": ["Multilayer perceptron", "Hidden Markov model", "Artificial neural network", "Mutual information", "Probability distribution", "Stochastic modelling", "Mean squared error", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2125610452", "references": ["2129244720", "2028656089", "2148371116", "2103075368", "2143855671", "2148099973", "2071781501", "2121551440", "2061068689", "2168961642"], "title": "Speech communication : human and machine", "abstract": "", "citation_count": "0", "reference_count": "1,735", "date": "1987", "authors": ["Douglas O'Shaughnessy"], "related_topics": ["Speech processing", "Voice activity detection", "Speech corpus", "Speech synthesis", "Speech technology", "Speech recognition", "Computer science", "Speech communication"]}
{"id": "2135622428", "references": ["2130116522", "1984714339", "616418331", "1506342804", "2088900896", "2187548090", "2062914653", "1553004968", "2140153041", "210359992"], "title": "Introduction to matrix computations", "abstract": "Preliminaries. Practicalities. The Direct Solution of Linear Systems. Norms, Limits, and Condition Numbers. The Linear Least Squares Problem. Eigenvalues and Eigenvectors. The QR Algorithm. The Greek Alphabet and Latin Notational Correspondents. Determinants. Rounding-Error Analysis of Solution of Triangular Systems and of Gaussian Elimination. Of Things Not Treated. Bibliography. Index.", "citation_count": "0", "reference_count": "2,875", "date": "1973", "authors": ["G. W. Stewart"], "related_topics": ["QR algorithm", "QR decomposition", "Gaussian elimination", "Linear least squares", "Eigenvalues and eigenvectors", "Matrix (mathematics)", "Linear system", "Algebra", "Bibliography", "Mathematics"]}
{"id": "2114552889", "references": ["2042986967", "1540760152", "2070357213", "5720473", "2400401042", "2056898464", "2072789827", "2033662447", "2115049345", "2327315803"], "title": "Solving Large-Scale Symmetric Travelling Salesman Problems to Optimality", "abstract": "We report the solution to optimality of ten large-scale symmetric travelling salesman problems. The travelling salesman problem (TSP) is one of the standard problems of the Operations Research/Management Science literature and is cited in virtually every textbook on this subject. The TSP is a hard combinatorial optimization problem for which to date no technically good algorithm is known. All known algorithmic approaches to the TSP that are amenable to a worst-case analysis have yielded nothing for this problem that is as satisfactory as the existing solution procedures for, as an example, flow problems in networks. Furthermore, recent theoretical work has shown that the TSP is algorithmically equivalent to the general zero-one decision problem with linear constraints and objective function. The phrase \u201cThis problem is as hard as a travelling salesman problem\u201d reflects this fact and expresses the slim chances of finding an optimal solution to the problem at hand. It is therefore all the more surprising th...", "citation_count": "11", "reference_count": "397", "date": "1980", "authors": ["H Crowder", "M Pedberg"], "related_topics": ["Lin\u2013Kernighan heuristic", "Travelling salesman problem", "2-opt", "Bottleneck traveling salesman problem", "Function problem", "Christofides algorithm", "Quadratic assignment problem", "Vehicle routing problem", "Mathematical optimization", "Mathematics"]}
{"id": "2148673189", "references": ["2106378689", "2058937865", "2056898464", "1969186119", "1994863634"], "title": "Computer solutions of the traveling salesman problem", "abstract": "Two algorithms for solving the (symmetric distance) traveling salesman problem have been programmed for a high-speed digital computer. The first produces guaranteed optimal solution for problems involving no more than 13 cities; the time required (IBM 7094 II) varies from 60 milliseconds for a 9-city problem to 1.75 seconds for a 13-city problem. The second algorithm produces precisely characterized, locally optimal solutions for large problems (up to 145 cities) in an extremely short time and is based on a general heuristic approach believed to be of general applicability to various optimization problems. The average time required to obtain a locally optimal solution is under 30n3 microseconds where n is the number of cities involved. Repeated runs on a problem from random initial tours result in a high probability of finding the optimal solution among the locally optimal solutions obtained. For large problems where many locally optimal solutions have to be obtained in order to be reasonably assured of having the optimal solution, an efficient reduction scheme is incorporated in the program to reduce the total computation time by a substantial amount.", "citation_count": "5", "reference_count": "2,570", "date": "1965", "authors": ["Shen Lin"], "related_topics": ["Greedy algorithm", "Local search (optimization)", "2-opt", "Bottleneck traveling salesman problem", "Traveling purchaser problem", "Optimization problem", "Travelling salesman problem", "Lin\u2013Kernighan heuristic", "Mathematical optimization", "Algorithm", "Mathematics"]}
{"id": "2022494241", "references": ["2022068533", "2020065097", "2066359652", "2014952973", "2022820481", "1983272724", "2332514279", "2068491219", "1990755770", "1967084746"], "title": "Infinite-ranged models of spin-glasses", "abstract": "", "citation_count": "27", "reference_count": "1,108", "date": "1978", "authors": ["Scott Kirkpatrick", "David Sherrington"], "related_topics": ["Spin glass", "Dynamic Monte Carlo method", "Condensed matter physics", "Physics"]}
{"id": "2014068360", "references": ["2058937865", "1489991626", "2797039129", "2564555264", "1965680834", "1975224834", "1994863634", "2904374521", "1968778433", "2064791171"], "title": "The shortest path through many points", "abstract": "We prove that the length of the shortest closed path through n points in a bounded plane region of area v is \u2018almost always\u2019 asymptotically proportional to \u221a(nv) for large n; and we extend this result to bounded Lebesgue sets in k\u2013dimensional Euclidean space. The constants of proportionality depend only upon the dimensionality of the space, and are independent of the shape of the region. We give numerical bounds for these constants for various values of k; and we estimate the constant in the particular case k = 2. The results are relevant to the travelling-salesman problem, Steiner's street network problem, and the Loberman\u2014Weinberger wiring problem. They have possible generalizations in the direction of Plateau's problem and Douglas' problem.", "citation_count": "14", "reference_count": "1,164", "date": "1959", "authors": ["Jillian Beardwood", "J. H. Halton", "J. M. Hammersley"], "related_topics": ["Euclidean shortest path", "Shortest path problem", "Widest path problem", "Longest path problem", "Bounded function", "Distance", "K shortest path routing", "Yen's algorithm", "Combinatorics", "Mathematics"]}
{"id": "2014952973", "references": ["2022494241", "2103216274", "3105807095", "2999185184", "2581275558", "2000449275", "2073633823", "2013520638", "2061743560"], "title": "Frustration and ground-state degeneracy in spin glasses", "abstract": "", "citation_count": "0", "reference_count": "317", "date": "1977", "authors": ["Scott Kirkpatrick"], "related_topics": ["Frustration", "Geometrical frustration", "Degeneracy (biology)", "Ground state", "Spin glass", "Condensed matter physics", "Physics"]}
{"id": "86906884", "references": ["2108536179", "1920649228", "2067288944", "2149884725", "2120970098", "2581275558", "3125007682", "2143391442", "2105406958", "2098502330"], "title": "Computers and In stractability: A Guide to the Theory of NP-Completeness. W. H Freeman, San Fran", "abstract": "", "citation_count": "0", "reference_count": "752", "date": "1979", "authors": ["Michael Randolph Garey", "David S. Johnson"], "related_topics": ["Completeness (order theory)", "Computer science", "Calculus", "Artificial intelligence"]}
{"id": "2022820481", "references": ["1534790618", "2061905891", "1506090423", "54778000"], "title": "Solvable Model of a Spin-Glass", "abstract": "We consider an Ising model in which the spins are coupled by infinite-ranged random interactions independently distributed with a Gaussian probability density. Both \"spinglass\" and ferromagnetic phases occur. The competition between the phases and the type of order present in each are studied.", "citation_count": "4", "reference_count": "4,290", "date": "1975", "authors": ["David Sherrington", "Scott Kirkpatrick"], "related_topics": ["Spin glass", "Ising model", "Square-lattice Ising model", "Replica trick", "Random energy model", "Spins", "Probability density function", "Gaussian", "Condensed matter physics", "Quantum mechanics", "Physics"]}
{"id": "2143037347", "references": ["2042986967", "2009803313", "2058748455", "2066194644", "1992928037", "2294603245", "2055569927", "1969186119", "2117226423", "2007610503"], "title": "Probabilistic Analysis of Partitioning Algorithms for the Traveling-Salesman Problem in the Plane", "abstract": "We consider partitioning algorithms for the approximate solution of large instances of the traveling-salesman problem in the plane. These algorithms subdivide the set of cities into small groups, construct an optimum tour through each group, and then patch the subtours together to form a tour through all the cities. If the number of cities in the problem is n, and the number of cities in each group is t, then the worst-case error is  $O\\sqrt{n/t}$ . If the cities are randomly distributed, then the relative error is Ot-1/2 with probability one. Hybrid schemes are suggested, in which partitioning is used in conjunction with existing heuristic algorithms. These hybrid schemes may be expected to give near-optimum solutions to problems with thousands of cities.", "citation_count": "16", "reference_count": "602", "date": "1977", "authors": ["Richard M. Karp"], "related_topics": ["Lin\u2013Kernighan heuristic", "Travelling salesman problem", "Approximation algorithm", "Probabilistic analysis of algorithms", "Heuristic (computer science)", "Geometric probability", "Approximation error", "Group (mathematics)", "Combinatorics", "Algorithm", "Mathematics"]}
{"id": "2190154063", "references": ["2618530766", "2103496339", "2143612262", "2145287260", "2293747114", "1586335931", "2117014758", "1665214252", "2064675550", "2117829824"], "title": "Modeling time series data with deep Fourier neural networks", "abstract": "Abstract   We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and the regularization term, such that both stability and efficient training are achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.", "citation_count": "42", "reference_count": "19", "date": "2016", "authors": ["Michael S. Gashler", "Stephen C. Ashmore"], "related_topics": ["Artificial neural network", "Fourier series", "Fourier transform", "Fast Fourier transform", "Regularization (mathematics)", "Curve fitting", "Extrapolation", "Nonlinear system", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2293185259", "references": ["162588823", "179875071", "2110415041", "2171928131", "1598327410", "2136939460", "2165125849", "1527772862", "2132339004", "2292896937"], "title": "Recurrent Neural Network based Language Modeling in Meeting Recognition", "abstract": "We use recurrent neural network (RNN) based language models to improve the BUT English meeting recognizer. On the baseline setup using the original language models we decrease word error rate (WER) more than 1% absolute by n-best list rescoring and language model adaptation. When n-gram language models are trained on the same moderately sized data set as the RNN models, improvements are higher yielding a system which performs comparable to the baseline. A noticeable improvement was observed with unsupervised adaptation of RNN models. Furthermore, we examine the influence of word history on WER and show how to speed-up rescoring by caching common prefix strings. Index Terms: automatic speech recognition, language modeling, recurrent neural networks, rescoring, adaptation", "citation_count": "12", "reference_count": "172", "date": "2011", "authors": ["Stefan Kombrink", "Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget"], "related_topics": ["Language model", "Recurrent neural network", "Word error rate", "Word (computer architecture)", "Speech recognition", "Natural language processing", "Adaptation (computer science)", "Data set", "Computer science", "Artificial intelligence"]}
{"id": "3046044791", "references": ["2042422091", "2150355110", "2798878556", "2554302513", "1599105596", "2016574277", "2783525259", "2300242332", "2164653071", "2064675550"], "title": "Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks", "abstract": "The emergence of brain-inspired neuromorphic computing as a paradigm for edge AI is motivating the search for high-performance and efficient spiking neural networks to run on this hardware. However, compared to classical neural networks in deep learning, current spiking neural networks lack competitive performance in compelling areas. Here, for sequential and streaming tasks, we demonstrate how a novel type of adaptive spiking recurrent neural network (SRNN) is able to achieve state-of-the-art performance compared to other spiking neural networks and almost reach or exceed the performance of classical recurrent neural networks (RNNs) while exhibiting sparse activity. From this, we calculate a &gt; 100x energy improvement for our SRNNs over classical RNNs on the harder tasks. To achieve this, we model standard and adaptive multiple-timescale spiking neurons as self-recurrent neural units, and leverage surrogate gradients and auto-differentiation in the PyTorch Deep Learning framework to efficiently implement backpropagation-through-time, including learning of the important spiking neuron parameters to adapt our spiking neurons to the tasks.", "citation_count": "25", "reference_count": "14", "date": "2020", "authors": ["Bojian Yin", "Federico Corradi", "Sander M. Boht\u00e9"], "related_topics": ["Spiking neural network", "Recurrent neural network", "Deep learning", "Artificial neural network", "Backpropagation through time", "Neuromorphic engineering", "Artificial intelligence", "Leverage (statistics)", "Computer science", "Computation", "Neuron"]}
{"id": "2106725254", "references": ["2156838815", "2077776048", "2017588182", "2753461371", "2157169955", "2001141328", "2151425871", "2053186076", "2040135606", "1595132914"], "title": "Temporal nonlinear dimensionality reduction", "abstract": "Existing Nonlinear dimensionality reduction (NLDR) algorithms make the assumption that distances between observations are uniformly scaled. Unfortunately, with many interesting systems, this assumption does not hold. We present a new technique called Temporal NLDR (TNLDR), which is specifically designed for analyzing the high-dimensional observations obtained from random-walks with dynamical systems that have external controls. It uses the additional information implicit in ordered sequences of observations to compensate for non-uniform scaling in observation space. We demonstrate that TNLDR computes more accurate estimates of intrinsic state than regular NLDR, and we show that accurate estimates of state can be used to train accurate models of dynamical systems.", "citation_count": "21", "reference_count": "15", "date": "2011", "authors": ["Mike Gashler", "Tony Martinez"], "related_topics": ["Dynamical systems theory", "Nonlinear dimensionality reduction", "Stochastic process", "Algorithm", "Scaling", "Theoretical computer science", "Space (mathematics)", "State (functional analysis)", "Mathematics"]}
{"id": "2569140349", "references": ["2155541015", "2139501017", "1810943226", "1895577753", "2919115771", "1999965501", "2095705004", "2156387975", "2293063825", "2962741254"], "title": "Application of Recurrent Neural Networks for Drought Projections in California", "abstract": "Abstract   We use recurrent neural networks (RNNs) to investigate the complex interactions between the long-term trend in dryness and a projected, short but intense, period of wetness due to the 2015\u20132016 El Nino. Although it was forecasted that this El Nino season would bring significant rainfall to the region, our long-term projections of the Palmer Z Index (PZI) showed a continuing drought trend, contrasting with the 1998\u20131999 El Nino event. RNN training considered PZI data during 1896\u20132006 that was validated against the 2006\u20132015 period to evaluate the potential of extreme precipitation forecast. We achieved a statistically significant correlation of 0.610 between forecasted and observed PZI on the validation set for a lead time of 1\u00a0month. This gives strong confidence to the forecasted precipitation indicator. The 2015\u20132016 El Nino season proved to be relatively weak as compared with the 1997\u20131998, with a peak PZI anomaly of 0.242 standard deviations below historical averages, continuing drought conditions.", "citation_count": "47", "reference_count": "23", "date": "2017", "authors": ["J. A. Le", "Hesham el-Askary", "Mohamed Allali", "Daniele C Struppa"], "related_topics": ["Quantitative precipitation forecast", "Anomaly (natural sciences)", "Precipitation", "Climatology", "Training (meteorology)", "Standard deviation", "Dryness", "Recurrent neural network", "Environmental science"]}
{"id": "2754791538", "references": ["2046079134", "2126574503", "2010399676", "2158169396", "3137695714", "2176950688", "1983364832", "2034328688", "24089286", "2064675550"], "title": "Lifelong learning of human actions with deep neural network self-organization", "abstract": "Lifelong learning is fundamental in autonomous robotics for the acquisition and fine-tuning of knowledge through experience. However, conventional deep neural models for action recognition from videos do not account for lifelong learning but rather learn a batch of training data with a predefined number of action classes and samples. Thus, there is the need to develop learning systems with the ability to incrementally process available perceptual cues and to adapt their responses over time. We propose a self-organizing neural architecture for incrementally learning to classify human actions from video sequences. The architecture comprises growing self-organizing networks equipped with recurrent neurons for processing time-varying patterns. We use a set of hierarchically arranged recurrent networks for the unsupervised learning of action representations with increasingly large spatiotemporal receptive fields. Lifelong learning is achieved in terms of prediction-driven neural dynamics in which the growth and the adaptation of the recurrent networks are driven by their capability to reconstruct temporally ordered input sequences. Experimental results on a classification task using two action benchmark datasets show that our model is competitive with state-of-the-art methods for batch learning also when a significant number of sample labels are missing or corrupted during training sessions. Additional experiments show the ability of our model to adapt to non-stationary input avoiding catastrophic interference.", "citation_count": "81", "reference_count": "68", "date": "2017", "authors": ["German Ignacio Parisi", "Jun Tani", "Cornelius Weber", "Stefan Wermter"], "related_topics": ["Competitive learning", "Unsupervised learning", "Deep learning", "Semi-supervised learning", "Robot learning", "Instance-based learning", "Artificial neural network", "Catastrophic interference", "Lifelong learning", "Machine learning", "Artificial intelligence", "Computer science", "Training set"]}
{"id": "2124592697", "references": ["2102605133", "2618530766", "2147880316", "2022508996", "2310919327", "1903029394", "2110158442", "2037227137", "2962835968", "2155893237"], "title": "Conditional Random Fields as Recurrent Neural Networks", "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.", "citation_count": "64", "reference_count": "2,426", "date": "2015", "authors": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip H. S. Torr"], "related_topics": ["Deep learning", "Recurrent neural network", "Convolutional neural network", "Image segmentation", "Conditional random field", "Graphical model", "Scale-space segmentation", "Approximate inference", "Probabilistic logic", "Machine learning", "Artificial intelligence", "Computer science", "Gaussian"]}
{"id": "2083703071", "references": ["114517082", "2154642048", "2112036188", "3023071679", "2079735306", "2098432798", "2135046866", "2119862467", "2016589492", "196214544"], "title": "Automated design of complex dynamic systems.", "abstract": "Several fields of study are concerned with uniting the concept of computation with that of the design of physical systems. For example, a recent trend in robotics is to design robots in such a way that they require a minimal control effort. Another example is found in the domain of photonics, where recent efforts try to benefit directly from the complex nonlinear dynamics to achieve more efficient signal processing. The underlying goal of these and similar research efforts is to internalize a large part of the necessary computations within the physical system itself by exploiting its inherent non-linear dynamics. This, however, often requires the optimization of large numbers of system parameters, related to both the system's structure as well as its material properties. In addition, many of these parameters are subject to fabrication variability or to variations through time. In this paper we apply a machine learning algorithm to optimize physical dynamic systems. We show that such algorithms, which are normally applied on abstract computational entities, can be extended to the field of differential equations and used to optimize an associated set of parameters which determine their behavior. We show that machine learning training methodologies are highly useful in designing robust systems, and we provide a set of both simple and complex examples using models of physical dynamical systems. Interestingly, the derived optimization method is intimately related to direct collocation a method known in the field of optimal control. Our work suggests that the application domains of both machine learning and optimal control have a largely unexplored overlapping area which envelopes a novel design methodology of smart and highly complex physical systems.", "citation_count": "42", "reference_count": "18", "date": "2014", "authors": ["Michiel Hermans", "Benjamin Schrauwen", "Peter Bienstman", "Joni Dambre"], "related_topics": ["Systems design", "Dynamical systems theory", "Optimal control", "Nonlinear system", "Recurrent neural network", "Domain (software engineering)", "Field (computer science)", "Robot", "Dynamical system", "Control engineering", "Computer science", "Differential equation"]}
{"id": "2783190965", "references": ["2107878631", "2150355110", "2399941526", "2560162835", "2414564754", "2099940443", "2171865010", "2752901856", "2064675550", "2473868734"], "title": "Deep Learning Based Multi-Channel Intelligent Attack Detection for Data Security", "abstract": "Deep learning methods, e.g., convolutional neural networks (CNNs) and Recurrent Neural Networks (RNNs), have achieved great success in image processing and natural language processing especially in high level vision applications such as recognition and understanding. However, it is rarely used to solve information security problems such as attack detection studied in this paper. Here, we move forward a step and propose a novel multi-channel intelligent attack detection method based on long short term memory recurrent neural networks (LSTM-RNNs). To achieve high detection rate, data preprocessing, feature abstraction, and multi-channel training and detection are seamlessly integrated into an end-to-end detection framework. Data preprocessing provides high-quality data for subsequent processing, then different types of features are extracted from the processed data. Multi-channel processing is used to generate classifiers by training neural networks with different types of features, which preserve attack features of input vectors and classify the attack from normal data. With the results of the classifier's attack detection, we introduce a voting algorithm to decide whether the input data is an attack or not. Experimental results validate that the proposed attack detection method greatly outperforms several attack detection methods that use feature detection and Bayesian or SVM classifiers.", "citation_count": "48", "reference_count": "113", "date": "2020", "authors": ["Feng Jiang", "Yunsheng Fu", "B. B. Gupta", "Yongsheng Liang", "Seungmin Rho", "Fang Lou", "Fanzhi Meng", "Zhihong Tian"], "related_topics": ["Deep learning", "Recurrent neural network", "Convolutional neural network", "Artificial neural network", "Intrusion detection system", "Feature extraction", "Data pre-processing", "Support vector machine", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2003357516", "references": ["2065127848", "1976990135", "1652505363", "2119423166", "2010526455", "3036751298", "1505136099", "2082560699", "2088744610", "2293063825"], "title": "The recent excitement about neural networks", "abstract": "The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects.", "citation_count": "13", "reference_count": "874", "date": "1989", "authors": ["Francis Crick"], "related_topics": ["Artificial neural network", "Artificial intelligence", "Computer science"]}
{"id": "1529808766", "references": ["2135346934"], "title": "Accelerated learning in layered neural networks", "abstract": "Abst ract . Learning in layered neu ral networks is posed as the mini\u00ad miz at ion of an error function defined over t he training set. A proba\u00ad bilistic interpretation of the target act ivities sugges ts th e use of rela\u00ad t ive entro py as an error measure. We investigate t he merits of using this error function over t he traditional quad ratic function for gradient descent learni ng. Com parative numerical sim ulations for the conrf\u00ad guity problem show marked redu ct ion s in learn ing t imes. This im \u00ad provement is explained in terms of the characteristic steepness of the landscape defined by the error function in configuration space.", "citation_count": "1", "reference_count": "300", "date": "1988", "authors": ["Sara A. Solla", "Esther Levin", "Michael Fleisher"], "related_topics": ["Error function", "Artificial neural network", "Gradient descent", "Function (mathematics)", "Algorithm", "Artificial intelligence", "Computer science", "Accelerated learning", "Training set"]}
{"id": "2010029425", "references": ["2047278710", "1944672", "2017170340", "2165758113", "1491711721", "2099579348", "2097580026", "2168405694", "2014165366", "2076118331"], "title": "Convergence of stochastic processes", "abstract": "I Functional on Stochastic Processes.- 1. Stochastic Processes as Random Functions.- Notes.- Problems.- II Uniform Convergence of Empirical Measures.- 1. Uniformity and Consistency.- 2. Direct Approximation.- 3. The Combinatorial Method.- 4. Classes of Sets with Polynomial Discrimination.- 5. Classes of Functions.- 6. Rates of Convergence.- Notes.- Problems.- III Convergence in Distribution in Euclidean Spaces.- 1. The Definition.- 2. The Continuous Mapping Theorem.- 3. Expectations of Smooth Functions.- 4. The Central Limit Theorem.- 5. Characteristic Functions.- 6. Quantile Transformations and Almost Sure Representations.- Notes.- Problems.- IV Convergence in Distribution in Metric Spaces.- 1. Measurability.- 2. The Continuous Mapping Theorem.- 3. Representation by Almost Surely Convergent Sequences.- 4. Coupling.- 5. Weakly Convergent Subsequences.- Notes.- Problems.- V The Uniform Metric on Spaces of Cadlag Functions.- 1. Approximation of Stochastic Processes.- 2. Empirical Processes.- 3. Existence of Brownian Bridge and Brownian Motion.- 4. Processes with Independent Increments.- 5. Infinite Time Scales.- 6. Functional of Brownian Motion and Brownian Bridge.- Notes.- Problems.- VI The Skorohod Metric on D(0, ?).- 1. Properties of the Metric.- 2. Convergence in Distribution.- Notes.- Problems.- VII Central Limit Theorems.- 1. Stochastic Equicontinuity.- 2. Chaining.- 3. Gaussian Processes.- 4. Random Covering Numbers.- 5. Empirical Central Limit Theorems.- 6. Restricted Chaining.- Notes.- Problems.- VIII Martingales.- 1. A Central Limit Theorem for Martingale-Difference Arrays.- 2. Continuous Time Martingales.- 3. Estimation from Censored Data.- Notes.- Problems.- Appendix A Stochastic-Order Symbols.- Appendix B Exponential Inequalities.- Notes.- Problems.- Appendix C Measurability.- Notes.- Problems.- References.- Author Index.", "citation_count": "0", "reference_count": "3,891", "date": "1984", "authors": ["David Pollard"], "related_topics": ["Convergence of random variables", "Continuous mapping theorem", "Uniform convergence", "Brownian bridge", "Stochastic equicontinuity", "Almost surely", "Central limit theorem", "Metric space", "Discrete mathematics", "Mathematics"]}
{"id": "2084544490", "references": ["1652505363", "2171277043", "1530699444", "2019363670", "2165758113", "2114766824", "2056099894", "3017143921", "1988520084", "1594031697"], "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "abstract": "We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x in X and an outcome y in Y , and tries to find a hypothesis h : X --&lt; A , where h in H , that specifies the appropriate action a in A to take for each instance x , in order to minimize the expectation of a loss l(y,a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X times Y . Special cases include the problem of learning a function from X into Y , the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain hypothesis spaces H , and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Pollard''s work, and a notion of *capacity* and *metric dimension* for classes of functions that map into a bounded metric space. (Supersedes 89-30 and 90-52.) [Also in \"Information and Computation\", Vol. 100, No.1, September 1992]", "citation_count": "77", "reference_count": "1,230", "date": "1992", "authors": ["David Haussler"], "related_topics": ["Conditional probability distribution", "Joint probability distribution", "Bounded function", "Probably approximately correct learning", "VC dimension", "Metric space", "Artificial neural network", "Metric dimension", "Discrete mathematics", "Algorithm", "Mathematics"]}
{"id": "2181111061", "references": ["2135293965", "2019207321", "2143287292", "2151535223", "2132731342", "2150242464", "2178814133", "2125329357", "2102054518", "2137461900"], "title": "Neural nets for adaptive filtering and adaptive pattern recognition", "abstract": "The adaptive linear combiner (ALC) is described, and practical applications of the ALC in signal processing and pattern recognition are presented. Six signal processing examples are given, which are system modeling, statistical prediction, noise canceling, echo canceling, universe modeling, and channel equalization. Adaptive pattern recognition using neural nets is then discussed. The concept involves the use of an invariance net followed by a trainable classifier. It makes use of a multilayer adaptation algorithm that descrambles output and reproduces original patterns. &gt;", "citation_count": "0", "reference_count": "691", "date": "1990", "authors": ["Bernard Widrow", "Rodney Winter"], "related_topics": ["Adaptive filter", "Pattern recognition (psychology)", "Artificial neural network", "Signal processing", "Active noise control", "Systems modeling", "Classifier (linguistics)", "Pattern recognition", "Echo (computing)", "Computer science", "Artificial intelligence"]}
{"id": "1984375561", "references": ["2154642048", "2151058089", "1991848143", "1994405449", "2565808444", "2007431958", "2177721432", "2000449275", "2293063825", "2310420328"], "title": "Dynamics and architecture for neural computation", "abstract": "Abstract   Useful computation can be performed by systematically exploiting the phenomenology of nonlinear dynamical systems. Two dynamical phenomena are isolated into primitive architectural components which perform the operations of continuous nonlinear transformation and autoassociative recall. Backpropagation techniques for programming the architectural components are presented in a formalism appropriate for a collective nonlinear dynamical system. It is shown that conventional recurrent backpropagation is not capable of storing multiple patterns in an associative memory which starts out with an insufficient number of point attractors. It is shown that a modified algorithm can solve this problem by introducing new attractors near the to-be-stored patterns. Two primitive components are assembled into an elementary machine and trained to perform invariant pattern recognition with respect to small arbitrary transformations of the input pattern, provided the transformations are sufficiently small. The machine realizes modular learning since error signals do not propagate across the boundaries of the components.", "citation_count": "19", "reference_count": "318", "date": "1988", "authors": ["Fernando J. Pineda"], "related_topics": ["Models of neural computation", "Content-addressable memory", "Backpropagation", "Attractor", "Computation", "Modular design", "Algorithm", "Architecture", "Mathematics", "Invariant pattern recognition"]}
{"id": "1881179843", "references": ["2950898568", "2150355110", "2786738752", "2076063813", "2116341502", "2907492528", "1553004968", "2064675550", "2116424792", "2016589492"], "title": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment", "abstract": "", "citation_count": "0", "reference_count": "464", "date": "1990", "authors": ["Luis B. Almeida"], "related_topics": ["Learning rule", "Perceptron", "Asynchronous communication", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "1984205520", "references": ["1959983357", "2154642048", "1984375561", "1881179843", "1529155465", "2160853866", "1523732620", "2119796132", "2895674046", "2016589492"], "title": "Experimental Analysis of the Real-time Recurrent Learning Algorithm", "abstract": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi...", "citation_count": "10", "reference_count": "389", "date": "1989", "authors": ["Ronald J. Williams", "David Zipser"], "related_topics": ["Weighted Majority Algorithm", "Computation", "Task (computing)", "Algorithm", "Power (physics)", "Computer science", "Series (mathematics)", "Matrix (mathematics)", "Sample (statistics)", "Structure (mathematical logic)"]}
{"id": "2119796132", "references": ["2081965739", "1505652865", "2504871398", "2016589492"], "title": "Learning Sequential Structure in Simple Recurrent Networks", "abstract": "We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-1, together with element t, to predict element t+1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses of the hidden-layer patterns of activation showed that they encode prediction-relevant information about the entire path traversed through the network. We illustrate the phases of learning with cluster analyses performed at different points during training.", "citation_count": "4", "reference_count": "241", "date": "1988", "authors": ["David Servan-Schreiber", "Axel Cleeremans", "James L. McClelland"], "related_topics": ["Network architecture", "Artificial neural network", "Grammar", "Rule-based machine translation", "Theoretical computer science", "Natural language", "Embedding", "Computer science"]}
{"id": "2058791286", "references": ["2158442843", "1959983357", "1690319996", "2110485445", "2008404566", "2068059068", "2145394183", "2117823925", "2086252718", "2019385508"], "title": "Up to year 2020 load forecasting using neural nets", "abstract": "Abstract   Prediction of peak electric loads in Japan up to year 2020 is discussed using the artificial neural networks (ANNs). In this study, total system load forecast reflecting current and future trends is carried out for nine power companies in Japan. Two ANNs, a three-layered back-propagation and a recurrent neural network, were designed and tested for the purpose. Predictions were done for target years 1999, 2000, 2005, 2010, 2015, and 2020, respectively. Two case studies, preservation of the status and structure reform, were also tested for predicting the loads of years 2010 and 2020. Unlike short-term load forecasting, long-term load forecasting is mainly affected by economical factors rather than weather conditions. This study focuses on economical data that seem to influence long-term electric load demands. Here, 10 factors are selected as inputs for the proposed ANNs: (1) gross national product, (2) gross domestic product, (3) population, (4) number of households, (5) number of air-conditioners, (6) amount of CO2 pollution, (7) index of industrial production, (8) oil price, (9) energy consumption, and (10) electricity price. The data used are: actual yearly, incremental growth rate from the previous year, and both together (actual and incremental growth rate from the previous year). As a result, the demands for 2010 and 2020 are predicted to be 225.779 and 249.617\u00a0GW, respectively (preservation of the status). With structure reform, the demands for 2010 and 2020 are predicted to be 219.259 and 244.508\u00a0GW.", "citation_count": "20", "reference_count": "237", "date": "2002", "authors": ["Bahman Kermanshahi", "Hiroshi Iwamiya"], "related_topics": ["Demand forecasting", "Population", "Gross national product", "Economic data", "Electrical load", "Index of industrial production", "Gross domestic product", "Energy consumption", "Statistics", "Operations research", "Mathematics"]}
{"id": "2999879503", "references": ["2618530766", "2108598243", "2194775991", "2919115771", "2340897893", "2095705004", "2964121744", "1903029394", "2962835968", "2064675550"], "title": "A Hybrid Compact Neural Architecture for Visual Place Recognition", "abstract": "State-of-the-art algorithms for visual place recognition, and related visual navigation systems, can be broadly split into two categories: computer-science-oriented models including deep learning or image retrieval-based techniques with minimal biological plausibility, and neuroscience-oriented dynamical networks that model temporal properties underlying spatial navigation in the brain. In this letter, we propose a new compact and high-performing place recognition model that bridges this divide for the first time. Our approach comprises two key neural models of these categories: (1)  FlyNet , a compact, sparse two-layer neural network inspired by brain architectures of fruit flies,  Drosophila melanogaster , and (2) a one-dimensional continuous attractor neural network (CANN). The resulting  FlyNet+CANN  network incorporates the compact pattern recognition capabilities of our FlyNet model with the powerful temporal filtering capabilities of an equally compact CANN, replicating entirely in a hybrid neural implementation the functionality that yields high performance in algorithmic localization approaches like SeqSLAM. We evaluate our model, and compare it to three state-of-the-art methods, on two benchmark real-world datasets with small viewpoint variations and extreme environmental changes \u2013 achieving 87% AUC results under day to night transitions compared to 60% for Multi-Process Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times faster, respectively.", "citation_count": "66", "reference_count": "24", "date": "2020", "authors": ["Marvin Chancan", "Luis Hernandez-Nunez", "Ajay Narendra", "Andrew B. Barron", "Michael Milford"], "related_topics": ["Deep learning", "Artificial neural network", "Image retrieval", "Benchmark (computing)", "Pattern recognition", "Key (cryptography)", "Computer science", "Artificial intelligence"]}
{"id": "1934184906", "references": ["2618530766", "2097117768", "2108598243", "2143612262", "2310919327", "2095705004", "3118608800", "2962835968", "2963173190", "2963911037"], "title": "Recurrent convolutional neural network for object recognition", "abstract": "In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.", "citation_count": "55", "reference_count": "837", "date": "2015", "authors": ["Ming Liang", "Xiaolin Hu"], "related_topics": ["Deep learning", "Convolutional neural network", "Time delay neural network", "Recurrent neural network", "MNIST database", "Cognitive neuroscience of visual object recognition", "Context (language use)", "Benchmark (computing)", "Artificial intelligence", "Computer science"]}
{"id": "2007800656", "references": ["1597286183", "2154642048", "1652505363", "2110485445", "2581275558", "2177721432", "1507849272", "2293063825", "22297218", "1498436455"], "title": "Deep dyslexia: A case study of connectionist neuropsychology", "abstract": "Abstract Deep dyslexia is an acquired reading disorder marked by the Occurrence of semantic errors (e.g. reading RIVER as \u201cocean\u201d). In addition, patients exhibit a number of other symptoms, including visual and morphological effects in their errors, a part-of-speech effect, and an advantage for concrete over abstract words. Deep dyslexia poses a distinct challenge for cognitive neuropsychology because there is little understanding of why such a variety of symptoms should co-occur in virtually all known patients. Hinton and Shallice (1991) replicated the co-occurrence of visual and semantic errors by lesioning a recurrent connectionist network trained to map from orthography to semantics. Although the success of their simulations is encouraging. there is little understanding of what underlying principles are responsible for them. In this paper we evaluate and, where possible, improve on the most important design decisions made by Hinton and Shallice, relating to the task, the network architecture, the trai...", "citation_count": "149", "reference_count": "1,126", "date": "1993", "authors": ["David C. Plaut", "Tim Shallice"], "related_topics": ["Deep dyslexia", "Dyslexia", "Cognitive neuropsychology", "Reading (process)", "Neuropsychology", "Orthography", "Word recognition", "Semantics", "Cognitive psychology", "Cognitive science", "Psychology"]}
{"id": "2155041441", "references": ["2154642048", "2150355110", "2110485445", "2798333393", "2124776405", "2173629880", "2138484437", "2064675550", "2116424792", "2016589492"], "title": "On-line learning algorithms for locally recurrent neural networks", "abstract": "This paper focuses on online learning procedures for locally recurrent neural nets with emphasis on multilayer perceptron (MLP) with infinite impulse response (IIR) synapses and its variations which include generalized output and activation feedback multilayer networks (MLN). We propose a new gradient-based procedure called recursive backpropagation (RBP) whose online version, causal recursive backpropagation (CRBP), has some advantages over other online methods. CRBP includes as particular cases backpropagation (BP), temporal BP, Back-Tsoi algorithm (1991) among others, thereby providing a unifying view on gradient calculation for recurrent nets with local feedback. The only learning method known for locally recurrent nets with no architectural restriction is the one by Back and Tsoi. The proposed algorithm has better stability and faster convergence with respect to the Back-Tsoi algorithm. The computational complexity of the CRBP is comparable with that of the Back-Tsoi algorithm, e.g., less that a factor of 1.5 for usual architectures and parameter settings. The superior performance of the new algorithm, however, easily justifies this small increase in computational burden. In addition, the general paradigms of truncated BPTT and RTRL are applied to networks with local feedback and compared with CRBP. CRBP exhibits similar performances and the detailed analysis of complexity reveals that CRBP is much simpler and easier to implement, e.g., CRBP is local in space and in time while RTRL is not local in space.", "citation_count": "54", "reference_count": "211", "date": "1999", "authors": ["P. Campolucci", "A. Uncini", "F. Piazza", "B.D. Rao"], "related_topics": ["Multilayer perceptron", "Recurrent neural network", "Artificial neural network", "Backpropagation", "Perceptron", "Stability (learning theory)", "Computational complexity theory", "Algorithm", "System identification", "Artificial intelligence", "Computer science", "Recurrent neural nets"]}
{"id": "2019385508", "references": ["1959983357", "2110485445", "2042898838", "2117823925", "2071789187", "2086252718", "2094592509", "2212424269"], "title": "Recurrent neural network for forecasting next 10 years loads of nine Japanese utilities", "abstract": "Abstract   In this paper, two artificial neural networks, a recurrent neural network (RNN) and a three-layer feed-forward back-propagation (BP), are applied for long-term load forecasting. The RNN is designed to forecast the loads of 1\u00a0yr ahead and the BP is used to forecast the next 5 and 10 years loads. The proposed networks are trained with the past 20 years (1975\u20131994) of actual data and are tested for target years 1995\u20131997, 2000, and 2005. In addition to the target year load forecasting, a sliding window training method is proposed for the continuous retraining of the RNN. The actual data is used for forecasting the loads of 1975\u20131994. However, forecasted data is applied for forecasting the loads beyond 1994. Since the weather condition data is not available for longer than two weeks ahead, a sensitivity program is developed to produce the future temperature from the present one. Very reasonable results have been obtained for the seen (inner sample) and unseen (out-of-sample or loads of target years) data. In this study, total system load forecast reflecting current and future trends, tempered with good judgement which is the key to all planning and indeed financial success is carried out for nine utilities in Japan.", "citation_count": "8", "reference_count": "143", "date": "1998", "authors": ["Bahman Kermanshahi"], "related_topics": ["Recurrent neural network", "Artificial neural network", "Sliding window protocol", "Sample (statistics)", "Sensitivity (control systems)", "Operations research", "Computer science", "Load forecasting"]}
{"id": "1996605950", "references": ["1652505363", "2110485445", "1979575715", "2137983211", "3125696988", "1554663460", "1586335931", "2117014758", "1498436455"], "title": "A dynamic artificial neural network model for forecasting time series events", "abstract": "Abstract   Neural networks have shown to be an effective method for forecasting time series events. Traditional research in this area uses a network with a sequential iterative learning process based on the feed-forward, back-propagation approach. In this paper we present a dynamic neural network model for forecasting time series events that uses a different architecture than traditional models. To assess the effectiveness of this method, we forecasted a number of standard benchmarks in time series research from forecasting literature. Results show that this approach is more accurate and performs significantly better than the traditional neural network and autoregressive integrated moving average (ARIMA) models.", "citation_count": "52", "reference_count": "329", "date": "2005", "authors": ["M. Ghiassi", "H. Saidane", "D.K. Zimbra"], "related_topics": ["Time delay neural network", "Autoregressive integrated moving average", "Probabilistic neural network", "Recurrent neural network", "Types of artificial neural networks", "Stochastic neural network", "Artificial neural network", "Backpropagation", "Machine learning", "Computer science", "Econometrics", "Artificial intelligence"]}
{"id": "2546314413", "references": ["3146166473", "2127141656", "2143612262", "2044503966", "2919115771", "2153635508", "2294059674", "2064675550", "1498436455"], "title": "Temporal Models for Predicting Student Dropout in Massive Open Online Courses", "abstract": "Over the past few years, the rapid emergence of massive open online courses (MOOCs) has sparked a great deal of research interest in MOOC data analytics. Dropout prediction, or identifying students at risk of dropping out of a course, is an important problem to study due to the high attrition rate commonly found on many MOOC platforms. The methods proposed recently for dropout prediction apply relatively simple machine learning methods like support vector machines and logistic regression, using features that reflect such student activities as lecture video watching and forum activities on a MOOC platform during the study period of a course. Since the features are captured continuously for each student over a period of time, dropout prediction is essentially a time series prediction problem. By regarding dropout prediction as a sequence classification problem, we propose some temporal models for solving it. In particular, based on extensive experiments conducted on two MOOCs offered on Coursera and edX, a recurrent neural network (RNN) model with long short-term memory (LSTM) cells beats the baseline methods as well as our other proposed methods by a large margin.", "citation_count": "33", "reference_count": "174", "date": "2015", "authors": ["Mi Fei", "Dit-Yan Yeung"], "related_topics": ["Student dropout", "Dropout (neural networks)", "Student activities", "Recurrent neural network", "Margin (machine learning)", "Support vector machine", "Machine learning", "Hidden Markov model", "Time series", "Data science", "Computer science", "Artificial intelligence"]}
{"id": "2045597501", "references": ["1987124984", "2075213419", "2010085418", "2240086165", "3041214984", "2139109112", "1976707985", "1984314602", "1967670055", "1773016195"], "title": "Toward a theory of automatic information processing in reading", "abstract": "Abstract   A model of information processing in reading is described in which visual information is transformed through a series of processing stages involving visual, phonological and episodic memory systems until it is finally comprehended in the semantic system. The processing which occurs at each stage is assumed to be learned and the degree of this learning is evaluated with respect to two criteria:  accuracy  and  automaticity . At the accuracy level of performance, attention is assumed to be necessary for processing; at the automatic level it is not. Experimental procedures are described which attempt to measure the degree of automaticity achieved in perceptual and associative learning tasks. Factors which may influence the development of automaticity in reading are discussed.", "citation_count": "41", "reference_count": "4,833", "date": "1974", "authors": ["David LaBerge", "S.Jay Samuels"], "related_topics": ["Automaticity", "Associative learning", "Information processing", "Reading (process)", "Episodic memory", "Perception", "Cognition", "Cognitive psychology", "Degree (music)", "Computer science"]}
{"id": "2106654511", "references": ["1986795801", "2018415829", "3040704687", "2019828626", "1994063459", "1966913759", "2075177070", "2047307179", "2047613852", "2037723078"], "title": "High-Speed Scanning in Human Memory", "abstract": "When subjects judge whether a test symbol is contained in a short memorized sequence of symbols, their mean reaction-time increases linearly with the length of the sequence. The linearity and slope of the function imply the existence of an internal serial-comparison process whose average rate is between 25 and 30 symbols per second.", "citation_count": "11", "reference_count": "4,588", "date": "1966", "authors": ["Saul Sternberg"], "related_topics": ["Sequence", "Linearity", "Symbol (programming)", "Function (mathematics)", "Process (computing)", "Arithmetic", "Mathematics", "Human memory"]}
{"id": "2135255848", "references": ["2109933556", "2106654511", "2011161709", "2014344082", "1989415743", "1990948551", "2127978962", "2142565826", "1502139053", "2064332540"], "title": "A spreading-activation theory of semantic processing", "abstract": "This paper presents a spreading-acti vation theory of human semantic processing, which can be applied to a wide range of recent experimental results. The theory is based on Quillian's theory of semantic memory search and semantic preparation, or priming. In conjunction with this, several of the miscondeptions concerning Qullian's theory are discussed. A number of additional assumptions are proposed for his theory in order to apply it to recent experiments. The present paper shows how the extended theory can account for results of several production experiments by Loftus, Juola and Atkinson's multiple-category experiment, Conrad's sentence-verification experiments, and several categorization experiments on the effect of semantic relatedness and typicality by Holyoak and Glass, Rips, Shoben, and Smith, and Rosch. The paper also provides a critique of the Smith, Shoben, and Rips model for categorization judgments. Some years ago, Quillian1 (1962, 1967) proposed a spreading-acti vation theory of human semantic processing that he tried to implement in computer simulations of memory search (Quillian, 1966) and comprehension (Quillian, 1969). The theory viewed memory search as activation spreading from two or more concept nodes in a semantic network until an intersection was found. The effects of preparation (or priming) in semantic memory were also explained in terms of spreading activation from the node of the primed concept. Rather than a theory to explain data, it was a theory designed to show how to build human semantic structure and processing into a computer.", "citation_count": "44", "reference_count": "12,060", "date": "1975", "authors": ["Allan M. Collins", "Elizabeth F. Loftus"], "related_topics": ["Semantic computing", "Semantic similarity", "Spreading activation", "Semantic memory", "Semantic search", "Semantic equivalence", "Semantic network", "Semantic compression", "Natural language processing", "Cognitive science", "Computer science", "Artificial intelligence"]}
{"id": "2094493170", "references": ["2077763860", "2075213419", "2318629766", "2073447756", "1980342340", "2094261607", "2060032378", "2040091250", "2015245944", "2094136133"], "title": "On data-limited and resource-limited processes", "abstract": "Abstract   This paper analyzes the effect on performance when several active processes compete for limited processing resources. The principles discussed show that conclusions about the interactions among psychological processes must be made with caution, and some existing assumptions may be unwarranted. When two (or more) processes use the same resources at the same time, they may both interfere with one another, neither may interfere with the other, or one may interfere with a second without any interference from the second process to the first. The important principles are that a process can be limited in its performance either by limits in the amount of available processing resources (such as memory or processing effort) or by limits in the quality of the data available to it. Competition among processes can affect a resource-limited process, but not a data-limited one. If a process continually makes preliminary results available even before it has completed all its operations, then it is possible to compute performance-resource operating characteristics that show how processes interact. A number of experiments from the psychological literature are examined according to these processing principles, resulting in some new interpretations of interactions among competing psychological processes.", "citation_count": "28", "reference_count": "3,382", "date": "1975", "authors": ["Donald A Norman", "Daniel G Bobrow"], "related_topics": ["Process (engineering)", "Quality (business)", "Risk analysis (engineering)", "Data processing", "Competition (economics)", "Cognition", "Computer science", "Management science", "Perception", "Data limited", "Limited resources"]}
{"id": "2147311265", "references": ["2082622165", "3142790270", "2414458198", "2023182894", "2117731089", "2147502381", "2011238950", "2116360511", "2013239224", "22297218"], "title": "Distinctive features, categorical perception, and probability learning: some applications of a neural model", "abstract": "A previously proposed model for memory based on neurophysiolo gical considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to \"categorical perception.\" Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy.", "citation_count": "50", "reference_count": "1,393", "date": "1988", "authors": ["James A. Anderson", "Jack W. Silverstein", "Stephen A. Ritz", "Randall S. Jones"], "related_topics": ["Categorical perception", "Set (psychology)", "Neuron", "Artificial intelligence", "Binary number", "Product (mathematics)", "Matrix (mathematics)", "Nervous system", "Positive feedback", "Psychology"]}
{"id": "1502139053", "references": ["2139666201"], "title": "Retrieval time from semantic memory", "abstract": "To ascertain the truth of a sentence such as \u201cA canary can fly,\u201d people utilize long-term memory. Consider two possible organizations of this memory. First, people might store with each kind of bird that flies (e.g., canary) the fact that it can fly. Then they could retrieve this fact directly to decide the sentence is true. An alternative organization would be to store only the generalization that  birds  can fly, and to infer that \u201cA canary can fly\u201d from the stored information that a canary is a bird and birds can fly. The latter organization is much more economical in terms of storage space but should require longer retrieval times when such inferences are necessary. The results of a true-false reaction-time task were found to support the latter hypothesis about memory organization.", "citation_count": "0", "reference_count": "5,316", "date": "1995", "authors": ["Allan M. Collins", "M. Ross Quillian"], "related_topics": ["Semantic memory", "Semantic computing", "Sentence", "Memory organisation", "Task (project management)", "Generalization", "Space (commercial competition)", "Natural language processing", "Computer science", "Artificial intelligence"]}
{"id": "1529340823", "references": ["2107033637", "2106654511", "1987873266", "1605773274", "2073588290", "2056622185", "2232925767", "1878893887", "2053566542", "1773016195"], "title": "Memory-scanning: mental processes revealed by reaction-time experiments.", "abstract": "", "citation_count": "36", "reference_count": "2,522", "date": "1969", "authors": ["Sternberg S"], "related_topics": ["Information system", "Text mining", "Computer science", "Information retrieval"]}
{"id": "1784695092", "references": ["1978662219", "1524144700", "2155653793", "2104960492", "2164084182", "2090267299", "2110143060", "2169134378", "2149273804", "2157825442"], "title": "Signal Detection Theory and Psychophysics", "abstract": "Book on statistical decision theory and sensory processes in signal detection theory and psychophysics", "citation_count": "0", "reference_count": "20,597", "date": "1974", "authors": ["David Marvin Green", "John Arthur Swets"], "related_topics": ["Psychophysics", "Detection theory", "Ideal observer analysis", "Perception", "Psychometric function", "Sensory system", "Speech recognition", "Computer science", "Perceptual decision", "Statistical decision theory"]}
{"id": "1967670055", "references": ["2106654511", "1593056665", "1990242729", "1605773274", "2049733042", "2021142183", "2139109112", "2056622185", "2076731491", "2037723078"], "title": "The discovery of processing stages: Extensions of Donders' method", "abstract": "A new method is proposed for using reaction-time (RT) measurements to study stages of information processing. It overcomes limitations of Donders' and more recent methods, and permits the discovery of stages, assessment of their properties, and separate testing of the additivity and stochastic independence of stage durations. The main feature of the additive-factor method is the search for non-interacting effects of experimental factors on mean RT. The method is applied to several binary-classification experiments, where it leads to a four-stage model, and to an identification experiment, where it distinguishes two stages. The sets of stages inferred from both these and other data are shown to carry substantive implications. It is demonstrated that stage-durations may be additive without being stochastically independent, a result that is relevant to the formulation of mathematical models of RT.", "citation_count": "45", "reference_count": "4,156", "date": "1969", "authors": ["Saul Sternberg"], "related_topics": ["Information processing", "Feature (computer vision)", "Identification (information)", "Mathematical model", "Algorithm", "Additive function", "Psychological refractory period", "Computer science", "Stochastic independence", "Two stages"]}
{"id": "2796837256", "references": ["2020044743", "2121016876", "2109945199", "2109635530", "2123632763", "2104599718", "2110575115", "1689445748", "2295821368", "2049981393"], "title": "Numerical recipes in C. The art of scientific computing", "abstract": "", "citation_count": "0", "reference_count": "4,189", "date": "1987", "authors": ["Max Planitz", "W. H. Press", "B. P. Flannery", "S. A. Teukolsky", "W. T. Vetterling"], "related_topics": ["Computer science", "Data science"]}
{"id": "2077658674", "references": ["2030723843", "2186428165", "2123871098", "2139212933", "1804110266", "1596717185", "2614081736", "1993170675", "1964357740", "2120145199"], "title": "Practical Methods of Optimization", "abstract": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index.", "citation_count": "0", "reference_count": "14,091", "date": "1988", "authors": ["Roger Fletcher"], "related_topics": ["Continuous optimization", "Derivative-free optimization", "Quadratic programming", "Nonlinear programming", "Random optimization", "Discrete optimization", "Quadratically constrained quadratic program", "Vector optimization", "Mathematical optimization", "Mathematics"]}
{"id": "2325850497", "references": ["1983959689", "2170816806", "2121016876", "2102717583", "2115167851", "1989672018", "2151812852", "2057502998", "2057622447"], "title": "Numerical recipes: the art of scientific computing", "abstract": "", "citation_count": "0", "reference_count": "795", "date": "1987", "authors": ["W.H. Press", "B.P. Flannery", "S.A. Teukolsky", "W.T. Vetterling", "J.R. Chipperfield"], "related_topics": ["Management science", "Chemistry"]}
{"id": "1527096151", "references": ["2042587503", "2008825922", "2164684058"], "title": "Sequential encoding and decoding for the discrete memoryless channel", "abstract": "\"August 12, 1960.\" Issued also as thesis, M.I.T. Dept. of Electrical Engineering, August 22, 1960.", "citation_count": "3", "reference_count": "17", "date": "1960", "authors": ["Barney. Reiffen"], "related_topics": ["Decoding methods", "Communication channel", "Coding theory", "Encoding (memory)", "Electronic engineering", "Computer science"]}
{"id": "2034274945", "references": ["172410088", "1593045043", "1804959266", "2011495938", "1624804034", "2153657884", "1494099594", "1780185704", "1667950888", "2166490929"], "title": "Transmission of information", "abstract": "", "citation_count": "0", "reference_count": "646", "date": "1961", "authors": ["Robert M. Fano", "W. T. Wintringham"], "related_topics": ["Transmission (telecommunications)", "Computer science", "Electronic engineering"]}
{"id": "2087362480", "references": ["1491140334", "1995875735", "2128765501", "2127514716", "2167100360"], "title": "A heuristic discussion of probabilistic decoding", "abstract": "This is another in a series of invited tutorial, status and survey papers that are being regularly solicited by the PTGIT Committee on Special Papers. We invited Profess01 Fano to commit to paprr his elegant but, unelaborate explanation of the principles of sequential decoding, a scheme which is currently contending for a position as the most practical implementation to dale of Shannon\u2019s theory of noisy communication channels. -&amp;e&amp;l Pcqwrs Committw.", "citation_count": "5", "reference_count": "695", "date": "1963", "authors": ["R. Fano"], "related_topics": ["Sequential decoding", "List decoding", "Probabilistic logic", "Decoding methods", "Heuristic", "Berlekamp\u2013Welch algorithm", "Scheme (programming language)", "Computer science", "Theoretical computer science", "Communication channel", "Error floor"]}
{"id": "2005530146", "references": ["2947000318", "2034274945", "2008825922", "2064759901", "1980073965", "1995875735", "2128765501", "2042587503", "2164684058", "1993944611"], "title": "Lower bounds to error probability for coding on discrete memoryless channels. II", "abstract": "New lower bounds are presented for the minimum error probability that can be achieved through the use of block coding on noisy discrete memoryless channels. Like previous upper bounds, these lower bounds decrease exponentially with the block length N. The coefficient of N in the exponent is a convex function of the rate. From a certain rate of transmission up to channel capacity, the exponents of the upper and lower bounds coincide. Below this particular rate, the exponents of the upper and lower bounds differ, although they approach the same limit as the rate approaches zero. Examples are given and various incidental results and techniques relating to coding theory are developed. The paper is presented in two parts: the first, appearing here, summarizes the major results and treats the case of high transmission rates in detail; the second, to appear in the subsequent issue, treats the case of low transmission rates.", "citation_count": "22", "reference_count": "542", "date": "1967", "authors": ["Claude E. Shannon", "Robert G. Gallager", "Elwyn R. Berlekamp"], "related_topics": ["Upper and lower bounds", "Coding gain", "Channel capacity", "Probability distribution", "Block code", "Coding theory", "Convex function", "Exponent", "Discrete mathematics", "Mathematics"]}
{"id": "1527268325", "references": ["2034274945", "2000241254", "2059944969", "2013366686", "2141927780"], "title": "Channel state testing in information decoding.", "abstract": "Massachusetts Institute of Technology. Dept. of Electrical Engineering. Thesis. 1965. Ph.D.", "citation_count": "5", "reference_count": "15", "date": "1965", "authors": ["Howard Louis Yudkin"], "related_topics": ["Decoding methods", "Communication channel", "State (computer science)", "Electrical engineering", "Engineering"]}
{"id": "1976797517", "references": ["2993383518", "2751862591", "2087362480", "2106215079", "2256505251", "2102567944", "2127002947", "2079656678", "1993944611"], "title": "Sequential decoding \u2014 the computation problem", "abstract": "Sequential decoding is a technique for encoding and decoding at moderate cost with a decoding reliability which approximates that of the optimum, and expensive, maximum-likelihood decoder. The several known sequential decoding algorithms enjoy a cost advantage over the maximum-likelihood decoder because they allow the level of the channel noise to regulate the level of the decoding computation. Since the average level of the required decoding computation for sequential decoders is small for source rates below a rate R comp , such a decoder can be realized for these rates with a relatively small logic unit and a buffer. The logic unit is normally designed to handle computation rates which are less than two or three times the average computation rate; the buffer serves to store data during those noisy periods when the required computation rate exceeds the computation rate of the logic unit. If the periods of high computation, which are caused by noise, are too frequent or too long, the buffer, which is necessarily finite in capacity, will fill and overflow. Since data are lost during an overflow, continuity in the decoding process cannot be maintained. The decoder, then, cannot continue to decode without error. For this reason, buffer overflow is an important event. In addition, since errors in the absence of overflow are much less frequent than are overflows themselves, the overflow event is of primary concern in the design of a sequential decoder. This paper presents some recent analytical results concerning the probability of a buffer overflow. In particular, it is shown that this probability is relatively insensitive to both the buffer capacity and the maximum speed of the logic unit for moderate capacities and speeds. By contrast, it is shown that the overflow probability decreases rapidly with a decrease in the source", "citation_count": "9", "reference_count": "79", "date": "1966", "authors": ["J. E. Savage"], "related_topics": ["Sequential decoding", "Soft-decision decoder", "Buffer overflow", "Decoding methods", "Computation", "Communication channel", "Encoding (memory)", "Noise", "Algorithm", "Computer science"]}
{"id": "1993944611", "references": ["2044180609", "2059944969", "2167210891", "1995875735", "2164684058"], "title": "A simple derivation of the coding theorem and some applications", "abstract": "Upper bounds are derived on the probability of error that can be achieved by using block codes on general time-discrete memoryless channels. Both amplitude-discrete and amplitude-continuous channels are treated, both with and without input constraints. The major advantages of the present approach are the simplicity of the derivations and the relative simplicity of the results; on the other hand, the exponential behavior of the bounds with block length is the best known for all transmission rates between 0 and capacity. The results are applied to a number of special channels, including the binary symmetric channel and the additive Gaussian noise channel.", "citation_count": "5", "reference_count": "988", "date": "1965", "authors": ["R. Gallager"], "related_topics": ["Channel capacity", "Block code", "Binary symmetric channel", "Gaussian noise", "Communication channel", "Capacity planning", "Applied mathematics", "Discrete mathematics", "Coding (social sciences)", "Mathematics", "Probability of error"]}
{"id": "2341171179", "references": ["2963641140", "2121863487", "2964161785", "2076063813", "1978956894", "1993962865", "2119567691", "2100235918"], "title": "Dynamic Programming", "abstract": "From the Publisher:\r\nAn introduction to the mathematical theory of multistage decision processes, this text takes a functional equation approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls a rich lode of applications and research topics. 1957 edition. 37 figures.", "citation_count": "0", "reference_count": "25,447", "date": "1957", "authors": ["Richard Ernest Bellman"], "related_topics": ["Mathematical theory", "Uniqueness", "Dynamic programming", "Optimal substructure", "Differential dynamic programming", "Mathematical economics", "Problem set", "Bottleneck", "Dynamic treatment regime", "Mathematics"]}
{"id": "2137095888", "references": ["2161414194", "2160031103", "1966812932", "1491238342", "2142416747", "1553004968", "2748207967", "2463456957", "2022635558"], "title": "The HARPY speech recognition system", "abstract": "Abstract : The Harpy connected speech recognition system is the result of an attempt to understand the relative importance of various design choices of two earlier speech recognition systems developed at Carnegie-Mellon University: The Hearsay-1 system and the Dragon system. Knowledge is represented in the Hearsay- 1 system as procedures and in the Dragon system as a Markov network with a- priori transition probabilities between states. Systematic performance analysis of various design choices of these two systems resulted in the HARPY system, in which knowledge is represented as a finite state transition network but without the a-priori transition probabilities. Harpy searches only a few 'best' syntactic (and acoustic) paths in parallel to determine the optimal path, and uses segmentation to effectively reduce the utterance length, thereby reducing the number of state probability updates that must be done. Several new heuristics have been added to the HARPY system to improve its performance and speed: detection of common sub-nets and collapsing them to reduce overall network size and complexity, eliminating the need for doing an acoustic match for all phonemic types at every time sample, and semi-automatic techniques for learning the lexical representations (that are needed for a steady-state system of this type) and the phonemic templates from training data, thus automatically accounting for the commonly occurring intra-word coarticulation and juncture phenomena. Inter-word phenomena are handled by the use of juncture rules which are applied at network generation time, thereby eliminating the need for repetitive and time consuming application of phonological rules during the recognition phase.", "citation_count": "0", "reference_count": "621", "date": "1976", "authors": ["Bruce T. Lowerre"], "related_topics": ["Heuristics", "Juncture", "Rule-based machine translation", "Coarticulation", "Markov chain", "Speech recognition", "Connected speech", "Path (graph theory)", "Computer science", "Segmentation"]}
{"id": "2142384583", "references": ["1562979145", "1991133427", "2122683098", "2106185713", "2045407304", "2131086249", "2153810958", "2134360027", "2161457263", "2035227369"], "title": "The viterbi algorithm", "abstract": "The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.", "citation_count": "38", "reference_count": "7,958", "date": "1973", "authors": ["Jr. G.D. Forney"], "related_topics": ["Soft output Viterbi algorithm", "Iterative Viterbi decoding", "Viterbi algorithm", "Forward algorithm", "Viterbi decoder", "Algorithm design", "Markov model", "Sequential decoding", "Algorithm", "Theoretical computer science", "Computer science"]}
{"id": "2157477135", "references": ["1991133427", "2074460300", "2134587001", "2169944289", "2035227369"], "title": "Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition", "abstract": "A model for channels in which an input sequence can produce output sequences of varying length is described. An efficient computational procedure for calculating Pr \\{Y\\mid X\\} is devised, where X = x_1,x_2,\\cdots,x_M and Y = y_1,y_2,\\cdots,y_N are the input and output of the channel. A stack decoding algorithm for decoding on such channels is presented. The appropriate likelihood function is derived. Channels with memory are considered. Some applications to speech and character recognition are discussed.", "citation_count": "5", "reference_count": "258", "date": "1975", "authors": ["L. Bahl", "F. Jelinek"], "related_topics": ["Decoding methods", "Sequence", "Likelihood function", "Stack (mathematics)", "Communication channel", "Speech recognition", "Computer science", "Insertion deletion"]}
{"id": "1989226853", "references": ["2963460797", "2125838338", "1966812932", "2397682113", "2003333103", "1665196592", "2165959773", "1553004968", "1990005915"], "title": "Continuous speech recognition via centisecond acoustic states", "abstract": "Continuous speech was treated as if produced by a finite\u2010state machine making a transition every centisecond. The observable output from state transitions was considered to be a power spectrum\u2014a probabilistic function of the target state of each transition. Using this model, observed sequences of power spectra from real speech were decoded as sequences of acoustic states by means of the Viterbi trellis algorithm. The finite\u2010state machine used as a representation of the speech source was composed of machines representing words, combined according to a \u201clanguage model.\u201d When trained to the voice of a particular speaker, the decoder recognized seven\u2010digit telephone numbers correctly 96% of the time, with a better than 99% per\u2010digit accuracy. Results for other tests of the system, including syllable and phoneme recognition, will also be given.", "citation_count": "0", "reference_count": "246", "date": "1976", "authors": ["R. Bakis"], "related_topics": ["Speech processing", "Voice activity detection", "Centisecond", "Viterbi algorithm", "Syllable", "Speech recognition", "Trellis (graph)", "Probabilistic logic", "Function (mathematics)", "Computer science"]}
{"id": "2163929346", "references": ["2341171179", "2138250146", "2110693321", "303664091", "2077574412"], "title": "The DRAGON system--An overview", "abstract": "This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model--that of a probabilistic function of a Markov process--is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.", "citation_count": "5", "reference_count": "704", "date": "1975", "authors": ["J. Baker"], "related_topics": ["Acoustic model", "Probabilistic logic", "Function (engineering)", "Markov process", "Markov chain", "Machine learning", "Computer science", "Structure (mathematical logic)", "Speech recognition", "Vocabulary", "Simple (abstract algebra)", "Artificial intelligence"]}
{"id": "2029491572", "references": ["2019529630", "2156539109", "2171161819", "646297848", "2041818183", "1966812932", "2020026127", "2115867364", "2038542953", "1973826788"], "title": "Introduction to Theoretical Linguistics", "abstract": "This is a comprehensive introduction to theoretical linguistics. It presupposes no previous knowledge and terms are defined as they are introduced; but it gives a rigorous and technical treatment of  a wide range of topics, and brings the reader to an advanced level of understanding. Since its first publication in 1968 Introduction to Theoretical Linguistics has been one of the classic introductions to the discipline. In a field which is often seen as rapidly moving, it will continue to be used by students seeking an overview of the central areas of linguistics - phonetics and phonology, grammar and semantics - and to be of great value to anyone interested in the ways in which theory can help to explain the key problems of human language.", "citation_count": "0", "reference_count": "2,554", "date": "1968", "authors": ["John Lyons"], "related_topics": ["Theoretical linguistics", "Grammar", "Phonology", "Semantics", "Phonetics", "Grammaticalization", "Linguistics", "Grammatical category", "Value (ethics)", "Sociology"]}
{"id": "2035227369", "references": ["1577906631", "2223581789", "2038058830", "2087362480", "1496151908", "585928584", "2128700225", "2004767718"], "title": "Fast sequential decoding algorithm using a stack", "abstract": "In this paper a new sequential decoding algorithm is introduced that uses stack storage at the receiver. It is much simpler to describe and analyze than the Fano algorithm, and is about six times faster than the latter at transmission rates equal to Rcomp the rate below which the average number of decoding steps is bounded by a constant. Practical problems connected with implementing the stack algorithm are discussed and a scheme is described that facilitates satisfactory performance even with limited stack storage capacity. Preliminary simulation results estimating the decoding effort and the needed stack siazree presented.", "citation_count": "8", "reference_count": "861", "date": "1969", "authors": ["F. Jelinek"], "related_topics": ["Sequential decoding", "Decoding methods", "Stack (abstract data type)", "Transmission (telecommunications)", "Algorithm", "Bounded function", "Constant (mathematics)", "Fano plane", "Scheme (programming language)", "Mathematics"]}
{"id": "2052810501", "references": ["1922629554", "2154815292", "2618208446", "3103713775", "1999488180", "1988150410", "3037683188", "2014311222"], "title": "The renormalization group: Critical phenomena and the Kondo problem", "abstract": "This review covers several topics involving renormalization group ideas. The solution of the $s$-wave Kondo Hamiltonian, describing a single magnetic impurity in a nonmagnetic metal, is explained in detail. See Secs. VII-IX. \"Block spin\" methods, applied to the two dimensional Ising model, are explained in Sec. VI. The first three sections give a relatively short review of basic renormalization group ideas, mainly in the context of critical phenomena. The relationship of the modern renormalization group to the older problems of divergences in statistical mechanics and field theory and field theoretic renormalization is discussed in Sec. IV. In Sec. V the special case of \"marginal variables\" is discussed in detail, along with the relationship of the modern renormalization group to its original formulation by Gell-Mann and Low and others.", "citation_count": "0", "reference_count": "4,954", "date": "1975", "authors": ["Kenneth G. Wilson"], "related_topics": ["Functional renormalization group", "Density matrix renormalization group", "Renormalization group", "Renormalization", "Critical phenomena", "Kondo model", "Kondo effect", "Critical dimension", "Theoretical physics", "Quantum mechanics", "Physics"]}
{"id": "1989544735", "references": ["2116397829", "2141128914", "2069035105", "2054536235", "2154234100", "2048330959", "2582614493", "2117430149", "2136113379"], "title": "An iterative prediction and correction method for automatic stereocomparison", "abstract": "In image processing of aerial photographs in cartography and of remote sensor data in earth resource surveys, one of the most basic parameters is the elevation of the ground estimated by means of parallax measurements made on stereo pairs of aerial images. In conventional methods, measurement by manual stereocomparators is precise but timeconsuming. Existing automatic stereocomparators do not work well because they use too simple a correlation method to get sufficiently precise measurements. Each of the two corresponding patterns from a stereo pair is so distorted by parallax that the calculated correlation value does not always attain the maximum at the point from which we can obtain the correct parallax.  As a result of the present study, it appears that an iterative prediction and correction method can be suitable for the efficient and accurate measurement of elevation from stereo imagery. The conventional methods do not fully utilize the information which is presented in the image itself or is  a priori  known. The first prediction of elevation is carried out only for well-contrasting parts of the image, which usually means roads, coast, forest edges, etc. Then this prediction is expanded to the neighboring parts, making use of the continuity of elevation. Using a temporary predicted elevation map, one of the stereo pair patterns is modified to lessen the distortion compared with the other pattern. Correction of the elevation map is performed using the correlation values calculated from the modified patterns. This feedback procedure is iterated until no correction is needed.  This new method is applied to digitized aerial photographs taken at a height of 3,000 m. Finally, a planned automatic image processing system is explained briefly, consisting of a newly developed scanner which can access every point in a scan field at high positional accuracy.", "citation_count": "0", "reference_count": "90", "date": "1973", "authors": ["Ken-ichi Mori", "Masatsugu Kidode", "Haruo Asada"], "related_topics": ["Image processing", "Parallax", "Elevation", "Distortion", "Point (geometry)", "A priori and a posteriori", "Scanner", "Computer vision", "Field (computer science)", "Computer science", "Artificial intelligence"]}
{"id": "1997494543", "references": ["2037819914", "1546972969", "2146934839", "1995756857", "1991012411", "2027935272", "1686754659", "1981076008", "2159772167", "2135347478"], "title": "Foundations of Cyclopean Perception", "abstract": "This classic work on cyclopean perception has influenced a generation of vision researchers, cognitive scientists, and neuroscientists and has inspired artists, designers, and computer graphics pioneers. In Foundations of Cyclopean Perception (first published in 1971 and unavailable for years), Bela Julesz traced the visual information flow in the brain, analyzing how the brain combines separate images received from the two eyes to produce depth perception. Julesz developed novel tools to do this: random-dot stereograms and cinematograms, generated by early digital computers at Bell Labs. These images, when viewed with the special glasses that came with the book, revealed complex, three-dimensional surfaces; this mode of visual stimulus became a paradigm for research in vision and perception. This reprint edition includes all 48 color random-dot designs from the original, as well as the special 3-D glasses required to view them.Foundations of Cyclopean Perception has had a profound impact on the vision studies community. It was chosen as one of the one hundred most influential works in cognitive science in a poll conducted by the University of Minnesota's Center for Cognitive Sciences. Many copies are \"permanently borrowed\" from college libraries; used copies are sought after online. Now, with this facsimile of the 1971 edition, the book is available again to cognitive scientists, neuroscientists, vision researchers, artists, and designers.", "citation_count": "0", "reference_count": "4,257", "date": "1971", "authors": ["Bela Julesz"], "related_topics": ["Cyclopean image", "Autostereogram", "Perception", "Vision Disparity", "Depth perception", "Random dot stereogram", "Cognition", "Stereoblindness", "Cognitive science", "Aesthetics", "Psychology"]}
{"id": "1975068880", "references": ["1980664748", "2014763046", "1552946839", "1649296624", "2086126894", "2048330959", "2036424138", "2092144432", "2072700524"], "title": "Cooperative phenomena in systems far from thermal equilibrium and in nonphysical systems", "abstract": "This article consists of two parts. The first part presents a tutorial approach to cooperative phenomena in systems far from thermal equilibrium and in nonphysical systems. Particular emphasis is placed on the question of how order and self-organization may arise. The following example is treated among others: the ordered phase of the laser giving rise to both coherently oscillating atomic dipole moments and coherent light emission. A complete analogy of the laser light distribution function to that of the Ginzburg-Landau theory of superconductivity is found mathematically which allows us to interpret the laser threshold as a quasi-second-order phase transition with soft modes, critical slowing down, etc. Similar phenomena, again closely resembling phase transitions, are found in tunnel diodes and in the nonlinear wave interaction which occurs, for example, in nonlinear optics. Remarkable analogies between the instability of the laser and those in hydro-dynamics are elaborated. While these phenomena show pronounced analogies to phase transitions in thermal equilibrium, there are further classes of instabilities and new phases which rather resemble hard excitations known in electrical engineering. Chemical oscillations are particularly important examples. In addition, the first part of this article contains the example of the cooperative behavior of neuron networks and shows the applicability of simple physical concepts, e.g., the Ising model, to the problem of the dynamics of social groups. All these above-mentioned examples demonstrate clearly that rather complex phenomena brought about by the cooperation of many subsystems can be understood and described by a few simple concepts. One of the main concepts is the order parameter; another is the adiabatic elimination of the variables of the subsystems, which is based upon a hierarchy of time constants present in most systems. The second part of this article gives a systematic account of the mathematical tools which allow us to deal with fluctuations. It contains the master equation, the Fokker-Planck equation, the generalized Fokker-Planck equation, and the Langevin equations, and gives several general methods for deriving the stationary and, in certain cases, the nonstationary solutions of master equations and the Fokker-Planck equations. Such general classes comprise those in which detailed balance is present or in which the coupling to the reservoirs is weak. In the quantum mechanical domain, the density matrix and the projection formalism for its reduction are presented. Finally, it is shown how the principle of quantum-classical correspondence allows us to translate quantum statistical problems completely into the classical domain.", "citation_count": "0", "reference_count": "1,164", "date": "1975", "authors": ["H. Haken"], "related_topics": ["Master equation", "Nonlinear system", "Ising model", "Detailed balance", "Thermal equilibrium", "Density matrix", "Quantum", "Phase transition", "Statistical physics", "Physics"]}
{"id": "2089840306", "references": ["1949099618", "2086441427", "1986713974", "2084061362", "1985072454", "2145511217", "2016350172", "2033757357", "2185205937", "1801947108"], "title": "A theory of biological pattern formation.", "abstract": "One of the elementary processes in morphogenesis is the formation of a spatial pattern of tissue structures, starting from almost homogeneous tissue. It will be shown that relatively simple molecular mechanisms based on auto- and cross catalysis can account for a primary pattern of morphogens to determine pattern formation of the tissue. The theory is based on short range activation, long range inhibition, and a distinction between activator and inhibitor concentrations on one hand, and the densities of their sources on the other. While source density is expected to change slowly, e.g. as an effect of cell differentiation, the concentration of activators and inhibitors can change rapidly to establish the primary pattern; this results from auto- and cross catalytic effects on the sources, spreading by diffusion or other mechanisms, and degradation. Employing an approximative equation, a criterium is derived for models, which lead to a striking pattern, starting from an even distribution of morphogens, and assuming a shallow source gradient. The polarity of the pattern depends on the direction of the source gradient, but can be rather independent of other features of source distribution. Models are proposed which explain size regulation (constant proportion of the parts of the pattern irrespective of total size). Depending on the choice of constants, aperiodic patterns, implying a one-to-one correlation between morphogen concentration and position in the tissue, or nearly periodic patterns can be obtained. The theory can be applied not only to multicellular tissues, but also to intracellular differentiation, e.g. of polar cells. The theory permits various molecular interpretations. One of the simplest models involves bimolecular activation and monomolecular inhibition. Source gradients may be substituted by, or added to, sink gradients, e.g. of degrading enzymes. Inhibitors can be substituted by substances required for, and depleted by activation. Sources may be either synthesizing systems or particulate structures releasing activators and inhibitors. Calculations by computer are presented to exemplify the main features of the theory proposed. The theory is applied to quantitative data on hydra \u2014 a suitable one-dimensional model for pattern formation \u2014 and is shown to account for activation and inhibition of secondary head formation.", "citation_count": "21", "reference_count": "3,844", "date": "1972", "authors": ["Alfred Gierer", "Hans Meinhardt"], "related_topics": ["Pattern formation", "Lateral inhibition", "Morphogen", "Autocatalysis", "Biological system", "Common spatial pattern", "Aperiodic graph", "Morphogenesis", "Complex system"]}
{"id": "1992476998", "references": ["2153782322", "2070900790", "2253776861", "1594551768", "1997494543", "2117731089", "2016354087", "2314890394", "1546859207", "22297218"], "title": "A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue", "abstract": "It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli. A mathematical model of simple cortical and thalamic nervous tissue is consequently developed, comprising two types of neurons (excitatory and inhibitory), homogeneously distributed in planar sheets, and interacting by way of recurrent lateral connexions. Following a discussion of certain anatomical and physiological restrictions on such interactions, numerical solutions of the relevant non-linear integro-differential equations are obtained. The results fall conveniently into three categories, each of which is postulated to correspond to a distinct type of tissue: sensory neo-cortex, archior prefrontal cortex, and thalamus. The different categories of solution are referred to as dynamical modes. The mode appropriate to thalamus involves a variety of non-linear oscillatory phenomena. That appropriate to archior prefrontal cortex is defined by the existence of spatially inhomogeneous stable steady states which retain contour information about prior stimuli. Finally, the mode appropriate to sensory neo-cortex involves active transient responses. It is shown that this particular mode reproduces some of the phenomenology of visual psychophysics, including spatial modulation transfer function determinations, certain metacontrast effects, and the spatial hysteresis phenomenon found in stereopsis.", "citation_count": "66", "reference_count": "2,021", "date": "1973", "authors": ["H. R. Wilson", "J. D. Cowan"], "related_topics": ["Sensory system", "Thalamus", "Prefrontal cortex", "Wilson\u2013Cowan model", "Cerebral cortex", "Electrophysiology", "Mathematical theory", "Visual Psychophysics", "Neuroscience"]}
{"id": "2087895317", "references": ["1983519005", "2026911129", "1998093640", "2740556548", "1973108021", "1995875735", "2108704847", "1484822889", "2799137445", "1553893196"], "title": "Selforganization of matter and the evolution of biological macromolecules", "abstract": "", "citation_count": "97", "reference_count": "5,078", "date": "1971", "authors": ["Manfred Eigen"], "related_topics": ["Macromolecular Substances", "Abiogenesis", "Macromolecule", "Polymer science", "Biological evolution", "Lethal mutagenesis", "Prebiotic evolution", "Quasispecies theory", "RNA Nucleotidyltransferases"]}
{"id": "1981520343", "references": ["2323067647", "2123335490", "2151189852", "2003370853", "1995756857", "2048330959", "2171727778", "2943083682", "2108138754"], "title": "The neural mechanism of binocular depth discrimination", "abstract": "1. Binocularly driven units were investigated in the cat's primary visual cortex.\r\n\r\n\r\n\r\n2. It was found that a stimulus located correctly in the visual fields of both eyes was more effective in driving the units than a monocular stimulus, and much more effective than a binocular stimulus which was correctly positioned in only one eye: the response to the correctly located image in one eye is vetoed if the image is incorrectly located in the other eye.\r\n\r\n\r\n\r\n3. The vertical and horizontal disparities of the paired retinal images that yielded the maximum response were measured in 87 units from seven cats: the range of horizontal disparities was 6\u00b76\u00b0, of vertical disparities 2\u00b72\u00b0.\r\n\r\n\r\n\r\n4. With fixed convergence, different units will be optimally excited by objects lying at different distances. This may be the basic mechanism underlying depth discrimination in the cat.", "citation_count": "0", "reference_count": "1,584", "date": "1967", "authors": ["H. B. Barlow", "C. Blakemore", "J. D. Pettigrew"], "related_topics": ["Stereopsis", "Binocular neurons", "Monocular", "Visual cortex", "Stimulus (physiology)", "Computer vision", "Communication", "Computer science", "Artificial intelligence", "Retinal Disparity"]}
{"id": "2001963156", "references": ["1981520343", "913592313", "2117731089", "2418905964", "1991232523"], "title": "Stereoscopic vision in macaque monkey. Cells sensitive to binocular depth in area 18 of the macaque monkey cortex.", "abstract": "Stereoscopic Vision in Macaque Monkey: Cells sensitive to Binocular Depth in Area 18 of the Macaque Monkey Cortex", "citation_count": "5", "reference_count": "762", "date": "1970", "authors": ["D. H. Hubel", "T. N. Wiesel"], "related_topics": ["Macaque", "Binocular neurons", "Cortex (anatomy)", "Stereopsis", "Neuroscience", "Biology"]}
{"id": "1594551768", "references": ["2149194912", "2119814172", "2112332687", "1624854622", "2036109700", "2067393309", "2951617899", "2098580305", "2119885245"], "title": "RECEPTIVE FIELDS AND FUNCTIONAL ARCHITECTURE IN TWO NONSTRIATE VISUAL AREAS (18 AND 19) OF THE CAT.", "abstract": "", "citation_count": "0", "reference_count": "3,635", "date": "1965", "authors": ["David H. Hubel", "Torsten N. Wiesel"], "related_topics": ["Receptive field", "Binocular neurons", "Orientation column", "Visual cortex", "Occipital lobe", "Neuroscience", "Computer science", "Coloring agents", "Geniculate body"]}
{"id": "1588340522", "references": ["1990840741", "2103212315", "1594551768", "2134598304", "2072581810", "2146567096", "2117731089", "1982280357", "2047496335", "2116360511"], "title": "Functional architecture of macaque monkey visual cortex", "abstract": "", "citation_count": "67", "reference_count": "1,741", "date": "1977", "authors": ["D. R. Rubel", "T. N. Wiesel"], "related_topics": ["Macaque", "Visual cortex", "Neuroscience", "Biology"]}
{"id": "2053120614", "references": ["1990840741", "1594551768", "2134598304", "2072581810", "2146567096", "2150232640", "2168570287", "2047496335", "2084577485", "1869259979"], "title": "Ferrier Lecture: Functional Architecture of Macaque Monkey Visual Cortex", "abstract": "Of the many possible functions of the macaque monkey primary visual cortex (striate cortex, area 17) two are now fairly well understood. First, the incoming information from the lateral geniculate bodies is rearranged so that most cells in the striate cortex respond to specifically oriented line segments, and, second, information originating from the two eyes converges upon single cells. The rearrangement and convergence do not take place immediately, however: in layer IVc, where the bulk of the afferents terminate, virtually all cells have fields with circular symmetry and are strictly monocular, driven from the left eye or from the right, but not both; at subsequent stages, in layers above and below IVc, most cells show orientation specificity, and about half are binocular. In a binocular cell the receptive fields in the two eyes are on corresponding regions in the two retinas and are identical in structure, but one eye is usually more effective than the other in influencing the cell; all shades of ocular dominance are seen. These two functions are strongly reflected in the architecture of the cortex, in that cells with common physiological properties are grouped together in vertically organized systems of columns. In an ocular dominance column all cells respond preferentially to the same eye. By four independent anatomical methods it has been shown that these columns have the from of vertically disposed alternating left-eye and right-eye slabs, which in horizontal section form alternating stripes about 400 $\\mu $m thick, with occasional bifurcations and blind endings. Cells of like orientation specificity are known from physiological recordings to be similarly grouped in much narrower vertical sheeet-like aggregations, stacked in orderly sequences so that on traversing the cortex tangentially one normally encounters a succession of small shifts in orientation, clockwise or counterclockwise; a 1 mm traverse is usually accompanied by one or several full rotations through 180 degrees, broken at times by reversals in direction of rotation and occasionally by large abrupt shifts. A full complement of columns, of either type, left-plus-right eye or a complete 180 degrees sequence, is termed a hypercolumn. Columns (and hence hypercolumns) have roughly the same width throughout the binocular part of the cortex. The two independent systems of hypercolumns are engrafted upon the well known topographic representation of the visual field. The receptive fields mapped in a vertical penetration through cortex show a scatter in position roughly equal to the average size of the fields themselves, and the area thus covered, the aggregate receptive field, increases with distance from the fovea. A parallel increase is seen in reciprocal magnification (the number of degrees of visual field corresponding to 1 mm of cortex). Over most or all of the striate cortex a movement of 1-2 mm, traversing several hypercolumns, is accompanied by a movement through the visual field about equal in size to the local aggregate receptive field. Thus any 1-2 mm block of cortex contains roughly the machinery needed to subserve an aggregate receptive field. In the cortex the fall-off in detail with which the visual field is analysed, as one moves out from the foveal area, is accompanied not by a reduction in thickness of layers, as is found in the retina, but by a reduction in the area of cortex (and hence the number of columnar units) devoted to a given amount of visual field: unlike the retina, the striate cortex is virtually uniform morphologically but varies in magnification. In most respects the above description fits the newborn monkey just as well as the adult, suggesting that area 17 is largely genetically programmed. The ocular dominance columns, however, are not fully developed at birth, since the geniculate terminals belonging to one eye occupy layer IVc throughout its length, segregating out into separate columns only after about the first 6 weeks, whether or not the animal has visual experience. If one eye is sutured closed during this early period the columns belonging to that eye become shrunken and their companions correspondingly expanded. This would seem to be at least in part the result of interference with normal maturation, though sprouting and retraction of axon terminals are not excluded.", "citation_count": "56", "reference_count": "3,475", "date": "1977", "authors": ["David Hunter Hubel", "Torsten Nils Wiesel"], "related_topics": ["Orientation column", "Ocular dominance column", "Visual cortex", "Binocular neurons", "Receptive field", "Visual system", "Cortex (anatomy)", "Ocular dominance", "Anatomy", "Geography"]}
{"id": "22889343", "references": ["2116360511", "1594551768", "2010315761"], "title": "Self-organization of a neural network which gives position-invariant response", "abstract": "In this paper, I propose a new algorithm for self-organizing a multilayered neural network which has an ability to recognize patterns based on the geometrical similarity of their shapes. This network, whose nickname is \"neo-cognitron\", has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of a photoreceptor layer followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \"S-cells\", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \"C-cells\" similar to complex cells or higher order hypercomplex cells. The input synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We don't need any \"teacher\" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer. The network has been simulated on a digital computer. After completion of self-organization, the stimulus patterns has become to elicit their own response from the last C-cell layer. That is, the response of the last C-cell layer changes without fail, if a stimulus patterns of a different category is presented to the input layer. The response of that layer, however, is not affected by the pattern's position at all. Neither is it affected by a certain amount of changes of the pattern's shape or size.", "citation_count": "3", "reference_count": "2", "date": "1979", "authors": ["Kunihiko Fukushima"], "related_topics": ["Artificial neural network", "Invariant (mathematics)", "Unsupervised learning", "Self-organization", "Hypercomplex number", "Stimulus (physiology)", "Modular design", "Cascade", "Pattern recognition", "Artificial intelligence", "Mathematics"]}
{"id": "2324189819", "references": ["2092695921", "2071239231", "2101926813"], "title": "(Invited) An Advanced MOS-IC Process Technology Using Oxidation of Oxygen-Doped Polycrystalline Silicon Films", "abstract": "", "citation_count": "0", "reference_count": "3", "date": "1978", "authors": ["Shuichi Sato", "Tadanori Yamaguchi", "Douglas Ritchie", "Karen Seaward", "Jack Sachitano", "Peter Burke"], "related_topics": ["Polycrystalline silicon", "Doping", "Oxygen", "Materials science", "Optoelectronics", "Scientific method"]}
{"id": "2010315761", "references": ["2008625057", "2887242076", "2103212315", "1594551768", "2048205984", "2322002063", "2091546412", "2116360511", "2155051950", "2058568633"], "title": "Cognitron: A self-organizing multilayered neural network", "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \"The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y\". By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \"cognitron\", is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \"teacher\" which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.", "citation_count": "12", "reference_count": "569", "date": "1975", "authors": ["Kunihiko Fukushima"], "related_topics": ["Receptive field", "Perceptron", "Artificial neural network", "Synapse", "Stimulus (physiology)", "Neuron", "Artificial intelligence", "Complex system", "Computer science", "Stimulus pattern"]}
{"id": "2091546412", "references": ["2155986444", "2034219019", "2028031284", "1977187949", "2009172409", "2039360139", "2101926813", "2056570598", "2010315761", "2048419276"], "title": "CURRENT OBSERVATIONS ON THE ACTION OF SODIUM LAURYL SULFATE ON THE CELL MEMBRANE OF E. COLI", "abstract": "", "citation_count": "0", "reference_count": "217", "date": "1965", "authors": ["E Boydelatour", "A Bolle", "E Kellenberger"], "related_topics": ["Sodium dodecyl sulfate", "Sodium", "Sulfate", "Escherichia coli", "Cell membrane", "Chromatography", "Chemistry", "Current (fluid)"]}
{"id": "2272360941", "references": ["2092048435", "1594551768", "2005322711", "1799331832", "2088643164", "2117731089", "2170274374", "1981520343", "2116360511", "1979285635"], "title": "Visual properties of neurons in inferotemporal cortex of the Macaque.", "abstract": "IN THE LAST DEC,ADE, considerable progress has been made in understanding the physiology of one of the most fundamental aspects of human experience: perception of the visual world. It is now clear that the retina and visual pathways do not simply transmit a mosaic of Iight and dark to some central sensorium. Rather, even at the retinal level, specific features of visual stimuli are detected and their presence communicated to the next level. In cats and monkeys, the geniculostriate visual system consists of a series of converging and diverging connections such that at each successive tier of processing mechanism, single neurons respond to increasingly more specific visual stimuli falling on an increasingly wider area of the retina (19-Z). How far does this analytical-synthetic process continue whereby individual cells have more and more specific trigger features? Are there regions of the brain beyond striate and prestriatel cortex where this processing of visual information is carrie,d further? If so, how far and in what way? Are there cells that are concerned with the storage of visual information as well as its analysis? There are several lines of evidence suggesting that a possible site for further processing of visual information and perhaps even for storage of such information might, in the monkey, be inferotemporal cortexthe cortex on the inferior convexity of the temporal lobe. First, this area receives afferents from prestriate cortex which itself processes visual information received from", "citation_count": "33", "reference_count": "1,822", "date": "1972", "authors": ["C G Gross", "C E Rocha-Miranda", "D B Bender"], "related_topics": ["Visual system", "Visual perception", "Form perception", "Cortex (anatomy)", "Cognitive neuroscience of visual object recognition", "Temporal lobe", "Color vision", "Macaque", "Neuroscience", "Psychology"]}
{"id": "2170973209", "references": ["2618530766", "2251939518", "2130942839", "179875071", "1832693441", "1810943226", "1895577753", "2131744502", "2132339004", "2064675550"], "title": "Semi-supervised Sequence Learning", "abstract": "We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.", "citation_count": "38", "reference_count": "794", "date": "2015", "authors": ["Andrew M. Dai", "Quoc V. Le"], "related_topics": ["Sequence learning", "Autoencoder", "Language model", "Sequence", "Machine learning", "Pattern recognition", "Computer science", "Point (typography)", "Artificial intelligence"]}
{"id": "3123753580", "references": ["2154642048", "2156163116", "194249466", "2170393096", "2110798204", "2310919327", "2159737176", "2139427956", "3121926921", "2064675550"], "title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "abstract": "Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35% error rate on the famous MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images, and graphics cards to greatly speed up learning.", "citation_count": "12", "reference_count": "807", "date": "2010", "authors": ["Dan Claudiu Ciresan", "Ueli Meier", "Luca Maria Gambardella", "Juergen Schmidhuber"], "related_topics": ["MNIST database", "Artificial neural network", "Perceptron", "Word error rate", "Benchmark (computing)", "Pattern recognition", "Speedup", "Computer science", "Artificial intelligence"]}
{"id": "22297218", "references": ["2042264548", "2076063813", "2963305465", "2002016471", "2148138104", "2788388592", "2066785847", "1928882148", "2166667242"], "title": "The organization of behavior", "abstract": "", "citation_count": "0", "reference_count": "14,153", "date": "1988", "authors": ["Donald O. Hebb"], "related_topics": ["Computer science", "Cognitive science"]}
{"id": "1572161815", "references": ["2150535417", "2202312251", "2092861539", "2101959500", "2136064843", "2138484437", "2108695617", "2104958104", "2111367974", "2049933349"], "title": "Stable Adaptive Systems", "abstract": "Stability theory simple adaptive systems adaptive observers the control problem persistent excitation error models robust adaptive control the control problem - relaxation of assumptions multivariable adaptive systems applications of adaptive control.", "citation_count": "0", "reference_count": "4,923", "date": "1989", "authors": ["K. S. Narendra", "A. M. Annaswamy"], "related_topics": ["Adaptive control", "Adaptive system", "Multivariable calculus", "Relaxation (approximation)", "Stability theory", "Control theory", "Control engineering", "Computer science"]}
{"id": "2174984063", "references": ["2147060498", "1545318875", "2128351324", "2054354066", "2101482846", "2158831443", "2145942961", "2103086465", "2141029212", "2066340926"], "title": "Simple \u201cNeural\u201d optimization networks: an A/D converter, signal decision circuit, and a linear programming circuit", "abstract": "We describe how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors. Analog-to-digital (A/D) conversion was considered as a simple optimization problem, and an A/D converter of novel architecture was designed. A/D conversion is a simple example of a more general class of signal-decision problems which we show could also be solved by appropriately constructed networks. Circuits to solve these problems were designed using general principles which result from an understanding of the basic collective computational properties of a specific class of analog-processor networks. We also show that a network which solves linear programming problems can be understood from the same concepts.", "citation_count": "0", "reference_count": "2,752", "date": "1988", "authors": ["David W. Tank", "John J. Hopfield"], "related_topics": ["Optimization problem", "Circuit extraction", "Linear circuit", "Linear programming", "Equivalent circuit", "Discrete circuit", "Electronic circuit", "Simple (abstract algebra)", "Topology", "Electronic engineering", "Computer science"]}
{"id": "2122136962", "references": ["2035943077", "2120512179", "2099681040", "2031459654", "2012532135", "1516750454", "147667152", "2104002432", "1998112257", "1979554781"], "title": "Adaptive control of linearizable systems", "abstract": "The authors give some initial results on the adaptive control of minimum-phase nonlinear systems which are exactly input-output linearizable by state feedback. Parameter adaptation is used as a technique to make robust the exact cancellation of nonlinear terms, which is called for in the linearization technique. The application of the adaptive technique to control of robot manipulators is discussed. Only the continuous-time case is considered; extensions to the discrete-time and sampled-data cases are not obvious. &gt;", "citation_count": "16", "reference_count": "1,428", "date": "1989", "authors": ["S.S. Sastry", "A. Isidori"], "related_topics": ["Adaptive control", "Linearization", "Robot control", "Control system", "Nonlinear system", "Robustness (computer science)", "Control theory", "Linear equation", "Robot", "Mathematics"]}
{"id": "2095425517", "references": ["1597286183", "2287077337", "2414319931", "2129830031", "2177721432", "2177040213", "2111926818", "2293063825", "1554576613", "2310420328"], "title": "Bidirectional associative memories", "abstract": "Stability and encoding properties of two-layer nonlinear feedback neural networks are examined. Bidirectionality is introduced in neural nets to produce two-way associative search for stored associations. The bidirectional associative memory (BAM) is the minimal two-layer nonlinear feedback network. The author proves that every n-by-p matrix M is a bidirectionally stable heteroassociative content-addressable memory for both binary/bipolar and continuous neurons. When the BAM neutrons are activated, the network quickly evolves to a stable state of two-pattern reverberation, or resonance. The stable reverberation corresponds to a system energy local minimum. Heteroassociative information is encoded in a BAM by summing correlation matrices. The BAM storage capacity for reliable recall is roughly m&lt;min (n,p). It is also shown that it is better on average to use bipolar (-1,1) coding than binary", "citation_count": "29", "reference_count": "2,583", "date": "1988", "authors": ["B. Kosko"], "related_topics": ["Bidirectional associative memory", "Content-addressable memory", "Content-addressable storage", "Encoding (memory)", "Artificial neural network", "Associative property", "Matrix (mathematics)", "Nonlinear system", "Topology", "Recall", "Artificial intelligence", "Computer science"]}
{"id": "2134673975", "references": ["1561941139", "2166735509", "3102923851", "1965324089", "1604822211", "1976410223", "1978956894", "1563938275", "2798813531", "1986922155"], "title": "A survey of industrial model predictive control technology", "abstract": "This paper provides an overview of commercially available model predictive control (MPC) technology, both linear and nonlinear, based primarily on data provided by MPC vendors. A brief history of industrial MPC technology is presented first, followed by results of our vendor survey of MPC control and identification technology. A general MPC control algorithm is presented, and approaches taken by each vendor for the different aspects of the calculation are described. Identification technology is reviewed to determine similarities and differences between the various approaches. MPC applications performed by each vendor are summarized by application area. The final section presents a vision of the next generation of MPC technology, with an emphasis on potential business and research opportunities. r 2002 Elsevier Science Ltd. All rights reserved.", "citation_count": "60", "reference_count": "5,445", "date": "2003", "authors": ["S.Joe Qin", "Thomas A. Badgwell"], "related_topics": ["Model predictive control", "Vendor", "Manufacturing engineering", "Control (management)", "Engineering", "Control algorithm", "Economic model predictive control", "Identification technology", "Nonlinear predictive control", "Research opportunities"]}
{"id": "1521785144", "references": ["3029645440", "2160337655", "2159080219", "2049633694", "2161406034", "1483307070", "2049981393", "2798766386", "2098613108", "3145128584"], "title": "Principles of Robot Motion: Theory, Algorithms, and Implementations", "abstract": "A text that makes the mathematical underpinnings of robot motion accessible and relates low-level details of implementation to high-level algorithmic concepts. Robot motion planning has become a major focus of robotics. Research findings can be applied not only to robotics but to planning routes on circuit boards, directing digital actors in computer graphics, robot-assisted surgery and medicine, and in novel areas such as drug design and protein folding. This text reflects the great advances that have taken place in the last ten years, including sensor-based planning, probabalistic planning, localization and mapping, and motion planning for dynamic and nonholonomic systems. Its presentation makes the mathematical underpinnings of robot motion accessible to students of computer science and engineering, rleating low-level implementation details to high-level algorithmic concepts.", "citation_count": "232", "reference_count": "3,606", "date": "2005", "authors": ["Howie M. Choset", "Jean-Claude Latombe"], "related_topics": ["Motion planning", "Probabilistic roadmap", "Robotics", "Computer graphics", "Nonholonomic system", "Implementation", "Human\u2013computer interaction", "Computer science", "Artificial intelligence", "Focus (computing)", "Presentation"]}
{"id": "1588998206", "references": ["2610857016", "2121117794", "2037612300", "2042106612", "2032173185", "2039867272", "1534096605", "1488435683", "1868559974", "2798909945"], "title": "Essentials of Robust Control", "abstract": "1. Introduction. 2. Linear Algebra. 3. Linear Systems. 4. H2 and Ha Spaces. 5. Internal Stability. 6. Performance Specifications and Limitations. 7. Balanced Model Reduction. 8. Uncertainty and Robustness. 9. Linear Fractional Transformation. 10. m and m- Synthesis. 11. Controller Parameterization. 12. Algebraic Riccati Equations. 13. H2 Optimal Control. 14. Ha Control. 15. Controller Reduction. 16. Ha Loop Shaping. 17. Gap Metric and ...u- Gap Metric. 18. Miscellaneous Topics. Bibliography. Index.", "citation_count": "187", "reference_count": "5,091", "date": "1997", "authors": ["Kemin Zhou", "John Comstock Doyle"], "related_topics": ["Linear system", "Robust control", "Linear algebra", "Optimal control", "Linear fractional transformation", "Robustness (computer science)", "Algebraic number", "Control theory", "Mathematics", "Bibliography"]}
{"id": "2065297540", "references": ["2133361850", "2148970883", "2070959866", "2089876099", "1887006513", "1964373008", "2798813531", "2032128836", "2146560209", "1980662086"], "title": "Flatness and defect of non-linear systems: introductory theory and examples", "abstract": "We introduce flat systems, which are equivalent to linear ones via a special type of feedback called endogenous. Their physical properties are subsumed by a linearizing output and they might be regarded as providing another nonlinear extension of Kalman's controllability. The distance to flatness is measured by a non-negative integer, the defect. We utilize differential algebra where flatness- and defect are best defined without distinguishing between input, state, output and other variables. Many realistic classes of examples are flat. We treat two popular ones: the crane and the car with n trailers, the motion planning of which is obtained via elementary properties of plane curves. The three non-flat examples, the simple, double and variable length pendulums, are borrowed from non-linear physics. A high frequency control strategy is proposed such that the averaged systems become flat.", "citation_count": "48", "reference_count": "3,623", "date": "1995", "authors": ["Michel Fliess", "Jean L\u00e9vine", "Philippe Martin", "Pierre Rouchon"], "related_topics": ["Flatness (systems theory)", "Controllability", "Nonlinear system", "Differential algebra", "Plane curve", "Differential equation", "Kalman filter", "Control system", "Topology", "Control theory", "Mathematics"]}
{"id": "2912155302", "references": ["2132984323", "2232702494", "2116013899", "1965324089", "2042371054", "1490632837", "2049633694", "2798813531", "2107790757", "1991605728"], "title": "Dynamic Textures", "abstract": "Dynamic textures are sequences of images of moving scenes that exhibit certain stationarity properties in times these include sea-waves, smoke, foliage, whirlwind etc. We present a characterization of dynamic textures that poses the problems of modeling, learning, recognizing and synthesizing dynamic textures on a firm analytical footing. We borrow tools from system identification to capture the \u201cessence\u201d of dynamic texturess we do so by learning (i.e. identifying) models that are optimal in the sense of maximum likelihood or minimum prediction error variance. For the special case of second-order stationary processes, we identify the model sub-optimally in closed-form. Once learned, a model has predictive power and can be used for extrapolating synthetic sequences to infinite length with negligible computational cost. We present experimental evidence that, within our framework, even low-dimensional models can capture very complex visual phenomena.", "citation_count": "44", "reference_count": "1,151", "date": "2003", "authors": ["Gianfranco Doretto", "Alessandro Chiuso", "Ying Nian Wu", "Stefano Soatto"], "related_topics": ["Generative model", "System identification", "Minimum description length", "Autoregressive\u2013moving-average model", "Algorithm", "Canonical correlation", "Image compression", "Special case", "Computer science", "Whirlwind"]}
{"id": "2149072817", "references": ["2132984323", "2166982406", "2041752335", "2798813531", "2096684483", "2116988482", "2093112233", "2098914003", "1996021349", "2103504761"], "title": "Wavelets and filter banks: theory and design", "abstract": "The wavelet transform is compared with the more classical short-time Fourier transform approach to signal analysis. Then the relations between wavelets, filter banks, and multiresolution signal processing are explored. A brief review is given of perfect reconstruction filter banks, which can be used both for computing the discrete wavelet transform, and for deriving continuous wavelet bases, provided that the filters meet a constraint known as regularity. Given a low-pass filter, necessary and sufficient conditions for the existence of a complementary high-pass filter that will permit perfect reconstruction are derived. The perfect reconstruction condition is posed as a Bezout identity, and it is shown how it is possible to find all higher-degree complementary filters based on an analogy with the theory of Diophantine equations. An alternative approach based on the theory of continued fractions is also given. These results are used to design highly regular filter banks, which generate biorthogonal continuous wavelet bases with symmetries. &gt;", "citation_count": "34", "reference_count": "2,709", "date": "1992", "authors": ["M. Vetterli", "C. Herley"], "related_topics": ["Discrete wavelet transform", "Wavelet", "Filter design", "Continuous wavelet", "Quadrature mirror filter", "Second-generation wavelet transform", "Filter bank", "Lifting scheme", "Algorithm", "Arithmetic", "Mathematics"]}
{"id": "2012445782", "references": ["2164278908", "2156909104", "2160643434", "1973948212", "2107396783", "2135046866", "1875112053", "2100556411", "2296319761", "1663973292"], "title": "Adaptation, Learning, and Optimization Over Networks", "abstract": "This work deals with the topic of information processing over graphs. The presentation is largely self-contained and covers results that relate to the analysis and design of multi-agent networks for the distributed solution of optimization, adaptation, and learning problems from streaming data through localized interactions among agents. The results derived in this work are useful in comparing network topologies against each other, and in comparing networked solutions against centralized or batch implementations. There are many good reasons for the peaked interest in distributed implementations, especially in this day and age when the word \"network\" has become commonplace whether one is referring to social networks, power networks, transportation networks, biological networks, or other types of networks. Some of these reasons have to do with the benefits of cooperation in terms of improved performance and improved resilience to failure. Other reasons deal with privacy and secrecy considerations where agents may not be comfortable sharing their data with remote fusion centers. In other situations, the data may already be available in dispersed locations, as happens with cloud computing. One may also be interested in learning through data mining from big data sets. Motivated by these considerations, this work examines the limits of performance of distributed solutions and discusses procedures that help bring forth their potential more fully. The presentation adopts a useful statistical framework and derives performance results that elucidate the mean-square stability, convergence, and steady-state behavior of the learning networks. At the same time, the work illustrates how distributed processing over graphs gives rise to some revealing phenomena due to the coupling effect among the agents. These phenomena are discussed in the context of adaptive networks, along with examples from a variety of areas including distributed sensing, intrusion detection, distributed estimation, online adaptation, network system theory, and machine learning.", "citation_count": "267", "reference_count": "537", "date": "2014", "authors": ["Ali Sayed"], "related_topics": ["Evolving networks", "Biological network", "Big data", "Stability (learning theory)", "Network topology", "Statistical learning theory", "Cloud computing", "Information processing", "Computer science", "Distributed computing"]}
{"id": "1986922155", "references": ["1492106702", "2006868529", "2026886971", "2347117266", "1969964504", "2798813531", "2071264748", "2154831602", "578614108", "161168623"], "title": "Generalized predictive control\u2014Part I. The basic algorithm", "abstract": "Current self-tuning algorithms lack robustness to prior choices of either dead-time or model order. A novel method\u2014generalized predictive control or GPC\u2014is developed which is shown by simulation studies to be superior to accepted techniques such as generalized minimum-variance and pole-placement. This receding-horizon method depends on predicting the plant's output over several steps based on assumptions about future control actions. One assumption\u2014that there is a \u201ccontrol horizon\u201d beyond which all control increments become zero\u2014is shown to be beneficial both in terms of robustness and for providing simplified calculations. Choosing particular values of the output and control horizons produces as subsets of the method various useful algorithms such as GMV, EPSAC, Peterka's predictive controller (1984, Automatica, 20, 39\u201350) and Ydstie's extended-horizon design (1984, IFAC 9th World Congress, Budapest, Hungary). Hence GPC can be used either to control a \u201csimple\u201d plant (e.g. open-loop stable) with little prior knowledge or a more complex plant such as nonminimum-phase, open-loop unstable and having variable dead-time. In particular GPC seems to be unaffected (unlike pole-placement strategies) if the plant model is overparameterized. Furthermore, as offsets are eliminated by the consequence of assuming a CARIMA plant model, GPC is a contender for general self-tuning applications. This is verified by a comparative simulation study.", "citation_count": "18", "reference_count": "5,893", "date": "1987", "authors": ["D W Clarke", "C Mohtadi", "P S Tuffs"], "related_topics": ["Adaptive control", "Model predictive control", "Carima", "Robustness (computer science)", "Control theory", "Algorithm", "Engineering", "Model order", "Multivariable predictive control", "Predictive controller"]}
{"id": "2099839128", "references": ["2005314985", "2100440346", "2611147814", "187211397", "2798813531", "1489872560", "1488435683", "1592648094", "1500921805", "2798909945"], "title": "Linear Matrix Inequalities in System and Control Theory", "abstract": "Preface 1. Introduction Overview A Brief History of LMIs in Control Theory Notes on the Style of the Book Origin of the Book 2. Some Standard Problems Involving LMIs. Linear Matrix Inequalities Some Standard Problems Ellipsoid Algorithm Interior-Point Methods Strict and Nonstrict LMIs Miscellaneous Results on Matrix Inequalities Some LMI Problems with Analytic Solutions 3. Some Matrix Problems. Minimizing Condition Number by Scaling Minimizing Condition Number of a Positive-Definite Matrix Minimizing Norm by Scaling Rescaling a Matrix Positive-Definite Matrix Completion Problems Quadratic Approximation of a Polytopic Norm Ellipsoidal Approximation 4. Linear Differential Inclusions. Differential Inclusions Some Specific LDIs Nonlinear System Analysis via LDIs 5. Analysis of LDIs: State Properties. Quadratic Stability Invariant Ellipsoids 6. Analysis of LDIs: Input/Output Properties. Input-to-State Properties State-to-Output Properties Input-to-Output Properties 7. State-Feedback Synthesis for LDIs. Static State-Feedback Controllers State Properties Input-to-State Properties State-to-Output Properties Input-to-Output Properties Observer-Based Controllers for Nonlinear Systems 8. Lure and Multiplier Methods. Analysis of Lure Systems Integral Quadratic Constraints Multipliers for Systems with Unknown Parameters 9. Systems with Multiplicative Noise. Analysis of Systems with Multiplicative Noise State-Feedback Synthesis 10. Miscellaneous Problems. Optimization over an Affine Family of Linear Systems Analysis of Systems with LTI Perturbations Positive Orthant Stabilizability Linear Systems with Delays Interpolation Problems The Inverse Problem of Optimal Control System Realization Problems Multi-Criterion LQG Nonconvex Multi-Criterion Quadratic Problems Notation List of Acronyms Bibliography Index.", "citation_count": "181", "reference_count": "10,861", "date": "1987", "authors": ["E.E. Yaz"], "related_topics": ["Linear system", "Linear-quadratic-Gaussian control", "Condition number", "State-transition matrix", "Linear matrix inequality", "Ellipsoid method", "Kalman\u2013Yakubovich\u2013Popov lemma", "Nonlinear system", "Control theory", "Mathematics"]}
{"id": "1506119886", "references": ["1607633954", "2610857016", "1586658033", "2156909104", "2798813531", "2099839128", "2014172170", "1868559974", "2011039300", "2798909945"], "title": "Stability of Time-Delay Systems", "abstract": "Preface, Notations 1.Introduction to Time-Delay Systems I.Frequency-Domain Approach 2.Systems with Commensurate Delays 3.Systems withIncommensurate Delays 4.Robust Stability Analysis II.Time Domain Approach 5.Systems with Single Delay 6.Robust Stability Analysis 7.Systems with Multiple and Distributed Delays III.Input-Output Approach 8.Input-output stability A.Matrix Facts B.LMI and Quadratic Integral Inequalities Bibliography Index", "citation_count": "217", "reference_count": "5,675", "date": "2003", "authors": ["Kequin Gu", "Jie Chen", "Vladimir Kharitonov"], "related_topics": ["Quadratic integral", "Stability (probability)", "Control theory", "Domain (software engineering)", "Bibliography", "Mathematics", "Delay dependent", "Delay margin", "Lyapunov krasovskii", "Wirtinger inequality"]}
{"id": "1603277681", "references": ["1975013860", "2007250360", "2102380305", "2734408173", "1825869920", "2014018052", "2036407535", "2172172920", "2166670624"], "title": "Dynamic System Identification: Experiment Design and Data Analysis", "abstract": "", "citation_count": "0", "reference_count": "2,202", "date": "2012", "authors": ["Gc Goodwin", "Goodwin Gc", "Payne Rl"], "related_topics": ["System identification", "Experimental data", "Design of experiments", "Computer science", "Data mining"]}
{"id": "2103496339", "references": ["2042264548", "18965947", "1652505363", "1594563152", "2019363670", "2165758113", "2137983211", "3038830718", "2066789935", "1971735090"], "title": "Approximation by superpositions of a sigmoidal function", "abstract": "In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.", "citation_count": "19", "reference_count": "15,140", "date": "1989", "authors": ["George Cybenko"], "related_topics": ["Universal approximation theorem", "Feedforward neural network", "Continuous function", "Stochastic neural network", "Rectifier (neural networks)", "Sigmoid function", "Recurrent neural network", "Types of artificial neural networks", "Applied mathematics", "Discrete mathematics", "Mathematics"]}
{"id": "2102380305", "references": ["1603277681", "1975424366", "2034562813", "2102041666", "2078841894", "2028847290", "2088051354", "1650765400", "2035100132", "2011406020"], "title": "Orthogonal least squares methods and their application to non-linear system identification", "abstract": "Abstract Identification algorithms based on the well-known linear least squares methods of gaussian elimination, Cholesky decomposition, classical Gram-Schmidt, modified Gram-Schmidt, Householder transformation, Givens method, and singular value decomposition are reviewed. The classical Gram-Schmidt, modified Gram-Schmidt, and Householder transformation algorithms are then extended to combine structure determination, or which terms to include in the model, and parameter estimation in a very simple and efficient manner for a class of multivariate discrete-time non-linear stochastic systems which are linear in the parameters.", "citation_count": "25", "reference_count": "1,901", "date": "1989", "authors": ["S. Chen", "S. A. Billings", "W. Luo"], "related_topics": ["Householder transformation", "Non-linear least squares", "Householder's method", "QR decomposition", "Linear least squares", "Cholesky decomposition", "Generalized least squares", "Singular value decomposition", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "1540723801", "references": ["2121863487", "1520710862", "2753542457", "1801704802", "2286699414", "2004537679", "1874027545", "2025653905"], "title": "Theory and Practice of Recursive Identification", "abstract": "Methods of recursive identification deal with the problem of building mathematical models of signals and systems on-line, at the same time as data is being collected. Such methods, which are also k ...", "citation_count": "0", "reference_count": "4,821", "date": "1983", "authors": ["Lennart Ljung", "Torsten S\u00f6derstr\u00f6m"], "related_topics": ["Identification (information)", "Mathematical model", "Machine learning", "Computer science", "Artificial intelligence", "Recursive prediction error"]}
{"id": "1650765400", "references": ["2057670157", "2330913512", "2101196233", "2075665712", "2798500587", "3047768732", "2028631419", "2122512809", "2170081783", "2009287270"], "title": "Factorization methods for discrete sequential estimation", "abstract": "", "citation_count": "17", "reference_count": "2,154", "date": "1977", "authors": ["Gerald J. Bierman"], "related_topics": ["Sequential estimation", "Incomplete LU factorization", "Dixon's factorization method", "Factorization", "Estimation theory", "Square root of a matrix", "Kalman filter", "Smoothing", "Algorithm", "Mathematical optimization", "Mathematics"]}
{"id": "2121029939", "references": ["2107878631", "194249466", "2154890045", "1674799117", "1971129545", "2057653135", "2100649405", "2064675550", "2036317923", "2136848157"], "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.", "citation_count": "24", "reference_count": "632", "date": "2001", "authors": ["F.A. Gers", "E. Schmidhuber"], "related_topics": ["Recurrent neural network", "Context (language use)", "Context-free language", "Context-sensitive language", "Artificial neural network", "Regular language", "Learning automata", "Natural language processing", "Hidden Markov model", "Computer science", "Artificial intelligence", "Recurrent neural nets"]}
{"id": "1525783482", "references": ["2964304707", "2108069432", "2170973209", "2964110616", "2076063813", "2130942839", "2144499799", "1810943226", "1689711448", "2141125852"], "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies", "abstract": "", "citation_count": "0", "reference_count": "1,666", "date": "2001", "authors": ["Sepp Hochreiter", "Yoshua Bengio"], "related_topics": ["Term (time)", "Short-term memory", "Computer science", "Artificial intelligence", "Machine learning", "Balanced flow", "Time lag"]}
{"id": "2113325037", "references": ["2102605133", "2618530766", "2128271252", "1994529670", "2131263044", "2146502635", "2030536784", "1976948919", "2022699039", "2135533529"], "title": "DeepPose: Human Pose Estimation via Deep Neural Networks", "abstract": "We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.", "citation_count": "24", "reference_count": "2,046", "date": "2014", "authors": ["Alexander Toshev", "Christian Szegedy"], "related_topics": ["Pose", "Deep learning", "Artificial neural network", "Machine learning", "Artificial intelligence", "Computer science", "Deep neural networks"]}
{"id": "1535689967", "references": [], "title": "Threshold models in non-linear time series analysis", "abstract": "One Introduction.- 1. Time Series Model Building.- 2. Stationarity.- 3. Linear Gaussian Models.- 4. Some Advantages and Some Limitations of Arma Models.- 5. What Next?.- Two Some Basic Concepts.- 1. Orientation.- 2. Limit Cycles.- 3. Some Examples of Threshold Models.- 4. Time Delay.- 5. Discussion.- Three Threshold Models.- 1. A Canonical Form.- 2. Generality of Setar Models.- 3. Non-Linear Difference Equations.- 4. Threshold Models and Discrete-Time Non-Linear Vibrations.- 5. Ergodicity.- 6. Stationary Distributions and Moments.- 7. Cyclical Structure and Multi-Step-Ahead Forecasting.- Four Identification.- 1. A General Principle.- 2. Estimation of Parameters.- 3. Sampling Properties.- 3.1 A General Result.- 3.2 Application of General Result to SETAR.- 3.3 Some Simulation Studies.- 3.4 Standard Errors of Some Parameter Estimates.- 3.5 Unsolved Problems.- 4. Diagnostics and Graphical Methods.- 5. Miscellanea.- Five Some Case Studies.- 1. Analysis of Some Ecological Data.- 1.1 Ecological Background.- 1.2 Some Graphical Results.- 1.3 A Full Identification.- 1.4 Diagnostics.- 1.5 Transformation.- 1.6 Extension to Two Species.- 2. Analysis of the Sunspot Numbers.- 2.1 Some Background.- 2.2 SETAR Modelling.- 2.3 Transformation.- 2.4 Multi-step-ahead Forecasting.- 2.5 Some Discussion.- 3. Analysis of Some Riverflow Data.- 3.1 Some Background.- 3.2 TARSO Modelling of Vatnsdalsa River, Iceland (1972-1974).- 3.3 TARSO Modelling of Jokulsa River, Iceland (1972-1974).- 3.4 Some Discussion.- 4. A Case Study with Laboratory Data.- 5. A Fuzzy Extension.- 6. Concluding Remarks.- Appedices.- References.- Author Index.", "citation_count": "0", "reference_count": "2,294", "date": "1983", "authors": ["Howell Tong"], "related_topics": ["SETAR", "Threshold model", "Time series", "Limit (mathematics)", "Gaussian", "Transformation (function)", "Canonical form", "Ergodicity", "Applied mathematics", "Statistics", "Mathematics"]}
{"id": "2136391815", "references": ["2075156252", "1994529670", "2161969291", "2535410496", "2963542991", "2120419212", "753012316", "2113325037", "36903255", "2124386111"], "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.", "citation_count": "31", "reference_count": "1,202", "date": "2014", "authors": ["Jonathan J Tompson", "Arjun Jain", "Yann LeCun", "Christoph Bregler"], "related_topics": ["Markov random field", "Pose", "Graphical model", "Domain (software engineering)", "Machine learning", "Computer vision", "Computer science", "Monocular", "Joint (audio engineering)", "Artificial intelligence"]}
{"id": "2082628174", "references": ["2001745976", "2012105858", "1993744672"], "title": "\u201cIgnition\u201d phenomena in random nets", "abstract": "The spread of excitation in a \u201crandom net\u201d is investigated. It is shown that if the thresholds of individual neurons in the net are equal to unity, a positive steady state of excitation will be reached equal to \u03b3, which previously had been computed as the weak connectivity of the net. If, however, the individual thresholds are greater than unity, either no positive steady state exists, or two such states depending on the magnitude of the axone density. In the latter case the smaller of the two steady states is unstable and hence resembles an \u201cignition point\u201d of the net. If the initial stimulation (assumed instantaneous) exceeds the \u201cignition point,\u201d the excitation of the net eventually assumes the greater steady state.", "citation_count": "3", "reference_count": "69", "date": "1952", "authors": ["Anatol Rapoport"], "related_topics": ["Steady state (electronics)", "Ignition system", "Excitation", "Mechanics", "Magnitude (mathematics)", "Net (mathematics)", "Control theory", "Mathematics", "Ignition point"]}
{"id": "2125812768", "references": [], "title": "Analysis of recursive stochastic algorithms", "abstract": "Recursive algorithms where random observations enter are studied in a fairly general framework. An important feature is that the observations my depend on previous \"outputs\" of the algorithm. The considered class of algorithms contains, e.g., stochastic approximation algorithm, recursive identification algorithm, and algorithms for adaptive control of linear systems. It is shown how a deterministic differential equation can be associated with the algorithm. Problems like convergence with probability one, possible convergence points and asymptotic behavior of the algorithm can all be studied in terms of this differential equation. Theorems stating the precise relationships between the differential equation and the algorithm are given as well as examples of applications of the results to problems in identification and adaptive control.", "citation_count": "34", "reference_count": "1,832", "date": "1977", "authors": ["L. Ljung"], "related_topics": ["Stochastic differential equation", "Stochastic partial differential equation", "Stochastic approximation", "Differential equation", "Stochastic optimization", "Stochastic control", "Adaptive control", "Continuous-time stochastic process", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "2159116087", "references": ["2082628174", "1554576613", "2037529930", "2124242926", "2072972096", "2159187100", "2079319526", "1587799944", "2798566718", "2918149778"], "title": "Characteristics of randomly connected threshold-element networks and network systems", "abstract": "The characteristics of the networks which are composed of many randomly connected threshold elements are investigated with the intention of understanding some aspects of information processing in nervous systems. In these networks, the statistical properties of connection are sufficient to determine the characteristics. Information is carried by the activity level of a network which designates the rate of exciting elements. Dynamics of the activity level is studied. Two statistical parameters, which are sufficient to determine the characteristics of networks, are extracted and the networks are categorized into three classes by these parameters. One is monostable, having only one stable activity level. Another is monostable or bistable according to the average threshold value of the elements. The third is astable or monostable. The characteristics of these three kinds of networks are analyzed in detail. Various systems can be obtained by connecting random networks, where the average thresholds of component networks can be controlled by other networks. The system performances are given. A stable oscillation of a long period is shown to exist in a system composed of two kinds of elements, i.e., excitatory and inhibitory elements, by randomly connecting them. A model for association of ideas is presented.", "citation_count": "11", "reference_count": "208", "date": "1971", "authors": ["S.-I. Amari"], "related_topics": ["Stochastic process", "Bistability", "Statistical parameter", "Association (object-oriented programming)", "Topology", "Information processing", "Multivibrator", "Boolean function", "Oscillation (cell signaling)", "Electronic engineering", "Computer science"]}
{"id": "1995030605", "references": ["2253776861", "1757333299", "2212384750", "2170274374", "2116360511", "1546859207", "1554576613", "22297218", "2894865365", "2055870134"], "title": "Brain functions and neural dynamics", "abstract": "Anatomical and physiological evidence is cited for the existence in the CNS of more or less discrete populations of interconnected neurons. These are given the term netlets. A model based on these observations is presented, in which it is assumed that the netlets are the fundamental building blocks out of which nets of considerable complexity may be as embled. The connectivity within each netlet is assumed to be random. Neuronal macrostates are defined in which the fractions of neurons active in each netlet are the dynamical variables. Thus the temporal and spatial fine structure of neuronal activity are considered to be of secondary significance and are disregarded.  These assumptions bring about an enormous reduction in complexity. Thus calculations and computer simulation studies become possible for systems hitherto inaccessible to quantitative description. It is hoped that the features retained in the model play a sufficiently significant role in the functioning of real neural nets to make these results meaningful.  The mathematical formalism and detailed numerical results appear in another paper of this issue ( Anninos, 1970 ). Some of these results are anticipated in this paper and their implications for our model are discussed. The study proceeds from a treatment of isolated probabilistic netlets to the dynamics of interacting netlets. Of particular interest are the conditions under which a netlet will go into sustained activity and the often extremely delicate control exerted by afferent excitatory or inhibitory biases. Hysteresis effects are common and may represent a type of short-term memory.  A variety of neural functions are listed to which some of these mechanisms may be applied. Among these are the modulating effects of the brain stem reticular formation on cortical and spinal neuron populations and the \u201cenergizing\u201d of cortical centers by spontaneous activity in sensory systems. Finally the concepts of netlet interaction are applied in conjunction with the principle of synaptic facilitation to information processing in the cortex. Examples given are sensory-sensory cortical conditioning and the formation of the classical conditioned reflex.", "citation_count": "41", "reference_count": "209", "date": "1970", "authors": ["E.M. Harth", "T.J. Csermely", "B. Beek", "R.D. Lindsay"], "related_topics": ["Sensory system", "Neural facilitation", "Neuroscience", "Artificial neural network", "Premovement neuronal activity", "Information processing", "Reticular formation", "Spinal neuron", "Probabilistic logic", "Computer science"]}
{"id": "2072972096", "references": ["2060666586", "2082628174", "2046196779", "2011937794"], "title": "Maintained Activity in Neural Nets", "abstract": "", "citation_count": "4", "reference_count": "71", "date": "1962", "authors": ["D. R. Smith", "C. H. Davidson"], "related_topics": ["Computer science", "Artificial neural network"]}
{"id": "2159187100", "references": ["1554576613"], "title": "Tests on a cell assembly theory of the action of the brain, using a large digital computer", "abstract": "Theories by D.O. Hebb and P.M. Milner on how the brain works were tested by simulating neuron nets on the IBM Type 704 Electronic Calculator. The formation of cell assemblies from an unorganized net of neurons was demonstrated, as well as a plausible mechanism for short-term memory and the phenomena of growth and fractionation of cell assemblies. The cell assemblies do not yet act just as the theory requires, but changes in the theory and the simulation offer promise for further experimentation.", "citation_count": "1", "reference_count": "351", "date": "1956", "authors": ["N. Rochester", "J. Holland", "L. Haibt", "W. Duda"], "related_topics": ["Artificial neural network", "Parallel computing", "Artificial intelligence", "Computer science", "Action (philosophy)", "Neuron", "Cell assembly", "Digital computer"]}
{"id": "2060666586", "references": ["2005087711", "2048616236", "2970228278", "1986694516", "2030341526", "2318077492", "2072521261", "2046801524", "1554576613", "2046196779"], "title": "Properties of a Mass of Cells Capable of Regenerating Pulses", "abstract": "Cells having some properties similar to those of neurons are considered. A mass of such cells, randomly placed together with a uniform volume density, appears capable of supporting various simple forms of activity, including plane waves, spherical and circular waves and vortex effects. The propagation of a plane wave of activity has been considered in some detail. It is shown that a wave may be initiated in a mass of such cells by a number of individual stimuli. The mass has a very sensitive threshold to such stimulation. This threshold depends on cell properties, and by altering the threshold a mass of cells may be made to act as an on/off switch. The switching of waves may be compared with the shifting of attention in a living organism. Particularly interesting phenomena emerge if some property of the individual cells, e.g. size, extent of axon or dendrite structure, or threshold, changes with repeated use. The mass of cells may then exhibit an ability to modify its response according to past experience in a manner similar to that of living organisms. Trial and error learning, conditioned responses, and the ability to regenerate internally a sequence of past events may be demonstrated with very little complication of the form of the mass of cells.", "citation_count": "15", "reference_count": "470", "date": "1956", "authors": ["R. L. Beurle"], "related_topics": ["Dendrite", "Plane wave", "Axon", "Vortex", "Stimulation", "Biophysics", "Physics", "Volume density"]}
{"id": "1968014724", "references": ["2947000318", "2014354687", "2041291259", "1995030605", "1588065767", "2072972096", "2021514745", "2159187100", "2060666586", "22297218"], "title": "Dynamics of neural structures.", "abstract": "Neural nets are constructed of discrete populations of formal neurons. The dynamics of such systems are investigated beginning with the study of single probabilistic nets which are called  netlets . The time course of neural activity is found by analytic methods and by computer simulation for a variety of parameters. A brief discussion of the problem of ergodicity brings out the fact that for the range of parameters considered in this study, a high probability exists for cycling, involving just two firing states of the netlet. The dynamics of netlets is extended to include steady or slowly varying excitatory or inhibitory inputs, spontaneous random activity and coupled pairs of netlets. Quantitative results are given for systems described by selected sets of parameters. Finally a formalism is presented for the quantitative discussion of systems combining arbitrary numbers of interconnected netlets.", "citation_count": "14", "reference_count": "234", "date": "1970", "authors": ["P.A. Anninos", "B. Beek", "T.J. Csermely", "E.M. Harth", "G. Pertile"], "related_topics": ["Artificial neural network", "Ergodicity", "Probabilistic logic", "Statistical physics", "Computer science", "Formalism (philosophy of mathematics)", "Neural activity", "Time course"]}
{"id": "582196039", "references": ["2171054066", "2171390740", "188857997", "2068894548", "2177721432", "1984474582", "291143483", "2150936521", "2071868441", "2159755890"], "title": "Modeling and analysis in biomedicine", "abstract": "", "citation_count": "0", "reference_count": "12", "date": "1984", "authors": ["Claudio A. Nicolini"], "related_topics": ["Biomedicine", "Nanotechnology", "Engineering"]}
{"id": "1973108021", "references": ["2016320357", "2130518506", "2063186269", "2501046150", "2177721432", "654350894", "2964153669", "2067870911", "2006960725", "2080601147"], "title": "Thermodynamic theory of structure, stability and fluctuations", "abstract": "", "citation_count": "0", "reference_count": "3,348", "date": "1971", "authors": ["Paul Glansdorff", "Ilya Prigogine"], "related_topics": ["Thermodynamic system", "Thermodynamic equations", "Thermodynamic process", "Non-equilibrium thermodynamics", "Chemical stability", "Thermodynamics", "Stability (probability)", "Structure (category theory)", "Physics"]}
{"id": "3148194443", "references": ["2578352087", "1535810436"], "title": "The truck backer-upper: an example of self-learning in neural networks", "abstract": "Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems. &gt;", "citation_count": "2", "reference_count": "663", "date": "1989", "authors": ["Nguyen", "Widrow"], "related_topics": ["Artificial neural network", "Nonlinear control", "Control theory", "Artificial intelligence", "Truck", "Task (project management)", "Computer science", "DOCK", "Variety (cybernetics)", "Trailer truck"]}
{"id": "2068484625", "references": ["2113651538", "2003949305", "1804110266", "2148138104", "2093229042", "1984753492", "2963433607", "2117595234", "2102372792", "114517082"], "title": "Numerical methods for unconstrained optimization and nonlinear equations", "abstract": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index.", "citation_count": "0", "reference_count": "10,840", "date": "1983", "authors": ["J. E. Dennis", "Robert B. Schnabel"], "related_topics": ["Secant method", "Muller's method", "Symmetric rank-one", "Sidi's generalized secant method", "Davidon\u2013Fletcher\u2013Powell formula", "Quasi-Newton method", "Local convergence", "Newton's method", "Applied mathematics", "Mathematical optimization", "Mathematics"]}
{"id": "1487148666", "references": ["1980109769", "131739482", "3004559681", "2159973263", "2116360511", "2007284782", "1514068937", "3025826919", "2044465660", "1967847028"], "title": "Comparing different neural network architectures for classifying handwritten digits", "abstract": "An evaluation is made of several neural network classifiers, comparing their performance on a typical problem, namely handwritten digit recognition. For this purpose, the authors use a database of handwritten digits, with relatively uniform handwriting styles. The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results. This approach works in conjunction with various techniques including: layered networks with one or several layers of adaptive connections, fully connected recursive networks, ad hoc networks with no adaptive connections, and architectures with second-degree polynomial decision surfaces. &gt;", "citation_count": "11", "reference_count": "80", "date": "1989", "authors": ["Guyon", "Poujaud", "Personnaz", "Dreyfus", "Denker", "Le Cun"], "related_topics": ["Neocognitron", "Recurrent neural network", "Time delay neural network", "Artificial neural network", "Nervous system network models", "Probabilistic neural network", "Wireless ad hoc network", "Network architecture", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "2118415523", "references": ["2110485445", "2103496339", "2019207321", "2012611887", "2110075977", "2173629880", "2138484437", "2116308743", "1570834090", "2016589492"], "title": "A recurrent fuzzy-neural model for dynamic system identification", "abstract": "This paper presents a fuzzy modeling approach for identification of dynamic systems. In particular, a new fuzzy model, the Dynamic Fuzzy Neural Network (DFNN), consisting of recurrent TSK rules, is developed. The premise and defuzzification parts are static while the consequent parts of the fuzzy rules are recurrent neural networks with internal feedback and time delay synapses. The network is trained by means of a novel learning algorithm, named Dynamic-Fuzzy Neural Constrained Optimization Method (D-FUNCOM), based on the concept of constrained optimization. The proposed algorithm is general since it can be applied to locally as well as fully recurrent networks, regardless of their structures. An adaptation mechanism of the maximum parameter change is presented as well. The proposed dynamic model, equipped with the learning algorithm, is applied to several temporal problems, including modeling of a NARMA process and the noise cancellation problem. Performance comparisons are conducted with a series of static and dynamic systems and some existing recurrent fuzzy models. Simulation results show that DFNN compares favorably with its competing rivals and thus it can be considered for efficient system identification.", "citation_count": "46", "reference_count": "341", "date": "2002", "authors": ["P.A. Mastorocostas", "J.B. Theocharis"], "related_topics": ["Neuro-fuzzy", "Defuzzification", "Recurrent neural network", "Fuzzy logic", "Fuzzy control system", "Artificial neural network", "System identification", "Constrained optimization", "Adaptive control", "Artificial intelligence", "Machine learning", "Dynamical system", "Computer science", "Recurrent neural nets"]}
{"id": "2993446282", "references": ["1480376833", "2121863487", "2156909104", "2076063813", "2130942839", "2525778437", "1604938182", "1986014385", "1497256448", "1663973292"], "title": "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions", "abstract": "We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [63] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also also conceptually simplify an approach [60] for teaching a robot to imitate humans. First videotape humans imitating the robot's current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.", "citation_count": "69", "reference_count": "11", "date": "2019", "authors": ["Juergen Schmidhuber"], "related_topics": ["Reinforcement learning", "Supervised learning", "Babbling", "Robot", "Artificial intelligence", "Action (philosophy)", "Computable function", "Computer science"]}
{"id": "2167224731", "references": ["3140457657", "2008791833", "3108739439", "2148323578", "1945445380", "1916189941", "1529008516"], "title": "ALVINN: An Autonomous Land Vehicle in a Neural Network", "abstract": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.", "citation_count": "7", "reference_count": "1,619", "date": "1988", "authors": ["Dean A. Pomerleau"], "related_topics": ["Autonomous Navigation System", "Artificial neural network", "Automatic control", "Adaptive system", "Remote control", "Computer vision", "Task (project management)", "Representation (mathematics)", "Engineering", "Artificial intelligence", "Field conditions"]}
{"id": "2115072676", "references": ["2042264548", "2165275919", "2173629880", "1512416947", "1529008516"], "title": "Fir and iir synapses, a new neural network architecture for time series modeling", "abstract": "A new neural network architecture involving either local feedforward global feedforward, and/or local recurrent global feedforward structure is proposed. A learning rule minimizing a mean square error criterion is derived. The performance of this algorithm (local recurrent global feedforward architecture) is compared with a local-feedforward global-feedforward architecture. It is shown that the local-recurrent global-feedforward model performs better than the local-feedforward global-feedforward model.", "citation_count": "5", "reference_count": "303", "date": "1991", "authors": ["A. D. Back", "A. C. Tsoi"], "related_topics": ["Feedforward neural network", "Probabilistic neural network", "Time delay neural network", "Learning rule", "Feed forward", "Infinite impulse response", "Mean squared error", "Control theory", "Architecture", "Computer science"]}
{"id": "2156562940", "references": ["1597286183", "2042264548", "2110485445", "1991848143", "2138484437", "2581275558", "2293063825", "2895674046", "1554576613", "1971735090"], "title": "Theory and applications of neural networks for industrial control systems", "abstract": "The theory and the applications of artificial neural networks, especially in a control field, are described. Recurrent networks and feedforward networks are discussed. Application to pattern recognition, information processing, design, planning, diagnosis, and control are examined. Hybrid systems using the neural networks, fuzzy sets, and artificial intelligence (AI) technologies are surveyed. &gt;", "citation_count": "93", "reference_count": "526", "date": "1992", "authors": ["T. Fukuda", "T. Shibata"], "related_topics": ["Intelligent control", "Neuro-fuzzy", "Types of artificial neural networks", "Time delay neural network", "Cellular neural network", "Nervous system network models", "Artificial neural network", "Industrial control system", "Hybrid system", "Fuzzy set", "Control system", "Feed forward", "Artificial intelligence", "Machine learning", "Computer science"]}
{"id": "2152503618", "references": ["2143956139", "2110485445", "2103496339", "1554576613", "2173629880", "2137983211", "2138484437", "2133671888", "2016589492"], "title": "Locally recurrent globally feedforward networks: a critical review of architectures", "abstract": "In this paper, we will consider a number of local-recurrent-global-feedforward (LRGF) networks that have been introduced by a number of research groups in the past few years. We first analyze the various architectures, with a view to highlighting their differences. Then we introduce a general LRGF network structure that includes most of the network architectures that have been proposed to date. Finally we will indicate some open issues concerning these types of networks. &gt;", "citation_count": "50", "reference_count": "397", "date": "1994", "authors": ["Ah Chung Tsoi", "A.D. Back"], "related_topics": ["Network architecture", "Artificial neural network", "Systems architecture", "Artificial intelligence", "Feed forward", "Computer science", "Dynamical system", "Recurrent neural nets", "Research groups"]}
{"id": "2188233853", "references": ["2102605133", "2156909104", "2097117768", "2130942839", "2076063813", "2919115771", "2100495367", "1849277567", "2064675550", "2145339207"], "title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "abstract": "This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially \"learning to think.\" The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as \"mirror neurons.\" Experimental results will be described in separate papers.", "citation_count": "243", "reference_count": "73", "date": "2015", "authors": ["Juergen Schmidhuber"], "related_topics": ["Reinforcement learning", "Recurrent neural network", "Algorithmic information theory", "Artificial intelligence", "Machine learning", "Computer science", "Mirror neuron"]}
{"id": "2056655352", "references": ["1967377907", "2120512179", "2150535417", "2083483041", "1613359937", "2138484437", "2004131797", "1981297107", "1583833196", "1971735090"], "title": "Neural network control for a closed-loop System using Feedback-error-learning", "abstract": "This paper presents new learning schemes using feedback-error-learning for a neural network model applied to adaptive nonlinear feedback control. Feedback-error-learning was proposed as a learning method for forming a feedforward controller that uses the output of a feedback controller as the error for training a neural network model. Using new schemes for nonlinear feedback control, the actual responses after learning correspond to the desired responses which are defined by an inverse reference model implemented as a conventional feedback controller. In this respect, these methods are similar to Model Reference Adaptive Control (MRAC) applied to linear or linearized systems. It is shown that learning impedance control is derived when one proposed scheme is used in Cartesian space. We show the results of applying these learning schemes to an inverted pendulum and a 2-link manipulator. We also discuss the convergence properties of the neural network models employed in these learning schemes by applying the Lyapunov method to the averaged equations associated with the stochastic differential equations which describe the system dynamics.", "citation_count": "34", "reference_count": "387", "date": "1993", "authors": ["Hiroaki Gomi", "Mitsuo Kawato"], "related_topics": ["Nonlinear control", "Feed forward", "Artificial neural network", "Adaptive control", "Control theory", "Inverted pendulum", "Impedance control", "Nonlinear system", "Lyapunov function", "Control theory", "Stochastic differential equation", "Computer science"]}
{"id": "2102496649", "references": ["2132152975"], "title": "Neural network classifier to threshold images from 3D microcomputed tomography", "abstract": "Summary form only given. The authors discuss the use of a neural network to perform context-dependent thresholding of gray-scale three-dimensional images of trabecular and cortical bone as measured in vitro by high-resolution X-ray computed tomography. A classifier is constructed on the basis of a simple model of the blurring necessarily associated with the tomographic measurement. &gt;", "citation_count": "0", "reference_count": "1", "date": "1991", "authors": ["L.A. Feldkamp", "G. Jesion", "G.V. Puskorius", "D.J. Kubinski"], "related_topics": ["Thresholding", "Artificial neural network", "Classifier (UML)", "Cortical bone", "Computer vision", "Computer science", "Artificial intelligence", "Computed tomography", "Microcomputed tomography", "Neural network classifier"]}
{"id": "2077493113", "references": ["2154642048", "2140207729", "2146528014", "1971129545", "2293807537", "2148845786", "2160699933"], "title": "MEKA-a fast, local algorithm for training feedforward neural networks", "abstract": "It is noted that the training of feedforward networks using the conventional backpropagation algorithm is plagued by poor convergence and misadjustment. The authors introduce the multiple extended Kalman algorithm (MEKA) to train feedforward networks. It is based on the idea of partitioning the global problem of finding the weights into a set of manageable nonlinear subproblems. The algorithm is local at the neuron level. The superiority of MEKA over the global extended Kalman algorithm in terms of convergence and quality of solution obtained on two benchmark problems is demonstrated. The superior performance can be attributed to the nonlinear localized approach. In fact, the nonconvex nature of the local performance surface reduces the chances of getting trapped into a local minima", "citation_count": "7", "reference_count": "97", "date": "1990", "authors": ["S. Shah", "F. Palmieri"], "related_topics": ["Local algorithm", "Feedforward neural network", "Backpropagation", "Artificial neural network", "Computational complexity theory", "Convergence (routing)", "Maxima and minima", "Benchmark (computing)", "Mathematical optimization", "Mathematics"]}
{"id": "2122568838", "references": ["2154642048", "2005047329", "1969166509", "3036751298", "2236623899", "3150413596", "2986444355", "2068484625", "2116418681", "2160699933"], "title": "An adaptive least squares algorithm for the efficient training of artificial neural networks", "abstract": "A novel learning algorithm is developed for the training of multilayer feedforward neural networks, based on a modification of the Marquardt-Levenberg least-squares optimization method. The algorithm updates the input weights of each neuron in the network in an effective parallel way. An adaptive distributed selection of the convergence rate parameter is presented, using suitable optimization strategies. The algorithm has better convergence properties than the conventional backpropagation learning technique. Its performance is illustrated, using examples from digital image halftoning and logical operations such as the XOR function. &gt;", "citation_count": "20", "reference_count": "271", "date": "1989", "authors": ["S. Kollias", "D. Anastassiou"], "related_topics": ["Rprop", "Population-based incremental learning", "Feedforward neural network", "Backpropagation", "Artificial neural network", "Rate of convergence", "Convergence (routing)", "Function (mathematics)", "Artificial intelligence", "Computer science"]}
{"id": "3147404844", "references": ["2042264548", "2154642048", "1995169133", "2076506437", "1587362683", "2085536277", "1507849272", "2002182716", "1573503290"], "title": "Statistical pattern recognition with neural networks: benchmarking studies.", "abstract": "Successful recognition of natural signals, e.g., speech recognition, requires substantial statistical pattern recognition capabilities. This is at odds with the fact that the bulk of work on applying neural networks to pattern recognition has concentrated on non-statistical problems. Three basic types of neural-like networks (Backpropagation network, Boltzmann machine, and Learning Vector Qumtization), were applied in this work to two representative artificial statistical pattern recognition tasks, each with varying dimensionality. The performance of each network's different approach to solving the tasks was evaluated and compared, both to the performance of the other two networks, and to the theoretical limit. The Learning Vector Quantization was further benchmarked against the parametric Bayes classifier and the k-nearestneighbor classifier using natural speech data. A novel Learning Vector Quantization classifier (LVQ2) is introduced the first time in this work.", "citation_count": "9", "reference_count": "735", "date": "1988", "authors": ["Teuvo Kohonen", "Gy\u00f6rgy Barna", "Ronald L. Chrisley"], "related_topics": ["Artificial neural network", "Learning vector quantization", "Bayes classifier", "Pattern recognition (psychology)", "Boltzmann machine", "Backpropagation", "Classifier (linguistics)", "Parametric statistics", "Pattern recognition", "Computer science", "Artificial intelligence"]}
{"id": "1761621746", "references": ["2042264548", "2154642048", "2003454866", "1995169133", "2076506437", "1587362683", "2085536277", "1507849272", "3121926921", "2002182716"], "title": "Statistical pattern recognition with neural networks: benchmarking studies", "abstract": "Three basic types of neural-like networks (backpropagation network, Boltzmann machine, and learning vector quantization), were applied to two representative artificial statistical pattern recognition tasks, each with varying dimensionality. The performance of each network's approach to solving the tasks was evaluated and compared, both to the performance of the other two networks and to the theoretical limit. The learning vector quantization was further benchmarked against the parametric Bayes classifier and the k-nearest-neighbor classifier using natural speech data. A novel learning vector quantization classifier called LVQ2 is introduced. &gt;", "citation_count": "11", "reference_count": "537", "date": "1988", "authors": ["Barna", "Chrisley"], "related_topics": ["Learning vector quantization", "Margin classifier", "Vector quantization", "Quadratic classifier", "Bayes classifier", "Competitive learning", "Learning classifier system", "Artificial neural network", "Pattern recognition", "Machine learning", "Computer science", "Artificial intelligence"]}
{"id": "107400462", "references": ["2917878399", "2058522416", "2083402998", "1579583813", "2105934661", "2070256701", "2171612575", "2094227853", "2024511625", "2799137445"], "title": "Kalman Filtering Techniques", "abstract": "Publisher Summary   The technique that Gauss suggested for obtaining the approximations (or estimates) of the unknown quantities has come to be known as the method of least squares. He called for approximate knowledge of the orbit to determine the least-squares estimates. These estimates were then used to correct the reference orbit. This chapter discusses the modern version of Gauss' least-squares technique that has been developed. It also discusses the two major differences between the classical and modern techniques. It describes important theoretical considerations relating to the linear estimation theory as developed by Kalman and others. The major portion of the presentation is concerned with the time-discrete model as it seems to be the most natural version for implementation on a digital computer. It describes the common physical situation in which measurement data are obtained at discrete instants of time. The chapter also presents filter equations for the time-continuous model.", "citation_count": "36", "reference_count": "303", "date": "1966", "authors": ["H.W. Sorenson"], "related_topics": ["Kalman filter", "Gauss", "Orbit (control theory)", "Filter (signal processing)", "Algorithm", "Computer science", "Simulation", "Digital computer", "Linear estimation"]}
{"id": "2076086013", "references": ["2130753017", "2105171561", "1600066860", "2075039721", "2145373072", "2066749892", "2076425068", "590542256", "2083474994", "2045010554"], "title": "A new approach to the sensitivity problem in multivariable feedback system design", "abstract": "A new point of view for the parameter variation problem in linear multivariable systems is proposed. The output deviations due to parameter variations for an open-loop realization are related by a sensitivity matrix to the output deviations due to parameter variations for a closed-loop (feedback) realization. Using a time-domain integral of the square of the error as a performance index, frequency-domain criteria involving the sensitivity matrix are derived. The criteria are sufficient for insuring that the feedback realization is less affected by parameter variations than an open-loop realization having the same nominal transfer characteristic. Furthermore, the criteria are independent of the integration interval involved in the performance index. A numerical example shows how the criteria may be used in designing multivariable control systems.", "citation_count": "10", "reference_count": "262", "date": "1964", "authors": ["J. Cruz", "W. Perkins"], "related_topics": ["Realization (systems)", "Sensitivity (control systems)", "Multivariable calculus", "Linear system", "Matrix (mathematics)", "Systems design", "Square (algebra)", "Control theory", "Point (geometry)", "Mathematics"]}
{"id": "2016261381", "references": ["2096299879", "2068039102", "2886017837", "2039022777"], "title": "Multiparameter self-optimizing systems using correlation techniques", "abstract": "A class of self-optimizing systems which continually alter their parameters to reduce a mean-square performance criterion is described. The change in each parameter is determined from an error gradient in parameter space computed by cross-correlation methods which are independent of signal spectra and require no test signal or parameter perturbation. Applications of this technique to both open-loop and closed-loop systems are included and it is shown that a combination of such self-optimizing systems is a possible solution to the adaptive control problem. Computer simulation results are included to demonstrate the practicality of the proposed systems.", "citation_count": "4", "reference_count": "77", "date": "1964", "authors": ["K. Narendra", "L. McBride"], "related_topics": ["Adaptive control", "Linear-quadratic-Gaussian control", "Parameter space", "Optimal control", "Adaptive system", "Automatic control", "Control theory", "Perturbation (astronomy)", "Computer science", "Correlation"]}
{"id": "2062870975", "references": ["2016261381", "2093304251", "2072714069", "2099867470", "2124734274", "1996647346", "2065324104"], "title": "Optimization of time-varying systems", "abstract": "A method of self-optimization using system models to compute error-criterion gradients in a parameter space is extended to time-varying systems. When the parameters are permitted to vary only slowly, the gradient computer is similar to that used for stationary systems of fixed configuration. When the parameters vary more rapidly, it is found that only the gradient with respect to the plant input function is meaningful. This influence function is obtained as the output from a model which can be defined whether or not a state-variable representation for the plant is known; a procedure for computing optimal control functions in a variety of linear and nonlinear systems is thus obtained.", "citation_count": "7", "reference_count": "15", "date": "1965", "authors": ["L. McBride", "K. Narendra"], "related_topics": ["Time-invariant system", "Linear system", "Nonlinear system", "Optimal control", "Parameter space", "Representation (mathematics)", "Applied mathematics", "Control theory", "Variety (universal algebra)", "Mathematics", "Influence function"]}
{"id": "2051992922", "references": ["2046432185", "2154642048", "2477186228", "1535810436", "2155051950", "825574500"], "title": "The modified Kanerva model for automatic speech recognition", "abstract": "Abstract   A parallel processing network derived from Kanerva's associative memory theory Kanerva 1984 is shown to be able to train rapidly on connected speech data and recognize further speech data with a label error rate of 0\u00b768%. This modified Kanerva model can be trained substantially faster than other networks with comparable pattern discrimination properties.  Kanerva presented his theory of a self-propagating search in 1984, and showed theoretically that large-scale versions of his model would have powerful pattern matching properties. This paper describes how the design for the modified Kanerva model is derived from Kanerva's original theory. Several designs are tested to discover which form may be implemented fastest while still maintaining versatile recognition performance. A method is developed to deal with the time varying nature of the speech signal by recognizing static patterns together with a fixed quantity of contextual information.  In order to recognize speech features in different contexts it is necessary for a network to be able to model disjoint pattern classes. This type of modelling cannot be performed by a single layer of links. Network research was once held back by the inability of single-layer networks to solve this sort of problem, and the lack of a training algorithm for multi-layer networks.  Rumelhart, Hinton &amp; Williams 1985 provided one solution by demonstrating the \u201cback propagation\u201d training algorithm for multi-layer networks. A second alternative is used in the modified Kanerva model. A non-linear fixed transformation maps the pattern space into a space of higher dimensionality in which the speech features are linearly separable. A single-layer network may then be used to perform the recognition. The advantage of this solution over the other using multi-layer networks lies in the greater power and speed of the single-layer network training algorithm.", "citation_count": "6", "reference_count": "88", "date": "1989", "authors": ["R.W. Prager", "F. Fallside"], "related_topics": ["Sparse distributed memory", "Word error rate", "Pattern matching", "Content-addressable memory", "Backpropagation", "Linear separability", "Transformation (function)", "Artificial intelligence", "sort", "Speech recognition", "Computer science"]}
{"id": "2105934661", "references": ["2887053799", "610132389", "2075549445", "3007191719", "2062328787", "71817473", "1999276251", "2112172398", "1519460252", "2799137445"], "title": "A New Approach to Linear Filtering and Prediction Problems", "abstract": "", "citation_count": "19", "reference_count": "36,136", "date": "1960", "authors": ["R. E. Kalman"], "related_topics": ["Filtering problem", "Fast Kalman filter", "Recursive Bayesian estimation", "Kalman filter", "Linear filter", "Invariant extended Kalman filter", "Alpha beta filter", "Unscented transform", "Algorithm", "Computer science"]}
